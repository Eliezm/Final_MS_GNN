The file thin_merge_env.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
THIN MERGE ENVIRONMENT - Updated for Enhanced Features
========================================================
Now handles 15-dim node features and 10-dim edge features from C++.
Uses improved reward function with h* preservation focus.
"""

import os
import sys
import json
import time
import glob
import subprocess
import tempfile
import logging
from pathlib import Path
from typing import Dict, Any, Tuple, Optional, List

import numpy as np
import gymnasium as gym
from gymnasium import spaces

logger = logging.getLogger(__name__)


class ThinMergeEnv(gym.Env):
    """
    Thin Client Environment with Enhanced Features.

    Now uses:
    - 15-dimensional node features (was 7)
    - 10-dimensional edge features from C++ (was derived)
    - h* preservation focused reward
    """

    metadata = {"render_modes": []}

    # Updated dimensions
    NODE_FEATURE_DIM = 15  # Expanded from 7
    EDGE_FEATURE_DIM = 10  # New: from C++
    MAX_NODES = 100
    MAX_EDGES = 1000

    # New reward weights (h* focused)
    DEFAULT_REWARD_WEIGHTS = {
        'w_h_preservation': 0.40,  # Primary signal!
        'w_shrinkability': 0.25,
        'w_state_control': 0.20,
        'w_solvability': 0.15,
    }

    def __init__(
            self,
            domain_file: str,
            problem_file: str,
            downward_dir: Optional[str] = None,
            max_merges: int = 50,
            timeout_per_step: float = 120.0,
            reward_weights: Optional[Dict[str, float]] = None,
            debug: bool = False,
    ):
        super().__init__()

        self.domain_file = os.path.abspath(domain_file)
        self.problem_file = os.path.abspath(problem_file)

        if not os.path.exists(self.domain_file):
            raise FileNotFoundError(f"Domain file not found: {self.domain_file}")
        if not os.path.exists(self.problem_file):
            raise FileNotFoundError(f"Problem file not found: {self.problem_file}")

        if downward_dir is None:
            script_dir = Path(__file__).parent.absolute()
            self.downward_dir = script_dir / "downward"
        else:
            self.downward_dir = Path(downward_dir).absolute()

        if not self.downward_dir.exists():
            raise FileNotFoundError(f"Fast Downward directory not found: {self.downward_dir}")

        self.max_merges = max_merges
        self.timeout_per_step = timeout_per_step
        self.debug = debug
        self.reward_weights = reward_weights or self.DEFAULT_REWARD_WEIGHTS.copy()

        self.fd_output_dir = self.downward_dir / "fd_output"
        self.gnn_output_dir = self.downward_dir / "gnn_output"
        self._setup_directories()

        # Updated observation space
        self.observation_space = spaces.Dict({
            "x": spaces.Box(
                low=-np.inf, high=np.inf,
                shape=(self.MAX_NODES, self.NODE_FEATURE_DIM),
                dtype=np.float32
            ),
            "edge_index": spaces.Box(
                low=0, high=self.MAX_NODES,
                shape=(2, self.MAX_EDGES),
                dtype=np.int64
            ),
            "edge_features": spaces.Box(  # NEW!
                low=-np.inf, high=np.inf,
                shape=(self.MAX_EDGES, self.EDGE_FEATURE_DIM),
                dtype=np.float32
            ),
            "num_nodes": spaces.Box(low=0, high=self.MAX_NODES, shape=(), dtype=np.int32),
            "num_edges": spaces.Box(low=0, high=self.MAX_EDGES, shape=(), dtype=np.int32),
        })

        self.action_space = spaces.Discrete(self.MAX_EDGES)

        self.current_iteration = -1
        self.process: Optional[subprocess.Popen] = None
        self.fd_log_file = None
        self._cached_edge_index: Optional[np.ndarray] = None
        self._last_observation: Optional[Dict] = None

        logger.info(f"[THIN_ENV] Initialized with {self.NODE_FEATURE_DIM} node features, "
                    f"{self.EDGE_FEATURE_DIM} edge features")

    def _setup_directories(self):
        self.fd_output_dir.mkdir(parents=True, exist_ok=True)
        self.gnn_output_dir.mkdir(parents=True, exist_ok=True)

    def _cleanup_files(self):
        patterns = [
            str(self.fd_output_dir / "*.json"),
            str(self.gnn_output_dir / "*.json"),
            str(self.fd_output_dir / "*.tmp"),
            str(self.gnn_output_dir / "*.tmp"),
        ]
        deleted = 0
        for pattern in patterns:
            for filepath in glob.glob(pattern):
                try:
                    os.remove(filepath)
                    deleted += 1
                except:
                    pass
        logger.debug(f"[THIN_ENV] Cleaned up {deleted} files")

    def _terminate_process(self):
        if self.process and self.process.poll() is None:
            try:
                self.process.terminate()
                self.process.wait(timeout=3.0)
            except subprocess.TimeoutExpired:
                self.process.kill()
            except:
                pass
        self.process = None

        if self.fd_log_file:
            try:
                self.fd_log_file.close()
            except:
                pass
            self.fd_log_file = None

    def _launch_fd_process(self):
        """Launch Fast Downward with GNN merge strategy."""
        logger.info("[THIN_ENV] Launching Fast Downward...")

        translator_script = self.downward_dir / "builds" / "release" / "bin" / "translate" / "translate.py"
        if not translator_script.exists():
            raise FileNotFoundError(f"Translator not found: {translator_script}")

        translate_cmd = [
            sys.executable,
            str(translator_script),
            self.domain_file,
            self.problem_file,
            "--sas-file", "output.sas"
        ]

        translate_result = subprocess.run(
            translate_cmd,
            cwd=str(self.downward_dir),
            capture_output=True,
            timeout=60,
            text=True
        )

        if translate_result.returncode != 0:
            raise RuntimeError(f"Translator failed: {translate_result.stderr}")

        downward_exe = self.downward_dir / "builds" / "release" / "bin" / "downward.exe"
        if not downward_exe.exists():
            raise FileNotFoundError(f"Downward executable not found: {downward_exe}")

        search_config = (
            "astar(merge_and_shrink("
            "merge_strategy=merge_gnn(),"
            "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
            "label_reduction=exact(before_shrinking=true,before_merging=false),"
            "max_states=50000,"
            "threshold_before_merge=1))"
        )

        downward_cmd = [str(downward_exe), "--search", search_config]

        log_path = self.fd_output_dir / "downward.log"
        self.fd_log_file = open(log_path, "w", buffering=1)

        self.process = subprocess.Popen(
            downward_cmd,
            cwd=str(self.downward_dir),
            stdin=subprocess.PIPE,
            stdout=self.fd_log_file,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        sas_path = self.downward_dir / "output.sas"
        with open(sas_path, "r") as f:
            sas_content = f.read()

        try:
            self.process.stdin.write(sas_content)
            self.process.stdin.close()
        except BrokenPipeError:
            logger.error("[THIN_ENV] Failed to write SAS file to process")
            raise

        logger.info(f"[THIN_ENV] FD process started (PID: {self.process.pid})")

    def _wait_for_observation(self, iteration: int, timeout: Optional[float] = None) -> Dict[str, Any]:
        if timeout is None:
            timeout = self.timeout_per_step

        obs_path = self.fd_output_dir / f"observation_{iteration}.json"
        start_time = time.time()
        last_error = None

        while time.time() - start_time < timeout:
            elapsed = time.time() - start_time

            if elapsed > 2.0 and self.process and self.process.poll() is not None:
                return_code = self.process.returncode
                if obs_path.exists():
                    try:
                        time.sleep(0.05)
                        with open(obs_path, 'r') as f:
                            content = f.read()
                        if content.strip():
                            return json.loads(content)
                    except:
                        pass
                raise RuntimeError(f"FD process died with code {return_code}")

            if obs_path.exists():
                try:
                    time.sleep(0.01)
                    with open(obs_path, 'r') as f:
                        content = f.read()
                    if content.strip():
                        data = json.loads(content)
                        return data
                except json.JSONDecodeError as e:
                    last_error = f"JSON decode: {e}"
                except PermissionError as e:
                    last_error = f"Permission: {e}"
                except IOError as e:
                    last_error = f"IO error: {e}"

            time.sleep(0.05)

        raise TimeoutError(f"Timeout waiting for observation_{iteration}.json: {last_error}")

    def _send_merge_decision(self, iteration: int, merge_pair: Tuple[int, int]) -> None:
        import platform

        decision = {
            "iteration": iteration,
            "merge_pair": list(merge_pair),
            "timestamp": time.time()
        }

        decision_path = self.gnn_output_dir / f"merge_{iteration}.json"
        temp_path = decision_path.with_suffix('.tmp')

        with open(temp_path, 'w') as f:
            json.dump(decision, f, indent=2)
            f.flush()
            os.fsync(f.fileno())

        max_rename_attempts = 5
        for attempt in range(max_rename_attempts):
            try:
                os.replace(temp_path, decision_path)
                break
            except (PermissionError, OSError) as e:
                if attempt < max_rename_attempts - 1:
                    time.sleep(0.05)
                else:
                    raise RuntimeError(f"Failed to write merge decision: {e}")

        if platform.system() == 'Windows':
            time.sleep(0.02)

    def _observation_to_tensors(self, raw_obs: Dict[str, Any]) -> Dict[str, np.ndarray]:
        """Convert C++ observation JSON to numpy tensors - UPDATED for edge features."""

        # Node features (15-dim)
        x_raw = raw_obs.get('x', [])
        num_nodes = len(x_raw)

        x = np.zeros((self.MAX_NODES, self.NODE_FEATURE_DIM), dtype=np.float32)
        n = min(num_nodes, self.MAX_NODES)

        for i in range(n):
            if i < len(x_raw):
                features = x_raw[i]
                for j in range(min(len(features), self.NODE_FEATURE_DIM)):
                    x[i, j] = float(features[j])

        # Edge index
        edge_index_raw = raw_obs.get('edge_index', [[], []])
        num_edges = len(edge_index_raw[0]) if len(edge_index_raw) == 2 else 0

        edge_index = np.zeros((2, self.MAX_EDGES), dtype=np.int64)
        ne = min(num_edges, self.MAX_EDGES)

        if ne > 0 and len(edge_index_raw) == 2:
            edge_index[0, :ne] = np.array(edge_index_raw[0][:ne], dtype=np.int64)
            edge_index[1, :ne] = np.array(edge_index_raw[1][:ne], dtype=np.int64)

        self._cached_edge_index = edge_index_raw

        # Edge features (10-dim) - NEW from C++!
        edge_features_raw = raw_obs.get('edge_features', None)
        edge_features = np.zeros((self.MAX_EDGES, self.EDGE_FEATURE_DIM), dtype=np.float32)

        if edge_features_raw is not None and ne > 0:
            for i, ef in enumerate(edge_features_raw[:ne]):
                for j, val in enumerate(ef[:self.EDGE_FEATURE_DIM]):
                    edge_features[i, j] = float(val)

        return {
            "x": x,
            "edge_index": edge_index,
            "edge_features": edge_features,  # NEW!
            "num_nodes": np.int32(n),
            "num_edges": np.int32(ne),
        }

    # def _compute_reward(self, raw_obs: Dict[str, Any]) -> float:
    #     """
    #     Compute scalar reward with h* preservation focus.
    #
    #     Weighted combination:
    #     - w_h_preservation = 0.40 (PRIMARY!)
    #     - w_shrinkability = 0.25
    #     - w_state_control = 0.20
    #     - w_solvability = 0.15
    #     """
    #     signals = raw_obs.get('reward_signals', {})
    #
    #     # ========================================================================
    #     # EXTRACT SIGNALS
    #     # ========================================================================
    #
    #     # H* preservation (PRIMARY SIGNAL!)
    #     h_star_before = float(signals.get('h_star_before', 0))
    #     h_star_after = float(signals.get('h_star_after', 0))
    #     h_star_preservation = float(signals.get('h_star_preservation', 1.0))
    #
    #     # Shrinkability
    #     theoretical_product = int(signals.get('theoretical_product_size', 1))
    #     actual_size = int(signals.get('merged_size', 1))
    #     shrinkability = float(signals.get('shrinkability', 0.0))
    #
    #     # State control
    #     state_control_score = float(signals.get('state_control_score', 0.5))
    #     state_explosion_penalty = float(signals.get('state_explosion_penalty', 0.0))
    #
    #     # Solvability
    #     is_solvable = bool(signals.get('is_solvable', True))
    #
    #     # Dead-end ratio (new!)
    #     dead_end_ratio = float(signals.get('dead_end_ratio', 0.0))
    #
    #     # ========================================================================
    #     # LOG SIGNALS FOR DEBUGGING
    #     # ========================================================================
    #
    #     if self.debug:
    #         logger.debug(f"[REWARD] h* before={h_star_before}, after={h_star_after}, "
    #                      f"preservation={h_star_preservation:.3f}")
    #         logger.debug(f"[REWARD] shrinkability={shrinkability:.3f}, "
    #                      f"state_control={state_control_score:.3f}")
    #         logger.debug(f"[REWARD] solvable={is_solvable}, dead_ends={dead_end_ratio:.3f}")
    #
    #     # ========================================================================
    #     # COMPUTE WEIGHTED REWARD
    #     # ========================================================================
    #
    #     w_h = self.reward_weights.get('w_h_preservation', 0.40)
    #     w_shrink = self.reward_weights.get('w_shrinkability', 0.25)
    #     w_state = self.reward_weights.get('w_state_control', 0.20)
    #     w_solv = self.reward_weights.get('w_solvability', 0.15)
    #
    #     # h* preservation: 1.0 = preserved, > 1.0 = improved, < 1.0 = degraded
    #     h_component = min(1.0, h_star_preservation)
    #
    #     # Shrinkability: [-1, 1] -> [0, 1]
    #     shrink_component = max(0.0, shrinkability + 0.5)
    #
    #     # State control
    #     state_component = state_control_score
    #
    #     # Solvability
    #     solv_component = 1.0 if is_solvable else 0.0
    #
    #     # Weighted sum
    #     weighted_sum = (
    #             w_h * h_component +
    #             w_shrink * shrink_component +
    #             w_state * state_component +
    #             w_solv * solv_component
    #     )
    #
    #     reward = weighted_sum
    #
    #     # ========================================================================
    #     # BONUSES AND PENALTIES
    #     # ========================================================================
    #
    #     # Heavy penalty for losing solvability
    #     if not is_solvable:
    #         reward -= 1.0
    #
    #     # Penalty for high dead-end ratio
    #     if dead_end_ratio > 0.3:
    #         reward -= 0.1 * dead_end_ratio
    #
    #     # Bonus for h* improvement
    #     if h_star_preservation > 1.0:
    #         reward += 0.1 * (h_star_preservation - 1.0)
    #
    #     # Small step reward
    #     reward += 0.02
    #
    #     # Scale to reasonable RL range
    #     reward = (reward - 0.5) * 2.0
    #
    #     # Clamp
    #     reward = max(-5.0, min(2.0, float(reward)))
    #
    #     logger.debug(f"[REWARD] Final: weighted_sum={weighted_sum:.3f}, reward={reward:.3f}")
    #
    #     return reward

    # def _compute_reward(self, raw_obs: Dict[str, Any]) -> float:
    #     """
    #     Compute reward with clear positive/negative signals.
    #
    #     Design:
    #     - Range: [-2.0, +2.0] centered around 0
    #     - Positive rewards for good merges (h* preserved, controlled growth)
    #     - Negative rewards for bad merges (h* degradation, explosion, dead-ends)
    #     - Severe penalties for catastrophic failures (unsolvable)
    #
    #     Components:
    #     1. H* Preservation (±0.5): Primary signal for heuristic quality
    #     2. State Explosion (±0.4): Penalize uncontrolled growth
    #     3. Shrinkability (±0.15): Reward good shrinking
    #     4. Solvability (-1.0): Severe penalty for losing solvability
    #     5. Dead-End Ratio (±0.3): Penalize creating dead-ends
    #     6. Reachability (±0.2): Reward maintaining reachability
    #     7. F-Value Stability (±0.1): Reward stable heuristics
    #     8. Progress (+0.02): Small positive for each step
    #     """
    #     signals = raw_obs.get('reward_signals', {})
    #
    #     # ========================================================================
    #     # EXTRACT ALL SIGNALS FROM C++
    #     # ========================================================================
    #
    #     # Primary: h* preservation (most important!)
    #     h_star_before = float(signals.get('h_star_before', 0))
    #     h_star_after = float(signals.get('h_star_after', 0))
    #     h_star_preservation = float(signals.get('h_star_preservation', 1.0))
    #
    #     # State management
    #     states_before = int(signals.get('states_before', 1))
    #     states_after = int(signals.get('states_after', 1))
    #     state_explosion_penalty = float(signals.get('state_explosion_penalty', 0.0))
    #     shrinkability = float(signals.get('shrinkability', 0.0))
    #
    #     # Quality metrics
    #     is_solvable = bool(signals.get('is_solvable', True))
    #     dead_end_ratio = float(signals.get('dead_end_ratio', 0.0))
    #     reachability_ratio = float(signals.get('reachability_ratio', 1.0))
    #
    #     # F-value stability
    #     f_value_stability = float(signals.get('f_value_stability', 1.0))
    #     f_preservation_score = float(signals.get('f_preservation_score', 1.0))
    #
    #     # Transition metrics
    #     transition_density = float(signals.get('transition_density', 1.0))
    #     total_dead_ends = int(signals.get('total_dead_ends', 0))
    #
    #     # ========================================================================
    #     # REWARD COMPONENT 1: H* PRESERVATION (±0.5)
    #     # This is the PRIMARY signal - h* is the whole point!
    #     # ========================================================================
    #
    #     h_reward = 0.0
    #     if h_star_preservation >= 1.0:
    #         # GOOD: h* preserved or improved
    #         # Base reward + bonus for improvement
    #         improvement = min(1.0, h_star_preservation - 1.0)
    #         h_reward = 0.3 + 0.2 * improvement  # Range: [0.3, 0.5]
    #
    #         if self.debug:
    #             logger.debug(f"[REWARD] h* improved/preserved: +{h_reward:.3f}")
    #     else:
    #         # BAD: h* degraded - penalty proportional to loss
    #         degradation = 1.0 - h_star_preservation
    #         h_reward = -0.5 * degradation  # Range: [-0.5, 0]
    #
    #         # Extra penalty for severe degradation (>20% loss)
    #         if degradation > 0.2:
    #             h_reward -= 0.2 * (degradation - 0.2)  # Additional penalty
    #
    #         if self.debug:
    #             logger.debug(f"[REWARD] h* DEGRADED by {degradation:.1%}: {h_reward:.3f}")
    #
    #     # ========================================================================
    #     # REWARD COMPONENT 2: STATE EXPLOSION CONTROL (±0.4)
    #     # Penalize merges that cause excessive state growth
    #     # ========================================================================
    #
    #     explosion_reward = 0.0
    #     if states_before > 0:
    #         growth_ratio = states_after / max(1, states_before)
    #
    #         if growth_ratio > 10:
    #             # VERY BAD: Severe explosion (>10x growth)
    #             explosion_reward = -0.4  # Maximum penalty
    #             if self.debug:
    #                 logger.debug(f"[REWARD] SEVERE explosion ({growth_ratio:.1f}x): {explosion_reward:.3f}")
    #
    #         elif growth_ratio > 5:
    #             # BAD: Significant explosion (5-10x growth)
    #             explosion_reward = -0.2 - 0.2 * (growth_ratio - 5) / 5
    #             if self.debug:
    #                 logger.debug(f"[REWARD] Significant explosion ({growth_ratio:.1f}x): {explosion_reward:.3f}")
    #
    #         elif growth_ratio > 2:
    #             # MODERATE: Some growth (2-5x)
    #             explosion_reward = -0.05 - 0.15 * (growth_ratio - 2) / 3
    #             if self.debug:
    #                 logger.debug(f"[REWARD] Moderate growth ({growth_ratio:.1f}x): {explosion_reward:.3f}")
    #
    #         elif growth_ratio > 1:
    #             # MILD: Slight growth (1-2x)
    #             explosion_reward = -0.02 * (growth_ratio - 1)
    #
    #         else:
    #             # GOOD: Shrinking or no growth
    #             shrink_factor = 1.0 - growth_ratio
    #             explosion_reward = 0.1 * shrink_factor  # Bonus for shrinking
    #             if self.debug:
    #                 logger.debug(f"[REWARD] Good shrinking ({growth_ratio:.2f}x): +{explosion_reward:.3f}")
    #
    #     # ========================================================================
    #     # REWARD COMPONENT 3: SHRINKABILITY (±0.15)
    #     # Reward merges that allow effective shrinking
    #     # shrinkability: 1.0 = perfect, 0 = neutral, -1.0 = expansion
    #     # ========================================================================
    #
    #     shrink_reward = 0.0
    #     if shrinkability > 0:
    #         # GOOD: Effective shrinking possible
    #         shrink_reward = 0.15 * shrinkability
    #     else:
    #         # BAD: Poor shrinkability
    #         shrink_reward = 0.1 * shrinkability  # Penalty (shrinkability is negative)
    #
    #     # ========================================================================
    #     # REWARD COMPONENT 4: SOLVABILITY (-1.0 penalty)
    #     # Catastrophic failure - losing ability to find solutions
    #     # ========================================================================
    #
    #     solvability_reward = 0.0
    #     if not is_solvable:
    #         # CATASTROPHIC: Lost solvability - large penalty
    #         solvability_reward = -1.0
    #         if self.debug:
    #             logger.debug(f"[REWARD] CATASTROPHIC: Lost solvability: {solvability_reward:.3f}")
    #     else:
    #         # Small bonus for maintaining solvability
    #         solvability_reward = 0.05
    #
    #     # ========================================================================
    #     # REWARD COMPONENT 5: DEAD-END RATIO (±0.3)
    #     # Penalize creating dead-ends (states with no path to goal)
    #     # ========================================================================
    #
    #     dead_end_reward = 0.0
    #     if dead_end_ratio > 0.5:
    #         # VERY BAD: >50% dead-ends
    #         dead_end_reward = -0.3 * (dead_end_ratio - 0.5) / 0.5 - 0.1
    #         if self.debug:
    #             logger.debug(f"[REWARD] HIGH dead-ends ({dead_end_ratio:.1%}): {dead_end_reward:.3f}")
    #
    #     elif dead_end_ratio > 0.2:
    #         # BAD: 20-50% dead-ends
    #         dead_end_reward = -0.1 * (dead_end_ratio - 0.2) / 0.3
    #
    #     elif dead_end_ratio > 0.1:
    #         # MODERATE: 10-20% dead-ends - small penalty
    #         dead_end_reward = -0.02
    #
    #     else:
    #         # GOOD: <10% dead-ends
    #         dead_end_reward = 0.02  # Small bonus
    #
    #     # ========================================================================
    #     # REWARD COMPONENT 6: REACHABILITY (±0.2)
    #     # Reward maintaining high fraction of reachable states
    #     # ========================================================================
    #
    #     reach_reward = 0.0
    #     if reachability_ratio < 0.3:
    #         # VERY BAD: <30% reachable
    #         reach_reward = -0.2
    #         if self.debug:
    #             logger.debug(f"[REWARD] LOW reachability ({reachability_ratio:.1%}): {reach_reward:.3f}")
    #
    #     elif reachability_ratio < 0.5:
    #         # BAD: 30-50% reachable
    #         reach_reward = -0.15 * (0.5 - reachability_ratio) / 0.2
    #
    #     elif reachability_ratio < 0.7:
    #         # MODERATE: 50-70% reachable
    #         reach_reward = -0.05
    #
    #     elif reachability_ratio > 0.9:
    #         # GOOD: >90% reachable
    #         reach_reward = 0.05
    #
    #     # ========================================================================
    #     # REWARD COMPONENT 7: F-VALUE STABILITY (±0.1)
    #     # Reward stable f-values (indicates good abstraction quality)
    #     # ========================================================================
    #
    #     stability_avg = (f_value_stability + f_preservation_score) / 2.0
    #     stability_reward = 0.1 * (stability_avg - 0.5)  # Centered at 0.5
    #
    #     # ========================================================================
    #     # REWARD COMPONENT 8: PROGRESS REWARD (+0.02)
    #     # Small positive reward for each step to encourage exploration
    #     # ========================================================================
    #
    #     progress_reward = 0.02
    #
    #     # ========================================================================
    #     # COMBINE ALL COMPONENTS
    #     # ========================================================================
    #
    #     reward = (
    #             h_reward +  # ±0.5 (primary signal)
    #             explosion_reward +  # ±0.4
    #             shrink_reward +  # ±0.15
    #             solvability_reward +  # -1.0 or +0.05
    #             dead_end_reward +  # ±0.3
    #             reach_reward +  # ±0.2
    #             stability_reward +  # ±0.1
    #             progress_reward  # +0.02
    #     )
    #
    #     # ========================================================================
    #     # FINAL CLAMPING AND LOGGING
    #     # ========================================================================
    #
    #     # Clamp to [-2.0, +2.0]
    #     reward = max(-2.0, min(2.0, float(reward)))
    #
    #     if self.debug:
    #         logger.debug(
    #             f"[REWARD] Components: h*={h_reward:.3f}, explosion={explosion_reward:.3f}, "
    #             f"shrink={shrink_reward:.3f}, solv={solvability_reward:.3f}, "
    #             f"dead={dead_end_reward:.3f}, reach={reach_reward:.3f}, "
    #             f"stab={stability_reward:.3f}, prog={progress_reward:.3f}"
    #         )
    #         logger.debug(f"[REWARD] FINAL: {reward:.4f}")
    #
    #     return reward

    def _compute_reward(self, raw_obs: Dict[str, Any]) -> float:
        """
        ENHANCED REWARD FUNCTION - Theory-Informed

        Based on:
        1. Nissim et al. (2011) - Perfect Heuristics & Bisimulation
        2. Helmert et al. (2014) - M&S Implementation
        3. Katz & Hoffmann (2013) - M&S Lower Bounds

        Component Weights:
        - H* Preservation (50%)      [Greedy Bisimulation]
        - Transition Control (20%)   [Avoid Explosion]
        - Operator Projection (15%)  [Enable Compression]
        - Label Combinability (10%)  [Label Reduction]
        - Bonuses (5%)              [Architecture Signals]
        """
        from reward_function_enhanced import EnhancedRewardFunction

        if not hasattr(self, '_enhanced_reward_fn'):
            self._enhanced_reward_fn = EnhancedRewardFunction(debug=self.debug)

        return self._enhanced_reward_fn.compute_reward(raw_obs)

    def _action_to_merge_pair(self, action: int) -> Tuple[int, int]:
        if self._cached_edge_index is None or len(self._cached_edge_index) != 2:
            return (0, 1)

        src_list, tgt_list = self._cached_edge_index

        if len(src_list) == 0:
            return (0, 1)

        num_edges = len(src_list)
        action = max(0, min(action, num_edges - 1))

        src = int(src_list[action])
        tgt = int(tgt_list[action])

        if src > tgt:
            src, tgt = tgt, src

        return (src, tgt)

    def reset(self, *, seed=None, options=None) -> Tuple[Dict, Dict]:
        super().reset(seed=seed)

        logger.info(f"[THIN_ENV] === RESET: {os.path.basename(self.problem_file)} ===")

        self._terminate_process()
        self._cleanup_files()

        self.current_iteration = -1
        self._cached_edge_index = None
        self._last_observation = None

        self._launch_fd_process()

        raw_obs = self._wait_for_observation(iteration=-1)

        obs = self._observation_to_tensors(raw_obs)
        self._last_observation = obs

        info = {
            "iteration": -1,
            "num_active_systems": raw_obs.get('num_active_systems', 0),
            "reward_signals": raw_obs.get('reward_signals', {}),
        }

        logger.info(f"[THIN_ENV] Reset complete: {obs['num_nodes']} nodes, {obs['num_edges']} edges")

        return obs, info

    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        self.current_iteration += 1
        iteration = self.current_iteration

        merge_pair = self._action_to_merge_pair(action)
        logger.info(f"[THIN_ENV] Step {iteration}: action={action} -> merge_pair={merge_pair}")

        self._send_merge_decision(iteration, merge_pair)

        try:
            raw_obs = self._wait_for_observation(iteration)
        except TimeoutError as e:
            logger.error(f"[THIN_ENV] Timeout: {e}")
            return self._last_observation, -1.0, True, False, {"error": str(e)}
        except RuntimeError as e:
            logger.error(f"[THIN_ENV] Step failed: {e}")
            return self._last_observation, -1.0, True, False, {"error": str(e)}

        obs = self._observation_to_tensors(raw_obs)
        self._last_observation = obs

        reward = self._compute_reward(raw_obs)

        num_active = raw_obs.get('num_active_systems', 1)
        is_done = raw_obs.get('is_terminal', False)

        terminated = (num_active <= 1) or is_done
        truncated = (iteration >= self.max_merges - 1)

        info = {
            "iteration": iteration,
            "merge_pair": merge_pair,
            "num_active_systems": num_active,
            "reward_signals": raw_obs.get('reward_signals', {}),
        }

        if terminated or truncated:
            logger.info(f"[THIN_ENV] Episode done: reward={reward:.4f}, steps={iteration + 1}")
        # logger.info(f"[THIN_ENV] Episode done: reward={reward:.4f}, steps={iteration + 1}")

        return obs, reward, terminated, truncated, info

    def close(self):
        self._terminate_process()
        logger.info("[THIN_ENV] Environment closed")


def make_thin_env(domain_file: str, problem_file: str, **kwargs) -> ThinMergeEnv:
    return ThinMergeEnv(domain_file=domain_file, problem_file=problem_file, **kwargs)

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.cc code is in the following block:
#include "merge_and_shrink_algorithm.h"

#include "distances.h"
#include "factored_transition_system.h"
#include "fts_factory.h"
#include "label_reduction.h"
#include "labels.h"
#include "merge_and_shrink_representation.h"
#include "merge_strategy.h"
#include "merge_strategy_factory.h"
#include "shrink_strategy.h"
#include "transition_system.h"
#include "types.h"
#include "utils.h"

#include "../plugins/plugin.h"
#include "../task_utils/task_properties.h"
#include "../utils/component_errors.h"
#include "../utils/countdown_timer.h"
#include "../utils/markup.h"
#include "../utils/math.h"
#include "../utils/system.h"
#include "../utils/timer.h"

#include "merge_and_shrink_signals.h"

#include <cassert>
#include <iostream>
#include <limits>
#include <string>
#include <utility>
#include <vector>
#include <filesystem>
#include <nlohmann/json.hpp>
#include <fstream>
#include <cmath>
#include <limits>
#include <ctime>
#include <set>

using json = nlohmann::json;
using namespace std;
namespace fs = std::filesystem;

using plugins::Bounds;
using utils::ExitCode;

namespace merge_and_shrink {


// ============================================================================
// Logging Helpers
// ============================================================================

static void log_progress(const utils::Timer &timer, const string &msg, utils::LogProxy &log) {
    log << "M&S algorithm timer: " << timer << " (" << msg << ")" << endl;
}

// ============================================================================
// MergeAndShrinkAlgorithm Implementation
// ============================================================================

MergeAndShrinkAlgorithm::MergeAndShrinkAlgorithm(
    const shared_ptr<MergeStrategyFactory> &merge_strategy,
    const shared_ptr<ShrinkStrategy> &shrink_strategy,
    const shared_ptr<LabelReduction> &label_reduction,
    bool prune_unreachable_states, bool prune_irrelevant_states,
    int max_states, int max_states_before_merge,
    int threshold_before_merge, double main_loop_max_time,
    utils::Verbosity verbosity)
    : merge_strategy_factory(merge_strategy),
      shrink_strategy(shrink_strategy),
      label_reduction(label_reduction),
      max_states(max_states),
      max_states_before_merge(max_states_before_merge),
      shrink_threshold_before_merge(threshold_before_merge),
      prune_unreachable_states(prune_unreachable_states),
      prune_irrelevant_states(prune_irrelevant_states),
      log(utils::get_log_for_verbosity(verbosity)),
      main_loop_max_time(main_loop_max_time),
      starting_peak_memory(0) {
    handle_shrink_limit_defaults();
    assert(this->max_states_before_merge >= 1);
    assert(this->max_states >= this->max_states_before_merge);
}

void MergeAndShrinkAlgorithm::handle_shrink_limit_defaults() {
    if (max_states == -1 && max_states_before_merge == -1) {
        max_states = 50000;
    }

    if (max_states_before_merge == -1) {
        max_states_before_merge = max_states;
    } else if (max_states == -1) {
        if (utils::is_product_within_limit(
                max_states_before_merge, max_states_before_merge, INF)) {
            max_states = max_states_before_merge * max_states_before_merge;
        } else {
            max_states = INF;
        }
    }

    if (max_states_before_merge > max_states) {
        max_states_before_merge = max_states;
        if (log.is_warning()) {
            log << "WARNING: "
                << "max_states_before_merge exceeds max_states, "
                << "correcting max_states_before_merge." << endl;
        }
    }

    utils::verify_argument(max_states >= 1,
                           "Transition system size must be at least 1.");

    utils::verify_argument(max_states_before_merge >= 1,
                           "Transition system size before merge must be at least 1.");

    if (shrink_threshold_before_merge == -1) {
        shrink_threshold_before_merge = max_states;
    }

    utils::verify_argument(shrink_threshold_before_merge >= 1,
                           "Threshold must be at least 1.");

    if (shrink_threshold_before_merge > max_states) {
        shrink_threshold_before_merge = max_states;
        if (log.is_warning()) {
            log << "WARNING: "
                << "threshold exceeds max_states, "
                << "correcting threshold." << endl;
        }
    }
}

void MergeAndShrinkAlgorithm::report_peak_memory_delta(bool final) const {
    if (final)
        log << "Final";
    else
        log << "Current";
    log << " peak memory increase of merge-and-shrink algorithm: "
        << utils::get_peak_memory_in_kb() - starting_peak_memory << " KB"
        << endl;
}

void MergeAndShrinkAlgorithm::dump_options() const {
    if (log.is_at_least_normal()) {
        if (merge_strategy_factory) {
            merge_strategy_factory->dump_options();
            log << endl;
        }

        log << "Options related to size limits and shrinking: " << endl;
        log << "Transition system size limit: " << max_states << endl
            << "Transition system size limit right before merge: "
            << max_states_before_merge << endl;
        log << "Threshold to trigger shrinking right before merge: "
            << shrink_threshold_before_merge << endl;
        log << endl;

        shrink_strategy->dump_options(log);
        log << endl;

        log << "Pruning unreachable states: "
            << (prune_unreachable_states ? "yes" : "no") << endl;
        log << "Pruning irrelevant states: "
            << (prune_irrelevant_states ? "yes" : "no") << endl;
        log << endl;

        if (label_reduction) {
            label_reduction->dump_options(log);
        } else {
            log << "Label reduction disabled" << endl;
        }
        log << endl;

        log << "Main loop max time in seconds: " << main_loop_max_time << endl;
        log << endl;
    }
}

void MergeAndShrinkAlgorithm::warn_on_unusual_options() const {
    string dashes(79, '=');
    if (!label_reduction) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! You did not enable label reduction. " << endl
                << "This may drastically reduce the performance of merge-and-shrink!"
                << endl << dashes << endl;
        }
    } else if (label_reduction->reduce_before_merging() && label_reduction->reduce_before_shrinking()) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! You set label reduction to be applied twice in each merge-and-shrink" << endl
                << "iteration, both before shrinking and merging. This double computation effort" << endl
                << "does not pay off for most configurations!"
                << endl << dashes << endl;
        }
    } else {
        if (label_reduction->reduce_before_shrinking() &&
            (shrink_strategy->get_name() == "f-preserving"
             || shrink_strategy->get_name() == "random")) {
            if (log.is_warning()) {
                log << dashes << endl
                    << "WARNING! Bucket-based shrink strategies such as f-preserving random perform" << endl
                    << "best if used with label reduction before merging, not before shrinking!"
                    << endl << dashes << endl;
            }
        }
        if (label_reduction->reduce_before_merging() &&
            shrink_strategy->get_name() == "bisimulation") {
            if (log.is_warning()) {
                log << dashes << endl
                    << "WARNING! Shrinking based on bisimulation performs best if used with label" << endl
                    << "reduction before shrinking, not before merging!"
                    << endl << dashes << endl;
            }
        }
    }

    if (!prune_unreachable_states || !prune_irrelevant_states) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! Pruning is (partially) turned off!" << endl
                << "This may drastically reduce the performance of merge-and-shrink!"
                << endl << dashes << endl;
        }
    }
}

bool MergeAndShrinkAlgorithm::ran_out_of_time(
    const utils::CountdownTimer &timer) const {
    if (timer.is_expired()) {
        if (log.is_at_least_normal()) {
            log << "Ran out of time, stopping computation." << endl << endl;
        }
        return true;
    }
    return false;
}

// ============================================================================
// MAIN LOOP - COMPLETE REWRITE (SYNCHRONIZED PING-PONG)
// ============================================================================

void MergeAndShrinkAlgorithm::main_loop(
    FactoredTransitionSystem &fts,
    const TaskProxy &task_proxy) {

    utils::CountdownTimer timer(main_loop_max_time);
    if (log.is_at_least_normal()) {
        log << "Starting main loop ";
        if (main_loop_max_time == numeric_limits<double>::infinity()) {
            log << "without a time limit." << endl;
        } else {
            log << "with a time limit of " << main_loop_max_time << "s." << endl;
        }
    }

    int maximum_intermediate_size = 0;
    for (int i = 0; i < fts.get_size(); ++i) {
        int size = fts.get_transition_system(i).get_size();
        if (size > maximum_intermediate_size) {
            maximum_intermediate_size = size;
        }
    }

    if (label_reduction) {
        label_reduction->initialize(task_proxy);
    }

    unique_ptr<MergeStrategy> merge_strategy =
        merge_strategy_factory->compute_merge_strategy(task_proxy, fts);
    merge_strategy_factory = nullptr;

    auto log_main_loop_progress = [&timer, this](const string &msg) {
        log << "M&S algorithm main loop timer: "
            << timer.get_elapsed_time()
            << " (" << msg << ")" << endl;
    };

    std::string fd_output_dir = get_fd_output_directory();
    std::cout << "[M&S::MAIN] fd_output_dir: " << fd_output_dir << std::endl;

    try {
        std::filesystem::create_directories(fd_output_dir);
    } catch (const std::exception& e) {
        std::cerr << "[M&S::ERROR] Failed to create fd_output: " << e.what() << std::endl;
        throw;
    }

    int iteration = 0;

    while (fts.get_num_active_entries() > 1) {
        std::cout << "\n[M&S::MAIN] ============================================" << std::endl;
        std::cout << "[M&S::MAIN] Iteration " << iteration << " starting..." << std::endl;
        std::cout << "[M&S::MAIN] ============================================" << std::endl;

        // ✅ PHASE 1: GET NEXT MERGE PAIR FROM STRATEGY
        std::cout << "[M&S::MAIN] PHASE 1: Waiting for GNN decision (merge_"
                  << iteration << ".json)..." << std::endl;

        pair<int, int> merge_indices;
        try {
            merge_indices = merge_strategy->get_next();
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Failed to get merge decision: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("get_next failed: ") + e.what(), fd_output_dir);
            throw;
        }

        std::cout << "[M&S::MAIN] PHASE 1 COMPLETE: Received decision: ("
                  << merge_indices.first << ", " << merge_indices.second << ")" << std::endl;

        if (ran_out_of_time(timer)) break;

        int merge_index1 = merge_indices.first;
        int merge_index2 = merge_indices.second;

        // ✅ VALIDATE INDICES IMMEDIATELY
        if (!fts.is_active(merge_index1)) {
            std::cerr << "[M&S::ERROR] merge_index1=" << merge_index1 << " is NOT ACTIVE!" << std::endl;
            export_error_signal(iteration, "merge_index1 is not active", fd_output_dir);
            throw std::runtime_error("Invalid merge index 1");
        }
        if (!fts.is_active(merge_index2)) {
            std::cerr << "[M&S::ERROR] merge_index2=" << merge_index2 << " is NOT ACTIVE!" << std::endl;
            export_error_signal(iteration, "merge_index2 is not active", fd_output_dir);
            throw std::runtime_error("Invalid merge index 2");
        }

        std::cout << "[M&S::MAIN] Merge indices validated: both are active" << std::endl;

        assert(merge_index1 != merge_index2);
        if (log.is_at_least_normal()) {
            log << "Next pair of indices: ("
                << merge_index1 << ", " << merge_index2 << ")" << endl;
            if (log.is_at_least_verbose()) {
                fts.statistics(merge_index1, log);
                fts.statistics(merge_index2, log);
            }
            log_main_loop_progress("after computation of next merge");
        }

        // ✅ PHASE 2: CAPTURE BEFORE DATA (for statistics only)
        std::cout << "[M&S::MAIN] PHASE 2: Capturing pre-merge data..." << std::endl;

        json before_data;
        try {
            before_data = export_merge_before_data(
                fts, merge_index1, merge_index2, iteration, false, false
            );
            std::cout << "[M&S::MAIN] PHASE 2 COMPLETE: Pre-merge data captured" << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Failed to capture before data: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("before_data failed: ") + e.what(), fd_output_dir);
            throw;
        }

        // ✅ PHASE 3: LABEL REDUCTION (BEFORE SHRINKING)
        bool reduced = false;
        if (label_reduction && label_reduction->reduce_before_shrinking()) {
            std::cout << "[M&S::MAIN] PHASE 3: Label reduction (before shrinking)..." << std::endl;
            try {
                reduced = label_reduction->reduce(merge_indices, fts, log);
                if (log.is_at_least_normal() && reduced) {
                    log_main_loop_progress("after label reduction");
                }
                std::cout << "[M&S::MAIN] PHASE 3 COMPLETE: reduced=" << reduced << std::endl;
            } catch (const std::exception& e) {
                std::cerr << "[M&S::ERROR] Label reduction failed: " << e.what() << std::endl;
                export_error_signal(iteration, std::string("label_reduction failed: ") + e.what(), fd_output_dir);
                throw;
            }
        }

        if (ran_out_of_time(timer)) break;

        // ✅ PHASE 4: SHRINKING
        std::cout << "[M&S::MAIN] PHASE 4: Shrinking..." << std::endl;
        bool shrunk = false;
        try {
            shrunk = shrink_before_merge_step(
                fts,
                merge_index1,
                merge_index2,
                max_states,
                max_states_before_merge,
                shrink_threshold_before_merge,
                *shrink_strategy,
                log);
            if (log.is_at_least_normal() && shrunk) {
                log_main_loop_progress("after shrinking");
            }
            std::cout << "[M&S::MAIN] PHASE 4 COMPLETE: shrunk=" << shrunk << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Shrinking failed: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("shrinking failed: ") + e.what(), fd_output_dir);
            throw;
        }

        before_data["shrunk"] = shrunk;
        before_data["reduced"] = reduced;

        if (ran_out_of_time(timer)) break;

        // ✅ PHASE 5: LABEL REDUCTION (BEFORE MERGING)
        if (label_reduction && label_reduction->reduce_before_merging()) {
            std::cout << "[M&S::MAIN] PHASE 5: Label reduction (before merging)..." << std::endl;
            try {
                reduced = label_reduction->reduce(merge_indices, fts, log);
                if (log.is_at_least_normal() && reduced) {
                    log_main_loop_progress("after label reduction");
                }
                before_data["reduced"] = reduced;
                std::cout << "[M&S::MAIN] PHASE 5 COMPLETE: reduced=" << reduced << std::endl;
            } catch (const std::exception& e) {
                std::cerr << "[M&S::ERROR] Label reduction (before merge) failed: " << e.what() << std::endl;
                export_error_signal(iteration, std::string("label_reduction_before_merge failed: ") + e.what(), fd_output_dir);
                throw;
            }
        }

        if (ran_out_of_time(timer)) break;

        // ✅ PHASE 6: PERFORM ACTUAL MERGE
        std::cout << "[M&S::MAIN] PHASE 6: Performing merge..." << std::endl;
        int merged_index;
        try {
            merged_index = fts.merge(merge_index1, merge_index2, log);
            std::cout << "[M&S::MAIN] PHASE 6 COMPLETE: merged_index=" << merged_index << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Merge operation failed: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("merge failed: ") + e.what(), fd_output_dir);
            throw;
        }

        // ✅ PHASE 7: EXPORT GNN OBSERVATION FOR NEXT ITERATION
        // ⭐ CRITICAL: This is the state AFTER the merge, so iteration number matches!
        std::cout << "[M&S::MAIN] PHASE 7: Exporting GNN observation for iteration "
                  << iteration << "..." << std::endl;
        try {
            json obs_data = export_gnn_observation(fts, iteration);  // ✅ FIX: Use iteration directly!
            std::string obs_path = fd_output_dir + "/observation_" + std::to_string(iteration) + ".json";
            write_json_file_atomic(obs_data, obs_path);
            std::cout << "[M&S::MAIN] PHASE 7 COMPLETE: " << obs_path << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] GNN observation export failed: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("obs export failed: ") + e.what(), fd_output_dir);
            // Don't throw - continue anyway
            std::cerr << "[M&S::WARNING] Continuing without observation export" << std::endl;
        }

        // ✅ PHASE 8: POST-MERGE OPERATIONS
        iteration++;  // Move to next iteration for next loop

        int abs_size = fts.get_transition_system(merged_index).get_size();
        if (abs_size > maximum_intermediate_size) {
            maximum_intermediate_size = abs_size;
        }

        if (log.is_at_least_normal()) {
            if (log.is_at_least_verbose()) {
                fts.statistics(merged_index, log);
            }
            log_main_loop_progress("after merging");
        }

        if (ran_out_of_time(timer)) {
            break;
        }

        // Pruning
        if (prune_unreachable_states || prune_irrelevant_states) {
            bool pruned = prune_step(
                fts,
                merged_index,
                prune_unreachable_states,
                prune_irrelevant_states,
                log);
            if (log.is_at_least_normal() && pruned) {
                if (log.is_at_least_verbose()) {
                    fts.statistics(merged_index, log);
                }
                log_main_loop_progress("after pruning");
            }
        }

        if (!fts.is_factor_solvable(merged_index)) {
            if (log.is_at_least_normal()) {
                log << "Abstract problem is unsolvable, stopping computation." << endl << endl;
            }
            break;
        }

        if (ran_out_of_time(timer)) {
            break;
        }

        if (log.is_at_least_verbose()) {
            report_peak_memory_delta();
        }
        if (log.is_at_least_normal()) {
            log << endl;
        }
    }

    log << "End of merge-and-shrink algorithm, statistics:" << endl;
    log << "Main loop runtime: " << timer.get_elapsed_time() << endl;
    log << "Maximum intermediate abstraction size: "
        << maximum_intermediate_size << endl;
    shrink_strategy = nullptr;
    label_reduction = nullptr;
}

FactoredTransitionSystem MergeAndShrinkAlgorithm::build_factored_transition_system(
    const TaskProxy &task_proxy) {
    if (starting_peak_memory) {
        cerr << "Calling build_factored_transition_system twice is not "
             << "supported!" << endl;
        utils::exit_with(utils::ExitCode::SEARCH_CRITICAL_ERROR);
    }
    starting_peak_memory = utils::get_peak_memory_in_kb();

    utils::Timer timer;
    log << "Running merge-and-shrink algorithm..." << endl;
    task_properties::verify_no_axioms(task_proxy);
    dump_options();
    warn_on_unusual_options();
    log << endl;

    const bool compute_init_distances =
        shrink_strategy->requires_init_distances() ||
        merge_strategy_factory->requires_init_distances() ||
        prune_unreachable_states;
    const bool compute_goal_distances =
        shrink_strategy->requires_goal_distances() ||
        merge_strategy_factory->requires_goal_distances() ||
        prune_irrelevant_states;
    FactoredTransitionSystem fts =
        create_factored_transition_system(
            task_proxy,
            compute_init_distances,
            compute_goal_distances,
            log);
    if (log.is_at_least_normal()) {
        log_progress(timer, "after computation of atomic factors", log);
    }

    bool pruned = false;
    bool unsolvable = false;
    for (int index = 0; index < fts.get_size(); ++index) {
        assert(fts.is_active(index));
        if (prune_unreachable_states || prune_irrelevant_states) {
            bool pruned_factor = prune_step(
                fts,
                index,
                prune_unreachable_states,
                prune_irrelevant_states,
                log);
            pruned = pruned || pruned_factor;
        }
        if (!fts.is_factor_solvable(index)) {
            log << "Atomic FTS is unsolvable, stopping computation." << endl;
            unsolvable = true;
            break;
        }
    }
    if (log.is_at_least_normal()) {
        if (pruned) {
            log_progress(timer, "after pruning atomic factors", log);
        }
        log << endl;
    }

// ✅ EXPORT INITIAL OBSERVATION (iteration -1)
    {
        std::string fd_output_dir_str = get_fd_output_directory();
        try {
            json initial_obs = export_gnn_observation(fts, -1);
            std::string initial_obs_path = fd_output_dir_str + "/observation_-1.json";
            write_json_file_atomic(initial_obs, initial_obs_path);
            std::cout << "[M&S] ✅ Exported initial GNN observation (iteration -1)" << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S] ⚠️ Warning: Could not export initial observation: " << e.what() << std::endl;
        }
    }

    // ✅ FIX: Always run main_loop for GNN-guided M&S
    // The GNN needs to make merge decisions regardless of atomic solvability
    if (main_loop_max_time > 0 && fts.get_num_active_entries() > 1) {
        if (unsolvable) {
            std::cout << "[M&S] ⚠️ Atomic FTS detected as unsolvable, but continuing for GNN training" << std::endl;
        }
        main_loop(fts, task_proxy);
    }

    const bool final = true;
    report_peak_memory_delta(final);
    log << "Merge-and-shrink algorithm runtime: " << timer << endl;
    log << endl;
    return fts;
}

void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature) {
    feature.add_option<shared_ptr<MergeStrategyFactory>>(
        "merge_strategy",
        "See detailed documentation for merge strategies. "
        "We currently recommend SCC-DFP, which can be achieved using "
        "{{{merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector="
        "score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order"
        "]))}}}");

    feature.add_option<shared_ptr<ShrinkStrategy>>(
        "shrink_strategy",
        "See detailed documentation for shrink strategies. "
        "We currently recommend non-greedy shrink_bisimulation, which can be "
        "achieved using {not relevant}");

    feature.add_option<shared_ptr<LabelReduction>>(
        "label_reduction",
        "See detailed documentation for labels. There is currently only "
        "one 'option' to use label_reduction, which is {not relevant} "
        "Also note the interaction with shrink strategies.",
        plugins::ArgumentInfo::NO_DEFAULT);

    feature.add_option<bool>(
        "prune_unreachable_states",
        "If true, prune abstract states unreachable from the initial state.",
        "true");
    feature.add_option<bool>(
        "prune_irrelevant_states",
        "If true, prune abstract states from which no goal state can be "
        "reached.",
        "true");

    add_transition_system_size_limit_options_to_feature(feature);

    feature.add_option<double>(
        "main_loop_max_time",
        "A limit in seconds on the runtime of the main loop of the algorithm. "
        "If the limit is exceeded, the algorithm terminates, potentially "
        "returning a factored transition system with several factors. Also "
        "note that the time limit is only checked between transformations "
        "of the main loop, but not during, so it can be exceeded if a "
        "transformation is runtime-intense.",
        "infinity",
        Bounds("0.0", "infinity"));
}

tuple<shared_ptr<MergeStrategyFactory>, shared_ptr<ShrinkStrategy>,
      shared_ptr<LabelReduction>, bool, bool, int, int, int, double>
get_merge_and_shrink_algorithm_arguments_from_options(
    const plugins::Options &opts) {
    return tuple_cat(
        make_tuple(
            opts.get<shared_ptr<MergeStrategyFactory>>("merge_strategy"),
            opts.get<shared_ptr<ShrinkStrategy>>("shrink_strategy"),
            opts.get<shared_ptr<LabelReduction>>(
                "label_reduction", nullptr),
            opts.get<bool>("prune_unreachable_states"),
            opts.get<bool>("prune_irrelevant_states")),
        get_transition_system_size_limit_arguments_from_options(opts),
        make_tuple(opts.get<double>("main_loop_max_time"))
        );
}

void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature) {
    feature.add_option<int>(
        "max_states",
        "maximum transition system size allowed at any time point.",
        "-1",
        Bounds("-1", "infinity"));
    feature.add_option<int>(
        "max_states_before_merge",
        "maximum transition system size allowed for two transition systems "
        "before being merged to form the synchronized product.",
        "-1",
        Bounds("-1", "infinity"));
    feature.add_option<int>(
        "threshold_before_merge",
        "If a transition system, before being merged, surpasses this soft "
        "transition system size limit, the shrink strategy is called to "
        "possibly shrink the transition system.",
        "-1",
        Bounds("-1", "infinity"));
}

tuple<int, int, int>
get_transition_system_size_limit_arguments_from_options(
    const plugins::Options &opts) {
    return make_tuple(
        opts.get<int>("max_states"),
        opts.get<int>("max_states_before_merge"),
        opts.get<int>("threshold_before_merge")
        );
}
}

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.h code is in the following block:
#ifndef MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H
#define MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H

#include "../utils/logging.h"
#include "merge_and_shrink_signals.h"

#include <memory>
#include <string>

class TaskProxy;

namespace plugins {
class ConstructContext;
class Feature;
class Options;
}

namespace utils {
class CountdownTimer;
}

namespace merge_and_shrink {
class FactoredTransitionSystem;
class LabelReduction;
class MergeStrategyFactory;
class ShrinkStrategy;

// ============================================================================
// Helper Functions (declarations)
// ============================================================================

std::string get_fd_output_directory();

void write_json_file_atomic(
    const nlohmann::json& data,
    const std::string& file_path
);

void export_error_signal(
    int iteration,
    const std::string& error_message,
    const std::string& fd_output_dir
);

json export_merge_before_data(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id,
    int iteration,
    bool shrunk,
    bool reduced
);

// ============================================================================
// Main Algorithm Class
// ============================================================================

class MergeAndShrinkAlgorithm {
    // TODO: when the option parser supports it, the following should become
    // unique pointers.
    std::shared_ptr<MergeStrategyFactory> merge_strategy_factory;
    std::shared_ptr<ShrinkStrategy> shrink_strategy;
    std::shared_ptr<LabelReduction> label_reduction;

    // Options for shrinking
    // Hard limit: the maximum size of a transition system at any point.
    int max_states;
    // Hard limit: the maximum size of a transition system before being merged.
    int max_states_before_merge;
    /* A soft limit for triggering shrinking even if the hard limits
       max_states and max_states_before_merge are not violated. */
    int shrink_threshold_before_merge;

    // Options for pruning
    const bool prune_unreachable_states;
    const bool prune_irrelevant_states;

    mutable utils::LogProxy log;
    const double main_loop_max_time;

    long starting_peak_memory;

    void report_peak_memory_delta(bool final = false) const;
    void dump_options() const;
    void warn_on_unusual_options() const;
    bool ran_out_of_time(const utils::CountdownTimer &timer) const;
    void main_loop(
        FactoredTransitionSystem &fts,
        const TaskProxy &task_proxy);
    void handle_shrink_limit_defaults();
public:
    MergeAndShrinkAlgorithm(
        const std::shared_ptr<MergeStrategyFactory> &merge_strategy,
        const std::shared_ptr<ShrinkStrategy> &shrink_strategy,
        const std::shared_ptr<LabelReduction> &label_reduction,
        bool prune_unreachable_states, bool prune_irrelevant_states,
        int max_states, int max_states_before_merge,
        int threshold_before_merge, double main_loop_max_time,
        utils::Verbosity verbosity);
    FactoredTransitionSystem build_factored_transition_system(const TaskProxy &task_proxy);
};

extern void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature);
std::tuple<std::shared_ptr<MergeStrategyFactory>,
           std::shared_ptr<ShrinkStrategy>,
           std::shared_ptr<LabelReduction>, bool, bool, int, int, int,
           double>
get_merge_and_shrink_algorithm_arguments_from_options(
    const plugins::Options &opts);
extern void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature);
std::tuple<int, int, int>
get_transition_system_size_limit_arguments_from_options(
    const plugins::Options &opts);
}

#endif

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_signals.cc code is in the following block:
#include "merge_and_shrink_signals.h"
#include "merge_and_shrink_signals_enhanced.h"

#include "factored_transition_system.h"
#include "transition_system.h"
#include "distances.h"
#include "types.h"
#include <thread>

#include <filesystem>
#include <fstream>
#include <ctime>
#include <iostream>
#include <iomanip>
#include <cmath>
#include <numeric>
#include <algorithm>
#include <stdexcept>
#include <sstream>
#include <set>
#include <unordered_set>
#include <map>

namespace fs = std::filesystem;
using json = nlohmann::json;

namespace merge_and_shrink {

// ============================================================================
// STATIC STATE FOR H* TRACKING ACROSS ITERATIONS
// ============================================================================

// Track h* values for preservation computation
static std::map<int, int> prev_h_star_values;  // ts_id -> h* value
static int prev_combined_h_star = 0;
static int prev_iteration = -2;
static int prev_total_states = 0;
static std::vector<int> prev_f_values;

// ============================================================================
// DIRECTORY MANAGEMENT
// ============================================================================

std::string get_fd_output_directory() {
    fs::path fd_output = fs::current_path() / "fd_output";
    try {
        fs::create_directories(fd_output);
    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Warning: Could not create fd_output: " << e.what() << std::endl;
    }
    return fd_output.string();
}

std::string get_gnn_output_directory() {
    fs::path gnn_output = fs::current_path() / "gnn_output";
    try {
        fs::create_directories(gnn_output);
    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Warning: Could not create gnn_output: " << e.what() << std::endl;
    }
    return gnn_output.string();
}

// ============================================================================
// ATOMIC FILE I/O
// ============================================================================

void write_json_file_atomic(const json& data, const std::string& file_path) {
    try {
        fs::path final_path(file_path);
        fs::path dir_path = final_path.parent_path();

        fs::create_directories(dir_path);

        fs::path temp_path = final_path.string() + ".tmp";

        {
            std::ofstream temp_file(temp_path, std::ios::out | std::ios::trunc);
            if (!temp_file.is_open()) {
                throw std::runtime_error("Cannot open temp file: " + temp_path.string());
            }

            temp_file << data.dump(2);
            temp_file.flush();

            if (temp_file.fail()) {
                throw std::runtime_error("Failed to write/flush to temp file");
            }

            temp_file.close();
        }

        int rename_attempts = 0;
        const int max_rename_attempts = 10;

        while (rename_attempts < max_rename_attempts) {
            try {
                fs::rename(temp_path, final_path);
                break;
            } catch (const fs::filesystem_error& e) {
                rename_attempts++;
                if (rename_attempts >= max_rename_attempts) {
                    throw;
                }
                std::this_thread::sleep_for(std::chrono::milliseconds(50));
            }
        }

        std::cout << "[SIGNALS] Wrote: " << file_path << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Error writing JSON: " << e.what() << std::endl;
        throw;
    }
}

// ============================================================================
// F-VALUE STATISTICS COMPUTATION
// ============================================================================

json compute_f_statistics(const std::vector<int>& distances, int unreachable_marker) {
    std::vector<double> valid_distances;

    for (int d : distances) {
        if (d != unreachable_marker && d >= 0 && d < 1000000000) {
            valid_distances.push_back(static_cast<double>(d));
        }
    }

    json stats;

    if (valid_distances.empty()) {
        stats["min"] = unreachable_marker;
        stats["max"] = unreachable_marker;
        stats["mean"] = 0.0;
        stats["std"] = 0.0;
        stats["valid_count"] = 0;
    } else {
        double min_val = *std::min_element(valid_distances.begin(), valid_distances.end());
        stats["min"] = static_cast<int>(min_val);

        double max_val = *std::max_element(valid_distances.begin(), valid_distances.end());
        stats["max"] = static_cast<int>(max_val);

        double sum = std::accumulate(valid_distances.begin(), valid_distances.end(), 0.0);
        double mean = sum / valid_distances.size();
        stats["mean"] = mean;

        double variance = 0.0;
        for (double v : valid_distances) {
            variance += (v - mean) * (v - mean);
        }
        double std_dev = std::sqrt(variance / valid_distances.size());
        stats["std"] = std_dev;

        stats["valid_count"] = static_cast<int>(valid_distances.size());
    }

    return stats;
}

// ============================================================================
// ENHANCED 15-DIMENSIONAL NODE FEATURES
// ============================================================================

std::vector<float> compute_gnn_node_features_enhanced(
    const TransitionSystem& ts,
    const Distances& distances,
    int iteration,
    int max_state_count,
    int global_max_labels) {

    std::vector<float> features(NODE_FEATURE_DIM, 0.0f);  // 15 features

    int ts_size = ts.get_size();

    const auto& goal_distances = distances.get_goal_distances();
    const auto& init_distances = distances.get_init_distances();

    // ========================================================================
    // EXISTING FEATURES (0-6)
    // ========================================================================

    // Feature 0: Normalized size
    features[0] = (max_state_count > 0) ?
        static_cast<float>(ts_size) / max_state_count : 0.0f;

    // Feature 1: Is atomic
    features[1] = (iteration == -1) ? 1.0f : 0.0f;

    // Features 2-3: H-value statistics
    int reachable_count = 0;
    double sum_h_values = 0.0;
    double mean_h = 0.0;

    for (size_t i = 0; i < goal_distances.size(); ++i) {
        if (goal_distances[i] != INF && goal_distances[i] >= 0) {
            sum_h_values += goal_distances[i];
            reachable_count++;
        }
    }

    // Feature 2: Mean h-value (normalized)
    if (reachable_count > 0) {
        mean_h = sum_h_values / reachable_count;
        features[2] = std::min(1.0f, static_cast<float>(mean_h / (ts_size + 1)));
    } else {
        features[2] = 0.0f;
    }

    // Feature 3: Solvability (fraction of reachable states)
    features[3] = (ts_size > 0) ?
        static_cast<float>(reachable_count) / ts_size : 0.0f;

    // Feature 4: Diameter (max distance)
    int max_distance = 0;
    if (distances.are_goal_distances_computed()) {
        for (int d : goal_distances) {
            if (d != INF && d > max_distance) {
                max_distance = d;
            }
        }
    }
    features[4] = (ts_size > 0) ?
        std::min(1.0f, static_cast<float>(max_distance) / ts_size) : 0.0f;

    // Feature 5: Transition density
    int transition_count = 0;
    for (auto it = ts.begin(); it != ts.end(); ++it) {
        transition_count += static_cast<int>((*it).get_transitions().size());
    }
    features[5] = (ts_size > 0) ?
        static_cast<float>(transition_count) / ts_size : 0.0f;

    // Feature 6: Label count (normalized)
    std::set<int> unique_labels;
    for (auto it = ts.begin(); it != ts.end(); ++it) {
        const auto& label_group = (*it).get_label_group();
        for (int label : label_group) {
            unique_labels.insert(label);
        }
    }
    int label_count = static_cast<int>(unique_labels.size());
    features[6] = (global_max_labels > 0) ?
        static_cast<float>(label_count) / global_max_labels : 0.0f;

    // ========================================================================
    // NEW STRUCTURAL FEATURES (7-10)
    // ========================================================================

    // Feature 7: Initial state h-value (THE KEY METRIC for h* preservation)
    int init_state = ts.get_init_state();
    int h_init = INF;
    if (init_state >= 0 && init_state < static_cast<int>(goal_distances.size())) {
        h_init = goal_distances[init_state];
    }
    features[7] = (h_init != INF && h_init >= 0) ?
        std::min(1.0f, static_cast<float>(h_init) / (ts_size + 1)) : 1.0f;

    // Feature 8: Dead-end ratio (quality indicator)
    int dead_ends = 0;
    for (size_t i = 0; i < goal_distances.size() && i < init_distances.size(); ++i) {
        if (goal_distances[i] == INF && init_distances[i] != INF) {
            dead_ends++;
        }
    }
    features[8] = (reachable_count > 0) ?
        static_cast<float>(dead_ends) / reachable_count : 0.0f;

    // Feature 9: Number of goal states (normalized)
    int num_goals = 0;
    for (int i = 0; i < ts_size; ++i) {
        if (ts.is_goal_state(i)) {
            num_goals++;
        }
    }
    features[9] = (ts_size > 0) ?
        static_cast<float>(num_goals) / ts_size : 0.0f;

    // Feature 10: H-value variance (abstraction quality)
    double variance = 0.0;
    if (reachable_count > 0) {
        for (int d : goal_distances) {
            if (d != INF && d >= 0) {
                double diff = d - mean_h;
                variance += diff * diff;
            }
        }
        variance /= reachable_count;
    }
    features[10] = std::min(1.0f, static_cast<float>(std::sqrt(variance)) / (ts_size + 1));

    // ========================================================================
    // NEW LABEL-BASED FEATURES (11-14)
    // ========================================================================

    // Feature 11: Average transitions per label (label in-degree proxy)
    double avg_trans_per_label = 0.0;
    if (!unique_labels.empty()) {
        avg_trans_per_label = static_cast<double>(transition_count) / unique_labels.size();
    }
    features[11] = std::min(1.0f, static_cast<float>(avg_trans_per_label) / (ts_size + 1));

    // Feature 12: Self-loop ratio (indicates identity transitions)
    int self_loops = 0;
    int total_trans = 0;
    for (auto it = ts.begin(); it != ts.end(); ++it) {
        for (const auto& trans : (*it).get_transitions()) {
            total_trans++;
            if (trans.src == trans.target) {
                self_loops++;
            }
        }
    }
    features[12] = (total_trans > 0) ?
        static_cast<float>(self_loops) / total_trans : 0.0f;

    // Feature 13: Number of incorporated variables (normalized)
    const auto& vars = ts.get_incorporated_variables();
    features[13] = std::min(1.0f, static_cast<float>(vars.size()) / 20.0f);

    // Feature 14: Branching factor / determinism indicator
    // Higher branching = less deterministic
    double branching = (reachable_count > 0 && total_trans > 0) ?
        static_cast<double>(total_trans) / reachable_count : 1.0;
    // Normalize: bf of 1 is fully deterministic, higher is less
    features[14] = std::min(1.0f, 1.0f / static_cast<float>(branching));

    // ========================================================================
    // CLAMP ALL FEATURES TO [0, 1]
    // ========================================================================
    for (int i = 0; i < NODE_FEATURE_DIM; ++i) {
        if (std::isnan(features[i]) || std::isinf(features[i])) {
            features[i] = 0.0f;
        }
        features[i] = std::max(0.0f, std::min(1.0f, features[i]));
    }

    return features;
}

// ============================================================================
// EDGE FEATURES COMPUTATION (C++ SIDE)
// ============================================================================

json compute_edge_features(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json edge_feats;

    // Validate indices
    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        edge_feats["error"] = "Invalid transition system indices";
        return edge_feats;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    // ========================================================================
    // LABEL SYNCHRONIZATION FEATURES (CRITICAL for merge complexity!)
    // ========================================================================

    // Get label sets for each TS
    std::set<int> labels1, labels2;
    for (auto it = ts1.begin(); it != ts1.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels1.insert(l);
        }
    }
    for (auto it = ts2.begin(); it != ts2.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels2.insert(l);
        }
    }

    // Shared labels
    std::set<int> shared_labels;
    std::set_intersection(labels1.begin(), labels1.end(),
                          labels2.begin(), labels2.end(),
                          std::inserter(shared_labels, shared_labels.begin()));

    int total_labels = static_cast<int>(labels1.size() + labels2.size() - shared_labels.size());

    // Feature 0: Label Jaccard similarity
    double jaccard = (total_labels > 0) ?
        static_cast<double>(shared_labels.size()) / total_labels : 0.0;
    edge_feats["label_jaccard"] = jaccard;

    // Feature 1: Shared label ratio
    int min_labels = static_cast<int>(std::min(labels1.size(), labels2.size()));
    double shared_ratio = (min_labels > 0) ?
        static_cast<double>(shared_labels.size()) / min_labels : 0.0;
    edge_feats["shared_label_ratio"] = shared_ratio;

    // Feature 2: Synchronization factor estimate
    // High shared labels = more synchronization = slower product growth
    // Low shared labels = independent = product is Cartesian
    double sync_factor = 1.0 - jaccard;  // 0 = full sync, 1 = no sync
    edge_feats["sync_factor"] = sync_factor;

    // ========================================================================
    // PRODUCT SIZE ESTIMATE
    // ========================================================================

    int ts1_size = ts1.get_size();
    int ts2_size = ts2.get_size();
    int max_product = ts1_size * ts2_size;

    // Feature 3: Product size (log normalized)
    edge_feats["max_product_size"] = max_product;
    edge_feats["product_size_log"] = std::log10(static_cast<double>(max_product) + 1);

    // ========================================================================
    // VARIABLE RELATIONSHIP FEATURES
    // ========================================================================

    const auto& vars1 = ts1.get_incorporated_variables();
    const auto& vars2 = ts2.get_incorporated_variables();

    // Feature 4: Shares variables? (do they have overlapping variables)
    std::set<int> vars1_set(vars1.begin(), vars1.end());
    bool shares_vars = false;
    for (int v : vars2) {
        if (vars1_set.count(v)) {
            shares_vars = true;
            break;
        }
    }
    edge_feats["shares_variables"] = shares_vars ? 1.0 : 0.0;

    // Feature 5: Total variables after merge (normalized)
    std::set<int> all_vars(vars1.begin(), vars1.end());
    all_vars.insert(vars2.begin(), vars2.end());
    edge_feats["combined_var_count"] = static_cast<double>(all_vars.size());

    // ========================================================================
    // HEURISTIC QUALITY FEATURES
    // ========================================================================

    const Distances& dist1 = fts.get_distances(ts1_id);
    const Distances& dist2 = fts.get_distances(ts2_id);

    // Get h* for each (init state h-value)
    int init1 = ts1.get_init_state();
    int init2 = ts2.get_init_state();

    const auto& goal_dist1 = dist1.get_goal_distances();
    const auto& goal_dist2 = dist2.get_goal_distances();

    int h1 = (init1 >= 0 && init1 < static_cast<int>(goal_dist1.size())) ?
             goal_dist1[init1] : INF;
    int h2 = (init2 >= 0 && init2 < static_cast<int>(goal_dist2.size())) ?
             goal_dist2[init2] : INF;

    bool h1_valid = (h1 != INF && h1 >= 0);
    bool h2_valid = (h2 != INF && h2 >= 0);

    // Feature 6: Combined h* (additive for admissible heuristics)
    int combined_h = 0;
    if (h1_valid) combined_h += h1;
    if (h2_valid) combined_h += h2;
    edge_feats["combined_h_star"] = combined_h;

    // Feature 7: H-value compatibility (similar h* = likely compatible)
    if (h1_valid && h2_valid && (h1 + h2) > 0) {
        edge_feats["h_compatibility"] = 1.0 - std::abs(h1 - h2) / static_cast<double>(h1 + h2);
    } else {
        edge_feats["h_compatibility"] = 0.5;
    }

    // Feature 8: Both solvable?
    edge_feats["both_solvable"] = (h1_valid && h2_valid) ? 1.0 : 0.0;

    // ========================================================================
    // STRUCTURAL COMPATIBILITY
    // ========================================================================

    // Feature 9: Size ratio (prefer balanced merges)
    double size_ratio = static_cast<double>(std::min(ts1_size, ts2_size)) /
                        std::max(ts1_size, ts2_size);
    edge_feats["size_balance"] = size_ratio;

    // Count transitions for density
    int ts1_transitions = 0;
    int ts2_transitions = 0;
    for (auto it = ts1.begin(); it != ts1.end(); ++it) {
        ts1_transitions += static_cast<int>((*it).get_transitions().size());
    }
    for (auto it = ts2.begin(); it != ts2.end(); ++it) {
        ts2_transitions += static_cast<int>((*it).get_transitions().size());
    }

    double density1 = (ts1_size > 0) ?
        static_cast<double>(ts1_transitions) / ts1_size : 0.0;
    double density2 = (ts2_size > 0) ?
        static_cast<double>(ts2_transitions) / ts2_size : 0.0;

    // Feature: Density ratio
    double density_ratio = (std::max(density1, density2) > 0) ?
        std::min(density1, density2) / std::max(density1, density2) : 1.0;
    edge_feats["density_ratio"] = density_ratio;

    return edge_feats;
}

std::vector<double> edge_features_to_vector(const json& edge_feats) {
    std::vector<double> vec(EDGE_FEATURE_DIM, 0.0);

    // Order must match Python expectation!
    vec[0] = edge_feats.value("label_jaccard", 0.0);
    vec[1] = edge_feats.value("shared_label_ratio", 0.0);
    vec[2] = edge_feats.value("sync_factor", 0.0);
    vec[3] = edge_feats.value("product_size_log", 0.0) / 10.0;  // Normalize
    vec[4] = edge_feats.value("shares_variables", 0.0);
    vec[5] = edge_feats.value("combined_h_star", 0.0) / 1000.0;  // Normalize
    vec[6] = edge_feats.value("h_compatibility", 0.5);
    vec[7] = edge_feats.value("both_solvable", 0.0);
    vec[8] = edge_feats.value("size_balance", 0.0);
    vec[9] = edge_feats.value("density_ratio", 0.0);

    // Clamp all to [0, 1]
    for (int i = 0; i < EDGE_FEATURE_DIM; ++i) {
        if (std::isnan(vec[i]) || std::isinf(vec[i])) {
            vec[i] = 0.0;
        }
        vec[i] = std::max(0.0, std::min(1.0, vec[i]));
    }

    return vec;
}

// ============================================================================
// COMPREHENSIVE A* SEARCH SIGNALS WITH H* PRESERVATION
// ============================================================================

json compute_comprehensive_astar_signals(
    const FactoredTransitionSystem& fts,
    const std::vector<int>& active_indices,
    int iteration) {

    json signals;

    // ========================================================================
    // AGGREGATE STATISTICS ACROSS ALL ACTIVE TRANSITION SYSTEMS
    // ========================================================================

    int total_states = 0;
    int total_reachable = 0;
    int total_transitions = 0;
    int total_goal_states = 0;
    int total_dead_ends = 0;

    // For f-value analysis
    std::vector<int> all_f_values;
    std::vector<double> all_h_values;

    // For search efficiency
    int total_nodes_expanded = 0;
    double sum_branching_factors = 0.0;
    int systems_with_branching = 0;

    // For solution quality
    int best_solution_cost = INF;
    bool any_solution_found = false;
    int total_search_depth = 0;
    int systems_with_depth = 0;

    // ========================================================================
    // H* TRACKING (THE MOST IMPORTANT SIGNAL!)
    // ========================================================================

    int current_combined_h_star = 0;
    std::map<int, int> current_h_star_values;

    // Reset tracking if this is a new episode
    if (iteration <= 0 || iteration != prev_iteration + 1) {
        prev_total_states = 0;
        prev_f_values.clear();
        prev_combined_h_star = 0;
        prev_h_star_values.clear();
        prev_iteration = -2;
    }

    // ========================================================================
    // COMPUTE SIGNALS FOR EACH ACTIVE TRANSITION SYSTEM
    // ========================================================================

    for (int idx : active_indices) {
        if (!fts.is_active(idx)) continue;

        const TransitionSystem& ts = fts.get_transition_system(idx);
        const Distances& distances = fts.get_distances(idx);

        int ts_size = ts.get_size();
        total_states += ts_size;

        // Get distances
        const auto& init_dist = distances.get_init_distances();
        const auto& goal_dist = distances.get_goal_distances();

        // ====================================================================
        // TRACK H* FOR THIS TS (init state h-value)
        // ====================================================================
        int init_state = ts.get_init_state();
        int h_star = INF;
        if (init_state >= 0 && init_state < static_cast<int>(goal_dist.size())) {
            h_star = goal_dist[init_state];
        }
        if (h_star != INF && h_star >= 0) {
            current_h_star_values[idx] = h_star;
            current_combined_h_star += h_star;
        }

        // Count transitions
        int ts_transitions = 0;
        for (auto it = ts.begin(); it != ts.end(); ++it) {
            ts_transitions += static_cast<int>((*it).get_transitions().size());
        }
        total_transitions += ts_transitions;

        // Count goal states
        for (int i = 0; i < ts_size; ++i) {
            if (ts.is_goal_state(i)) {
                total_goal_states++;
            }
        }

        // Compute f-values and analyze reachability
        int reachable_count = 0;
        int best_goal_f = INF;
        double sum_h = 0.0;
        int max_depth = 0;

        for (size_t i = 0; i < init_dist.size() && i < goal_dist.size(); ++i) {
            // Track dead-ends
            if (goal_dist[i] == INF && init_dist[i] != INF) {
                total_dead_ends++;
            }

            if (init_dist[i] != INF && goal_dist[i] != INF) {
                reachable_count++;
                total_nodes_expanded++;

                int f = init_dist[i] + goal_dist[i];
                all_f_values.push_back(f);

                if (goal_dist[i] < 1000000) {
                    all_h_values.push_back(static_cast<double>(goal_dist[i]));
                    sum_h += goal_dist[i];
                }

                if (init_dist[i] > max_depth && init_dist[i] < INF) {
                    max_depth = init_dist[i];
                }

                if (ts.is_goal_state(static_cast<int>(i))) {
                    if (f < best_goal_f) {
                        best_goal_f = f;
                    }
                    any_solution_found = true;
                }
            }
        }

        total_reachable += reachable_count;

        // Branching factor
        if (reachable_count > 0 && ts_transitions > 0) {
            double bf = static_cast<double>(ts_transitions) / static_cast<double>(reachable_count);
            if (!std::isnan(bf) && !std::isinf(bf) && bf >= 1.0) {
                sum_branching_factors += bf;
                systems_with_branching++;
            }
        }

        // Search depth
        if (reachable_count > 0) {
            total_search_depth += max_depth;
            systems_with_depth++;
        }

        // Best solution cost
        if (best_goal_f < best_solution_cost) {
            best_solution_cost = best_goal_f;
        }
    }

    // ========================================================================
    // COMPUTE H* PRESERVATION (THE KEY METRIC!)
    // ========================================================================

    double h_star_preservation = 1.0;
    int h_star_before = prev_combined_h_star;
    int h_star_after = current_combined_h_star;

    if (prev_combined_h_star > 0) {
        // h* should be preserved or improved (higher = better heuristic)
        // Ratio > 1 means h* improved, < 1 means it degraded
        h_star_preservation = static_cast<double>(h_star_after) / prev_combined_h_star;
        // Clamp to reasonable range
        h_star_preservation = std::max(0.0, std::min(2.0, h_star_preservation));
    }

    signals["h_star_before"] = h_star_before;
    signals["h_star_after"] = h_star_after;
    signals["h_star_preservation"] = h_star_preservation;

    // ========================================================================
    // COMPUTE SHRINKABILITY RATIO
    // ========================================================================

    double shrinkability = 0.0;
    int theoretical_product_size = 0;

    // Estimate theoretical product size from previous state
    if (prev_total_states > 0 && total_states > 0) {
        // Simple heuristic: if we merged, theoretical = prev_total * some factor
        theoretical_product_size = prev_total_states;  // Simplified
        if (theoretical_product_size > 0) {
            shrinkability = 1.0 - static_cast<double>(total_states) / theoretical_product_size;
            shrinkability = std::max(-1.0, std::min(1.0, shrinkability));
        }
    }

    signals["theoretical_product_size"] = theoretical_product_size;
    signals["merged_size"] = total_states;
    signals["shrinkability"] = shrinkability;

    // ========================================================================
    // COMPUTE DEAD-END RATIO
    // ========================================================================

    double dead_end_ratio = (total_reachable > 0) ?
        static_cast<double>(total_dead_ends) / total_reachable : 0.0;
    signals["dead_end_ratio"] = dead_end_ratio;
    signals["total_dead_ends"] = total_dead_ends;

    // ========================================================================
    // COMPUTE AGGREGATED SIGNALS (existing logic)
    // ========================================================================

    // --- SEARCH EFFICIENCY SIGNALS ---
    signals["nodes_expanded"] = total_nodes_expanded;

    double avg_search_depth = (systems_with_depth > 0) ?
        static_cast<double>(total_search_depth) / systems_with_depth : 0.0;
    signals["search_depth"] = avg_search_depth;

    double avg_branching_factor = (systems_with_branching > 0) ?
        sum_branching_factors / systems_with_branching : 1.0;
    signals["branching_factor"] = avg_branching_factor;

    double search_efficiency = (total_states > 0) ?
        1.0 - (static_cast<double>(total_nodes_expanded) / total_states) : 0.0;
    search_efficiency = std::max(0.0, std::min(1.0, search_efficiency));
    signals["search_efficiency_score"] = search_efficiency;

    // --- SOLUTION QUALITY SIGNALS ---
    signals["solution_found"] = any_solution_found;
    signals["solution_cost"] = any_solution_found ? best_solution_cost : 0;

    double solution_quality = 0.0;
    if (any_solution_found && best_solution_cost < INF) {
        solution_quality = 1.0 - std::min(1.0, static_cast<double>(best_solution_cost) / 1000.0);
    }
    signals["solution_quality_score"] = solution_quality;

    // --- STATE CONTROL SIGNALS ---
    signals["states_before"] = prev_total_states;
    signals["states_after"] = total_states;

    int delta_states = total_states - prev_total_states;
    signals["delta_states"] = delta_states;

    double state_explosion_penalty = 0.0;
    if (prev_total_states > 0) {
        double ratio = static_cast<double>(total_states) / prev_total_states;
        if (ratio > 1.0) {
            state_explosion_penalty = std::min(1.0, (ratio - 1.0));
        }
    }
    signals["state_explosion_penalty"] = state_explosion_penalty;

    double transition_density = (total_states > 0) ?
        static_cast<double>(total_transitions) / total_states : 0.0;
    signals["transition_density"] = transition_density;

    double reachability_ratio = (total_states > 0) ?
        static_cast<double>(total_reachable) / total_states : 0.0;
    signals["reachability_ratio"] = reachability_ratio;

    double state_control_score = reachability_ratio * (1.0 - state_explosion_penalty);
    signals["state_control_score"] = state_control_score;

    // --- F-VALUE STABILITY SIGNALS ---
    double f_value_stability = 1.0;
    int num_significant_f_changes = 0;
    double avg_f_change = 0.0;
    double max_f_change = 0.0;
    double f_preservation_score = 1.0;

    if (!prev_f_values.empty() && !all_f_values.empty()) {
        double prev_mean = 0.0, curr_mean = 0.0;
        double prev_std = 0.0, curr_std = 0.0;

        for (int f : prev_f_values) {
            if (f < INF) prev_mean += f;
        }
        prev_mean /= prev_f_values.size();

        for (int f : prev_f_values) {
            if (f < INF) {
                prev_std += (f - prev_mean) * (f - prev_mean);
            }
        }
        prev_std = std::sqrt(prev_std / prev_f_values.size());

        for (int f : all_f_values) {
            if (f < INF) curr_mean += f;
        }
        curr_mean /= all_f_values.size();

        for (int f : all_f_values) {
            if (f < INF) {
                curr_std += (f - curr_mean) * (f - curr_mean);
            }
        }
        curr_std = std::sqrt(curr_std / all_f_values.size());

        if (prev_mean > 0) {
            double mean_change = std::abs(curr_mean - prev_mean) / prev_mean;
            f_value_stability = 1.0 - std::min(1.0, mean_change);
        }

        if (prev_mean > 0 && std::abs(curr_mean - prev_mean) / prev_mean > 0.1) {
            num_significant_f_changes = 1;
        }

        avg_f_change = std::abs(curr_mean - prev_mean);
        max_f_change = std::max(std::abs(curr_mean - prev_mean),
                                std::abs(curr_std - prev_std));

        double std_ratio = (prev_std > 0) ? curr_std / prev_std : 1.0;
        f_preservation_score = f_value_stability *
            std::min(1.0, 1.0 / (1.0 + std::abs(1.0 - std_ratio)));
    }

    signals["f_value_stability"] = f_value_stability;
    signals["num_significant_f_changes"] = num_significant_f_changes;
    signals["avg_f_change"] = avg_f_change;
    signals["max_f_change"] = max_f_change;
    signals["f_preservation_score"] = f_preservation_score;

    double f_stability_score = (f_value_stability + f_preservation_score) / 2.0;
    signals["f_stability_score"] = f_stability_score;

    // --- ADDITIONAL RAW METRICS ---
    signals["total_states"] = total_states;
    signals["total_reachable"] = total_reachable;
    signals["total_transitions"] = total_transitions;
    signals["total_goal_states"] = total_goal_states;
    signals["num_active_systems"] = static_cast<int>(active_indices.size());
    signals["is_solvable"] = any_solution_found || (total_reachable > 0 && total_goal_states > 0);

    if (!all_f_values.empty()) {
        signals["f_stats"] = compute_f_statistics(all_f_values);
    }

    // ========================================================================
    // COMPUTE WEIGHTED REWARD COMPONENTS (Updated with h* focus)
    // ========================================================================

    // 1. H* Preservation (w=0.40) - THE PRIMARY SIGNAL
    double w_h_star = 0.40;
    double h_star_component = std::min(1.0, h_star_preservation);
    signals["weighted_h_preservation"] = h_star_component;

    // 2. Shrinkability (w=0.25) - How well did shrinking work?
    double w_shrink = 0.25;
    double shrink_component = std::max(0.0, shrinkability + 0.5);  // Shift to [0, 1]
    signals["weighted_shrinkability"] = shrink_component;

    // 3. State Control (w=0.20) - Avoid explosion
    double w_state_ctrl = 0.20;
    double state_ctrl_component = state_control_score;
    signals["weighted_state_control"] = state_ctrl_component;

    // 4. Solvability Maintenance (w=0.15)
    double w_solv = 0.15;
    double solv_component = signals["is_solvable"].get<bool>() ? 1.0 : 0.0;
    signals["weighted_solvability"] = solv_component;

    // Final weighted score
    double weighted_total = w_h_star * h_star_component +
                           w_shrink * shrink_component +
                           w_state_ctrl * state_ctrl_component +
                           w_solv * solv_component;
    signals["weighted_total_score"] = weighted_total;

    // Store weights for Python reference
    signals["weights"] = {
        {"h_preservation", w_h_star},
        {"shrinkability", w_shrink},
        {"state_control", w_state_ctrl},
        {"solvability", w_solv}
    };

    // ========================================================================
    // ADDITIONAL DETAILED SIGNALS FOR REWARD SHAPING
    // ========================================================================

    // Growth ratio (explicit for Python)
    double growth_ratio = 1.0;
    if (prev_total_states > 0) {
        growth_ratio = static_cast<double>(total_states) / prev_total_states;
    }
    signals["growth_ratio"] = growth_ratio;

    // Is this merge "good" by simple heuristics?
    bool is_good_merge = (h_star_preservation >= 0.95) &&
                         (growth_ratio < 5.0) &&
                         (dead_end_ratio < 0.3) &&
                         signals["is_solvable"].get<bool>();
    signals["is_good_merge"] = is_good_merge;

    // Is this merge "bad"?
    bool is_bad_merge = (h_star_preservation < 0.8) ||
                        (growth_ratio > 10.0) ||
                        (dead_end_ratio > 0.5) ||
                        !signals["is_solvable"].get<bool>();
    signals["is_bad_merge"] = is_bad_merge;

    // Merge quality score (simple heuristic: 0-1 range)
    double merge_quality = 0.0;
    merge_quality += 0.35 * std::min(1.0, h_star_preservation);  // h* component
    merge_quality += 0.25 * std::max(0.0, 1.0 - state_explosion_penalty);  // explosion
    merge_quality += 0.20 * (shrinkability + 1.0) / 2.0;  // shrinkability normalized
    merge_quality += 0.10 * (1.0 - dead_end_ratio);  // dead-end avoidance
    merge_quality += 0.10 * reachability_ratio;  // reachability
    signals["merge_quality_score"] = std::max(0.0, std::min(1.0, merge_quality));

    // Absolute h* change (useful for debugging)
    signals["h_star_delta"] = h_star_after - h_star_before;

    // Was there actual shrinking?
    signals["did_shrink"] = (total_states < prev_total_states);

    // Transition growth (another explosion indicator)
    double transition_growth = 1.0;
    // Note: would need to track prev_transitions for this
    signals["transition_growth"] = transition_growth;

    std::cout << "[SIGNALS::DEBUG]   growth_ratio=" << growth_ratio
              << ", is_good=" << is_good_merge
              << ", is_bad=" << is_bad_merge << std::endl;

    // ========================================================================
    // UPDATE TRACKING STATE FOR NEXT ITERATION
    // ========================================================================

    prev_total_states = total_states;
    prev_f_values = all_f_values;
    prev_combined_h_star = current_combined_h_star;
    prev_h_star_values = current_h_star_values;
    prev_iteration = iteration;

    // Debug logging
    std::cout << "[SIGNALS::DEBUG] iteration=" << iteration << std::endl;
    std::cout << "[SIGNALS::DEBUG]   total_states=" << total_states
              << " (prev=" << signals["states_before"].get<int>() << ")" << std::endl;
    std::cout << "[SIGNALS::DEBUG]   h_star=" << current_combined_h_star
              << " (prev=" << h_star_before << ", preservation=" << h_star_preservation << ")" << std::endl;
    std::cout << "[SIGNALS::DEBUG]   reachability=" << reachability_ratio
              << ", dead_ends=" << total_dead_ends << std::endl;
    std::cout << "[SIGNALS::DEBUG]   weighted_total=" << weighted_total << std::endl;

    return signals;
}

// ============================================================================
// INTERACTION GRAPH EDGES
// ============================================================================

std::vector<std::pair<int, int>> build_interaction_graph_edges(
    const FactoredTransitionSystem& fts) {

    std::vector<std::pair<int, int>> edges;

    std::vector<int> active_indices;
    int num_systems = fts.get_size();

    for (int i = 0; i < num_systems; ++i) {
        if (fts.is_active(i)) {
            active_indices.push_back(i);
        }
    }

    int num_active = static_cast<int>(active_indices.size());
    bool use_all_pairs = (num_active <= 50);

    if (use_all_pairs) {
        for (size_t i = 0; i < active_indices.size(); ++i) {
            for (size_t j = i + 1; j < active_indices.size(); ++j) {
                int idx_i = active_indices[i];
                int idx_j = active_indices[j];
                edges.push_back(std::make_pair(idx_i, idx_j));
            }
        }
    } else {
        // Use variable sharing for larger graphs
        for (int i = 0; i < num_systems; ++i) {
            if (!fts.is_active(i)) continue;

            const TransitionSystem& ts_i = fts.get_transition_system(i);
            const auto& vars_i = ts_i.get_incorporated_variables();
            std::set<int> vars_i_set(vars_i.begin(), vars_i.end());

            for (int j = i + 1; j < num_systems; ++j) {
                if (!fts.is_active(j)) continue;

                const TransitionSystem& ts_j = fts.get_transition_system(j);
                const auto& vars_j = ts_j.get_incorporated_variables();

                bool shares_variables = false;
                for (int var : vars_j) {
                    if (vars_i_set.count(var) > 0) {
                        shares_variables = true;
                        break;
                    }
                }

                if (shares_variables) {
                    edges.push_back(std::make_pair(i, j));
                }
            }
        }

        if (edges.empty() && num_active >= 2) {
            for (size_t i = 0; i + 1 < active_indices.size(); ++i) {
                edges.push_back(std::make_pair(active_indices[i], active_indices[i + 1]));
            }
        }
    }

    return edges;
}

// ============================================================================
// MAIN GNN OBSERVATION EXPORT (Updated with Edge Features)
// ============================================================================

json export_gnn_observation(
    const FactoredTransitionSystem& fts,
    int iteration) {

    json observation;
    observation["iteration"] = iteration;
    observation["timestamp"] = static_cast<long>(std::time(nullptr));

    try {
        // PHASE 1: Get active transition systems
        std::vector<int> active_indices;
        int num_systems = fts.get_size();

        for (int i = 0; i < num_systems; ++i) {
            if (fts.is_active(i)) {
                active_indices.push_back(i);
            }
        }

        int num_active = static_cast<int>(active_indices.size());
        observation["num_active_systems"] = num_active;

        std::cout << "[SIGNALS::GNN] Active systems: " << num_active << std::endl;

        // PHASE 2: Compute statistics for normalization
        int max_state_count = 1;
        int global_max_labels = 1;

        for (int idx : active_indices) {
            const TransitionSystem& ts = fts.get_transition_system(idx);
            int ts_size = ts.get_size();
            max_state_count = std::max(max_state_count, ts_size);

            std::set<int> unique_labels;
            for (auto it = ts.begin(); it != ts.end(); ++it) {
                const auto& label_group = (*it).get_label_group();
                for (int label : label_group) {
                    unique_labels.insert(label);
                }
            }
            global_max_labels = std::max(global_max_labels, static_cast<int>(unique_labels.size()));
        }

        // PHASE 3: Compute ENHANCED node features (15 features)
        json x_features = json::array();

        for (int idx : active_indices) {
            const TransitionSystem& ts = fts.get_transition_system(idx);
            const Distances& distances = fts.get_distances(idx);

            auto features = compute_gnn_node_features_enhanced(
                ts, distances, iteration, max_state_count, global_max_labels
            );

            json node_features = json::array();
            for (float f : features) {
                node_features.push_back(f);
            }
            x_features.push_back(node_features);
        }

        observation["x"] = x_features;
        observation["node_feature_dim"] = NODE_FEATURE_DIM;

        std::cout << "[SIGNALS::GNN] Computed " << NODE_FEATURE_DIM
                  << " features for " << num_active << " nodes" << std::endl;

        // PHASE 4: Build edges WITH FEATURES (C++ computed!)
        auto edges = build_interaction_graph_edges(fts);

        json edge_index = json::array();
        json sources = json::array();
        json targets = json::array();
        json edge_features = json::array();  // NEW!

        for (const auto& [src, tgt] : edges) {
            sources.push_back(src);
            targets.push_back(tgt);

            // Compute edge features for this merge candidate
            json ef = compute_edge_features(fts, src, tgt);

            // Convert to feature vector
            std::vector<double> edge_feat_vec = edge_features_to_vector(ef);
            edge_features.push_back(edge_feat_vec);
        }

        edge_index.push_back(sources);
        edge_index.push_back(targets);

        observation["edge_index"] = edge_index;
        observation["edge_features"] = edge_features;  // NEW!
        observation["edge_feature_dim"] = EDGE_FEATURE_DIM;
        observation["num_edges"] = static_cast<int>(edges.size());

        std::cout << "[SIGNALS::GNN] Interaction graph: " << edges.size()
                  << " edges with " << EDGE_FEATURE_DIM << " features each" << std::endl;

        // PHASE 5: Compute COMPREHENSIVE A* SEARCH SIGNALS
//        json reward_signals = compute_comprehensive_astar_signals(fts, active_indices, iteration);
//        observation["reward_signals"] = reward_signals;


        // PHASE 5: Compute COMPREHENSIVE MERGE QUALITY SIGNALS
        json reward_signals = compute_comprehensive_astar_signals(fts, active_indices, iteration);

        // After computing reward_signals, add enhanced signals
        if (active_indices.size() >= 2) {
            // Compute comprehensive merge quality analysis for the candidate merge pair
            json enhanced_signals = MergeQualityAnalyzer::compute_comprehensive_merge_signals(
                fts, active_indices[0], active_indices[1], iteration
            );

            // Merge enhanced signals into reward_signals
            for (auto& [key, value] : enhanced_signals.items()) {
                if (key != "iteration" && key != "ts1_id" && key != "ts2_id") {
                    reward_signals[key] = value;
                }
            }

            // Extract and promote top-level scores for easy access
            try {
                if (enhanced_signals.contains("operator_projection") &&
                    enhanced_signals["operator_projection"].contains("opp_score")) {
                    reward_signals["opp_score"] = enhanced_signals["operator_projection"]["opp_score"];
                }

                if (enhanced_signals.contains("label_combinability") &&
                    enhanced_signals["label_combinability"].contains("combinability_score")) {
                    reward_signals["label_combinability_score"] =
                        enhanced_signals["label_combinability"]["combinability_score"];
                }

                if (enhanced_signals.contains("greedy_bisimulation") &&
                    enhanced_signals["greedy_bisimulation"].contains("gb_error")) {
                    reward_signals["gb_error"] = enhanced_signals["greedy_bisimulation"]["gb_error"];
                }

                if (enhanced_signals.contains("causal_graph") &&
                    enhanced_signals["causal_graph"].contains("causal_proximity")) {
                    reward_signals["causal_proximity_score"] =
                        enhanced_signals["causal_graph"]["causal_proximity"];
                }

                if (enhanced_signals.contains("label_support") &&
                    enhanced_signals["label_support"].contains("support_overlap")) {
                    reward_signals["label_support_overlap"] =
                        enhanced_signals["label_support"]["support_overlap"];
                }

                if (enhanced_signals.contains("landmark_preservation") &&
                    enhanced_signals["landmark_preservation"].contains("landmark_preservation")) {
                    reward_signals["landmark_preservation"] =
                        enhanced_signals["landmark_preservation"]["landmark_preservation"];
                }

                if (enhanced_signals.contains("transition_explosion") &&
                    enhanced_signals["transition_explosion"].contains("density_ratio")) {
                    reward_signals["transition_density_ratio"] =
                        enhanced_signals["transition_explosion"]["density_ratio"];
                }

                std::cout << "[SIGNALS::GNN] Enhanced merge quality signals computed for pair ("
                          << active_indices[0] << ", " << active_indices[1] << ")" << std::endl;

            } catch (const std::exception& e) {
                std::cerr << "[SIGNALS::GNN] Warning: Failed to extract enhanced signals: "
                          << e.what() << std::endl;
            }
        }

        observation["reward_signals"] = reward_signals;


        // PHASE 6: Terminal condition
        observation["is_terminal"] = (num_active <= 1);

        std::cout << "[SIGNALS::GNN] ✅ Observation exported for iteration "
                  << iteration << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS::GNN] ERROR: " << e.what() << std::endl;
        observation["error"] = std::string(e.what());

        // Export minimal reward signals even on error
        json reward_signals;
        reward_signals["weighted_total_score"] = 0.0;
        reward_signals["h_star_preservation"] = 1.0;
        reward_signals["is_solvable"] = false;
        observation["reward_signals"] = reward_signals;
    }

    return observation;
}


// ============================================================================
// LEGACY COMPUTE_ASTAR_SIGNALS (kept for compatibility)
// ============================================================================

json compute_astar_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    const std::vector<int>& init_distances,
    const std::vector<int>& goal_distances) {

    const TransitionSystem& ts = fts.get_transition_system(merged_index);

    int reachable_count = 0;
    int unreachable_count = 0;
    long long sum_goal_dist = 0;
    int reachable_goal_count = 0;
    int best_goal_f = INF;

    for (size_t i = 0; i < init_distances.size(); ++i) {
        if (init_distances[i] != INF && goal_distances[i] != INF) {
            reachable_count++;
            sum_goal_dist += goal_distances[i];

            if (ts.is_goal_state(i)) {
                reachable_goal_count++;
                int f = init_distances[i] + goal_distances[i];
                if (f < best_goal_f) {
                    best_goal_f = f;
                }
            }
        } else {
            unreachable_count++;
        }
    }

    int search_depth = 0;
    if (reachable_goal_count > 0) {
        search_depth = static_cast<int>(std::round(
            static_cast<double>(sum_goal_dist) / reachable_goal_count
        ));
    }

    double branching_factor = 1.0;
    int num_transitions = 0;
    for (auto it = ts.begin(); it != ts.end(); ++it) {
        num_transitions += (*it).get_transitions().size();
    }

    if (reachable_count > 0 && num_transitions > 0) {
        branching_factor = static_cast<double>(num_transitions) /
                          static_cast<double>(reachable_count);

        if (std::isnan(branching_factor) || std::isinf(branching_factor)) {
            branching_factor = 1.0;
        }
        if (branching_factor < 1.0) {
            branching_factor = 1.0;
        }
    }

    bool solution_found = (best_goal_f != INF);

    json signals;
    signals["nodes_expanded"] = reachable_count;
    signals["unreachable_states"] = unreachable_count;
    signals["search_depth"] = search_depth;
    signals["branching_factor"] = branching_factor;
    signals["solution_cost"] = solution_found ? best_goal_f : 0;
    signals["solution_found"] = solution_found;

    return signals;
}

// ============================================================================
// PRODUCT STATE MAPPING
// ============================================================================

json build_product_mapping(int ts1_size, int ts2_size) {
    json mapping;

    int counter = 0;
    for (int s = 0; s < ts1_size * ts2_size; ++s) {
        int s1 = s / ts2_size;
        int s2 = s % ts2_size;

        mapping[std::to_string(s)] = {
            {"s1", s1},
            {"s2", s2}
        };

        counter++;

        if (counter > 100000) {
            mapping["_note"] = "Product mapping truncated (too large)";
            break;
        }
    }

    return mapping;
}

// ============================================================================
// MERGE BEFORE DATA EXPORT
// ============================================================================

json export_merge_before_data(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id,
    int iteration,
    bool shrunk,
    bool reduced) {

    json before_data;
    before_data["iteration"] = iteration;
    before_data["ts1_id"] = ts1_id;
    before_data["ts2_id"] = ts2_id;

    try {
        if (!fts.is_active(ts1_id)) {
            std::cerr << "[SIGNALS] ERROR: ts1_id=" << ts1_id << " is NOT active!" << std::endl;
            before_data["error"] = "ts1_id is not active";
            before_data["ts1_active"] = false;
            return before_data;
        }
        if (!fts.is_active(ts2_id)) {
            std::cerr << "[SIGNALS] ERROR: ts2_id=" << ts2_id << " is NOT active!" << std::endl;
            before_data["error"] = "ts2_id is not active";
            before_data["ts2_active"] = false;
            return before_data;
        }

        before_data["ts1_active"] = true;
        before_data["ts2_active"] = true;

        const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
        const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

        const Distances& dist1 = fts.get_distances(ts1_id);
        const Distances& dist2 = fts.get_distances(ts2_id);

        const auto& init1 = dist1.get_init_distances();
        const auto& goal1 = dist1.get_goal_distances();
        const auto& init2 = dist2.get_init_distances();
        const auto& goal2 = dist2.get_goal_distances();

        std::vector<int> f1(init1.size());
        std::vector<int> f2(init2.size());

        for (size_t i = 0; i < f1.size(); ++i) {
            if (i < init1.size() && i < goal1.size()) {
                if (init1[i] != INF && goal1[i] != INF) {
                    f1[i] = init1[i] + goal1[i];
                } else {
                    f1[i] = INF;
                }
            } else {
                f1[i] = INF;
            }
        }

        for (size_t j = 0; j < f2.size(); ++j) {
            if (j < init2.size() && j < goal2.size()) {
                if (init2[j] != INF && goal2[j] != INF) {
                    f2[j] = init2[j] + goal2[j];
                } else {
                    f2[j] = INF;
                }
            } else {
                f2[j] = INF;
            }
        }

        int ts1_transitions = 0;
        int ts2_transitions = 0;
        for (auto it = ts1.begin(); it != ts1.end(); ++it) {
            ts1_transitions += static_cast<int>((*it).get_transitions().size());
        }
        for (auto it = ts2.begin(); it != ts2.end(); ++it) {
            ts2_transitions += static_cast<int>((*it).get_transitions().size());
        }

        int ts1_goals = 0, ts2_goals = 0;
        for (int i = 0; i < ts1.get_size(); ++i) {
            if (ts1.is_goal_state(i)) ts1_goals++;
        }
        for (int j = 0; j < ts2.get_size(); ++j) {
            if (ts2.is_goal_state(j)) ts2_goals++;
        }

        before_data["ts1_size"] = static_cast<int>(f1.size());
        before_data["ts2_size"] = static_cast<int>(f2.size());
        before_data["expected_product_size"] =
            static_cast<int>(f1.size()) * static_cast<int>(f2.size());

        before_data["ts1_transitions"] = ts1_transitions;
        before_data["ts2_transitions"] = ts2_transitions;
        before_data["ts1_density"] =
            static_cast<double>(ts1_transitions) / std::max(static_cast<int>(f1.size()), 1);
        before_data["ts2_density"] =
            static_cast<double>(ts2_transitions) / std::max(static_cast<int>(f2.size()), 1);

        before_data["ts1_goal_states"] = ts1_goals;
        before_data["ts2_goal_states"] = ts2_goals;

        before_data["ts1_f_values"] = f1;
        before_data["ts2_f_values"] = f2;

        before_data["ts1_f_stats"] = compute_f_statistics(f1);
        before_data["ts2_f_stats"] = compute_f_statistics(f2);

        before_data["ts1_variables"] = ts1.get_incorporated_variables();
        before_data["ts2_variables"] = ts2.get_incorporated_variables();

        int product_size = static_cast<int>(f1.size()) * static_cast<int>(f2.size());
        if (product_size <= 100000) {
            before_data["product_mapping"] = build_product_mapping(
                static_cast<int>(f1.size()),
                static_cast<int>(f2.size())
            );
        } else {
            before_data["product_mapping_skipped"] = true;
            before_data["product_mapping_reason"] = "Product too large";
        }

        before_data["shrunk"] = shrunk;
        before_data["reduced"] = reduced;
        before_data["timestamp"] = static_cast<long>(std::time(nullptr));

        std::cout << "[SIGNALS] export_merge_before_data: ts1_size=" << f1.size()
                  << ", ts2_size=" << f2.size() << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] ERROR in export_merge_before_data: " << e.what() << std::endl;
        before_data["error"] = std::string(e.what());
    }

    return before_data;
}

// ============================================================================
// MERGE AFTER DATA EXPORT
// ============================================================================

json export_merge_after_data(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int iteration) {

    json after_data;
    after_data["iteration"] = iteration;

    try {
        const TransitionSystem& ts = fts.get_transition_system(merged_index);
        const Distances& distances = fts.get_distances(merged_index);

        const auto& init_dist = distances.get_init_distances();
        const auto& goal_dist = distances.get_goal_distances();

        std::vector<int> f_after(init_dist.size());
        for (size_t s = 0; s < f_after.size(); ++s) {
            if (init_dist[s] != INF && goal_dist[s] != INF) {
                f_after[s] = init_dist[s] + goal_dist[s];
            } else {
                f_after[s] = INF;
            }
        }

        int reachable_count = 0, unreachable_count = 0;
        for (size_t i = 0; i < f_after.size(); ++i) {
            if (init_dist[i] != INF && goal_dist[i] != INF) {
                reachable_count++;
            } else {
                unreachable_count++;
            }
        }

        int num_goals = 0;
        for (int i = 0; i < ts.get_size(); ++i) {
            if (ts.is_goal_state(i)) num_goals++;
        }

        int merged_transitions = 0;
        for (auto it = ts.begin(); it != ts.end(); ++it) {
            merged_transitions += (*it).get_transitions().size();
        }

        after_data["merged_size"] = static_cast<int>(f_after.size());
        after_data["merged_goal_states"] = num_goals;
        after_data["merged_transitions"] = merged_transitions;
        after_data["merged_density"] =
            static_cast<double>(merged_transitions) /
            std::max(static_cast<int>(f_after.size()), 1);

        after_data["reachable_states"] = reachable_count;
        after_data["unreachable_states"] = unreachable_count;
        after_data["reachability_ratio"] =
            static_cast<double>(reachable_count) /
            std::max(static_cast<int>(f_after.size()), 1);

        after_data["f_values"] = f_after;
        after_data["f_stats"] = compute_f_statistics(f_after);

        after_data["shrinking_ratio"] =
            static_cast<double>(f_after.size()) /
            std::max(1, fts.get_transition_system(merged_index).get_size());

        json astar_signals = compute_astar_signals(
            fts, merged_index, init_dist, goal_dist
        );
        after_data["search_signals"] = astar_signals;

        after_data["timestamp"] = static_cast<long>(std::time(nullptr));

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Error in export_merge_after_data: " << e.what() << std::endl;
        after_data["error"] = std::string(e.what());
    }

    return after_data;
}

// ============================================================================
// TRANSITION SYSTEM DEFINITION EXPORT
// ============================================================================

json export_ts_data(
    const FactoredTransitionSystem& fts,
    int ts_index,
    int iteration) {

    json ts_json;
    ts_json["iteration"] = iteration;

    try {
        const TransitionSystem& ts = fts.get_transition_system(ts_index);

        ts_json["ts_index"] = ts_index;
        ts_json["num_states"] = ts.get_size();
        ts_json["init_state"] = ts.get_init_state();

        std::vector<int> goal_states;
        for (int i = 0; i < ts.get_size(); ++i) {
            if (ts.is_goal_state(i)) {
                goal_states.push_back(i);
            }
        }
        ts_json["goal_states"] = goal_states;

        ts_json["incorporated_variables"] = ts.get_incorporated_variables();

        std::vector<json> transitions;
        int transition_count = 0;
        const int MAX_TRANSITIONS_TO_EXPORT = 10000;

        for (auto it = ts.begin(); it != ts.end(); ++it) {
            const auto& info = *it;
            const auto& label_group = info.get_label_group();
            const auto& trans_vec = info.get_transitions();

            for (int label : label_group) {
                for (const auto& trans : trans_vec) {
                    if (transition_count < MAX_TRANSITIONS_TO_EXPORT) {
                        transitions.push_back({
                            {"src", trans.src},
                            {"target", trans.target},
                            {"label", label}
                        });
                        transition_count++;
                    }
                }
            }
        }

        ts_json["transitions"] = transitions;

        if (transition_count >= MAX_TRANSITIONS_TO_EXPORT) {
            ts_json["_transitions_note"] = "Transitions truncated (too many)";
        }

        ts_json["timestamp"] = static_cast<long>(std::time(nullptr));

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Error in export_ts_data: " << e.what() << std::endl;
        ts_json["error"] = std::string(e.what());
    }

    return ts_json;
}

// ============================================================================
// FD INDEX MAPPING EXPORT
// ============================================================================

json export_fd_index_mapping_data(
    const FactoredTransitionSystem& fts,
    int iteration) {

    json index_mapping;
    index_mapping["iteration"] = iteration;
    index_mapping["timestamp"] = static_cast<long>(std::time(nullptr));

    json systems = json::array();

    try {
        for (int fd_idx = 0; fd_idx < fts.get_size(); ++fd_idx) {
            if (!fts.is_active(fd_idx)) {
                continue;
            }

            const TransitionSystem& ts = fts.get_transition_system(fd_idx);

            json system_entry;
            system_entry["fd_index"] = fd_idx;
            system_entry["num_states"] = ts.get_size();
            system_entry["incorporated_variables"] = ts.get_incorporated_variables();
            system_entry["is_active"] = true;

            systems.push_back(system_entry);
        }

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Error in export_fd_index_mapping_data: "
                  << e.what() << std::endl;
        index_mapping["error"] = std::string(e.what());
    }

    index_mapping["systems"] = systems;

    return index_mapping;
}

// ============================================================================
// ERROR SIGNAL EXPORT
// ============================================================================

void export_error_signal(
    int iteration,
    const std::string& error_message,
    const std::string& fd_output_dir) {

    try {
        json error_data;
        error_data["iteration"] = iteration;
        error_data["error"] = true;
        error_data["message"] = error_message;
        error_data["timestamp"] = static_cast<long>(std::time(nullptr));

        std::string error_path = fd_output_dir + "/gnn_error_" +
                                 std::to_string(iteration) + ".json";

        write_json_file_atomic(error_data, error_path);

        std::cerr << "[SIGNALS] Error signal exported: " << error_path << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Failed to export error signal: " << e.what() << std::endl;
    }
}

// ============================================================================
// MAIN EXPORT FUNCTION
// ============================================================================

void export_merge_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int ts1_id,
    int ts2_id,
    const std::string& fd_output_dir,
    int iteration,
    bool shrunk,
    bool reduced) {

    std::cout << "\n[SIGNALS] ========================================" << std::endl;
    std::cout << "[SIGNALS] Exporting signals for iteration " << iteration << std::endl;
    std::cout << "[SIGNALS] ========================================\n" << std::endl;

    try {
        std::cout << "[SIGNALS] 1. Exporting merge_before..." << std::endl;
        json before_data = export_merge_before_data(
            fts, ts1_id, ts2_id, iteration, shrunk, reduced
        );
        std::string before_path = fd_output_dir + "/merge_before_" +
                                  std::to_string(iteration) + ".json";
        write_json_file_atomic(before_data, before_path);

        std::cout << "[SIGNALS] 2. Exporting merge_after..." << std::endl;
        json after_data = export_merge_after_data(fts, merged_index, iteration);
        std::string after_path = fd_output_dir + "/merge_after_" +
                                 std::to_string(iteration) + ".json";
        write_json_file_atomic(after_data, after_path);

        std::cout << "[SIGNALS] 3. Exporting ts definition..." << std::endl;
        json ts_data = export_ts_data(fts, merged_index, iteration);
        std::string ts_path = fd_output_dir + "/ts_" +
                              std::to_string(iteration) + ".json";
        write_json_file_atomic(ts_data, ts_path);

        std::cout << "[SIGNALS] 4. Exporting FD index mapping..." << std::endl;
        json mapping_data = export_fd_index_mapping_data(fts, iteration);
        std::string mapping_path = fd_output_dir + "/fd_index_mapping_" +
                                   std::to_string(iteration) + ".json";
        write_json_file_atomic(mapping_data, mapping_path);

        std::cout << "\n[SIGNALS] ✅ All signals exported for iteration "
                  << iteration << "\n" << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "\n[SIGNALS] ❌ CRITICAL ERROR exporting signals: "
                  << e.what() << std::endl;
        export_error_signal(iteration,
                           std::string("Signal export failed: ") + e.what(),
                           fd_output_dir);
        throw;
    }
}

}  // namespace merge_and_shrink

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_signals.h code is in the following block:
#ifndef MERGE_AND_SHRINK_SIGNALS_H
#define MERGE_AND_SHRINK_SIGNALS_H

#include <string>
#include <vector>
#include <nlohmann/json.hpp>

using json = nlohmann::json;

namespace merge_and_shrink {

// Forward declarations
class FactoredTransitionSystem;
class TransitionSystem;
class Distances;
class Labels;

// ============================================================================
// FEATURE DIMENSIONS - MUST MATCH PYTHON
// ============================================================================

constexpr int NODE_FEATURE_DIM = 15;    // Expanded from 7 to 15
constexpr int EDGE_FEATURE_DIM = 10;    // New: C++ computed edge features

// ============================================================================
// GNN OBSERVATION EXPORT - THE FEATURE ENGINE
// ============================================================================

/**
 * Export observation in GNN format with comprehensive A* signals
 * Now includes edge features computed in C++
 */
json export_gnn_observation(
    const FactoredTransitionSystem& fts,
    int iteration
);

/**
 * Compute comprehensive A* search signals for reward calculation
 * Enhanced with h* preservation tracking
 */
json compute_comprehensive_astar_signals(
    const FactoredTransitionSystem& fts,
    const std::vector<int>& active_indices,
    int iteration
);

// ============================================================================
// ENHANCED FEATURE COMPUTATION
// ============================================================================

/**
 * Compute enhanced 15-dimensional node features
 *
 * Features 0-6: Original features
 * Features 7-14: New structural and label-based features
 */
std::vector<float> compute_gnn_node_features_enhanced(
    const TransitionSystem& ts,
    const Distances& distances,
    int iteration,
    int max_state_count,
    int global_max_labels
);

/**
 * Compute edge features for a merge candidate pair
 *
 * Returns 10 features including:
 * - Label synchronization (Jaccard, shared ratio, sync factor)
 * - Product size estimates
 * - Variable relationships
 * - Heuristic quality
 * - Structural compatibility
 */
json compute_edge_features(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id
);

/**
 * Convert edge feature JSON to vector for export
 */
std::vector<double> edge_features_to_vector(const json& edge_feats);

// ============================================================================
// MERGE DATA EXPORTS
// ============================================================================

json export_merge_before_data(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id,
    int iteration,
    bool shrunk,
    bool reduced
);

json export_merge_after_data(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int iteration
);

// ============================================================================
// HELPER FUNCTIONS
// ============================================================================

std::vector<std::pair<int, int>> build_interaction_graph_edges(
    const FactoredTransitionSystem& fts
);

json export_ts_data(
    const FactoredTransitionSystem& fts,
    int ts_index,
    int iteration
);

json export_fd_index_mapping_data(
    const FactoredTransitionSystem& fts,
    int iteration
);

json compute_f_statistics(
    const std::vector<int>& distances,
    int unreachable_marker = 2147483647
);

json compute_astar_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    const std::vector<int>& init_distances,
    const std::vector<int>& goal_distances
);

json build_product_mapping(int ts1_size, int ts2_size);

void write_json_file_atomic(
    const json& data,
    const std::string& file_path
);

std::string get_fd_output_directory();
std::string get_gnn_output_directory();

void export_error_signal(
    int iteration,
    const std::string& error_message,
    const std::string& fd_output_dir
);

void export_merge_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int ts1_id,
    int ts2_id,
    const std::string& fd_output_dir,
    int iteration,
    bool shrunk,
    bool reduced
);

}  // namespace merge_and_shrink

#endif

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_signals_enhanced.cc code is in the following block:
#include "merge_and_shrink_signals_enhanced.h"
#include "labels.h"
#include <algorithm>
#include <numeric>
#include <cmath>
#include <unordered_set>

namespace merge_and_shrink {

// ============================================================================
// OPERATOR PROJECTION POTENTIAL
// ============================================================================

double OperatorProjectionAnalyzer::compute_opp_score(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 0.0;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    const auto& vars1 = ts1.get_incorporated_variables();
    const auto& vars2 = ts2.get_incorporated_variables();

    std::set<int> merged_vars(vars1.begin(), vars1.end());
    merged_vars.insert(vars2.begin(), vars2.end());

    // Collect all remaining variables (NOT in merged set)
    std::set<int> remaining_vars;
    int num_vars = fts.get_num_active_entries();
    for (int i = 0; i < num_vars; ++i) {
        if (merged_vars.find(i) == merged_vars.end()) {
            remaining_vars.insert(i);
        }
    }

    if (remaining_vars.empty()) {
        // No variables left to project onto = bad for later merges
        return 0.5;
    }

    // Count operators projectable (affect ONLY merged variables)
    // These can be abstracted away post-merge
    int projectable_ops = 0;
    int total_ops = fts.get_labels().get_num_total_labels();

    for (int label = 0; label < total_ops; ++label) {
        bool is_projectable = true;

        // Check if this operator affects any remaining variable
        for (int var : remaining_vars) {
            // Would need access to operator preconditions/effects
            // For now, use heuristic: check if label appears in remaining systems
            bool affects_remaining = false;
            for (int i = 0; i < fts.get_size(); ++i) {
                if (!fts.is_active(i) || merged_vars.count(i)) continue;

                const TransitionSystem& ts_remaining = fts.get_transition_system(i);
                // Check if label is used in this TS
                for (auto it = ts_remaining.begin(); it != ts_remaining.end(); ++it) {
                    for (int l : (*it).get_label_group()) {
                        if (l == label) {
                            affects_remaining = true;
                            break;
                        }
                    }
                    if (affects_remaining) break;
                }
                if (affects_remaining) break;
            }

            if (affects_remaining) {
                is_projectable = false;
                break;
            }
        }

        if (is_projectable) {
            projectable_ops++;
        }
    }

    double opp_score = (total_ops > 0) ?
        static_cast<double>(projectable_ops) / total_ops : 0.0;

    return std::max(0.0, std::min(1.0, opp_score));
}

json OperatorProjectionAnalyzer::analyze_operator_projection(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json analysis;
    analysis["opp_score"] = compute_opp_score(fts, ts1_id, ts2_id);
    analysis["metric_name"] = "Operator Projection Potential";
    analysis["interpretation"] = "High (>0.7) = many projectable operators = GOOD merge";
    return analysis;
}

// ============================================================================
// LABEL COMBINABILITY
// ============================================================================

double LabelCombinaibilityAnalyzer::compute_label_combinability_score(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 0.0;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    // Get labels for each TS
    std::set<int> labels1, labels2;
    for (auto it = ts1.begin(); it != ts1.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels1.insert(l);
        }
    }
    for (auto it = ts2.begin(); it != ts2.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels2.insert(l);
        }
    }

    // Labels that appear in both are "synchronizing" and will likely become equivalent
    std::set<int> shared_labels;
    std::set_intersection(labels1.begin(), labels1.end(),
                         labels2.begin(), labels2.end(),
                         std::inserter(shared_labels, shared_labels.begin()));

    // Score: ratio of shared labels
    // High = many labels already synchronized = they'll be combinable
    int total_unique = labels1.size() + labels2.size() - shared_labels.size();
    double score = (total_unique > 0) ?
        static_cast<double>(shared_labels.size()) / total_unique : 0.0;

    return std::max(0.0, std::min(1.0, score));
}

int LabelCombinaibilityAnalyzer::count_collapsible_labels(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 0;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    std::set<int> labels1, labels2;
    for (auto it = ts1.begin(); it != ts1.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels1.insert(l);
        }
    }
    for (auto it = ts2.begin(); it != ts2.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels2.insert(l);
        }
    }

    std::set<int> shared_labels;
    std::set_intersection(labels1.begin(), labels1.end(),
                         labels2.begin(), labels2.end(),
                         std::inserter(shared_labels, shared_labels.begin()));

    return static_cast<int>(shared_labels.size());
}

json LabelCombinaibilityAnalyzer::analyze_label_equivalence(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json analysis;
    analysis["combinability_score"] = compute_label_combinability_score(fts, ts1_id, ts2_id);
    analysis["collapsible_label_count"] = count_collapsible_labels(fts, ts1_id, ts2_id);
    analysis["metric_name"] = "Label Combinability";
    analysis["interpretation"] = "High score = labels will collapse post-merge = GOOD";
    return analysis;
}

// ============================================================================
// GREEDY BISIMULATION ERROR
// ============================================================================

double GreedyBisimulationAnalyzer::compute_greedy_bisimulation_error(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 1.0;  // Unknown = bad
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    const Distances& dist1 = fts.get_distances(ts1_id);
    const Distances& dist2 = fts.get_distances(ts2_id);

    const auto& h1 = dist1.get_goal_distances();
    const auto& h2 = dist2.get_goal_distances();

    // Get init states
    int init1 = ts1.get_init_state();
    int init2 = ts2.get_init_state();

    // Check h-value compatibility at init
    if (init1 < 0 || init1 >= static_cast<int>(h1.size()) ||
        init2 < 0 || init2 >= static_cast<int>(h2.size())) {
        return 1.0;
    }

    int h_init1 = h1[init1];
    int h_init2 = h2[init2];

    // If both are solvable and have similar h-values, good
    if (h_init1 != INF && h_init2 != INF && h_init1 >= 0 && h_init2 >= 0) {
        // h-value mismatch = potential bisimulation violation
        int max_h = std::max(h_init1, h_init2);
        if (max_h > 0) {
            double h_diff_ratio = static_cast<double>(std::abs(h_init1 - h_init2)) / max_h;
            // Scale to [0, 1]: 0 = perfect, 1 = severe conflict
            return std::min(1.0, h_diff_ratio);
        }
    }

    // One or both unsolvable = bad for heuristic
    if (h_init1 == INF || h_init2 == INF) {
        return 0.8;  // Not immediately fatal but risky
    }

    return 0.0;  // Good
}

bool GreedyBisimulationAnalyzer::violates_greedy_bisimulation(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id,
    double error_threshold) {

    double error = compute_greedy_bisimulation_error(fts, ts1_id, ts2_id);
    return error > error_threshold;
}

json GreedyBisimulationAnalyzer::analyze_h_value_compatibility(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json analysis;
    analysis["gb_error"] = compute_greedy_bisimulation_error(fts, ts1_id, ts2_id);
    analysis["violates_gb"] = violates_greedy_bisimulation(fts, ts1_id, ts2_id, 0.2);
    analysis["metric_name"] = "Greedy Bisimulation Error";
    analysis["interpretation"] = "Low error (0-0.2) = h-values compatible = GOOD";
    return analysis;
}

// ============================================================================
// OPERATOR COST VARIANCE
// ============================================================================

json OperatorCostAnalyzer::analyze_operator_costs(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json analysis;
    analysis["cost_variance_score"] = compute_cost_preservation_score(fts, ts1_id, ts2_id);
    analysis["metric_name"] = "Operator Cost Variance";
    return analysis;
}

double OperatorCostAnalyzer::compute_cost_preservation_score(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    // Simplified: Check if operators affecting both have non-uniform costs
    // Full implementation would need access to task.operators
    // For now, use label count variance as proxy

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 0.5;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    std::set<int> labels1, labels2;
    for (auto it = ts1.begin(); it != ts1.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels1.insert(l);
        }
    }
    for (auto it = ts2.begin(); it != ts2.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels2.insert(l);
        }
    }

    // Shared operators = ones that could benefit from merge
    std::set<int> shared;
    std::set_intersection(labels1.begin(), labels1.end(),
                         labels2.begin(), labels2.end(),
                         std::inserter(shared, shared.begin()));

    if (shared.empty()) {
        return 0.3;  // No shared operators = merging loses information
    }

    // Score = proportion of shared to unique
    int total = labels1.size() + labels2.size() - shared.size();
    return std::min(1.0, static_cast<double>(shared.size()) / total);
}

// ============================================================================
// CAUSAL GRAPH DISTANCE
// ============================================================================

double CausalGraphAnalyzer::compute_causal_proximity_score(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 0.0;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    const auto& vars1 = ts1.get_incorporated_variables();
    const auto& vars2 = ts2.get_incorporated_variables();

    // Check if variables directly interact (via shared operators)
    // This is a proxy for causal graph distance
    std::set<int> vars1_set(vars1.begin(), vars1.end());
    bool directly_related = false;

    for (int v : vars2) {
        if (vars1_set.count(v)) {
            directly_related = true;
            break;
        }
    }

    return directly_related ? 1.0 : 0.0;
}

bool CausalGraphAnalyzer::are_causally_adjacent(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    return compute_causal_proximity_score(fts, ts1_id, ts2_id) > 0.5;
}

json CausalGraphAnalyzer::analyze_causal_relationships(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json analysis;
    analysis["causal_proximity"] = compute_causal_proximity_score(fts, ts1_id, ts2_id);
    analysis["are_adjacent"] = are_causally_adjacent(fts, ts1_id, ts2_id);
    analysis["metric_name"] = "Causal Graph Proximity";
    analysis["interpretation"] = "Adjacent in causal graph = better operator projection";
    return analysis;
}

// ============================================================================
// LABEL SUPPORT CORRELATION
// ============================================================================

double LabelSupportAnalyzer::compute_operator_support_overlap(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 0.0;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    std::set<int> labels1, labels2;
    for (auto it = ts1.begin(); it != ts1.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels1.insert(l);
        }
    }
    for (auto it = ts2.begin(); it != ts2.end(); ++it) {
        for (int l : (*it).get_label_group()) {
            labels2.insert(l);
        }
    }

    // Jaccard similarity
    std::set<int> intersection, union_set;
    std::set_intersection(labels1.begin(), labels1.end(),
                         labels2.begin(), labels2.end(),
                         std::inserter(intersection, intersection.begin()));
    std::set_union(labels1.begin(), labels1.end(),
                   labels2.begin(), labels2.end(),
                   std::inserter(union_set, union_set.begin()));

    if (union_set.empty()) {
        return 0.0;
    }

    return static_cast<double>(intersection.size()) / union_set.size();
}

json LabelSupportAnalyzer::analyze_operator_support(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json analysis;
    analysis["support_overlap"] = compute_operator_support_overlap(fts, ts1_id, ts2_id);
    analysis["metric_name"] = "Label Support Overlap";
    analysis["interpretation"] = "High (>0.6) = variables entangled = merge preserves constraints";
    return analysis;
}

// ============================================================================
// LANDMARK PRESERVATION
// ============================================================================

double LandmarkAnalyzer::compute_landmark_preservation_score(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    // Simplified: Check if both TS have goal states
    // Full implementation would use actual landmark graph

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 0.5;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    int goals1 = 0, goals2 = 0;
    for (int i = 0; i < ts1.get_size(); ++i) {
        if (ts1.is_goal_state(i)) goals1++;
    }
    for (int i = 0; i < ts2.get_size(); ++i) {
        if (ts2.is_goal_state(i)) goals2++;
    }

    // If both have goal states, merge should preserve landmark achievement
    return (goals1 > 0 && goals2 > 0) ? 1.0 : 0.5;
}

json LandmarkAnalyzer::analyze_landmark_involvement(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json analysis;
    analysis["landmark_preservation"] = compute_landmark_preservation_score(fts, ts1_id, ts2_id);
    analysis["metric_name"] = "Landmark Preservation";
    return analysis;
}

// ============================================================================
// TRANSITION EXPLOSION PREDICTION
// ============================================================================

double TransitionExplosionPredictor::predict_transition_density_ratio(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    if (!fts.is_active(ts1_id) || !fts.is_active(ts2_id)) {
        return 1.0;
    }

    const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
    const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

    int ts1_size = ts1.get_size();
    int ts2_size = ts2.get_size();
    int ts1_trans = 0, ts2_trans = 0;

    for (auto it = ts1.begin(); it != ts1.end(); ++it) {
        ts1_trans += static_cast<int>((*it).get_transitions().size());
    }
    for (auto it = ts2.begin(); it != ts2.end(); ++it) {
        ts2_trans += static_cast<int>((*it).get_transitions().size());
    }

    // Estimate: product density ≈ min(density1, density2)
    // (synchronization reduces product density)
    double density1 = (ts1_size > 0) ? static_cast<double>(ts1_trans) / ts1_size : 0.0;
    double density2 = (ts2_size > 0) ? static_cast<double>(ts2_trans) / ts2_size : 0.0;

    double product_size = ts1_size * ts2_size;
    double estimated_product_trans = product_size * std::min(density1, density2);

    double ratio = (product_size > 0) ? estimated_product_trans / product_size : 1.0;
    return std::max(0.0, std::min(1.0, ratio));
}

json TransitionExplosionPredictor::predict_transition_explosion(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id) {

    json prediction;
    prediction["density_ratio"] = predict_transition_density_ratio(fts, ts1_id, ts2_id);
    prediction["metric_name"] = "Transition Explosion Prediction";
    prediction["interpretation"] = "Low ratio (<0.3) = good, High (>0.7) = explosion risk";
    return prediction;
}

// ============================================================================
// COMPREHENSIVE MERGE SIGNALS
// ============================================================================

json MergeQualityAnalyzer::compute_comprehensive_merge_signals(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id,
    int iteration) {

    json comprehensive;
    comprehensive["iteration"] = iteration;
    comprehensive["ts1_id"] = ts1_id;
    comprehensive["ts2_id"] = ts2_id;

    // Run all analyzers
    comprehensive["operator_projection"] =
        OperatorProjectionAnalyzer::analyze_operator_projection(fts, ts1_id, ts2_id);

    comprehensive["label_combinability"] =
        LabelCombinaibilityAnalyzer::analyze_label_equivalence(fts, ts1_id, ts2_id);

    comprehensive["greedy_bisimulation"] =
        GreedyBisimulationAnalyzer::analyze_h_value_compatibility(fts, ts1_id, ts2_id);

    comprehensive["operator_costs"] =
        OperatorCostAnalyzer::analyze_operator_costs(fts, ts1_id, ts2_id);

    comprehensive["causal_graph"] =
        CausalGraphAnalyzer::analyze_causal_relationships(fts, ts1_id, ts2_id);

    comprehensive["label_support"] =
        LabelSupportAnalyzer::analyze_operator_support(fts, ts1_id, ts2_id);

    comprehensive["landmark_preservation"] =
        LandmarkAnalyzer::analyze_landmark_involvement(fts, ts1_id, ts2_id);

    comprehensive["transition_explosion"] =
        TransitionExplosionPredictor::predict_transition_explosion(fts, ts1_id, ts2_id);

    // Compute composite "merge quality" score
    double score = 0.0;
    score += 0.25 * comprehensive["operator_projection"]["opp_score"].get<double>();
    score += 0.20 * comprehensive["label_combinability"]["combinability_score"].get<double>();
    score += 0.20 * (1.0 - comprehensive["greedy_bisimulation"]["gb_error"].get<double>());
    score += 0.15 * comprehensive["label_support"]["support_overlap"].get<double>();
    score += 0.10 * comprehensive["landmark_preservation"]["landmark_preservation"].get<double>();
    score += 0.10 * (1.0 - comprehensive["transition_explosion"]["density_ratio"].get<double>());

    comprehensive["composite_merge_quality"] = std::max(0.0, std::min(1.0, score));

    return comprehensive;
}

}  // namespace merge_and_shrink


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_signals_enhanced.h code is in the following block:
#ifndef MERGE_AND_SHRINK_SIGNALS_ENHANCED_H
#define MERGE_AND_SHRINK_SIGNALS_ENHANCED_H

#include "merge_and_shrink_signals.h"
#include "factored_transition_system.h"
#include "transition_system.h"
#include "distances.h"
#include <nlohmann/json.hpp>
#include <vector>
#include <set>
#include <map>

namespace merge_and_shrink {

using json = nlohmann::json;

// ============================================================================
// OPERATOR PROJECTION POTENTIAL (OPP)
// ============================================================================
/**
 * Computes how many operators will have ZERO effect on remaining variables.
 * These operators can be projected away, leading to massive compression.
 *
 * From Nissim et al. (2011):
 * "Label Projection is the Maximal Conservative Label Reduction"
 * Merging variables allows projection of operators affecting only those vars.
 */
class OperatorProjectionAnalyzer {
public:
    /**
     * Estimate the "Operator Projection Potential" score [0, 1]
     *
     * High value (>0.7): Many operators can be projected → merge is GOOD
     * Low value (<0.2): Few operators can be projected → merge is RISKY
     */
    static double compute_opp_score(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Get detailed breakdown of projectable operators
     */
    static json analyze_operator_projection(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );
};

// ============================================================================
// LABEL COMBINABILITY ANALYSIS
// ============================================================================
/**
 * From Helmert et al. (2014):
 * "Labels are combinable if they are locally equivalent in all other factors"
 *
 * High label combinability → post-merge label reduction potential
 * This directly impacts compressed size of merged system.
 */
class LabelCombinaibilityAnalyzer {
public:
    /**
     * Compute label combinability score [0, 1]
     *
     * Score = (% of labels that become equivalent) after merge
     *
     * High: Merging allows aggressive label reduction
     * Low: Labels remain distinct, system stays large
     */
    static double compute_label_combinability_score(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Count how many label groups will collapse
     */
    static int count_collapsible_labels(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Detailed breakdown of label equivalence
     */
    static json analyze_label_equivalence(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );
};

// ============================================================================
// GREEDY BISIMULATION ERROR
// ============================================================================
/**
 * From Nissim et al. (2011):
 * "Greedy Bisimulation only requires bisimulation on transitions where
 *  h*(s) = h*(s') + c(l)"
 *
 * Measures h-value incompatibility between merge candidates.
 * High error → merging destroys heuristic quality.
 */
class GreedyBisimulationAnalyzer {
public:
    /**
     * Compute greedy bisimulation error [0, 1]
     *
     * 0 = Perfect: All optimal-path transitions respect h-values
     * 1 = Severe: Many h-value conflicts on optimal paths
     *
     * This is a DENSE REWARD SIGNAL for h* preservation.
     */
    static double compute_greedy_bisimulation_error(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Check if merge candidates have conflicting h-values on optimal paths
     */
    static bool violates_greedy_bisimulation(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id,
        double error_threshold = 0.2
    );

    /**
     * Detailed h-value conflict analysis
     */
    static json analyze_h_value_compatibility(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );
};

// ============================================================================
// OPERATOR COST VARIANCE SIGNAL
// ============================================================================
/**
 * From Helmert et al. (2014):
 * "Cost distribution of transitions matters for abstraction quality"
 *
 * High-cost operators need special handling:
 * - If separated: each variable sees only its subset of costs
 * - If merged: full cost information is preserved
 */
class OperatorCostAnalyzer {
public:
    /**
     * Analyze cost distribution for operators affecting merge candidates
     */
    static json analyze_operator_costs(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Score [0, 1]: How well is cost distribution captured?
     * High: Operators affecting both candidates have wide cost range
     * Low: Operators are independent in cost
     */
    static double compute_cost_preservation_score(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );
};

// ============================================================================
// CAUSAL GRAPH DISTANCE
// ============================================================================
/**
 * From Nissim et al. (2011) - M&S-gop strategy:
 * "Variables closer in causal graph should merge earlier"
 *
 * Reason: Operator projection works better when dependent variables merge
 */
class CausalGraphAnalyzer {
public:
    /**
     * Compute causal graph distance [0, 1]
     *
     * 0 = Independent (far in graph)
     * 1 = Directly dependent (adjacent in graph)
     *
     * Merging closer variables → better operator projection
     */
    static double compute_causal_proximity_score(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Check if variables are adjacent in causal graph
     */
    static bool are_causally_adjacent(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Detailed causal relationship analysis
     */
    static json analyze_causal_relationships(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );
};

// ============================================================================
// LABEL SUPPORT CORRELATION
// ============================================================================
/**
 * From papers:
 * "Operator support overlap indicates variable entanglement"
 *
 * High support overlap = variables are heavily entangled
 * Merging such variables captures global constraints better
 */
class LabelSupportAnalyzer {
public:
    /**
     * Jaccard similarity of operators affecting each variable [0, 1]
     *
     * High (>0.6): Variables share many operators → highly entangled
     * Low (<0.2): Variables mostly independent
     */
    static double compute_operator_support_overlap(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Detailed operator support analysis
     */
    static json analyze_operator_support(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );
};

// ============================================================================
// LANDMARK ACHIEVER PRESERVATION
// ============================================================================
/**
 * From papers: Merging landmark-critical variables can destroy heuristic
 *
 * Landmark achievers are transitions on optimal plans
 * We want to preserve these to maintain heuristic quality
 */
class LandmarkAnalyzer {
public:
    /**
     * Check if merge candidates achieve important landmarks
     * Returns ratio of preserved landmark achievers [0, 1]
     *
     * High: Merge preserves landmark achievement capability
     * Low: Merge might destroy important pathways
     */
    static double compute_landmark_preservation_score(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Identify critical landmark achiever transitions
     */
    static json analyze_landmark_involvement(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );
};

// ============================================================================
// TRANSITION DENSITY & EXPLOSION PREDICTION
// ============================================================================
/**
 * From papers: "Number of transitions is the real killer"
 *
 * Predicts post-merge transition explosion more accurately
 */
class TransitionExplosionPredictor {
public:
    /**
     * Estimate actual transition count in merged product
     * (not just |S1| × |S2|)
     *
     * Returns ratio of expected_transitions / (|S1| × |S2|)
     * <0.1 = good (many states unreachable)
     * >0.9 = bad (dense product)
     */
    static double predict_transition_density_ratio(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );

    /**
     * Detailed transition explosion analysis
     */
    static json predict_transition_explosion(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id
    );
};

// ============================================================================
// COMPREHENSIVE MERGE QUALITY ANALYZER
// ============================================================================
/**
 * Integration point: Computes all signals together
 */
class MergeQualityAnalyzer {
public:
    /**
     * Compute comprehensive merge quality signals
     * Returns JSON with all analyzers' results
     */
    static json compute_comprehensive_merge_signals(
        const FactoredTransitionSystem& fts,
        int ts1_id,
        int ts2_id,
        int iteration
    );
};

}  // namespace merge_and_shrink

#endif


--------------------------------------------------------------------------------

The file communication_protocol.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COMMUNICATION PROTOCOL - Thin Client Architecture
==================================================
Simplified protocol for Thin Client / Fat Server communication.

Protocol:
1. C++ exports observation_{N}.json with pre-computed features + reward signals
2. Python reads observation, makes decision
3. Python writes merge_{N}.json with chosen merge pair indices
4. C++ reads decision, executes merge, exports next observation

This module handles all I/O between Python and C++.
"""

import os
import json
import time
import tempfile
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict

from common_utils import FD_OUTPUT_DIR, GNN_OUTPUT_DIR, ThinClientConfig

logger = logging.getLogger(__name__)


# ============================================================================
# ATOMIC FILE I/O
# ============================================================================

def write_json_atomic(data: Dict[str, Any], filepath: str) -> None:
    """
    Write JSON atomically using temp file + rename.

    Guarantees:
    - Complete write before visibility
    - No partial files visible to readers
    """
    filepath = Path(filepath)
    filepath.parent.mkdir(parents=True, exist_ok=True)

    # Write to temp file first
    fd, temp_path = tempfile.mkstemp(
        dir=str(filepath.parent),
        suffix='.tmp',
        prefix=filepath.stem + '_'
    )

    try:
        with os.fdopen(fd, 'w') as f:
            json.dump(data, f, indent=2)
            f.flush()
            os.fsync(f.fileno())

        # Atomic rename
        os.replace(temp_path, filepath)

    except Exception:
        # Clean up temp file on error
        try:
            os.remove(temp_path)
        except:
            pass
        raise


def read_json_robust(
        filepath: str,
        timeout: float = 30.0,
        poll_interval: float = 0.05
) -> Optional[Dict[str, Any]]:
    """
    Read JSON with retries until file is complete and valid.

    Args:
        filepath: Path to JSON file
        timeout: Maximum wait time in seconds
        poll_interval: Time between retries in seconds

    Returns:
        Parsed JSON dict, or None if timeout
    """
    filepath = Path(filepath)
    start = time.time()

    while time.time() - start < timeout:
        if not filepath.exists():
            time.sleep(poll_interval)
            continue

        try:
            with open(filepath, 'r') as f:
                content = f.read()

            if not content.strip():
                time.sleep(poll_interval)
                continue

            return json.loads(content)

        except (json.JSONDecodeError, IOError):
            time.sleep(poll_interval)
            continue

    return None


# ============================================================================
# MESSAGE DATA STRUCTURES
# ============================================================================

@dataclass
class Observation:
    """
    C++ → Python: Pre-computed observation with features.

    Contains:
    - x: Node feature matrix [N, 7]
    - edge_index: Adjacency in COO format [[sources], [targets]]
    - num_active_systems: Number of remaining transition systems
    - reward_signals: Metrics for reward computation
    - iteration: Current iteration number
    - is_terminal: Whether episode should end
    """
    iteration: int
    x: list  # [[f1, f2, ..., f7], ...]
    edge_index: list  # [[src1, src2, ...], [tgt1, tgt2, ...]]
    num_active_systems: int
    reward_signals: Dict[str, float]
    is_terminal: bool = False
    timestamp: float = 0.0

    @staticmethod
    def from_json(data: Dict[str, Any]) -> 'Observation':
        """Parse observation from JSON dict."""
        return Observation(
            iteration=data.get('iteration', -1),
            x=data.get('x', []),
            edge_index=data.get('edge_index', [[], []]),
            num_active_systems=data.get('num_active_systems', 0),
            reward_signals=data.get('reward_signals', {}),
            is_terminal=data.get('is_terminal', False),
            timestamp=data.get('timestamp', time.time()),
        )


@dataclass
class MergeDecision:
    """
    Python → C++: Chosen merge pair.

    Contains:
    - iteration: Which iteration this decision is for
    - merge_pair: [node_idx_1, node_idx_2] - indices into current node list
    """
    iteration: int
    merge_pair: Tuple[int, int]
    timestamp: float = 0.0

    def to_json(self) -> Dict[str, Any]:
        """Convert to JSON-serializable dict."""
        return {
            'iteration': self.iteration,
            'merge_pair': list(self.merge_pair),
            'timestamp': self.timestamp or time.time(),
        }


# ============================================================================
# CORE COMMUNICATION FUNCTIONS
# ============================================================================

def wait_for_observation(
        iteration: int,
        timeout: float = None
) -> Optional[Observation]:
    """
    Wait for C++ to export observation_{iteration}.json

    Args:
        iteration: Expected iteration number
        timeout: Maximum wait time (uses config default if None)

    Returns:
        Observation object, or None if timeout/error
    """
    if timeout is None:
        timeout = ThinClientConfig.OBSERVATION_TIMEOUT

    filepath = FD_OUTPUT_DIR / f"observation_{iteration}.json"

    logger.debug(f"[COMM] Waiting for: {filepath.name}")

    data = read_json_robust(str(filepath), timeout=timeout)

    if data is None:
        logger.error(f"[COMM] Timeout waiting for observation_{iteration}")
        return None

    # Validate iteration
    if data.get('iteration') != iteration:
        logger.error(
            f"[COMM] Iteration mismatch: expected {iteration}, "
            f"got {data.get('iteration')}"
        )
        return None

    return Observation.from_json(data)


def send_merge_decision(
        iteration: int,
        merge_pair: Tuple[int, int]
) -> bool:
    """
    Send merge decision to C++.

    Args:
        iteration: Current iteration
        merge_pair: (node_idx_1, node_idx_2)

    Returns:
        True if successful
    """
    decision = MergeDecision(
        iteration=iteration,
        merge_pair=merge_pair,
        timestamp=time.time()
    )

    filepath = GNN_OUTPUT_DIR / f"merge_{iteration}.json"

    try:
        write_json_atomic(decision.to_json(), str(filepath))
        logger.debug(f"[COMM] Sent merge decision: {merge_pair}")
        return True

    except Exception as e:
        logger.error(f"[COMM] Failed to send decision: {e}")
        return False


def cleanup_communication_files() -> int:
    """
    Remove all signal files from communication directories.

    Returns:
        Number of files deleted
    """
    deleted = 0

    for directory in [FD_OUTPUT_DIR, GNN_OUTPUT_DIR]:
        if not directory.exists():
            continue

        import glob
        for pattern in ["*.json", "*.tmp"]:
            for filepath in glob.glob(str(directory / pattern)):
                try:
                    os.remove(filepath)
                    deleted += 1
                except:
                    pass

    logger.debug(f"[COMM] Cleaned up {deleted} files")
    return deleted


def ensure_communication_directories() -> None:
    """Ensure communication directories exist."""
    FD_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    GNN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

--------------------------------------------------------------------------------

The file common_utils.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COMMON UTILITIES - Updated for Enhanced Features
=================================================
"""

import os
import logging
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# PATHS
# ============================================================================

PROJECT_ROOT = Path(__file__).parent.absolute()
DOWNWARD_DIR = PROJECT_ROOT / "downward"
FD_OUTPUT_DIR = DOWNWARD_DIR / "fd_output"
GNN_OUTPUT_DIR = DOWNWARD_DIR / "gnn_output"

FD_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
GNN_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


# ============================================================================
# THIN CLIENT CONFIG - UPDATED DIMENSIONS
# ============================================================================

class ThinClientConfig:
    """Configuration for thin client architecture."""

    # Node features (15 features from C++ - expanded!)
    NODE_FEATURE_DIM = 15  # Was 7, now 15

    FEATURE_NAMES = [
        # Original 7 features
        "normalized_size",
        "is_atomic",
        "mean_h_value",
        "solvability",
        "diameter",
        "transition_density",
        "label_count",
        # New 8 features
        "init_h_value",  # Feature 7: h* for init state
        "dead_end_ratio",  # Feature 8
        "goal_state_ratio",  # Feature 9
        "h_value_variance",  # Feature 10
        "avg_trans_per_label",  # Feature 11
        "self_loop_ratio",  # Feature 12
        "num_variables",  # Feature 13
        "determinism_indicator",  # Feature 14
    ]

    # Edge features (10 features from C++ - NEW!)
    EDGE_FEATURE_DIM = 10

    EDGE_FEATURE_NAMES = [
        "label_jaccard",
        "shared_label_ratio",
        "sync_factor",
        "product_size_log",
        "shares_variables",
        "combined_h_star",
        "h_compatibility",
        "both_solvable",
        "size_balance",
        "density_ratio",
    ]

    # Observation limits
    MAX_NODES = 100
    MAX_EDGES = 1000

    # Timeouts
    OBSERVATION_TIMEOUT = 120.0
    ACK_TIMEOUT = 30.0
    POLL_INTERVAL = 0.05


def ensure_directories():
    """Ensure all required directories exist."""
    directories = [
        FD_OUTPUT_DIR,
        GNN_OUTPUT_DIR,
        PROJECT_ROOT / "logs",
        PROJECT_ROOT / "models",
        PROJECT_ROOT / "tb_logs",
    ]
    for dir_path in directories:
        dir_path.mkdir(parents=True, exist_ok=True)


def cleanup_signal_files():
    """Remove all signal files."""
    deleted = 0
    for directory in [FD_OUTPUT_DIR, GNN_OUTPUT_DIR]:
        if not directory.exists():
            continue
        import glob
        for pattern in ["*.json", "*.tmp"]:
            for filepath in glob.glob(str(directory / pattern)):
                try:
                    os.remove(filepath)
                    deleted += 1
                except:
                    pass
    return deleted


ensure_directories()

--------------------------------------------------------------------------------

The file gnn_model.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GNN MODEL - Updated for Enhanced Features
==========================================
Now uses 15-dim node features and 10-dim edge features from C++.
"""

import torch
from torch import Tensor, nn
from torch_geometric.nn import GCNConv, GATConv
from typing import Tuple, Optional
import numpy as np

import logging
logger = logging.getLogger(__name__)

# ============================================================================
# UPDATED CONSTANTS
# ============================================================================

NODE_FEATURE_DIM = 15  # Expanded from 7
EDGE_FEATURE_DIM = 10  # From C++


class GCNWithAttention(nn.Module):
    """GCN backbone with multi-head attention."""

    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int = 3, n_heads: int = 4):
        super().__init__()

        layers = []
        dims = [input_dim] + [hidden_dim] * (n_layers - 1) + [hidden_dim]
        for i in range(n_layers):
            layers.append(GCNConv(dims[i], dims[i + 1]))
        self.convs = nn.ModuleList(layers)

        self.attention = GATConv(
            in_channels=hidden_dim,
            out_channels=hidden_dim,
            heads=n_heads,
            concat=True,
            dropout=0.1,
            add_self_loops=False
        )

        self.attention_proj = nn.Linear(hidden_dim * n_heads, hidden_dim)
        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:
        device = x.device
        edge_index = edge_index.to(device, dtype=torch.long)

        for conv in self.convs:
            x = self.activation(conv(x, edge_index))
            x = self.dropout(x)

        if edge_index.numel() > 0:
            try:
                attn_out = self.attention(x, edge_index)
                attn_out = self.activation(self.attention_proj(attn_out))
                x = x + attn_out * 0.3
            except Exception as e:
                logger.warning(f"Attention layer failed: {e}")

        return x


class EdgeFeatureEncoder(nn.Module):
    """Encodes C++ edge features."""

    def __init__(self, num_edge_features: int = EDGE_FEATURE_DIM, output_dim: int = 32):
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Linear(num_edge_features, 64),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(64, output_dim),
            nn.ReLU()
        )
        self.output_dim = output_dim

    def forward(self, edge_features: Tensor) -> Tensor:
        if edge_features.numel() == 0:
            return torch.zeros(0, self.output_dim, device=edge_features.device)
        return self.encoder(edge_features)


class AttentionWeightedEdgeScorer(nn.Module):
    """Score edges using attention + C++ edge features + node embeddings."""

    def __init__(self, hidden_dim: int, edge_feature_dim: int = 32):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.edge_feature_dim = edge_feature_dim

        total_dim = 2 * hidden_dim + edge_feature_dim

        self.mlp = nn.Sequential(
            nn.Linear(total_dim, 2 * total_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2 * total_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 1),
        )

        self.mlp_no_edge = nn.Sequential(
            nn.Linear(2 * hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
        )

        self.edge_attention = nn.Sequential(
            nn.Linear(edge_feature_dim, 32),
            nn.Tanh(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(
            self,
            node_embs: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tensor:
        if edge_index.numel() == 0 or edge_index.shape[1] == 0:
            return torch.zeros(0, device=node_embs.device, dtype=torch.float32)

        src_idx, tgt_idx = edge_index
        num_nodes = node_embs.shape[0]

        if len(src_idx) > 0:
            max_idx = max(src_idx.max().item(), tgt_idx.max().item())
            if max_idx >= num_nodes:
                src_idx = torch.clamp(src_idx, 0, num_nodes - 1)
                tgt_idx = torch.clamp(tgt_idx, 0, num_nodes - 1)

        src_emb = node_embs[src_idx]
        tgt_emb = node_embs[tgt_idx]

        if edge_features is not None and edge_features.numel() > 0:
            edge_attn_weights = self.edge_attention(edge_features)
            edge_feats_weighted = edge_features * edge_attn_weights
            edge_feat = torch.cat([src_emb, tgt_emb, edge_feats_weighted], dim=1)
            score = self.mlp(edge_feat).squeeze(-1)
        else:
            edge_feat = torch.cat([src_emb, tgt_emb], dim=1)
            score = self.mlp_no_edge(edge_feat).squeeze(-1)

        score = torch.clamp(score, min=-1e6, max=1e6)
        score = torch.nan_to_num(score, nan=0.0, posinf=1e6, neginf=-1e6)

        return score


class GNNModel(nn.Module):
    """Full GNN with C++ edge features support."""

    def __init__(
            self,
            input_dim: int = NODE_FEATURE_DIM,  # Updated to 15
            hidden_dim: int = 64,
            n_layers: int = 3,
            n_heads: int = 4,
            edge_feature_dim: int = EDGE_FEATURE_DIM,  # Updated to 10
            use_cpp_edge_features: bool = True,  # Use C++ features
    ):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.edge_feature_dim = edge_feature_dim
        self.use_cpp_edge_features = use_cpp_edge_features

        self.backbone = GCNWithAttention(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            n_layers=n_layers,
            n_heads=n_heads
        )

        # Edge encoder for C++ features
        encoded_edge_dim = 32
        self.edge_encoder = EdgeFeatureEncoder(
            num_edge_features=edge_feature_dim,
            output_dim=encoded_edge_dim
        )

        self.scorer = AttentionWeightedEdgeScorer(
            hidden_dim=hidden_dim,
            edge_feature_dim=encoded_edge_dim
        )

    def forward(
            self,
            x: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        """
        Forward pass.

        Args:
            x: [N, 15] node features (from C++)
            edge_index: [2, E] edge indices
            edge_features: [E, 10] edge features (from C++)

        Returns:
            edge_logits: [E] scores
            node_embs: [N, hidden_dim] embeddings
        """
        if x.dim() != 2:
            raise ValueError(f"Node features must be 2D, got {x.dim()}D")
        if edge_index.dim() != 2 or edge_index.shape[0] != 2:
            raise ValueError(f"Edge index must be [2, E], got {edge_index.shape}")

        device = x.device
        edge_index = edge_index.to(device, dtype=torch.long)

        num_nodes = x.shape[0]
        if edge_index.numel() > 0:
            max_idx = edge_index.max().item()
            if max_idx >= num_nodes:
                edge_index = torch.clamp(edge_index, 0, num_nodes - 1)

        # Get node embeddings
        node_embs = self.backbone(x, edge_index)

        if torch.isnan(node_embs).any():
            node_embs = torch.nan_to_num(node_embs, nan=0.0)

        # Encode C++ edge features
        encoded_edge_features = None
        if edge_features is not None and edge_features.numel() > 0 and self.use_cpp_edge_features:
            try:
                encoded_edge_features = self.edge_encoder(edge_features.float())
            except Exception as e:
                logger.warning(f"Could not encode edge features: {e}")

        # Score edges
        edge_logits = self.scorer(node_embs, edge_index, encoded_edge_features)

        return edge_logits, node_embs


if __name__ == "__main__":
    print("Testing GNNModel with C++ features...")
    print("=" * 60)

    model = GNNModel(input_dim=15, hidden_dim=64, edge_feature_dim=10)

    x = torch.randn(10, 15)  # 10 nodes, 15 features
    edge_index = torch.tensor([[0, 1, 2], [1, 2, 3]], dtype=torch.long)
    edge_features = torch.randn(3, 10)  # 3 edges, 10 features from C++

    edge_logits, node_embs = model(x, edge_index, edge_features)
    print(f"Node embeddings: {node_embs.shape}")
    print(f"Edge logits: {edge_logits.shape}")
    print("✓ All tests passed!")

--------------------------------------------------------------------------------

The file gnn_policy.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GNN POLICY - Updated for Enhanced Features
===========================================
Now handles 15-dim node features and 10-dim edge features from C++.
"""

import numpy as np
import torch
from torch import nn, Tensor
from torch.distributions import Categorical
from stable_baselines3.common.policies import ActorCriticPolicy
from typing import Tuple, Dict, Any, Optional

from gnn_model import GNNModel, NODE_FEATURE_DIM, EDGE_FEATURE_DIM

import logging

logger = logging.getLogger(__name__)


class GNNExtractor(nn.Module):
    """Feature extractor using enhanced GNN."""

    def __init__(
            self,
            input_dim: int = NODE_FEATURE_DIM,  # 15
            hidden_dim: int = 64,
            n_layers: int = 3,
            n_heads: int = 4,
            edge_feature_dim: int = EDGE_FEATURE_DIM,  # 10
    ):
        super().__init__()

        self.gnn = GNNModel(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            n_layers=n_layers,
            n_heads=n_heads,
            edge_feature_dim=edge_feature_dim,
            use_cpp_edge_features=True,
        )
        self.hidden_dim = hidden_dim

    def forward(
            self,
            x: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        return self.gnn(x, edge_index, edge_features)


class GNNPolicy(ActorCriticPolicy):
    """Actor-Critic policy using enhanced GNN."""

    def __init__(
            self,
            observation_space,
            action_space,
            lr_schedule,
            net_arch=None,
            activation_fn=nn.ReLU,
            hidden_dim: int = 64,
            n_layers: int = 3,
            n_heads: int = 4,
            **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            net_arch=[],
            activation_fn=activation_fn,
            **kwargs
        )

        self.node_feat_dim = NODE_FEATURE_DIM  # 15
        self.edge_feat_dim = EDGE_FEATURE_DIM  # 10
        self.hidden_dim = hidden_dim

        self.extractor = GNNExtractor(
            input_dim=self.node_feat_dim,
            hidden_dim=self.hidden_dim,
            n_layers=n_layers,
            n_heads=n_heads,
            edge_feature_dim=self.edge_feat_dim,
        )

        self.value_net = nn.Linear(self.hidden_dim, 1)
        self.action_net = nn.Identity()

        self.optimizer = self.optimizer_class(
            self.parameters(),
            lr=lr_schedule(1)
        )

    def _get_data_from_obs(self, obs: Dict[str, Any]) -> Tuple[Tensor, Tensor, Optional[Tensor], int, int]:
        """Extract data including edge features."""
        device = self.device

        x = torch.as_tensor(obs["x"], dtype=torch.float32, device=device)
        edge_index = torch.as_tensor(obs["edge_index"], dtype=torch.long, device=device)

        # Get edge features from C++
        edge_features = None
        if "edge_features" in obs:
            edge_features = torch.as_tensor(obs["edge_features"], dtype=torch.float32, device=device)

        def to_python_int(val):
            if isinstance(val, torch.Tensor):
                return int(val.cpu().item())
            elif isinstance(val, np.ndarray):
                return int(val.flat[0])
            elif hasattr(val, 'item'):
                return int(val.item())
            return int(val)

        num_nodes = to_python_int(obs.get("num_nodes", x.shape[-2]))
        num_edges = to_python_int(obs.get("num_edges", edge_index.shape[-1]))

        if x.dim() == 3:
            x = x[0]
            edge_index = edge_index[0] if edge_index.dim() == 3 else edge_index
            if edge_features is not None and edge_features.dim() == 3:
                edge_features = edge_features[0]

        num_nodes = max(1, min(num_nodes, x.shape[0]))
        num_edges = max(0, min(num_edges, edge_index.shape[1] if edge_index.dim() == 2 else 0))

        x = x[:num_nodes]
        edge_index = edge_index[:, :num_edges] if num_edges > 0 else edge_index[:, :0]

        if edge_features is not None:
            edge_features = edge_features[:num_edges]

        if num_edges > 0 and num_nodes > 0:
            max_valid_idx = num_nodes - 1
            edge_index = torch.clamp(edge_index, min=0, max=max_valid_idx)

        return x, edge_index, edge_features, num_nodes, num_edges

    def _mask_invalid_edges(self, logits: Tensor, num_edges: int) -> Tensor:
        E = logits.size(0)
        num_edges = max(1, min(num_edges, E))
        mask = torch.arange(E, device=logits.device) < num_edges
        if not mask.any():
            mask[0] = True
        masked = logits.clone()
        masked[~mask] = float('-inf')
        return masked

    def _sample_action(self, logits: Tensor, deterministic: bool) -> Tuple[Tensor, Tensor]:
        logits = torch.clamp(logits, min=-100, max=100)
        if torch.isinf(logits).all():
            logits = torch.zeros_like(logits)
        probs = torch.softmax(logits, dim=0)
        if torch.isnan(probs).any():
            probs = torch.ones_like(logits) / len(logits)
        dist = Categorical(probs=probs)
        if deterministic:
            action = probs.argmax()
        else:
            action = dist.sample()
        log_prob = dist.log_prob(action)
        return action, log_prob

    @torch.no_grad()
    def predict(self, observation: Dict[str, Any], state=None, episode_start=None, deterministic: bool = False):
        self.eval()
        try:
            x, edge_index, edge_features, num_nodes, num_edges = self._get_data_from_obs(observation)
            if num_edges == 0:
                return np.array([0]), None
            edge_logits, _ = self.extractor(x, edge_index, edge_features)
            masked_logits = self._mask_invalid_edges(edge_logits, num_edges)
            action, _ = self._sample_action(masked_logits, deterministic)
            return np.array([action.cpu().item()]), None
        except Exception as e:
            logger.error(f"Predict failed: {e}")
            return np.array([0]), None

    def forward(self, obs: Dict[str, Any], deterministic: bool = False) -> Tuple[Tensor, Tensor, Tensor]:
        device = self.device
        x, edge_index, edge_features, num_nodes, num_edges = self._get_data_from_obs(obs)

        edge_logits, node_embs = self.extractor(x, edge_index, edge_features)

        if node_embs.shape[0] > 0:
            value = self.value_net(node_embs.mean(dim=0, keepdim=True))
        else:
            value = torch.zeros(1, 1, device=device)

        if num_edges == 0:
            action = torch.zeros(1, dtype=torch.long, device=device)
            log_prob = torch.zeros(1, device=device)
        else:
            masked_logits = self._mask_invalid_edges(edge_logits, num_edges)
            action, log_prob = self._sample_action(masked_logits, deterministic)
            action = action.unsqueeze(0)
            log_prob = log_prob.unsqueeze(0)

        return action, value, log_prob

    def evaluate_actions(self, obs: Dict[str, Tensor], actions: Tensor) -> Tuple[Tensor, Tensor, Optional[Tensor]]:
        device = self.device

        def to_python_int(val):
            if isinstance(val, torch.Tensor):
                return int(val.cpu().item())
            elif isinstance(val, np.ndarray):
                return int(val.flat[0])
            elif hasattr(val, 'item'):
                return int(val.item())
            return int(val)

        x = obs["x"].to(device, dtype=torch.float32)
        edge_index = obs["edge_index"].to(device, dtype=torch.long)

        edge_features = None
        if "edge_features" in obs:
            edge_features = obs["edge_features"].to(device, dtype=torch.float32)

        if x.dim() == 3:
            batch_size = x.shape[0]
        else:
            batch_size = 1
            x = x.unsqueeze(0)
            edge_index = edge_index.unsqueeze(0)
            if edge_features is not None:
                edge_features = edge_features.unsqueeze(0)

        num_nodes_raw = obs.get("num_nodes", None)
        if num_nodes_raw is None:
            num_nodes_arr = [x.shape[1]] * batch_size
        elif isinstance(num_nodes_raw, torch.Tensor):
            num_nodes_arr = num_nodes_raw.cpu().numpy().flatten().tolist()
        elif isinstance(num_nodes_raw, np.ndarray):
            num_nodes_arr = num_nodes_raw.flatten().tolist()
        else:
            num_nodes_arr = [int(num_nodes_raw)] * batch_size

        num_edges_raw = obs.get("num_edges", None)
        if num_edges_raw is None:
            num_edges_arr = [edge_index.shape[-1]] * batch_size
        elif isinstance(num_edges_raw, torch.Tensor):
            num_edges_arr = num_edges_raw.cpu().numpy().flatten().tolist()
        elif isinstance(num_edges_raw, np.ndarray):
            num_edges_arr = num_edges_raw.flatten().tolist()
        else:
            num_edges_arr = [int(num_edges_raw)] * batch_size

        values, log_probs, entropies = [], [], []

        for i in range(batch_size):
            x_i = x[i] if x.dim() == 3 else x
            ei_i = edge_index[i] if edge_index.dim() == 3 else edge_index
            ef_i = edge_features[i] if edge_features is not None and edge_features.dim() == 3 else edge_features
            action_i = actions[i] if actions.dim() > 0 else actions

            num_nodes_i = int(num_nodes_arr[i]) if i < len(num_nodes_arr) else x_i.shape[0]
            num_edges_i = int(num_edges_arr[i]) if i < len(num_edges_arr) else ei_i.shape[-1]

            num_nodes_i = max(1, min(num_nodes_i, x_i.shape[0]))
            num_edges_i = max(0, min(num_edges_i, ei_i.shape[-1]))

            x_i = x_i[:num_nodes_i]
            ei_i = ei_i[:, :num_edges_i]
            if ef_i is not None:
                ef_i = ef_i[:num_edges_i]

            if num_edges_i > 0 and num_nodes_i > 0:
                max_idx = num_nodes_i - 1
                ei_i = torch.clamp(ei_i, min=0, max=max_idx)

            edge_logits, node_embs = self.extractor(x_i, ei_i, ef_i)

            if node_embs.shape[0] > 0:
                val_i = self.value_net(node_embs.mean(dim=0, keepdim=True))
            else:
                val_i = torch.zeros(1, device=device)

            if num_edges_i == 0 or edge_logits.shape[0] == 0:
                log_prob_i = torch.zeros(1, device=device, requires_grad=True)
                ent_i = torch.zeros(1, device=device, requires_grad=True)
            else:
                masked_logits = self._mask_invalid_edges(edge_logits, num_edges_i)
                if torch.isinf(masked_logits).all():
                    masked_logits = torch.zeros_like(edge_logits)
                dist = Categorical(logits=masked_logits)
                action_clamped = torch.clamp(action_i, 0, edge_logits.shape[0] - 1)
                log_prob_i = dist.log_prob(action_clamped)
                ent_i = dist.entropy()

            values.append(val_i.squeeze())
            log_probs.append(log_prob_i)
            entropies.append(ent_i)

        return torch.stack(values), torch.stack(log_probs), torch.stack(entropies)

    def predict_values(self, obs: Dict[str, Tensor]) -> Tensor:
        device = self.device
        x = obs["x"].to(device, dtype=torch.float32)
        edge_index = obs["edge_index"].to(device, dtype=torch.long)
        edge_features = obs.get("edge_features")
        if edge_features is not None:
            edge_features = edge_features.to(device, dtype=torch.float32)

        if x.dim() == 2:
            x = x.unsqueeze(0)
            edge_index = edge_index.unsqueeze(0)
            if edge_features is not None:
                edge_features = edge_features.unsqueeze(0)

        batch_size = x.shape[0]
        values = []

        for i in range(batch_size):
            x_i = x[i]
            ei_i = edge_index[i]
            ef_i = edge_features[i] if edge_features is not None else None

            _, node_embs = self.extractor(x_i, ei_i, ef_i)

            if node_embs.shape[0] > 0:
                val = self.value_net(node_embs.mean(dim=0, keepdim=True))
            else:
                val = torch.zeros(1, 1, device=device)

            values.append(val)

        return torch.cat(values, dim=0)

--------------------------------------------------------------------------------

The file reward_function_enhanced.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ENHANCED REWARD FUNCTION - Theory-Informed Design
==================================================

Based on:
1. Helmert et al. (2014) - Merge-and-Shrink Abstraction
2. Nissim et al. (2011) - Computing Perfect Heuristics
3. Katz & Hoffmann (2013) - Merge-and-Shrink Implementation

Core Principles:
- Bisimulation (especially Greedy Bisimulation) is the gold standard
- H* preservation is PRIMARY (w=0.50)
- Transition count matters more than state count
- Label reduction potential is critical
- Operator projection enables compression
"""

import numpy as np
from typing import Dict, Any, Tuple
import logging

logger = logging.getLogger(__name__)


class EnhancedRewardFunction:
    """
    Theory-informed reward function balancing Small & Accurate abstractions.

    Reward Components (in order of importance from literature):
    1. H* Preservation (50%)         - Greedy bisimulation
    2. Transition Control (20%)      - Avoid explosion
    3. Operator Projection (15%)     - Post-merge compression potential
    4. Label Combinability (10%)     - Label reduction potential
    5. Bonus Signals (5%)            - Architecture-specific insights

    Range: [-2.0, +2.0]
    """

    def __init__(self, debug: bool = False):
        self.debug = debug

        # Thresholds from literature
        self.H_STAR_CRITICAL_LOSS = 0.15  # 15% h* loss = severe penalty
        self.TRANSITION_EXPLOSION_THRESHOLD = 5.0  # 5x growth = bad
        self.REACHABILITY_MINIMUM = 0.3  # <30% reachable = very bad
        self.DEAD_END_DANGER_ZONE = 0.4  # >40% dead-ends = risky

    def compute_reward(self, raw_obs: Dict[str, Any]) -> float:
        """
        Compute scalar reward based on merge quality signals.

        Args:
            raw_obs: Observation from C++ including reward_signals

        Returns:
            reward: Scalar in [-2.0, +2.0]
        """
        signals = raw_obs.get('reward_signals', {})
        edge_features = raw_obs.get('edge_features', None)

        # ====================================================================
        # COMPONENT 1: H* PRESERVATION (50% weight) - PRIMARY SIGNAL
        # ====================================================================
        # From Nissim et al.: Bisimulation is gold standard
        # From papers: Greedy bisimulation only requires optimal-path preservation

        h_reward, h_details = self._compute_h_preservation_reward(signals)

        # ====================================================================
        # COMPONENT 2: TRANSITION EXPLOSION CONTROL (20% weight)
        # ====================================================================
        # From papers: "Transitions > states in complexity"
        # Penalize merges causing uncontrolled transition growth

        trans_reward, trans_details = self._compute_transition_control_reward(signals)

        # ====================================================================
        # COMPONENT 3: OPERATOR PROJECTION POTENTIAL (15% weight)
        # ====================================================================
        # From Nissim et al.: Label projection is maximal conservative reduction
        # High OPP score = merge enables label reduction

        opp_reward, opp_details = self._compute_operator_projection_reward(signals)

        # ====================================================================
        # COMPONENT 4: LABEL COMBINABILITY (10% weight)
        # ====================================================================
        # From papers: High label combinability → post-merge compression

        label_reward, label_details = self._compute_label_combinability_reward(signals)

        # ====================================================================
        # COMPONENT 5: BONUS SIGNALS (5% weight)
        # ====================================================================
        # Architecture-specific insights

        bonus_reward, bonus_details = self._compute_bonus_signals(signals, edge_features)

        # ====================================================================
        # WEIGHTED COMBINATION
        # ====================================================================

        final_reward = (
                0.50 * h_reward +  # PRIMARY: H* preservation
                0.20 * trans_reward +  # HIGH: Transition control
                0.15 * opp_reward +  # MEDIUM: Operator projection
                0.10 * label_reward +  # MEDIUM: Label combinability
                0.05 * bonus_reward  # LOW: Bonuses
        )

        # ====================================================================
        # CATASTROPHIC FAILURE PENALTIES
        # ====================================================================

        # Lost solvability = -1.0 (severe)
        if not signals.get('is_solvable', True):
            final_reward -= 1.0
            if self.debug:
                logger.debug("[REWARD] CATASTROPHIC: Lost solvability")

        # Extreme dead-end creation = -0.5
        if signals.get('dead_end_ratio', 0.0) > 0.7:
            final_reward -= 0.5
            if self.debug:
                logger.debug(f"[REWARD] SEVERE: Dead-end ratio {signals['dead_end_ratio']:.1%}")

        # ====================================================================
        # SCALE & CLAMP
        # ====================================================================

        final_reward = np.clip(final_reward, -2.0, 2.0)

        if self.debug:
            logger.debug(f"[REWARD] Components: h={h_reward:.3f}, trans={trans_reward:.3f}, "
                         f"opp={opp_reward:.3f}, label={label_reward:.3f}, bonus={bonus_reward:.3f}")
            logger.debug(f"[REWARD] Final: {final_reward:.4f}")

        return float(final_reward)

    def _compute_h_preservation_reward(self, signals: Dict) -> Tuple[float, Dict]:
        """
        H* Preservation Reward Component (50% weight)

        From Nissim et al. (2011):
        - Gold standard: h* = h^* (bisimulation)
        - Practical: Greedy bisimulation (h* on optimal paths)

        This is the PRIMARY signal for heuristic quality.
        """
        details = {}

        h_star_before = signals.get('h_star_before', 0)
        h_star_after = signals.get('h_star_after', 0)
        h_star_preservation = signals.get('h_star_preservation', 1.0)

        # h_star_preservation = after / before (ratio)
        # >1.0 = improved, =1.0 = preserved, <1.0 = degraded

        if h_star_preservation >= 1.0:
            # GOOD: H* preserved or improved
            # Reward = improvement bonus
            improvement = min(1.0, h_star_preservation - 1.0)
            reward = 0.4 + 0.3 * improvement  # Range: [0.4, 0.7]

            if self.debug:
                logger.debug(f"[REWARD-H*] Preserved/improved: {h_star_preservation:.3f} → +{reward:.3f}")

        elif h_star_preservation >= (1.0 - self.H_STAR_CRITICAL_LOSS):
            # MODERATE: Small h* degradation (<15%)
            # Tolerable if other signals are good
            degradation = 1.0 - h_star_preservation
            penalty = 0.3 * (degradation / self.H_STAR_CRITICAL_LOSS)
            reward = 0.2 - penalty  # Range: [0.05, 0.2]

            if self.debug:
                logger.debug(f"[REWARD-H*] Minor degradation {degradation:.1%}: {reward:.3f}")

        else:
            # BAD: Severe h* degradation (>15%)
            # Large penalty - heuristic quality compromised
            degradation = 1.0 - h_star_preservation
            penalty = 0.5 * min(1.0, degradation / 0.5)  # Scale by severity
            reward = -penalty  # Range: [-0.5, 0]

            if self.debug:
                logger.debug(f"[REWARD-H*] SEVERE degradation {degradation:.1%}: {reward:.3f}")

        details['h_star_preservation'] = h_star_preservation
        details['reward'] = reward

        return reward, details

    def _compute_transition_control_reward(self, signals: Dict) -> Tuple[float, Dict]:
        """
        Transition Explosion Control (20% weight)

        From papers: "Transitions are the real killer"
        - Penalize merges causing explosive transition growth
        - Reward merges that compress or stabilize transitions
        - Use prediction from C++ for explosion detection
        """
        details = {}

        states_before = signals.get('states_before', 1)
        states_after = signals.get('states_after', 1)

        # Transition density prediction
        density_ratio = signals.get('transition_density', 1.0)

        # Compute growth factor
        if states_before > 0:
            growth_ratio = states_after / max(1, states_before)
        else:
            growth_ratio = 1.0

        # Explosion severity
        if growth_ratio > 10.0 or density_ratio > 0.9:
            # SEVERE EXPLOSION
            reward = -0.8
            if self.debug:
                logger.debug(f"[REWARD-Trans] SEVERE explosion: growth={growth_ratio:.1f}x, "
                             f"density={density_ratio:.2f}")

        elif growth_ratio > 5.0 or density_ratio > 0.7:
            # MODERATE EXPLOSION
            reward = -0.3 - 0.2 * min(1.0, (growth_ratio - 5.0) / 5.0)
            if self.debug:
                logger.debug(f"[REWARD-Trans] Moderate explosion: growth={growth_ratio:.1f}x")

        elif growth_ratio > 2.0:
            # MILD GROWTH
            reward = -0.1 * (growth_ratio - 1.0)
            if self.debug:
                logger.debug(f"[REWARD-Trans] Mild growth: {growth_ratio:.1f}x")

        elif growth_ratio >= 1.0:
            # STABLE
            reward = 0.05

        else:
            # SHRINKING - bonus!
            shrink = 1.0 - growth_ratio
            reward = 0.15 * shrink  # Range: [0, 0.15]
            if self.debug:
                logger.debug(f"[REWARD-Trans] Good shrinking: {growth_ratio:.2f}x")

        details['growth_ratio'] = growth_ratio
        details['density_ratio'] = density_ratio
        details['reward'] = reward

        return reward, details

    def _compute_operator_projection_reward(self, signals: Dict) -> Tuple[float, Dict]:
        """
        Operator Projection Potential (15% weight)

        From Nissim et al. (2011) - Section on Label Projection:
        "Maximal conservative label reduction: project operators to merged vars"

        High OPP = many operators become internal-only = post-merge compression
        This is crucial for keeping merged systems "small"
        """
        details = {}

        # Extract OPP score from enhanced signals
        merge_quality = signals.get('merge_quality_score', 0.5)
        opp_score = signals.get('opp_score', 0.5)  # 0-1

        # Use OPP directly if available, otherwise estimate from quality
        if 'opp_score' in signals:
            opp_score = signals['opp_score']
        else:
            # Fallback: estimate from shrinkability
            opp_score = max(0.0, signals.get('shrinkability', 0.0) + 0.5)

        # Convert to reward: high OPP = high reward
        if opp_score > 0.7:
            # EXCELLENT: Many projectable operators
            reward = 0.2 + 0.1 * (opp_score - 0.7) / 0.3  # Range: [0.2, 0.3]
            if self.debug:
                logger.debug(f"[REWARD-OPP] Excellent projection potential: {opp_score:.2f}")

        elif opp_score > 0.4:
            # GOOD: Reasonable projection potential
            reward = 0.1 + 0.1 * (opp_score - 0.4) / 0.3
            if self.debug:
                logger.debug(f"[REWARD-OPP] Good projection potential: {opp_score:.2f}")

        elif opp_score > 0.2:
            # MODERATE: Some projection possible
            reward = 0.02 + 0.08 * (opp_score - 0.2) / 0.2

        else:
            # POOR: Few projectable operators
            # Merge might not benefit from compression
            reward = -0.1
            if self.debug:
                logger.debug(f"[REWARD-OPP] Poor projection potential: {opp_score:.2f}")

        details['opp_score'] = opp_score
        details['reward'] = reward

        return reward, details

    def _compute_label_combinability_reward(self, signals: Dict) -> Tuple[float, Dict]:
        """
        Label Combinability Reward (10% weight)

        From Helmert et al. (2014):
        "Labels that are locally equivalent in all other factors are combinable"

        High combinability = labels will collapse post-merge = compression
        """
        details = {}

        # Extract label combinability score
        label_comb = signals.get('label_combinability_score', 0.5)

        # Convert to reward
        if label_comb > 0.6:
            # EXCELLENT: Many labels will combine
            reward = 0.15 * label_comb  # Range: [0.09, 0.15]
            if self.debug:
                logger.debug(f"[REWARD-Label] Excellent combinability: {label_comb:.2f}")

        elif label_comb > 0.3:
            # GOOD: Reasonable combinability
            reward = 0.08 * label_comb

        elif label_comb > 0.0:
            # MODERATE
            reward = 0.03 * label_comb

        else:
            # POOR: No label combinability
            reward = -0.05
            if self.debug:
                logger.debug(f"[REWARD-Label] No combinability: merging independent systems")

        details['label_combinability'] = label_comb
        details['reward'] = reward

        return reward, details

    def _compute_bonus_signals(self, signals: Dict,
                               edge_features: Any) -> Tuple[float, Dict]:
        """
        Bonus Signals (5% weight)

        Architecture-specific insights that don't fit main categories.
        """
        details = {}
        reward = 0.0

        # Bonus 1: Causal graph proximity
        causal_proximity = signals.get('causal_proximity_score', 0.0)
        if causal_proximity > 0.8:
            reward += 0.02  # Small bonus for merging causally adjacent vars

        # Bonus 2: Greedy bisimulation compliance
        gb_error = signals.get('gb_error', 0.0)
        if gb_error < 0.1:
            reward += 0.02  # Good h-value compatibility

        # Bonus 3: Landmark preservation
        landmark_score = signals.get('landmark_preservation', 0.5)
        if landmark_score > 0.8:
            reward += 0.01

        # Bonus 4: Stability/F-value preservation
        f_stability = signals.get('f_value_stability', 0.5)
        if f_stability > 0.8:
            reward += 0.01

        # Penalty: Reachability collapse
        reachability = signals.get('reachability_ratio', 1.0)
        if reachability < 0.3:
            reward -= 0.05
            if self.debug:
                logger.debug(f"[REWARD-Bonus] Low reachability: {reachability:.1%}")

        details['causal_proximity'] = causal_proximity
        details['gb_error'] = gb_error
        details['landmark_score'] = landmark_score
        details['f_stability'] = f_stability
        details['reachability'] = reachability
        details['reward'] = reward

        return reward, details


# ============================================================================
# INTEGRATION: Replace in thin_merge_env.py
# ============================================================================

def create_enhanced_reward_function(debug: bool = False) -> EnhancedRewardFunction:
    """Factory function for reward function."""
    return EnhancedRewardFunction(debug=debug)


--------------------------------------------------------------------------------

The file shared_experiment_utils.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHARED EXPERIMENT UTILITIES - ENHANCED WITH THIN CLIENT INTEGRATION
====================================================================
Common code used across all experiments + evaluation framework integration.

Provides:
  - Checkpoint/resume functionality
  - Standardized logging
  - Common training/evaluation patterns
  - Result persistence
  - Experiment-to-evaluation conversion
  - Multi-experiment result aggregation
  - ThinMergeEnv integration with enhanced features
"""

import sys
import os
import json
import time
import traceback
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime
import numpy as np
from dataclasses import dataclass, asdict

import logging

logger = logging.getLogger(__name__)

# Ensure project root is in path
PROJECT_ROOT = Path(__file__).parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

# Ensure directories exist
os.makedirs("downward/gnn_output", exist_ok=True)
os.makedirs("downward/fd_output", exist_ok=True)
os.makedirs("logs", exist_ok=True)
os.makedirs("models", exist_ok=True)
os.makedirs("tb_logs", exist_ok=True)


# ============================================================================
# LOGGING & FORMATTING
# ============================================================================

def setup_logging(experiment_name: str, output_dir: str) -> logging.Logger:
    """Setup consistent logging for experiment."""
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    log_file = os.path.join(output_dir, f"{experiment_name}.log")

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)-8s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler(log_file, encoding='utf-8'),
        ],
        force=True
    )

    exp_logger = logging.getLogger(experiment_name)
    return exp_logger


def print_section(title: str, exp_logger: logging.Logger, width: int = 90):
    """Print formatted section header."""
    exp_logger.info("\n" + "=" * width)
    exp_logger.info(f"// {title.upper()}")
    exp_logger.info("=" * width + "\n")


def print_subsection(title: str, exp_logger: logging.Logger):
    """Print formatted subsection header."""
    exp_logger.info("\n" + "-" * 80)
    exp_logger.info(f">>> {title}")
    exp_logger.info("-" * 80 + "\n")


# ============================================================================
# CHECKPOINT & RESUME
# ============================================================================

class ExperimentCheckpoint:
    """Simple checkpoint manager for robust experiment recovery."""

    def __init__(self, checkpoint_dir: str):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.checkpoint_file = self.checkpoint_dir / "experiment_checkpoint.json"

    def save(self, state: Dict[str, Any]) -> None:
        """Save checkpoint state."""
        state['checkpoint_time'] = datetime.now().isoformat()

        with open(self.checkpoint_file, 'w') as f:
            json.dump(state, f, indent=2, default=str)

    def load(self) -> Optional[Dict[str, Any]]:
        """Load checkpoint if it exists."""
        if not self.checkpoint_file.exists():
            return None

        try:
            with open(self.checkpoint_file, 'r') as f:
                return json.load(f)
        except Exception:
            return None

    def clear(self) -> None:
        """Clear checkpoint (call after successful completion)."""
        if self.checkpoint_file.exists():
            self.checkpoint_file.unlink()


# ============================================================================
# THIN CLIENT REWARD WEIGHTS
# ============================================================================

DEFAULT_REWARD_WEIGHTS = {
    'w_h_preservation': 0.40,
    'w_shrinkability': 0.25,
    'w_state_control': 0.20,
    'w_solvability': 0.15,
}


# ============================================================================
# TRAINING HELPERS (UPDATED FOR THIN CLIENT)
# ============================================================================

def train_gnn_model(
        benchmarks: List[Tuple[str, str]],
        total_timesteps: int = 5000,
        timesteps_per_problem: int = 500,
        model_output_path: str = "models/gnn_model.zip",
        exp_logger: Optional[logging.Logger] = None,
        tb_log_name: str = "experiment_training",
        reward_weights: Optional[Dict[str, float]] = None,
        max_merges: int = 50,
        timeout_per_step: float = 120.0,
) -> Optional[str]:
    """
    Train GNN model on benchmarks using ThinMergeEnv.

    Returns:
        Path to trained model, or None if failed
    """
    if exp_logger is None:
        exp_logger = logging.getLogger("train_gnn")

    try:
        from stable_baselines3 import PPO
        from stable_baselines3.common.monitor import Monitor
        from gnn_policy import GNNPolicy
        from thin_merge_env import ThinMergeEnv

        exp_logger.info(f"Training on {len(benchmarks)} problem(s)")
        exp_logger.info(f"Total timesteps: {total_timesteps}")
        exp_logger.info(f"Timesteps per problem: {timesteps_per_problem}\n")

        os.makedirs(os.path.dirname(model_output_path) or ".", exist_ok=True)

        if reward_weights is None:
            reward_weights = DEFAULT_REWARD_WEIGHTS.copy()

        model = None
        total_steps = 0

        for step, (domain_file, problem_file) in enumerate(benchmarks):
            problem_name = os.path.basename(problem_file)
            exp_logger.info(f"  [{step + 1}/{len(benchmarks)}] {problem_name}")

            try:
                env = ThinMergeEnv(
                    domain_file=os.path.abspath(domain_file),
                    problem_file=os.path.abspath(problem_file),
                    max_merges=max_merges,
                    timeout_per_step=timeout_per_step,
                    reward_weights=reward_weights,
                    debug=False,
                )
                env = Monitor(env)

                if model is None:
                    exp_logger.info("    Creating new PPO model...")
                    model = PPO(
                        policy=GNNPolicy,
                        env=env,
                        learning_rate=0.0003,
                        n_steps=64,
                        batch_size=32,
                        ent_coef=0.01,
                        verbose=0,
                        tensorboard_log="tb_logs/",
                        policy_kwargs={"hidden_dim": 64},
                    )
                else:
                    model.set_env(env)

                exp_logger.info(f"    Training for {timesteps_per_problem} timesteps...")
                model.learn(
                    total_timesteps=timesteps_per_problem,
                    tb_log_name=f"{tb_log_name}_{step}",
                    reset_num_timesteps=False,
                )

                total_steps += timesteps_per_problem
                exp_logger.info(f"    ✓ Training complete (total: {total_steps} steps)")

                env.close()

                if total_steps >= total_timesteps:
                    break

            except Exception as e:
                exp_logger.error(f"    ⚠️ Problem failed: {e}")
                exp_logger.error(traceback.format_exc())
                continue

        if model is None:
            exp_logger.error("Training failed - no model created")
            return None

        model.save(model_output_path)
        exp_logger.info(f"\n✅ Model saved: {model_output_path}")

        return model_output_path

    except Exception as e:
        exp_logger.error(f"Training failed: {e}")
        exp_logger.error(traceback.format_exc())
        return None


def evaluate_model_on_problems(
        model_path: str,
        benchmarks: List[Tuple[str, str]],
        exp_logger: Optional[logging.Logger] = None,
        reward_weights: Optional[Dict[str, float]] = None,
        max_merges: int = 50,
        timeout_per_step: float = 120.0,
) -> Dict[str, Any]:
    """
    Quick evaluation of trained model on test problems using ThinMergeEnv.

    Returns:
        Dict with evaluation results
    """
    if exp_logger is None:
        exp_logger = logging.getLogger("evaluate")

    try:
        from stable_baselines3 import PPO
        from thin_merge_env import ThinMergeEnv

        model = PPO.load(model_path)
        exp_logger.info(f"Model loaded: {model_path}")

        if reward_weights is None:
            reward_weights = DEFAULT_REWARD_WEIGHTS.copy()

        results = {
            'total_problems': len(benchmarks),
            'solved_count': 0,
            'avg_reward': 0.0,
            'avg_time': 0.0,
            'details': []
        }

        rewards = []
        times = []

        for i, (domain_file, problem_file) in enumerate(benchmarks):
            problem_name = os.path.basename(problem_file)
            exp_logger.info(f"  [{i + 1}/{len(benchmarks)}] {problem_name}")

            try:
                env = ThinMergeEnv(
                    domain_file=os.path.abspath(domain_file),
                    problem_file=os.path.abspath(problem_file),
                    max_merges=max_merges,
                    timeout_per_step=timeout_per_step,
                    reward_weights=reward_weights,
                    debug=False,
                )

                start_time = time.time()
                obs, _ = env.reset()
                episode_reward = 0.0
                steps = 0

                for step in range(max_merges):
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, done, truncated, info = env.step(int(action))
                    episode_reward += reward
                    steps += 1

                    if done or truncated:
                        break

                elapsed = time.time() - start_time

                rewards.append(episode_reward)
                times.append(elapsed)
                results['solved_count'] += 1

                exp_logger.info(f"    ✓ Reward: {episode_reward:.4f}, Time: {elapsed:.2f}s, Steps: {steps}")

                # Extract additional info from final state
                reward_signals = info.get('reward_signals', {})

                results['details'].append({
                    'problem': problem_name,
                    'reward': float(episode_reward),
                    'time': float(elapsed),
                    'steps': steps,
                    'solved': True,
                    'h_star_preservation': reward_signals.get('h_star_preservation', 1.0),
                    'is_solvable': reward_signals.get('is_solvable', True),
                    'num_active_systems': info.get('num_active_systems', 1),
                })

                env.close()

            except Exception as e:
                exp_logger.warning(f"    ⚠️ Failed: {e}")
                results['details'].append({
                    'problem': problem_name,
                    'solved': False,
                    'error': str(e)[:100]
                })

        if rewards:
            results['avg_reward'] = float(np.mean(rewards))
            results['avg_time'] = float(np.mean(times))
            results['solve_rate'] = (results['solved_count'] / len(benchmarks)) * 100

        return results

    except Exception as e:
        exp_logger.error(f"Evaluation failed: {e}")
        exp_logger.error(traceback.format_exc())
        return {}


# ============================================================================
# DATASET UTILITIES
# ============================================================================

def load_and_split_problems(
        domain_file: str,
        problem_pattern: str,
        train_ratio: float = 0.8,
        random_seed: int = 42,
        exp_logger: Optional[logging.Logger] = None
) -> Tuple[List[str], List[str]]:
    """Load problems and split into train/test."""
    if exp_logger is None:
        exp_logger = logging.getLogger("dataset")

    import glob
    import random

    if not os.path.exists(domain_file):
        raise FileNotFoundError(f"Domain not found: {domain_file}")

    all_problems = sorted(glob.glob(problem_pattern))

    if not all_problems:
        raise ValueError(f"No problems found matching: {problem_pattern}")

    if len(all_problems) < 2:
        raise ValueError("Need at least 2 problems for train/test split")

    random.seed(random_seed)
    problems_shuffled = all_problems.copy()
    random.shuffle(problems_shuffled)

    split_idx = int(len(problems_shuffled) * train_ratio)
    train_problems = sorted(problems_shuffled[:split_idx])
    test_problems = sorted(problems_shuffled[split_idx:])

    exp_logger.info(f"Loaded {len(all_problems)} problems")
    exp_logger.info(f"Train/test split: {len(train_problems)} train, {len(test_problems)} test")

    return train_problems, test_problems


def load_benchmarks_by_difficulty(
        benchmarks_dir: str = "benchmarks",
        exp_logger: Optional[logging.Logger] = None
) -> Dict[str, List[Tuple[str, str]]]:
    """Load benchmarks organized by difficulty."""
    if exp_logger is None:
        exp_logger = logging.getLogger("dataset")

    import glob

    benchmarks_dir = os.path.abspath(benchmarks_dir)

    if not os.path.isdir(benchmarks_dir):
        exp_logger.warning(f"Benchmarks dir not found: {benchmarks_dir}")
        return {}

    all_benchmarks = {}

    for difficulty in ["small", "medium", "large"]:
        difficulty_dir = os.path.join(benchmarks_dir, difficulty)

        domain_file = os.path.join(difficulty_dir, "domain.pddl")
        if not os.path.exists(domain_file):
            exp_logger.warning(f"Domain not found for {difficulty}")
            all_benchmarks[difficulty] = []
            continue

        problems = sorted(glob.glob(os.path.join(difficulty_dir, "problem_*.pddl")))

        if not problems:
            exp_logger.warning(f"No problems found for {difficulty}")
            all_benchmarks[difficulty] = []
            continue

        benchmarks_list = [
            (os.path.abspath(domain_file), os.path.abspath(prob))
            for prob in problems
        ]

        all_benchmarks[difficulty] = benchmarks_list
        exp_logger.info(f"  {difficulty:<10}: {len(benchmarks_list)} problems")

    return all_benchmarks


def load_and_validate_benchmarks(
        benchmark_dir: str = "benchmarks",
        timeout_per_problem: int = 480,
        exp_logger: Optional[logging.Logger] = None
) -> Dict[str, List[Tuple[str, str]]]:
    """
    Load and validate benchmarks from directory structure.

    Expected structure:
        benchmarks/
        ├── blocksworld/
        │   ├── small/
        │   │   ├── domain.pddl
        │   │   ├── problem_small_00.pddl
        │   │   └── ...
        │   ├── medium/
        │   └── large/
        ├── logistics/
        └── parking/

    OR simple structure:
        benchmarks/
        ├── small/
        │   ├── domain.pddl
        │   └── problem_*.pddl
        ├── medium/
        └── large/

    Returns:
        Dict mapping key -> list of (domain_file, problem_file) tuples
    """
    if exp_logger is None:
        exp_logger = logging.getLogger("benchmark_loader")

    import glob

    exp_logger.info(f"Loading benchmarks from: {benchmark_dir}")

    benchmarks = {}

    # Check for simple structure first (benchmarks/small/, etc.)
    simple_sizes = ["small", "medium", "large"]
    simple_structure = any(
        os.path.isdir(os.path.join(benchmark_dir, size))
        for size in simple_sizes
    )

    if simple_structure:
        # Simple structure: benchmarks/small/domain.pddl
        for size_name in simple_sizes:
            size_dir = os.path.join(benchmark_dir, size_name)
            if not os.path.isdir(size_dir):
                continue

            domain_file = os.path.join(size_dir, "domain.pddl")
            if not os.path.exists(domain_file):
                exp_logger.warning(f"Domain file not found: {domain_file}")
                continue

            problems = sorted(glob.glob(os.path.join(size_dir, "problem_*.pddl")))

            if not problems:
                exp_logger.warning(f"No problems found in: {size_dir}")
                continue

            key = size_name
            benchmarks[key] = [
                (os.path.abspath(domain_file), os.path.abspath(p))
                for p in problems
            ]

            exp_logger.info(f"  {key}: {len(benchmarks[key])} problems")
    else:
        # Complex structure: benchmarks/domain/size/
        for domain_dir in glob.glob(os.path.join(benchmark_dir, "*")):
            if not os.path.isdir(domain_dir):
                continue

            domain_name = os.path.basename(domain_dir)

            for size_dir in glob.glob(os.path.join(domain_dir, "*")):
                if not os.path.isdir(size_dir):
                    continue

                size_name = os.path.basename(size_dir)

                domain_file = os.path.join(size_dir, "domain.pddl")
                if not os.path.exists(domain_file):
                    exp_logger.warning(f"Domain file not found: {domain_file}")
                    continue

                problems = sorted(glob.glob(os.path.join(size_dir, "problem_*.pddl")))

                if not problems:
                    exp_logger.warning(f"No problems found in: {size_dir}")
                    continue

                key = f"{domain_name}_{size_name}"
                benchmarks[key] = [
                    (os.path.abspath(domain_file), os.path.abspath(p))
                    for p in problems
                ]

                exp_logger.info(f"  {key}: {len(benchmarks[key])} problems")

    exp_logger.info(f"\n✅ Loaded {sum(len(v) for v in benchmarks.values())} total problems")

    return benchmarks


def filter_benchmarks_by_size(
        all_benchmarks: Dict[str, List[Tuple[str, str]]],
        sizes: List[str]
) -> Dict[str, List[Tuple[str, str]]]:
    """
    Filter benchmarks to only include specified sizes.

    Args:
        all_benchmarks: Dict from load_and_validate_benchmarks()
        sizes: List of sizes like ["small", "medium", "large"]

    Returns:
        Filtered benchmarks dict with only matching keys
    """
    filtered = {}
    for key, benchmark_list in all_benchmarks.items():
        for size in sizes:
            if key == size or key.endswith(f"_{size}"):
                filtered[key] = benchmark_list
                break

    logger.info(f"Filtered benchmarks by size {sizes}: {len(filtered)} combinations")
    return filtered


def get_benchmarks_for_sizes(
        all_benchmarks: Dict[str, List[Tuple[str, str]]],
        sizes: List[str],
        max_problems_per_combination: int = None
) -> List[Tuple[str, str]]:
    """
    Get flattened list of benchmarks for specified sizes.

    Args:
        all_benchmarks: Dict from load_and_validate_benchmarks()
        sizes: List of sizes like ["small", "medium"]
        max_problems_per_combination: Max problems per domain-size combo

    Returns:
        Flattened list of (domain_file, problem_file) tuples
    """
    benchmarks_list = []

    for key, benchmark_items in all_benchmarks.items():
        for size in sizes:
            if key == size or key.endswith(f"_{size}"):
                if max_problems_per_combination:
                    benchmarks_list.extend(benchmark_items[:max_problems_per_combination])
                else:
                    benchmarks_list.extend(benchmark_items)
                break

    logger.info(f"Collected {len(benchmarks_list)} benchmarks for sizes {sizes}")
    return benchmarks_list


# ============================================================================
# RESULT PERSISTENCE
# ============================================================================

def save_results_to_json(
        results: Dict[str, Any],
        output_path: str,
        exp_logger: Optional[logging.Logger] = None
) -> None:
    """Save results to JSON file."""
    if exp_logger is None:
        exp_logger = logging.getLogger("results")

    try:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2, default=str)

        exp_logger.info(f"✓ Results saved: {output_path}")

    except Exception as e:
        exp_logger.error(f"Failed to save results: {e}")


def save_results_to_txt(
        results: Dict[str, Any],
        output_path: str,
        experiment_name: str,
        exp_logger: Optional[logging.Logger] = None
) -> None:
    """Save human-readable results to text file."""
    if exp_logger is None:
        exp_logger = logging.getLogger("results")

    try:
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w') as f:
            f.write("=" * 90 + "\n")
            f.write(f"{experiment_name.upper()} - RESULTS SUMMARY\n")
            f.write("=" * 90 + "\n\n")

            f.write(f"Timestamp: {datetime.now().isoformat()}\n\n")

            for key, value in results.items():
                if key.startswith('_'):
                    continue

                if isinstance(value, dict):
                    f.write(f"\n{key.upper()}:\n")
                    f.write("-" * 50 + "\n")
                    for k, v in value.items():
                        f.write(f"  {k:<40} {v}\n")
                elif isinstance(value, list):
                    f.write(f"\n{key.upper()}:\n")
                    f.write("-" * 50 + "\n")
                    for idx, item in enumerate(value[:5], 1):
                        f.write(f"  [{idx}] {item}\n")
                    if len(value) > 5:
                        f.write(f"  ... and {len(value) - 5} more\n")
                else:
                    f.write(f"{key:<40} {value}\n")

            f.write("\n" + "=" * 90 + "\n")

        exp_logger.info(f"✓ Results saved: {output_path}")

    except Exception as e:
        exp_logger.error(f"Failed to save results: {e}")


# ============================================================================
# EXPERIMENT-TO-EVALUATION INTEGRATION
# ============================================================================

@dataclass
class ExperimentResultsMetrics:
    """Unified metrics format for evaluation."""
    experiment_type: str
    problem_name: str
    difficulty: str
    solved: bool
    solve_time: float
    reward: float
    merge_episodes: int
    num_problems_total: int
    num_problems_solved: int
    avg_reward: float
    avg_time: float
    solve_rate: float

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


def load_experiment_results_from_directory(
        experiment_dir: str,
        experiment_type: str,
        exp_logger: Optional[logging.Logger] = None
) -> List[Dict[str, Any]]:
    """
    Load results from a single experiment directory.

    Expected structure:
        experiment_dir/
        ├── results.json      # Contains evaluation data
        └── results.txt       # Human-readable summary
    """
    if exp_logger is None:
        exp_logger = logging.getLogger("experiment_loader")

    # Try multiple common result file names
    result_files = ["results.json", "overfit_summary.json", "experiment_results.json"]
    experiment_results = None

    for result_file in result_files:
        results_path = os.path.join(experiment_dir, result_file)
        if os.path.exists(results_path):
            try:
                with open(results_path, 'r') as f:
                    experiment_results = json.load(f)
                exp_logger.info(f"Loaded experiment results from {results_path}")
                break
            except Exception as e:
                exp_logger.warning(f"Failed to load {results_path}: {e}")

    if experiment_results is None:
        exp_logger.warning(f"No results file found in: {experiment_dir}")
        return []

    parsed_results = []

    # Parse evaluation details
    if 'evaluation' in experiment_results and 'details' in experiment_results['evaluation']:
        for detail in experiment_results['evaluation']['details']:
            parsed_results.append({
                'experiment_type': experiment_type,
                'problem_name': detail.get('problem', ''),
                'solved': detail.get('solved', False),
                'time': detail.get('time', 0),
                'reward': detail.get('reward', 0),
                'difficulty': _extract_difficulty_from_problem_name(detail.get('problem', '')),
            })

    elif 'details' in experiment_results:
        for detail in experiment_results['details']:
            parsed_results.append({
                'experiment_type': experiment_type,
                'problem_name': detail.get('problem', ''),
                'solved': detail.get('solved', False),
                'time': detail.get('time', 0),
                'reward': detail.get('reward', 0),
                'difficulty': _extract_difficulty_from_problem_name(detail.get('problem', '')),
            })

    elif 'per_problem_stats' in experiment_results:
        for stats in experiment_results['per_problem_stats']:
            parsed_results.append({
                'experiment_type': experiment_type,
                'problem_name': stats.get('problem_name', ''),
                'solved': stats.get('solve_rate', 0) > 0,
                'time': 0,
                'reward': stats.get('avg_reward', 0),
                'difficulty': _extract_difficulty_from_problem_name(stats.get('problem_name', '')),
            })

    exp_logger.info(f"Parsed {len(parsed_results)} problem results")

    return parsed_results


def aggregate_experiment_results(
        experiment_directories: Dict[str, str],
        exp_logger: Optional[logging.Logger] = None
) -> Dict[str, Any]:
    """
    Aggregate results from multiple experiments.

    Args:
        experiment_directories: Dict mapping experiment_type -> directory_path
                               E.g., {
                                   'overfit': 'overfit_experiment_results',
                                   'problem_gen': 'problem_generalization_results',
                                   'scale_gen': 'scale_generalization_results',
                                   'curriculum': 'curriculum_learning_results'
                               }

    Returns:
        Aggregated results dictionary with statistics
    """
    if exp_logger is None:
        exp_logger = logging.getLogger("experiment_aggregator")

    exp_logger.info("Aggregating experiment results...")

    all_results = []
    experiment_summaries = {}

    for exp_type, exp_dir in experiment_directories.items():
        if not os.path.exists(exp_dir):
            exp_logger.warning(f"Experiment directory not found: {exp_dir}")
            continue

        exp_logger.info(f"\nLoading {exp_type} from {exp_dir}...")

        results = load_experiment_results_from_directory(
            exp_dir,
            exp_type,
            exp_logger
        )

        if results:
            all_results.extend(results)

            solved_count = sum(1 for r in results if r['solved'])
            total_count = len(results)

            solved_times = [r['time'] for r in results if r['solved'] and r['time'] > 0]
            avg_time = np.mean(solved_times) if solved_times else 0

            all_rewards = [r['reward'] for r in results]
            avg_reward = np.mean(all_rewards) if all_rewards else 0

            experiment_summaries[exp_type] = {
                'total_problems': total_count,
                'solved': solved_count,
                'solve_rate': (solved_count / total_count * 100) if total_count > 0 else 0,
                'avg_time': float(avg_time) if not np.isnan(avg_time) else 0,
                'avg_reward': float(avg_reward) if not np.isnan(avg_reward) else 0,
            }

            exp_logger.info(f"  ✓ {exp_type}: {solved_count}/{total_count} solved "
                            f"({experiment_summaries[exp_type]['solve_rate']:.1f}%)")

    exp_logger.info(f"\n✅ Total results aggregated: {len(all_results)} problems")

    return {
        'all_results': all_results,
        'experiment_summaries': experiment_summaries,
        'timestamp': datetime.now().isoformat()
    }


def _extract_difficulty_from_problem_name(problem_name: str) -> str:
    """Extract difficulty level from problem name."""
    problem_lower = problem_name.lower()
    if 'small' in problem_lower:
        return 'small'
    elif 'medium' in problem_lower:
        return 'medium'
    elif 'large' in problem_lower or 'hard' in problem_lower:
        return 'large'
    else:
        return 'unknown'


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def ensure_directories_exist() -> None:
    """Ensure all required directories exist."""
    for directory in [
        "downward/gnn_output",
        "downward/fd_output",
        "logs",
        "models",
        "benchmarks",
        "tb_logs"
    ]:
        os.makedirs(directory, exist_ok=True)


def get_timestamp_str() -> str:
    """Get current timestamp as string."""
    return datetime.now().strftime("%Y%m%d_%H%M%S")


def format_duration(seconds: float) -> str:
    """Format duration nicely."""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        return f"{seconds / 60:.1f}m"
    else:
        return f"{seconds / 3600:.1f}h"


def cleanup_signal_files() -> int:
    """Remove all signal files from communication directories."""
    from pathlib import Path
    import glob

    deleted = 0
    for directory in ["downward/fd_output", "downward/gnn_output"]:
        dir_path = Path(directory)
        if not dir_path.exists():
            continue
        for pattern in ["*.json", "*.tmp"]:
            for filepath in glob.glob(str(dir_path / pattern)):
                try:
                    os.remove(filepath)
                    deleted += 1
                except:
                    pass
    return deleted

--------------------------------------------------------------------------------

The file analysis_and_visualization.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COMPREHENSIVE ANALYSIS & VISUALIZATION MODULE - RESEARCH GRADE
==================================================================
Advanced plots, statistical analysis, and GNN policy evaluation for
reinforcement learning-based merge strategy selection.

Features:
✓ GNN policy evaluation and comparison with baseline planners
✓ Merge strategy analysis and learned heuristic evaluation
✓ Statistical significance testing (Mann-Whitney U, Wilcoxon)
✓ Per-problem-difficulty breakdown and scaling analysis
✓ Efficiency frontier and Pareto analysis
✓ H* preservation metrics (GNN-specific)
✓ Robustness analysis across problem distributions
✓ Research-grade statistical dashboards
✓ CSV export of detailed results and metrics

Compatible with:
✓ evaluation_comprehensive.py
✓ run_full_evaluation.py
✓ All experiment types (overfit, problem_gen, scale_gen, curriculum)
✓ Enhanced feature dimensions (15-dim node, 10-dim edge)

Plots Generated:
1. Solve rate comparison (with 95% confidence intervals)
2. Time comparison (box plots, violin plots, scatter)
3. Expansions comparison (log scale with distribution)
4. Efficiency frontier (2D scatter with Pareto analysis)
5. Cumulative distribution of solve times
6. Performance profile with significance zones
7. H* preservation analysis (GNN-specific, multi-panel)
8. Statistical summary dashboard
9. Per-difficulty breakdown (4-panel)
10. Scaling analysis across problem sizes
11. Summary statistics CSV export
"""

import sys
import os
import logging
import argparse
import json
import glob
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Callable
import numpy as np
from datetime import datetime
from collections import defaultdict
import traceback

logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - %(message)s'
)

warnings.filterwarnings('ignore', category=FutureWarning)

try:
    import matplotlib

    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    from matplotlib import rcParams
    from matplotlib.gridspec import GridSpec
    import seaborn as sns

    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    logger.warning("matplotlib/seaborn not installed - plotting disabled")

try:
    import pandas as pd

    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    logger.warning("pandas not installed - advanced features disabled")

try:
    from scipy import stats

    HAS_SCIPY = True
except ImportError:
    HAS_SCIPY = False
    logger.warning("scipy not installed - statistical tests disabled")

# ============================================================================
# PLOT CONFIGURATION
# ============================================================================

PLOT_CONFIG = {
    "style": "seaborn-v0_8-whitegrid",
    "figsize": (14, 8),
    "figsize_wide": (16, 10),
    "figsize_large": (18, 12),
    "dpi": 150,
    "font_size": 11,
}

COLORS = {
    "GNN": "#2E86AB",
    "FD_LM-Cut": "#F18F01",
    "FD_Blind": "#C73E1D",
    "FD_Add": "#06A77D",
    "FD_Max": "#D62828",
    "FD_M&S_DFP": "#A23B72",
    "FD_M&S_SCC": "#9D4EDD",
}

# Statistical thresholds
SIGNIFICANCE_THRESHOLD = 0.05
MEDIAN_TIME_THRESHOLD = 10.0  # seconds


# ============================================================================
# DATA LOADING FUNCTIONS
# ============================================================================

def load_results_csv(csv_path: str) -> Optional['pd.DataFrame']:
    """Load evaluation results from CSV."""
    if not HAS_PANDAS:
        logger.warning("pandas not available - cannot load CSV")
        return None

    if not os.path.exists(csv_path):
        logger.error(f"CSV not found: {csv_path}")
        return None

    try:
        df = pd.read_csv(csv_path)
        logger.info(f"Loaded {len(df)} results from CSV")

        # Ensure required columns exist
        required_cols = ['planner_name', 'problem_name', 'solved']
        missing = [col for col in required_cols if col not in df.columns]
        if missing:
            logger.warning(f"Missing columns: {missing}. Has: {list(df.columns)}")

        return df
    except Exception as e:
        logger.error(f"Failed to load CSV: {e}")
        traceback.print_exc()
        return None


def load_experiment_results(
        experiment_dirs: Dict[str, str]
) -> Optional['pd.DataFrame']:
    """Load results from experiment directories."""
    if not HAS_PANDAS:
        logger.warning("pandas not available")
        return None

    logger.info("Loading experiment results...")

    all_data = []

    for exp_type, exp_dir in experiment_dirs.items():
        results_file = os.path.join(exp_dir, "results.json")

        if not os.path.exists(results_file):
            logger.warning(f"Results file not found: {results_file}")
            continue

        logger.info(f"  Loading: {exp_type}...")

        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                exp_results = json.load(f)

            # Parse experiment results
            if 'evaluation' in exp_results:
                eval_data = exp_results['evaluation']
                for detail in eval_data.get('details', []):
                    all_data.append({
                        'experiment_type': exp_type,
                        'planner_name': 'GNN',
                        'problem_name': detail.get('problem', ''),
                        'solved': detail.get('solved', False),
                        'wall_clock_time': detail.get('time', 0),
                        'h_star_preservation': detail.get('h_star_preservation', 1.0),
                        'reward': detail.get('reward', 0),
                        'nodes_expanded': detail.get('nodes_expanded', 0),
                        'plan_cost': detail.get('plan_cost', 0),
                    })

            logger.info(f"  ✓ Loaded {exp_type}")

        except Exception as e:
            logger.error(f"  Failed to load {exp_type}: {e}")
            traceback.print_exc()
            continue

    if not all_data:
        logger.error("No experiment data loaded")
        return None

    df = pd.DataFrame(all_data)
    logger.info(f"Total: {len(df)} results")
    return df


def add_derived_metrics(df: 'pd.DataFrame') -> 'pd.DataFrame':
    """Add derived metrics to dataframe for richer analysis."""
    if df is None:
        return None

    try:
        # Ensure all expected columns exist
        if 'nodes_expanded' not in df.columns:
            df['nodes_expanded'] = 0
        if 'plan_cost' not in df.columns:
            df['plan_cost'] = 0
        if 'wall_clock_time' not in df.columns:
            df['wall_clock_time'] = 0
        if 'h_star_preservation' not in df.columns:
            df['h_star_preservation'] = 1.0

        # Add efficiency metric (time × log nodes)
        df['efficiency'] = df['wall_clock_time'] * np.log1p(df['nodes_expanded'] + 1)

        # Add problem difficulty proxy based on expansions
        difficulty_map = {}
        for problem in df['problem_name'].unique():
            max_expansions = df[df['problem_name'] == problem]['nodes_expanded'].max()
            if max_expansions < 1000:
                difficulty_map[problem] = 'Easy'
            elif max_expansions < 100000:
                difficulty_map[problem] = 'Medium'
            else:
                difficulty_map[problem] = 'Hard'
        df['difficulty'] = df['problem_name'].map(difficulty_map)

        return df
    except Exception as e:
        logger.error(f"Failed to add derived metrics: {e}")
        return df


# ============================================================================
# STATISTICAL ANALYSIS FUNCTIONS
# ============================================================================

def compute_statistical_tests(
        df: 'pd.DataFrame',
        gnn_name: str = 'GNN'
) -> Dict[str, Dict]:
    """Compute statistical significance tests comparing GNN to baselines."""
    if not HAS_SCIPY or df is None:
        return {}

    results = {}
    df_gnn = df[df['planner_name'] == gnn_name]

    if len(df_gnn) == 0:
        logger.warning(f"No results found for {gnn_name}")
        return results

    gnn_times = df_gnn[df_gnn['solved']]['wall_clock_time'].values

    for baseline in df['planner_name'].unique():
        if baseline == gnn_name:
            continue

        df_baseline = df[df['planner_name'] == baseline]
        baseline_times = df_baseline[df_baseline['solved']]['wall_clock_time'].values

        if len(gnn_times) == 0 or len(baseline_times) == 0:
            continue

        try:
            # Mann-Whitney U test (non-parametric, no assumptions on distribution)
            u_stat, p_value_mw = stats.mannwhitneyu(gnn_times, baseline_times, alternative='two-sided')

            # Wilcoxon signed-rank test (if same number of samples)
            p_value_wilcoxon = None
            if len(gnn_times) == len(baseline_times):
                try:
                    wilcoxon_stat, p_value_wilcoxon = stats.wilcoxon(gnn_times, baseline_times)
                except:
                    pass

            # Effect size (Cohen's d)
            cohens_d = (np.mean(gnn_times) - np.mean(baseline_times)) / np.sqrt(
                (np.std(gnn_times) ** 2 + np.std(baseline_times) ** 2) / 2
            )

            results[baseline] = {
                'mann_whitney_p': p_value_mw,
                'wilcoxon_p': p_value_wilcoxon,
                'cohens_d': cohens_d,
                'gnn_mean': np.mean(gnn_times),
                'baseline_mean': np.mean(baseline_times),
                'gnn_median': np.median(gnn_times),
                'baseline_median': np.median(baseline_times),
                'gnn_n': len(gnn_times),
                'baseline_n': len(baseline_times),
                'significant': p_value_mw < SIGNIFICANCE_THRESHOLD,
                'speedup': np.mean(baseline_times) / np.mean(gnn_times)
            }
        except Exception as e:
            logger.warning(f"Failed to compute tests for {baseline}: {e}")

    return results


def extract_problem_size(problem_name: str) -> Optional[int]:
    """Extract size metric from problem name if available."""
    import re
    # Try various patterns
    patterns = [
        r'(\d+)x(\d+)',  # grid format
        r'size[_-]?(\d+)',  # size_N format
        r'n[_-]?(\d+)',  # n_N format
    ]
    for pattern in patterns:
        match = re.search(pattern, problem_name.lower())
        if match:
            if len(match.groups()) == 2:
                return int(match.group(1)) * int(match.group(2))
            else:
                return int(match.group(1))
    return None


# ============================================================================
# PLOT 1: SOLVE RATE COMPARISON
# ============================================================================

def plot_solve_rate_comparison(df: 'pd.DataFrame', output_path: str):
    """Bar chart comparing solve rates with confidence intervals."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        plt.style.use(PLOT_CONFIG["style"])
        fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

        solve_stats = []
        planner_names = []

        for planner in sorted(df['planner_name'].unique()):
            df_planner = df[df['planner_name'] == planner]
            n_total = len(df_planner)
            n_solved = df_planner['solved'].sum()
            rate = (n_solved / n_total) * 100 if n_total > 0 else 0

            # Compute 95% confidence interval using binomial proportion
            if n_total > 0:
                p = rate / 100
                ci = 1.96 * np.sqrt(p * (1 - p) / n_total) * 100
            else:
                ci = 0

            solve_stats.append({'rate': rate, 'ci': ci, 'n': n_total})
            planner_names.append(planner)

        rates = [s['rate'] for s in solve_stats]
        cis = [s['ci'] for s in solve_stats]

        colors_list = [COLORS.get(name, "#555") for name in planner_names]
        bars = ax.bar(range(len(rates)), rates, yerr=cis, capsize=10,
                      color=colors_list, alpha=0.8, edgecolor='black', linewidth=1.5)

        ax.set_ylabel("Solve Rate (%)", fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
        ax.set_xlabel("Planner", fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
        ax.set_xticks(range(len(planner_names)))
        ax.set_xticklabels(planner_names, rotation=45, ha='right', fontsize=PLOT_CONFIG["font_size"])
        ax.set_ylim([0, 110])
        ax.axhline(y=100, color='green', linestyle='--', alpha=0.4, linewidth=2, label='Perfect (100%)')
        ax.axhline(y=80, color='orange', linestyle='--', alpha=0.4, linewidth=2, label='Target (80%)')

        for i, (bar, rate, ci) in enumerate(zip(bars, rates, cis)):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2, height + ci + 2,
                    f'{rate:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')

        ax.set_title("Solve Rate Comparison (95% CI)", fontsize=PLOT_CONFIG["font_size"] + 3,
                     fontweight='bold', pad=20)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.legend(fontsize=PLOT_CONFIG["font_size"], loc='upper left')

        plt.tight_layout()
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate solve rate plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 2: TIME COMPARISON (ENHANCED)
# ============================================================================

def plot_time_comparison_enhanced(df: 'pd.DataFrame', output_path: str):
    """Enhanced visualization: box plots, violin plots, and scatter with statistics."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        plt.style.use(PLOT_CONFIG["style"])
        fig = plt.figure(figsize=(18, 10), dpi=PLOT_CONFIG["dpi"])
        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)

        df_solved = df[df['solved']]

        if len(df_solved) == 0:
            logger.warning("No solved problems for time comparison")
            plt.close()
            return

        planners = sorted(df_solved['planner_name'].unique())

        # Prepare data
        data_for_plots = {p: df_solved[df_solved['planner_name'] == p]['wall_clock_time'].values
                          for p in planners}

        # 1. Box plot
        ax1 = fig.add_subplot(gs[0, 0])
        bp = ax1.boxplot([data_for_plots[p] for p in planners], labels=planners, patch_artist=True)
        for patch, planner in zip(bp['boxes'], planners):
            patch.set_facecolor(COLORS.get(planner, "#555"))
            patch.set_alpha(0.7)
        ax1.set_ylabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax1.set_title("Time Distribution (Box Plot)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax1.set_xticklabels(planners, rotation=45, ha='right')
        ax1.grid(axis='y', alpha=0.3)

        # 2. Violin plot
        ax2 = fig.add_subplot(gs[0, 1])
        positions = range(len(planners))
        parts = ax2.violinplot([data_for_plots[p] for p in planners], positions=positions,
                               showmeans=True, showmedians=True)
        for pc in parts['bodies']:
            pc.set_facecolor('#2E86AB')
            pc.set_alpha(0.7)
        ax2.set_ylabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax2.set_title("Time Distribution (Violin Plot)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax2.set_xticks(positions)
        ax2.set_xticklabels(planners, rotation=45, ha='right')
        ax2.grid(axis='y', alpha=0.3)

        # 3. Scatter with jitter
        ax3 = fig.add_subplot(gs[1, 0])
        for i, planner in enumerate(planners):
            times = data_for_plots[planner]
            x = np.random.normal(i, 0.04, size=len(times))
            ax3.scatter(x, times, alpha=0.6, s=100,
                        color=COLORS.get(planner, "#555"), label=planner,
                        edgecolor='black', linewidth=0.5)
        ax3.set_ylabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax3.set_xlabel("Planner", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax3.set_xticks(range(len(planners)))
        ax3.set_xticklabels(planners, rotation=45, ha='right')
        ax3.set_title("Time Distribution (Scatter)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax3.grid(True, alpha=0.3)

        # 4. Summary statistics
        ax4 = fig.add_subplot(gs[1, 1])
        ax4.axis('off')

        summary_text = "TIME STATISTICS (SOLVED ONLY)\n" + "=" * 50 + "\n\n"
        for planner in planners:
            times = data_for_plots[planner]
            summary_text += f"{planner}:\n"
            summary_text += f"  N = {len(times)}\n"
            summary_text += f"  Mean = {np.mean(times):.3f}s\n"
            summary_text += f"  Median = {np.median(times):.3f}s\n"
            summary_text += f"  Std = {np.std(times):.3f}s\n"
            summary_text += f"  Min = {np.min(times):.3f}s\n"
            summary_text += f"  Max = {np.max(times):.3f}s\n"
            summary_text += f"  Q1 = {np.percentile(times, 25):.3f}s\n"
            summary_text += f"  Q3 = {np.percentile(times, 75):.3f}s\n"
            summary_text += "\n"

        ax4.text(0.05, 0.95, summary_text, fontsize=9, verticalalignment='top',
                 fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

        fig.suptitle("Comprehensive Time Comparison", fontsize=PLOT_CONFIG["font_size"] + 3,
                     fontweight='bold', y=0.995)
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate time comparison plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 3: EXPANSIONS COMPARISON
# ============================================================================

def plot_expansions_comparison(df: 'pd.DataFrame', output_path: str):
    """Enhanced expansions comparison with statistics and distribution."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        if 'nodes_expanded' not in df.columns:
            logger.warning("nodes_expanded not in dataframe - skipping expansions plot")
            return

        plt.style.use(PLOT_CONFIG["style"])
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), dpi=PLOT_CONFIG["dpi"])

        df_solved = df[df['solved']]

        if len(df_solved) == 0:
            logger.warning("No solved problems for expansions comparison")
            plt.close()
            return

        planners = sorted(df_solved['planner_name'].unique())

        expansions_stats = []
        for planner in planners:
            df_p = df_solved[df_solved['planner_name'] == planner]
            valid_exps = df_p[df_p['nodes_expanded'] > 0]['nodes_expanded']
            if len(valid_exps) > 0:
                expansions_stats.append({
                    'mean': valid_exps.mean(),
                    'median': valid_exps.median(),
                    'std': valid_exps.std(),
                    'n': len(valid_exps)
                })
            else:
                expansions_stats.append({'mean': 1, 'median': 1, 'std': 0, 'n': 0})

        means = [s['mean'] for s in expansions_stats]
        stds = [s['std'] for s in expansions_stats]

        colors_list = [COLORS.get(name, "#555") for name in planners]

        # Mean with error bars
        bars = ax1.bar(range(len(means)), means, yerr=stds, capsize=10,
                       color=colors_list, alpha=0.8, edgecolor='black', linewidth=1.5)
        ax1.set_ylabel("Avg Nodes Expanded (log scale)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax1.set_xlabel("Planner", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax1.set_yscale('log')
        ax1.set_xticks(range(len(planners)))
        ax1.set_xticklabels(planners, rotation=45, ha='right')
        ax1.grid(axis='y', alpha=0.3, which='both')
        ax1.set_title("Mean Nodes Expanded", fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')

        # Violin plot for distribution
        data_for_violin = [df_solved[df_solved['planner_name'] == p]['nodes_expanded'].values
                           for p in planners]
        parts = ax2.violinplot(data_for_violin, positions=range(len(planners)),
                               showmeans=True, showmedians=True)
        for pc in parts['bodies']:
            pc.set_facecolor('#2E86AB')
            pc.set_alpha(0.7)
        ax2.set_ylabel("Nodes Expanded (log scale)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax2.set_xlabel("Planner", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax2.set_yscale('log')
        ax2.set_xticks(range(len(planners)))
        ax2.set_xticklabels(planners, rotation=45, ha='right')
        ax2.set_title("Expansions Distribution", fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
        ax2.grid(axis='y', alpha=0.3, which='both')

        plt.tight_layout()
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate expansions plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 4: EFFICIENCY FRONTIER & PARETO ANALYSIS
# ============================================================================

def plot_efficiency_frontier(df: 'pd.DataFrame', output_path: str):
    """2D scatter with Pareto frontier analysis."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        if 'nodes_expanded' not in df.columns:
            logger.warning("nodes_expanded not in dataframe - skipping efficiency frontier")
            return

        plt.style.use(PLOT_CONFIG["style"])
        fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

        df_solved = df[df['solved']].copy()

        if len(df_solved) == 0:
            logger.warning("No solved problems for efficiency frontier")
            plt.close()
            return

        # Filter for reasonable bounds
        df_solved = df_solved[df_solved['wall_clock_time'] > 0]
        df_solved = df_solved[df_solved['nodes_expanded'] > 0]

        for planner in sorted(df_solved['planner_name'].unique()):
            df_p = df_solved[df_solved['planner_name'] == planner]

            # Plot all points
            ax.scatter(df_p['wall_clock_time'], df_p['nodes_expanded'],
                       label=planner, s=150, alpha=0.6, color=COLORS.get(planner, "#555"),
                       edgecolor='black', linewidth=1.5)

        ax.set_xlabel("Wall Clock Time (seconds)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax.set_ylabel("Nodes Expanded (log scale)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax.set_yscale('log')
        ax.set_xscale('log')
        ax.legend(fontsize=PLOT_CONFIG["font_size"], loc='best')
        ax.set_title("Efficiency Frontier (Time vs Expansions)",
                     fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
        ax.grid(True, alpha=0.3, which='both', linestyle='--')

        plt.tight_layout()
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate efficiency frontier plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 5: H* PRESERVATION (GNN-SPECIFIC)
# ============================================================================

def plot_h_star_preservation(df: 'pd.DataFrame', output_path: str):
    """Enhanced H* preservation analysis with detailed metrics."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        if 'h_star_preservation' not in df.columns:
            logger.warning("h_star_preservation not in data - skipping H* analysis")
            return

        df_gnn = df[df['planner_name'] == 'GNN']

        if len(df_gnn) == 0:
            logger.warning("No GNN results for H* preservation")
            return

        plt.style.use(PLOT_CONFIG["style"])
        fig = plt.figure(figsize=(16, 10), dpi=PLOT_CONFIG["dpi"])
        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)

        h_pres_values = df_gnn['h_star_preservation'].dropna().values
        h_pres_solved = df_gnn[df_gnn['solved']]['h_star_preservation'].dropna().values

        # 1. Histogram
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.hist(h_pres_values, bins=25, color='#2E86AB', alpha=0.7, edgecolor='black')
        ax1.axvline(x=1.0, color='red', linestyle='--', linewidth=2, label='Perfect (1.0)')
        ax1.axvline(x=np.mean(h_pres_values), color='green', linestyle='--', linewidth=2,
                    label=f'Mean ({np.mean(h_pres_values):.3f})')
        ax1.axvline(x=np.median(h_pres_values), color='orange', linestyle='--', linewidth=2,
                    label=f'Median ({np.median(h_pres_values):.3f})')
        ax1.set_xlabel("H* Preservation Ratio", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax1.set_ylabel("Frequency", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax1.set_title("H* Preservation Distribution (All)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax1.legend(fontsize=9)
        ax1.grid(axis='y', alpha=0.3)

        # 2. Solved vs Unsolved
        ax2 = fig.add_subplot(gs[0, 1])
        h_pres_unsolved = df_gnn[~df_gnn['solved']]['h_star_preservation'].dropna().values
        bp = ax2.boxplot([h_pres_solved, h_pres_unsolved],
                         labels=['Solved', 'Unsolved'],
                         patch_artist=True)
        for patch, color in zip(bp['boxes'], ['#06A77D', '#D62828']):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, linewidth=2)
        ax2.set_ylabel("H* Preservation Ratio", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax2.set_title("H* Preservation: Solved vs Unsolved", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax2.grid(axis='y', alpha=0.3)

        # 3. Time series
        ax3 = fig.add_subplot(gs[1, 0])
        ax3.plot(h_pres_values, marker='o', linestyle='-', color='#2E86AB',
                 alpha=0.7, linewidth=2, markersize=4)
        ax3.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, linewidth=2, label='Perfect')
        ax3.axhline(y=np.mean(h_pres_values), color='green', linestyle='--',
                    alpha=0.5, linewidth=2, label='Mean')
        ax3.fill_between(range(len(h_pres_values)), 0.95, 1.05, alpha=0.1,
                         color='green', label='Good region (±5%)')
        ax3.set_xlabel("Problem Index", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax3.set_ylabel("H* Preservation Ratio", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax3.set_title("H* Preservation Trend", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax3.legend(fontsize=9)
        ax3.grid(True, alpha=0.3)

        # 4. Statistics text
        ax4 = fig.add_subplot(gs[1, 1])
        ax4.axis('off')

        # Count preservation levels
        perfect = np.sum(h_pres_values == 1.0)
        good = np.sum((h_pres_values > 0.95) & (h_pres_values <= 1.0))
        acceptable = np.sum((h_pres_values > 0.9) & (h_pres_values <= 0.95))
        poor = np.sum(h_pres_values <= 0.9)

        stats_text = "H* PRESERVATION ANALYSIS\n" + "=" * 40 + "\n\n"
        stats_text += f"All Problems (N={len(h_pres_values)}):\n"
        stats_text += f"  Mean: {np.mean(h_pres_values):.4f}\n"
        stats_text += f"  Median: {np.median(h_pres_values):.4f}\n"
        stats_text += f"  Std: {np.std(h_pres_values):.4f}\n"
        stats_text += f"  Min: {np.min(h_pres_values):.4f}\n"
        stats_text += f"  Max: {np.max(h_pres_values):.4f}\n\n"
        stats_text += f"Solved Only (N={len(h_pres_solved)}):\n"
        if len(h_pres_solved) > 0:
            stats_text += f"  Mean: {np.mean(h_pres_solved):.4f}\n"
            stats_text += f"  Median: {np.median(h_pres_solved):.4f}\n"
        stats_text += f"\nQuality Distribution:\n"
        stats_text += f"  Perfect (=1.0): {perfect}\n"
        stats_text += f"  Good (0.95-1.0): {good}\n"
        stats_text += f"  Acceptable (0.9-0.95): {acceptable}\n"
        stats_text += f"  Poor (<0.9): {poor}\n"

        ax4.text(0.05, 0.95, stats_text, fontsize=9, verticalalignment='top',
                 fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

        fig.suptitle("H* Preservation Analysis (GNN Policy)", fontsize=PLOT_CONFIG["font_size"] + 3,
                     fontweight='bold', y=0.995)
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate H* preservation plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 6: CUMULATIVE DISTRIBUTION
# ============================================================================

def plot_cumulative_distribution(df: 'pd.DataFrame', output_path: str):
    """Cumulative distribution of solve times with reference lines."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        plt.style.use(PLOT_CONFIG["style"])
        fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

        df_solved = df[df['solved']]

        if len(df_solved) == 0:
            logger.warning("No solved problems for cumulative distribution")
            plt.close()
            return

        for planner in sorted(df_solved['planner_name'].unique()):
            df_p = df_solved[df_solved['planner_name'] == planner]['wall_clock_time'].sort_values()
            cumulative = np.arange(1, len(df_p) + 1) / len(df_p)

            ax.plot(df_p.values, cumulative, marker='o', label=planner, linewidth=2.5,
                    color=COLORS.get(planner, "#555"), markersize=5, alpha=0.8)

        # Add reference lines for practical time limits
        ax.axvline(x=1.0, color='gray', linestyle=':', alpha=0.5, linewidth=1.5, label='1 second')
        ax.axvline(x=10.0, color='gray', linestyle=':', alpha=0.5, linewidth=1.5, label='10 seconds')

        ax.set_xlabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax.set_ylabel("Cumulative Fraction Solved", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax.legend(fontsize=PLOT_CONFIG["font_size"], loc='lower right', ncol=2)
        ax.set_title("Cumulative Distribution of Solve Times", fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
        ax.grid(True, alpha=0.3, linestyle='--')
        ax.set_xscale('log')
        ax.set_ylim([0, 1.05])

        plt.tight_layout()
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate cumulative distribution plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 7: PERFORMANCE PROFILE
# ============================================================================

def plot_performance_profile(df: 'pd.DataFrame', output_path: str):
    """Performance profile with statistical significance zones."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        plt.style.use(PLOT_CONFIG["style"])
        fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

        df_solved = df[df['solved']]

        if len(df_solved) == 0:
            logger.warning("No solved problems for performance profile")
            plt.close()
            return

        planners = sorted(df_solved['planner_name'].unique())

        for planner in planners:
            df_p = df_solved[df_solved['planner_name'] == planner]
            performance_ratios = []

            for problem in df['problem_name'].unique():
                best_time = df[df['problem_name'] == problem]['wall_clock_time'].min()
                df_problem = df_p[df_p['problem_name'] == problem]

                if len(df_problem) > 0 and best_time > 0:
                    time_p = df_problem['wall_clock_time'].values[0]
                    ratio = time_p / best_time
                    performance_ratios.append(ratio)

            if performance_ratios:
                performance_ratios = sorted(performance_ratios)
                coverage = np.arange(1, len(performance_ratios) + 1) / len(performance_ratios)
                ax.plot(performance_ratios, coverage, marker='o', label=planner, linewidth=2.5,
                        color=COLORS.get(planner, "#555"), markersize=5, alpha=0.8)

        # Add significance zones
        ax.axvspan(1, 1.5, alpha=0.1, color='green', label='Excellent (1-1.5x)')
        ax.axvspan(1.5, 3, alpha=0.1, color='yellow', label='Good (1.5-3x)')
        ax.axvspan(3, 10, alpha=0.1, color='red', label='Poor (>3x)')

        ax.set_xscale('log')
        ax.set_xlabel('Performance Ratio τ (time / best_time)', fontsize=PLOT_CONFIG["font_size"] + 1,
                      fontweight='bold')
        ax.set_ylabel('% Problems Solved', fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax.set_title('Performance Profile (Solved Problems)', fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
        ax.legend(fontsize=PLOT_CONFIG["font_size"], loc='lower right', ncol=2)
        ax.grid(True, alpha=0.3, which='both', linestyle='--')
        ax.set_ylim([0, 1.05])

        plt.tight_layout()
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate performance profile plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 8: PER-DIFFICULTY BREAKDOWN
# ============================================================================

def plot_per_difficulty_analysis(df: 'pd.DataFrame', output_path: str):
    """Analyze performance broken down by problem difficulty."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        if 'difficulty' not in df.columns:
            logger.warning("difficulty not in dataframe - skipping per-difficulty analysis")
            return

        plt.style.use(PLOT_CONFIG["style"])
        fig = plt.figure(figsize=(16, 10), dpi=PLOT_CONFIG["dpi"])
        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)

        difficulties = sorted(df['difficulty'].unique())
        planners = sorted(df['planner_name'].unique())

        # 1. Solve rate by difficulty
        ax1 = fig.add_subplot(gs[0, 0])
        x = np.arange(len(difficulties))
        width = 0.8 / len(planners) if len(planners) > 0 else 0.8

        for i, planner in enumerate(planners):
            rates = []
            for diff in difficulties:
                df_subset = df[(df['difficulty'] == diff) & (df['planner_name'] == planner)]
                if len(df_subset) > 0:
                    rate = df_subset['solved'].sum() / len(df_subset) * 100
                else:
                    rate = 0
                rates.append(rate)

            ax1.bar(x + i * width, rates, width, label=planner,
                    color=COLORS.get(planner, "#555"), alpha=0.8, edgecolor='black')

        ax1.set_ylabel("Solve Rate (%)", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax1.set_xlabel("Problem Difficulty", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax1.set_xticks(x + width * (len(planners) - 1) / 2)
        ax1.set_xticklabels(difficulties)
        ax1.legend(fontsize=9)
        ax1.set_title("Solve Rate by Difficulty", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax1.set_ylim([0, 105])
        ax1.grid(axis='y', alpha=0.3)

        # 2. Mean time by difficulty
        ax2 = fig.add_subplot(gs[0, 1])
        for i, planner in enumerate(planners):
            times = []
            for diff in difficulties:
                df_subset = df[(df['difficulty'] == diff) & (df['planner_name'] == planner) & (df['solved'])]
                if len(df_subset) > 0:
                    time = df_subset['wall_clock_time'].mean()
                else:
                    time = 0
                times.append(time)

            ax2.bar(x + i * width, times, width, label=planner,
                    color=COLORS.get(planner, "#555"), alpha=0.8, edgecolor='black')

        ax2.set_ylabel("Mean Time (seconds)", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax2.set_xlabel("Problem Difficulty", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax2.set_xticks(x + width * (len(planners) - 1) / 2)
        ax2.set_xticklabels(difficulties)
        ax2.legend(fontsize=9)
        ax2.set_title("Mean Time by Difficulty (Solved)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax2.grid(axis='y', alpha=0.3)

        # 3. Box plot of times by difficulty
        ax3 = fig.add_subplot(gs[1, 0])
        df_solved = df[df['solved']]
        positions = []
        data_for_box = []
        labels_for_box = []
        pos = 0
        for diff in difficulties:
            for planner in planners:
                df_subset = df_solved[(df_solved['difficulty'] == diff) & (df_solved['planner_name'] == planner)]
                if len(df_subset) > 0:
                    data_for_box.append(df_subset['wall_clock_time'].values)
                    positions.append(pos)
                    pos += 1
            pos += 1

        if data_for_box:
            bp = ax3.boxplot(data_for_box, positions=positions, patch_artist=True, widths=0.6)
            for patch in bp['boxes']:
                patch.set_facecolor('#2E86AB')
                patch.set_alpha(0.7)
        ax3.set_ylabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax3.set_title("Time Distribution by Difficulty", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax3.grid(axis='y', alpha=0.3)

        # 4. Problem count by difficulty
        ax4 = fig.add_subplot(gs[1, 1])
        counts = []
        for diff in difficulties:
            count = len(df[df['difficulty'] == diff])
            counts.append(count)

        colors_map = {'Easy': '#06A77D', 'Medium': '#F18F01', 'Hard': '#D62828'}
        colors_bars = [colors_map.get(d, '#555') for d in difficulties]

        bars = ax4.bar(difficulties, counts, color=colors_bars, alpha=0.8, edgecolor='black')
        ax4.set_ylabel("Count", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax4.set_xlabel("Difficulty", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax4.set_title("Problem Distribution by Difficulty", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax4.grid(axis='y', alpha=0.3)

        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width() / 2, height,
                     f'{int(count)}', ha='center', va='bottom', fontsize=10, fontweight='bold')

        fig.suptitle("Per-Difficulty Analysis", fontsize=PLOT_CONFIG["font_size"] + 3,
                     fontweight='bold', y=0.995)
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate per-difficulty plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 9: STATISTICAL SUMMARY DASHBOARD
# ============================================================================

def plot_statistical_summary(df: 'pd.DataFrame', output_path: str):
    """Comprehensive statistical summary dashboard."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        plt.style.use(PLOT_CONFIG["style"])
        fig = plt.figure(figsize=(18, 12), dpi=PLOT_CONFIG["dpi"])
        gs = GridSpec(3, 3, figure=fig, hspace=0.35, wspace=0.35)

        df_solved = df[df['solved']]
        planners = sorted(df_solved['planner_name'].unique()) if len(df_solved) > 0 else []

        if len(planners) == 0:
            logger.warning("No solved problems for statistical summary")
            plt.close()
            return

        # 1. Solve rate
        ax1 = fig.add_subplot(gs[0, 0])
        solve_rates = [(df[df['planner_name'] == p]['solved'].sum() / len(df[df['planner_name'] == p]) * 100)
                       for p in planners]
        colors = [COLORS.get(p, "#555") for p in planners]
        bars = ax1.barh(planners, solve_rates, color=colors, alpha=0.8, edgecolor='black')
        ax1.set_xlabel("Solve Rate (%)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"])
        ax1.set_title("Solve Rate", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
        ax1.set_xlim([0, 105])
        ax1.grid(axis='x', alpha=0.3)
        for bar, rate in zip(bars, solve_rates):
            width = bar.get_width()
            ax1.text(width + 2, bar.get_y() + bar.get_height() / 2,
                     f'{rate:.1f}%', ha='left', va='center', fontsize=9, fontweight='bold')

        # 2. Mean time
        ax2 = fig.add_subplot(gs[0, 1])
        mean_times = [df_solved[df_solved['planner_name'] == p]['wall_clock_time'].mean() for p in planners]
        bars = ax2.barh(planners, mean_times, color=colors, alpha=0.8, edgecolor='black')
        ax2.set_xlabel("Mean Time (s)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"])
        ax2.set_title("Mean Time (Solved)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
        ax2.grid(axis='x', alpha=0.3)
        for bar, time in zip(bars, mean_times):
            width = bar.get_width()
            ax2.text(width, bar.get_y() + bar.get_height() / 2,
                     f'{time:.2f}s', ha='left', va='center', fontsize=9)

        # 3. Median time
        ax3 = fig.add_subplot(gs[0, 2])
        median_times = [df_solved[df_solved['planner_name'] == p]['wall_clock_time'].median() for p in planners]
        bars = ax3.barh(planners, median_times, color=colors, alpha=0.8, edgecolor='black')
        ax3.set_xlabel("Median Time (s)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"])
        ax3.set_title("Median Time (Solved)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
        ax3.grid(axis='x', alpha=0.3)
        for bar, time in zip(bars, median_times):
            width = bar.get_width()
            ax3.text(width, bar.get_y() + bar.get_height() / 2,
                     f'{time:.2f}s', ha='left', va='center', fontsize=9)

        # 4. Mean expansions
        ax4 = fig.add_subplot(gs[1, 0])
        if 'nodes_expanded' in df.columns:
            mean_exps = [df_solved[df_solved['planner_name'] == p]['nodes_expanded'].mean() for p in planners]
            bars = ax4.barh(planners, mean_exps, color=colors, alpha=0.8, edgecolor='black')
            ax4.set_xlabel("Mean Expansions", fontweight='bold', fontsize=PLOT_CONFIG["font_size"])
            ax4.set_title("Mean Nodes Expanded", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
            ax4.set_xscale('log')
            ax4.grid(axis='x', alpha=0.3, which='both')
        else:
            ax4.text(0.5, 0.5, 'N/A', ha='center', va='center', transform=ax4.transAxes, fontsize=12)
            ax4.set_title("Mean Nodes Expanded", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
            ax4.axis('off')

        # 5. Plan cost
        ax5 = fig.add_subplot(gs[1, 1])
        if 'plan_cost' in df.columns and df['plan_cost'].sum() > 0:
            mean_costs = [df_solved[df_solved['planner_name'] == p]['plan_cost'].mean() for p in planners]
            bars = ax5.barh(planners, mean_costs, color=colors, alpha=0.8, edgecolor='black')
            ax5.set_xlabel("Mean Plan Cost", fontweight='bold', fontsize=PLOT_CONFIG["font_size"])
            ax5.set_title("Mean Plan Cost", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
            ax5.grid(axis='x', alpha=0.3)
        else:
            ax5.text(0.5, 0.5, 'N/A', ha='center', va='center', transform=ax5.transAxes, fontsize=12)
            ax5.set_title("Mean Plan Cost", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
            ax5.axis('off')

        # 6. H* preservation (GNN only)
        ax6 = fig.add_subplot(gs[1, 2])
        if 'h_star_preservation' in df.columns and 'GNN' in planners:
            h_pres_vals = df_solved[df_solved['planner_name'] == 'GNN']['h_star_preservation'].dropna()
            if len(h_pres_vals) > 0:
                bars = ax6.barh(['GNN'], [h_pres_vals.mean()], color=[COLORS.get('GNN', "#555")],
                                alpha=0.8, edgecolor='black')
                ax6.axvline(x=1.0, color='red', linestyle='--', linewidth=2, alpha=0.5, label='Perfect')
                ax6.set_xlabel("H* Preservation", fontweight='bold', fontsize=PLOT_CONFIG["font_size"])
                ax6.set_title("H* Preservation (GNN)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
                ax6.set_xlim([0.8, 1.3])
                ax6.grid(axis='x', alpha=0.3)
            else:
                ax6.text(0.5, 0.5, 'N/A', ha='center', va='center', transform=ax6.transAxes, fontsize=12)
                ax6.set_title("H* Preservation (GNN)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
                ax6.axis('off')
        else:
            ax6.text(0.5, 0.5, 'N/A', ha='center', va='center', transform=ax6.transAxes, fontsize=12)
            ax6.set_title("H* Preservation (GNN)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
            ax6.axis('off')

        # 7. Standard deviations
        ax7 = fig.add_subplot(gs[2, 0])
        if 'wall_clock_time' in df.columns:
            stds = [df_solved[df_solved['planner_name'] == p]['wall_clock_time'].std() for p in planners]
            bars = ax7.barh(planners, stds, color=colors, alpha=0.8, edgecolor='black')
            ax7.set_xlabel("Std Dev Time (s)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"])
            ax7.set_title("Time Std Dev (Solved)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
            ax7.grid(axis='x', alpha=0.3)

        # 8. 95th percentile time
        ax8 = fig.add_subplot(gs[2, 1])
        if 'wall_clock_time' in df.columns:
            p95_times = [np.percentile(df_solved[df_solved['planner_name'] == p]['wall_clock_time'], 95)
                         for p in planners]
            bars = ax8.barh(planners, p95_times, color=colors, alpha=0.8, edgecolor='black')
            ax8.set_xlabel("95th Percentile Time (s)", fontweight='bold', fontsize=PLOT_CONFIG["font_size"])
            ax8.set_title("95th Percentile Time", fontweight='bold', fontsize=PLOT_CONFIG["font_size"] + 1)
            ax8.grid(axis='x', alpha=0.3)

        # 9. Summary text
        ax9 = fig.add_subplot(gs[2, 2])
        ax9.axis('off')

        summary_text = "SUMMARY\n" + "=" * 35 + "\n\n"
        for planner in planners:
            df_p = df[df['planner_name'] == planner]
            df_p_solved = df_p[df_p['solved']]
            n_solved = len(df_p_solved)
            n_total = len(df_p)
            summary_text += f"{planner}:\n"
            summary_text += f"  {n_solved}/{n_total} solved\n"
            summary_text += f"  {n_solved / n_total * 100:.1f}%\n"
            if n_solved > 0:
                med_time = df_p_solved['wall_clock_time'].median()
                summary_text += f"  Med: {med_time:.2f}s\n"
            summary_text += "\n"

        ax9.text(0.05, 0.95, summary_text, fontsize=9, verticalalignment='top',
                 fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))

        fig.suptitle("Statistical Summary Dashboard", fontsize=PLOT_CONFIG["font_size"] + 3,
                     fontweight='bold', y=0.995)
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate statistical summary plot: {e}")
        traceback.print_exc()


# ============================================================================
# PLOT 10: SCALING ANALYSIS
# ============================================================================

def plot_scaling_analysis(df: 'pd.DataFrame', output_path: str):
    """Analyze scaling behavior across problem sizes."""
    if not HAS_MATPLOTLIB or df is None:
        return

    try:
        plt.style.use(PLOT_CONFIG["style"])
        fig, axes = plt.subplots(2, 2, figsize=PLOT_CONFIG["figsize_wide"], dpi=PLOT_CONFIG["dpi"])

        df_copy = df.copy()
        df_copy['size'] = df_copy['problem_name'].apply(extract_problem_size)

        planners = sorted(df_copy['planner_name'].unique())

        # Filter out problems without size info
        df_sized = df_copy[df_copy['size'].notna()]

        if len(df_sized) == 0:
            logger.warning("No size information found in problem names - skipping scaling analysis")
            plt.close()
            return

        # 1. Solve rate vs size
        ax = axes[0, 0]
        for planner in planners:
            df_p = df_sized[df_sized['planner_name'] == planner]
            sizes = sorted(df_p['size'].unique())
            rates = []
            for size in sizes:
                df_size = df_p[df_p['size'] == size]
                rate = df_size['solved'].sum() / len(df_size) * 100 if len(df_size) > 0 else 0
                rates.append(rate)
            ax.plot(sizes, rates, marker='o', label=planner, linewidth=2.5,
                    color=COLORS.get(planner, "#555"), markersize=8)

        ax.set_xlabel("Problem Size", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax.set_ylabel("Solve Rate (%)", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax.set_title("Solve Rate vs Problem Size", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0, 105])

        # 2. Mean time vs size
        ax = axes[0, 1]
        df_solved_sized = df_sized[df_sized['solved']]
        for planner in planners:
            df_p = df_solved_sized[df_solved_sized['planner_name'] == planner]
            sizes = sorted(df_p['size'].unique())
            times = []
            for size in sizes:
                df_size = df_p[df_p['size'] == size]
                if len(df_size) > 0:
                    time = df_size['wall_clock_time'].mean()
                else:
                    time = 0
                times.append(time)
            if times:
                ax.plot(sizes, times, marker='o', label=planner, linewidth=2.5,
                        color=COLORS.get(planner, "#555"), markersize=8)

        ax.set_xlabel("Problem Size", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax.set_ylabel("Mean Time (s)", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax.set_title("Mean Time vs Problem Size", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')

        # 3. Expansions vs size
        ax = axes[1, 0]
        if 'nodes_expanded' in df.columns:
            for planner in planners:
                df_p = df_solved_sized[df_solved_sized['planner_name'] == planner]
                sizes = sorted(df_p['size'].unique())
                exps = []
                for size in sizes:
                    df_size = df_p[df_p['size'] == size]
                    if len(df_size) > 0:
                        exp_vals = df_size[df_size['nodes_expanded'] > 0]['nodes_expanded']
                        exp = exp_vals.mean() if len(exp_vals) > 0 else 1
                    else:
                        exp = 1
                    exps.append(exp)
                if exps:
                    ax.plot(sizes, exps, marker='o', label=planner, linewidth=2.5,
                            color=COLORS.get(planner, "#555"), markersize=8)

            ax.set_xlabel("Problem Size", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
            ax.set_ylabel("Mean Expansions", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
            ax.set_title("Mean Expansions vs Size", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
            ax.legend(fontsize=9)
            ax.grid(True, alpha=0.3, which='both')
            ax.set_yscale('log')

        # 4. Problems per size
        ax = axes[1, 1]
        sizes = sorted(df_sized['size'].unique())
        counts = [len(df_sized[df_sized['size'] == s]) for s in sizes]

        bars = ax.bar(range(len(sizes)), counts, color='#2E86AB', alpha=0.8, edgecolor='black')
        ax.set_xlabel("Problem Size", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax.set_ylabel("Count", fontsize=PLOT_CONFIG["font_size"], fontweight='bold')
        ax.set_title("Problem Distribution by Size", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
        ax.set_xticks(range(len(sizes)))
        ax.set_xticklabels([int(s) for s in sizes], rotation=45, ha='right')
        ax.grid(axis='y', alpha=0.3)

        for bar, count in zip(bars, counts):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2, height,
                    f'{int(count)}', ha='center', va='bottom', fontsize=9)

        fig.suptitle("Scaling Analysis", fontsize=PLOT_CONFIG["font_size"] + 3,
                     fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
        logger.info(f"✓ {output_path}")
        plt.close()
    except Exception as e:
        logger.error(f"Failed to generate scaling analysis plot: {e}")
        traceback.print_exc()


# ============================================================================
# EXPORT SUMMARY STATISTICS TO CSV
# ============================================================================

def export_summary_statistics(df: 'pd.DataFrame', output_path: str):
    """Export detailed summary statistics to CSV."""
    if not HAS_PANDAS or df is None:
        return

    try:
        planners = sorted(df['planner_name'].unique())

        rows = []

        for planner in planners:
            df_p = df[df['planner_name'] == planner]
            df_p_solved = df_p[df_p['solved']]

            row = {
                'Planner': planner,
                'Total_Problems': len(df_p),
                'Solved': len(df_p_solved),
                'Solve_Rate_%': len(df_p_solved) / len(df_p) * 100 if len(df_p) > 0 else 0,
                'Mean_Time_s': df_p_solved['wall_clock_time'].mean() if len(df_p_solved) > 0 else np.nan,
                'Median_Time_s': df_p_solved['wall_clock_time'].median() if len(df_p_solved) > 0 else np.nan,
                'Std_Time_s': df_p_solved['wall_clock_time'].std() if len(df_p_solved) > 0 else np.nan,
                'Min_Time_s': df_p_solved['wall_clock_time'].min() if len(df_p_solved) > 0 else np.nan,
                'Max_Time_s': df_p_solved['wall_clock_time'].max() if len(df_p_solved) > 0 else np.nan,
                'P25_Time_s': np.percentile(df_p_solved['wall_clock_time'], 25) if len(df_p_solved) > 0 else np.nan,
                'P75_Time_s': np.percentile(df_p_solved['wall_clock_time'], 75) if len(df_p_solved) > 0 else np.nan,
                'P95_Time_s': np.percentile(df_p_solved['wall_clock_time'], 95) if len(df_p_solved) > 0 else np.nan,
            }

            # Add expansions if available
            if 'nodes_expanded' in df.columns:
                valid_exps = df_p_solved[df_p_solved['nodes_expanded'] > 0]['nodes_expanded']
                row['Mean_Expansions'] = valid_exps.mean() if len(valid_exps) > 0 else np.nan
                row['Median_Expansions'] = valid_exps.median() if len(valid_exps) > 0 else np.nan

            # Add H* preservation if available
            if 'h_star_preservation' in df.columns:
                h_pres = df_p['h_star_preservation'].dropna()
                row['Mean_H*_Preservation'] = h_pres.mean() if len(h_pres) > 0 else np.nan

            rows.append(row)

        summary_df = pd.DataFrame(rows)
        summary_df.to_csv(output_path, index=False)
        logger.info(f"✓ Summary statistics exported to {output_path}")

    except Exception as e:
        logger.error(f"Failed to export summary statistics: {e}")
        traceback.print_exc()


# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Comprehensive GNN Policy Analysis & Visualization",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Analyze evaluation results
  python analysis_and_visualization.py \\
      --results evaluation_results/evaluation_results.csv \\
      --output plots/

  # Analyze experiment results
  python analysis_and_visualization.py \\
      --experiments overfit_results problem_gen_results \\
      --output plots/

  # Analyze with specific output naming
  python analysis_and_visualization.py \\
      --results results.csv \\
      --output results/analysis \\
      --title "GNN Policy Evaluation"
        """
    )

    parser.add_argument("--results", help="Path to evaluation_results.csv")
    parser.add_argument("--experiments", nargs='+', help="Experiment directories")
    parser.add_argument("--output", default="plots", help="Output directory")
    parser.add_argument("--title", default="", help="Analysis title")

    args = parser.parse_args()

    Path(args.output).mkdir(parents=True, exist_ok=True)

    df = None

    if args.results and os.path.exists(args.results):
        logger.info(f"Loading results from: {args.results}")
        df = load_results_csv(args.results)

    elif args.experiments:
        logger.info("Loading from experiments...")
        exp_dirs = {os.path.basename(d.rstrip('/')): d for d in args.experiments}
        df = load_experiment_results(exp_dirs)

    if df is None:
        logger.error("No results loaded!")
        return 1

    # Add derived metrics
    df = add_derived_metrics(df)

    logger.info("\n" + "=" * 80)
    logger.info("GENERATING PLOTS & ANALYSIS")
    logger.info("=" * 80 + "\n")

    # Generate all plots
    plot_solve_rate_comparison(df, os.path.join(args.output, "01_solve_rate.png"))
    plot_time_comparison_enhanced(df, os.path.join(args.output, "02_time_comparison.png"))
    plot_expansions_comparison(df, os.path.join(args.output, "03_expansions.png"))
    plot_efficiency_frontier(df, os.path.join(args.output, "04_efficiency_frontier.png"))
    plot_cumulative_distribution(df, os.path.join(args.output, "05_cumulative_dist.png"))
    plot_performance_profile(df, os.path.join(args.output, "06_performance_profile.png"))
    plot_h_star_preservation(df, os.path.join(args.output, "07_h_star_preservation.png"))
    plot_statistical_summary(df, os.path.join(args.output, "08_statistical_summary.png"))
    plot_per_difficulty_analysis(df, os.path.join(args.output, "09_per_difficulty.png"))
    plot_scaling_analysis(df, os.path.join(args.output, "10_scaling_analysis.png"))

    # Export summary statistics
    export_summary_statistics(df, os.path.join(args.output, "summary_statistics.csv"))

    # Compute and log statistical tests
    if HAS_SCIPY:
        logger.info("\n" + "=" * 80)
        logger.info("STATISTICAL SIGNIFICANCE TESTS")
        logger.info("=" * 80 + "\n")

        test_results = compute_statistical_tests(df)

        for baseline, stats in test_results.items():
            logger.info(f"\nGNN vs {baseline}:")
            logger.info(f"  Sample sizes: GNN={stats['gnn_n']}, {baseline}={stats['baseline_n']}")
            logger.info(f"  GNN:      mean={stats['gnn_mean']:.3f}s, median={stats['gnn_median']:.3f}s")
            logger.info(f"  {baseline}: mean={stats['baseline_mean']:.3f}s, median={stats['baseline_median']:.3f}s")
            logger.info(f"  Mann-Whitney U p-value: {stats['mann_whitney_p']:.6f}")
            if stats['wilcoxon_p'] is not None:
                logger.info(f"  Wilcoxon p-value: {stats['wilcoxon_p']:.6f}")
            logger.info(f"  Cohen's d (effect size): {stats['cohens_d']:.3f}")
            logger.info(f"  Speedup: {stats['speedup']:.2f}x")
            logger.info(f"  Significant at α=0.05: {'YES ✓' if stats['significant'] else 'NO'}")

    logger.info(f"\n✅ All analysis complete!")
    logger.info(f"📊 Results saved to: {os.path.abspath(args.output)}/")
    logger.info(f"\nGenerated files:")
    logger.info(f"  ✓ 01_solve_rate.png")
    logger.info(f"  ✓ 02_time_comparison.png")
    logger.info(f"  ✓ 03_expansions.png")
    logger.info(f"  ✓ 04_efficiency_frontier.png")
    logger.info(f"  ✓ 05_cumulative_dist.png")
    logger.info(f"  ✓ 06_performance_profile.png")
    logger.info(f"  ✓ 07_h_star_preservation.png")
    logger.info(f"  ✓ 08_statistical_summary.png")
    logger.info(f"  ✓ 09_per_difficulty.png")
    logger.info(f"  ✓ 10_scaling_analysis.png")
    logger.info(f"  ✓ summary_statistics.csv\n")

    return 0


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file evaluation_comprehensive.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COMPREHENSIVE EVALUATION FRAMEWORK - BASELINE COMPARISON EDITION
=================================================================
Complete evaluation system with rich baseline comparison and statistical analysis.

Compatible with:
✓ ThinMergeEnv (15-dim node features, 10-dim edge features)
✓ All 4 experiment types (overfit, problem_gen, scale_gen, curriculum)
✓ Baseline Fast Downward planners (7 different configurations)
✓ GNN policy with enhanced reward function
✓ Research-grade statistical analysis with significance testing
✓ Multiple output formats (CSV, JSON, TXT, plots)

Architecture:
- BaselineRunner: Execute FD with multiple heuristics
- GNNPolicyRunner: Execute GNN policy with ThinMergeEnv
- DetailedMetrics: 25+ metrics per run (comprehensive tracking)
- AggregateStatistics: Statistical analysis with hypothesis tests
- ComparisonAnalyzer: Head-to-head comparison and ranking
- EvaluationFramework: Orchestrates complete pipeline

Statistical Features:
- H* preservation as primary metric
- State explosion control
- Shrinkability analysis
- Statistical significance tests (t-test, Mann-Whitney U)
- Confidence intervals (95%)
- Per-problem rankings
- Speedup analysis (GNN vs baselines)
- Effect size calculations (Cohen's d)
"""

import sys
import os
import logging
import glob
import json
import subprocess
import time
import re
import csv
import argparse
import numpy as np
import traceback
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from dataclasses import dataclass, asdict, field
from collections import defaultdict
from scipy import stats
from itertools import combinations

sys.path.insert(0, os.getcwd())

# Setup logging with both console and file output
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("evaluation_comprehensive.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)





# ============================================================================
# CONFIGURATION - FRAMEWORK COMPATIBLE
# ============================================================================

class EvaluationConfig:
    """Configuration matching framework standards."""

    # Feature dimensions (match ThinMergeEnv)
    NODE_FEATURE_DIM = 15
    EDGE_FEATURE_DIM = 10

    # Reward weights (match thin_merge_env DEFAULT_REWARD_WEIGHTS)
    REWARD_WEIGHTS = {
        'w_h_preservation': 0.40,  # Primary signal
        'w_shrinkability': 0.25,
        'w_state_control': 0.20,
        'w_solvability': 0.15,
    }

    # Timeouts
    FD_TIMEOUT_SEC = 300
    GNN_TIMEOUT_SEC = 300

    # Statistical analysis
    CONFIDENCE_LEVEL = 0.95
    SIGNIFICANCE_LEVEL = 0.05

    # Baseline configurations (complete set)
    BASELINE_CONFIGS = [
        {
            "name": "FD_LM-Cut",
            "search": "astar(lmcut())",
            "description": "Fast Downward with LM-Cut heuristic"
        },
        {
            "name": "FD_Blind",
            "search": "astar(blind())",
            "description": "Fast Downward with blind search"
        },
        {
            "name": "FD_Add",
            "search": "astar(add())",
            "description": "Fast Downward with additive heuristic"
        },
        {
            "name": "FD_Max",
            "search": "astar(max())",
            "description": "Fast Downward with max heuristic"
        },
        {
            "name": "FD_M&S_DFP",
            "search": (
                "astar(merge_and_shrink("
                "merge_strategy=merge_stateless("
                "merge_selector=score_based_filtering("
                "scoring_functions=[goal_relevance(),dfp(),total_order()])),"
                "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
                "label_reduction=exact(before_shrinking=true,before_merging=false),"
                "max_states=50000,threshold_before_merge=1))"
            ),
            "description": "Merge-and-shrink with DFP scoring"
        },
        {
            "name": "FD_M&S_SCC",
            "search": (
                "astar(merge_and_shrink("
                "merge_strategy=merge_sccs("
                "order_of_sccs=topological,"
                "merge_selector=score_based_filtering("
                "scoring_functions=[goal_relevance(),dfp(),total_order()])),"
                "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
                "label_reduction=exact(before_shrinking=true,before_merging=false),"
                "max_states=50000,threshold_before_merge=1))"
            ),
            "description": "Merge-and-shrink with SCC merging"
        },
    ]


# ============================================================================
# DATA STRUCTURES - COMPREHENSIVE METRICS
# ============================================================================

@dataclass
class DetailedMetrics:
    """Complete set of metrics for a single run (25+ metrics)."""

    # Problem identification
    problem_name: str
    planner_name: str
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    # Solve status (primary)
    solved: bool = False
    wall_clock_time: float = 0.0

    # Planning quality
    plan_cost: int = 0
    plan_length: int = 0
    solution_cost: int = 0

    # Search metrics (expansion analysis)
    nodes_expanded: int = 0
    nodes_generated: int = 0
    search_depth: int = 0
    branching_factor: float = 1.0

    # Memory metrics
    peak_memory_kb: int = 0

    # Time breakdown
    search_time: float = 0.0
    translate_time: float = 0.0
    preprocess_time: float = 0.0
    total_time: float = 0.0

    # Solution quality (h* related for GNN)
    initial_heuristic: int = 0
    average_heuristic: float = 0.0
    final_heuristic: int = 0
    h_star_preservation: float = 1.0  # GNN-specific

    # State control metrics
    num_active_systems: int = 0
    merge_episodes: int = 0
    shrinkability: float = 0.0

    # Reachability and dead-ends
    reachability_ratio: float = 1.0
    dead_end_ratio: float = 0.0

    # Error tracking
    error_type: Optional[str] = None
    error_message: Optional[str] = None

    # Metadata
    timeout_occurred: bool = False
    evaluation_notes: str = ""

    def efficiency_score(self) -> float:
        """Efficiency score (lower is better)."""
        if not self.solved:
            return float('inf')
        if self.nodes_expanded == 0 and self.wall_clock_time == 0:
            return 0.0
        if self.plan_cost > 0:
            efficiency = (
                    (self.nodes_expanded / (self.plan_cost * 100.0)) * 0.4 +
                    (self.wall_clock_time / 10.0) * 0.3 +
                    (self.search_depth / 100.0) * 0.3
            )
        else:
            efficiency = self.wall_clock_time
        return float(efficiency)

    def quality_score(self) -> float:
        """Solution quality score (higher is better)."""
        if not self.solved:
            return 0.0
        if self.plan_cost <= 0:
            return 1.0
        quality = 1.0 / (1.0 + self.plan_cost / 100.0)
        return float(quality)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for CSV/JSON export."""
        d = asdict(self)
        d['efficiency_score'] = self.efficiency_score()
        d['quality_score'] = self.quality_score()
        return d


@dataclass
class AggregateStatistics:
    """Summary statistics across all problems (research-grade)."""

    planner_name: str
    num_problems_total: int
    num_problems_solved: int
    solve_rate_pct: float

    # Time statistics (solved only)
    mean_time_sec: float
    median_time_sec: float
    std_time_sec: float
    min_time_sec: float
    max_time_sec: float
    q1_time_sec: float
    q3_time_sec: float
    iqr_time_sec: float

    # Expansion statistics
    mean_expansions: int
    median_expansions: int
    std_expansions: int
    min_expansions: int
    max_expansions: int

    # Plan quality statistics
    mean_plan_cost: int
    median_plan_cost: int
    std_plan_cost: int

    # H* preservation (GNN-specific)
    mean_h_preservation: float = 1.0
    median_h_preservation: float = 1.0

    # Efficiency metrics
    mean_efficiency_score: float = 0.0
    mean_quality_score: float = 0.0

    # Coverage and errors
    unsolved_count: int = 0
    timeout_count: int = 0
    error_count: int = 0

    # Aggregate times
    total_wall_clock_time_sec: float = 0.0

    # Statistical tests
    statistical_significance: Optional[str] = None
    confidence_level: float = 0.95

    # Additional metrics
    solved_per_time_unit: float = 0.0  # problems/second
    avg_depth_solved: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


@dataclass
class ProblemComparison:
    """Per-problem comparison across planners."""
    problem_name: str
    best_planner: str
    best_time: float
    best_expansions: int
    results_by_planner: Dict[str, Dict[str, Any]] = field(default_factory=dict)

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


# ============================================================================
# BASELINE RUNNER - COMPATIBLE WITH FD
# ============================================================================

class BaselineRunner:
    """Runs baseline Fast Downward planners."""

    def __init__(self, timeout_sec: int = 300, downward_dir: Optional[str] = None):
        self.timeout_sec = timeout_sec

        if downward_dir:
            self.downward_dir = Path(downward_dir).absolute()
        else:
            self.downward_dir = Path(__file__).parent / "downward"

        self.fd_bin = self.downward_dir / "builds" / "release" / "bin" / "downward.exe"
        self.fd_translate = self.downward_dir / "builds" / "release" / "bin" / "translate" / "translate.py"

        if not self.fd_bin.exists():
            logger.warning(f"FD binary not found at: {self.fd_bin}")
        if not self.fd_translate.exists():
            logger.warning(f"FD translate not found at: {self.fd_translate}")

    def run(
            self,
            domain_file: str,
            problem_file: str,
            search_config: str,
            baseline_name: str = "FD"
    ) -> DetailedMetrics:
        """Run baseline planner."""
        problem_name = os.path.basename(problem_file)
        logger.info(f"[BASELINE] {baseline_name}: {problem_name}")

        try:
            # PHASE 1: Translate
            logger.debug(f"  [TRANSLATE] Starting translation...")
            translate_start = time.time()

            work_dir = Path("evaluation_temp")
            work_dir.mkdir(exist_ok=True)
            sas_file = work_dir / "output.sas"

            translate_result = subprocess.run(
                [
                    sys.executable,
                    str(self.fd_translate),
                    domain_file,
                    problem_file,
                    "--sas-file", str(sas_file)
                ],
                cwd=str(self.downward_dir),
                capture_output=True,
                text=True,
                timeout=self.timeout_sec
            )

            translate_time = time.time() - translate_start

            if translate_result.returncode != 0 or not sas_file.exists():
                logger.debug(f"  [TRANSLATE] Failed: {translate_result.stderr[:200]}")
                return DetailedMetrics(
                    problem_name=problem_name,
                    planner_name=baseline_name,
                    solved=False,
                    wall_clock_time=translate_time,
                    translate_time=translate_time,
                    error_type="translate_error",
                    error_message=translate_result.stderr[:500],
                    timeout_occurred=False
                )

            logger.debug(f"  [TRANSLATE] Success ({sas_file.stat().st_size} bytes)")

            # PHASE 2: Search
            logger.debug(f"  [SEARCH] Starting search...")
            search_start = time.time()

            search_result = subprocess.run(
                [str(self.fd_bin), "--search", search_config],
                stdin=open(sas_file, 'r'),
                cwd=str(self.downward_dir),
                capture_output=True,
                text=True,
                timeout=self.timeout_sec
            )

            search_time = time.time() - search_start
            total_time = translate_time + search_time

            output_text = search_result.stdout + search_result.stderr

            logger.debug(f"  [SEARCH] Completed in {search_time:.2f}s")

            # PHASE 3: Parse output
            if "Solution found" not in output_text and "Plan length:" not in output_text:
                logger.debug(f"  [PARSE] No solution found")
                return DetailedMetrics(
                    problem_name=problem_name,
                    planner_name=baseline_name,
                    solved=False,
                    wall_clock_time=total_time,
                    translate_time=translate_time,
                    search_time=search_time,
                    error_type="no_solution",
                    timeout_occurred=False
                )

            # Extract metrics from output
            metrics = self._parse_fd_output(output_text)

            if metrics is None:
                logger.debug(f"  [PARSE] Could not extract metrics")
                return DetailedMetrics(
                    problem_name=problem_name,
                    planner_name=baseline_name,
                    solved=True,
                    wall_clock_time=total_time,
                    translate_time=translate_time,
                    search_time=search_time,
                    error_type="parse_error",
                    timeout_occurred=False
                )

            # Build complete result
            result = DetailedMetrics(
                problem_name=problem_name,
                planner_name=baseline_name,
                solved=True,
                wall_clock_time=total_time,
                translate_time=translate_time,
                search_time=search_time,
                plan_cost=metrics.get('cost', 0),
                plan_length=metrics.get('cost', 0),
                nodes_expanded=metrics.get('expansions', 0),
                search_depth=metrics.get('search_depth', 0),
                branching_factor=metrics.get('branching_factor', 1.0),
                peak_memory_kb=metrics.get('memory', 0),
                timeout_occurred=False
            )

            logger.debug(f"  [SUCCESS] cost={result.plan_cost}, exp={result.nodes_expanded}")
            return result

        except subprocess.TimeoutExpired:
            logger.debug(f"  [TIMEOUT] Exceeded {self.timeout_sec}s")
            return DetailedMetrics(
                problem_name=problem_name,
                planner_name=baseline_name,
                solved=False,
                wall_clock_time=self.timeout_sec,
                error_type="timeout",
                timeout_occurred=True
            )

        except Exception as e:
            logger.error(f"  [ERROR] {e}")
            return DetailedMetrics(
                problem_name=problem_name,
                planner_name=baseline_name,
                solved=False,
                wall_clock_time=0,
                error_type="exception",
                error_message=str(e)[:500],
                timeout_occurred=False
            )

    @staticmethod
    def _parse_fd_output(output_text: str) -> Optional[Dict[str, Any]]:
        """Extract metrics from FD output."""
        metrics = {}

        # Plan cost
        match = re.search(r'Plan length:\s*(\d+)', output_text)
        if match:
            metrics['cost'] = int(match.group(1))

        # Expansions (take last)
        matches = list(re.finditer(r'Expanded\s+(\d+)\s+state', output_text))
        if matches:
            metrics['expansions'] = int(matches[-1].group(1))

        # Search depth
        match = re.search(r'Search depth:\s*(\d+)', output_text)
        if match:
            metrics['search_depth'] = int(match.group(1))

        # Branching factor
        match = re.search(r'Branching factor:\s*([\d.]+)', output_text)
        if match:
            metrics['branching_factor'] = float(match.group(1))

        # Memory
        match = re.search(r'Peak memory:\s*(\d+)\s*KB', output_text)
        if match:
            metrics['memory'] = int(match.group(1))

        if 'cost' not in metrics or 'expansions' not in metrics:
            return None

        return metrics


# ============================================================================
# GNN POLICY RUNNER - COMPATIBLE WITH ThinMergeEnv
# ============================================================================

class GNNPolicyRunner:
    """Runs GNN policy using ThinMergeEnv."""

    def __init__(
            self,
            model_path: str,
            timeout_sec: float = 300,
            downward_dir: Optional[str] = None
    ):
        self.model_path = model_path
        self.timeout_sec = timeout_sec

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")

        if downward_dir:
            self.downward_dir = downward_dir
        else:
            self.downward_dir = os.path.join(os.path.dirname(__file__), "downward")

        logger.info(f"[GNN] Initialized with model: {model_path}")

    def run(self, domain_file: str, problem_file: str) -> DetailedMetrics:
        """Run GNN policy on a problem."""
        problem_name = os.path.basename(problem_file)
        logger.info(f"[GNN] Evaluating: {problem_name}")

        try:
            from stable_baselines3 import PPO
            from thin_merge_env import ThinMergeEnv
        except ImportError as e:
            logger.error(f"[GNN] Import failed: {e}")
            return DetailedMetrics(
                problem_name=problem_name,
                planner_name="GNN",
                solved=False,
                wall_clock_time=0,
                error_type="import_error",
                error_message=str(e)
            )

        start_time = time.time()

        try:
            logger.debug(f"  [LOAD] Loading model...")
            model = PPO.load(self.model_path)
            logger.debug(f"  [LOAD] Model loaded successfully")

            logger.debug(f"  [ENV] Creating ThinMergeEnv...")
            env = ThinMergeEnv(
                domain_file=os.path.abspath(domain_file),
                problem_file=os.path.abspath(problem_file),
                max_merges=50,
                timeout_per_step=self.timeout_sec,
                debug=False,
            )
            logger.debug(f"  [ENV] Environment created")

            logger.debug(f"  [RESET] Resetting environment...")
            solve_start = time.time()
            obs, info = env.reset()
            logger.debug(f"  [RESET] Environment reset")

            total_episode_reward = 0.0
            steps = 0
            max_steps = 50

            # Track rewards from signal
            h_star_preservation = 1.0
            is_solvable = True
            num_active = info.get('num_active_systems', 0)

            logger.debug(f"  [INFERENCE] Starting policy inference...")

            while steps < max_steps:
                try:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, done, truncated, info = env.step(int(action))

                    total_episode_reward += reward
                    steps += 1

                    # Extract GNN-specific metrics
                    reward_signals = info.get('reward_signals', {})
                    h_star_preservation = float(reward_signals.get('h_star_preservation', 1.0))
                    is_solvable = bool(reward_signals.get('is_solvable', True))
                    num_active = info.get('num_active_systems', 0)

                    if done or truncated:
                        logger.debug(f"  [INFERENCE] Episode done at step {steps}")
                        break

                except KeyboardInterrupt:
                    logger.warning("  [INFERENCE] Interrupted by user")
                    break
                except Exception as e:
                    logger.error(f"  [INFERENCE] Step failed: {e}")
                    break

            solve_time = time.time() - solve_start
            total_time = time.time() - start_time

            logger.debug(f"  [INFERENCE] Completed in {steps} steps")

            # Build result
            result = DetailedMetrics(
                problem_name=problem_name,
                planner_name="GNN",
                solved=is_solvable,
                wall_clock_time=total_time,
                search_time=solve_time,
                h_star_preservation=h_star_preservation,
                num_active_systems=num_active,
                merge_episodes=steps,
                evaluation_notes=f"GNN policy: {steps} merge decisions"
            )

            logger.debug(f"  [SUCCESS] h*_preservation={h_star_preservation:.3f}, "
                         f"steps={steps}, solvable={is_solvable}")

            try:
                env.close()
            except:
                pass

            return result

        except subprocess.TimeoutExpired:
            logger.debug(f"  [TIMEOUT] Exceeded {self.timeout_sec}s")
            return DetailedMetrics(
                problem_name=problem_name,
                planner_name="GNN",
                solved=False,
                wall_clock_time=self.timeout_sec,
                error_type="timeout",
                timeout_occurred=True
            )

        except Exception as e:
            logger.error(f"  [ERROR] GNN run failed: {e}")
            logger.error(traceback.format_exc())

            try:
                env.close()
            except:
                pass

            return DetailedMetrics(
                problem_name=problem_name,
                planner_name="GNN",
                solved=False,
                wall_clock_time=time.time() - start_time,
                error_type="exception",
                error_message=str(e)[:500],
                timeout_occurred=False
            )


# ============================================================================
# COMPARISON ANALYZER - RICH BASELINE COMPARISON
# ============================================================================

class ComparisonAnalyzer:
    """Performs detailed statistical comparison between planners."""

    def __init__(self, results: List[DetailedMetrics]):
        self.results = results
        self.by_planner = defaultdict(list)
        for result in results:
            self.by_planner[result.planner_name].append(result)

    def get_per_problem_comparisons(self) -> List[ProblemComparison]:
        """Generate per-problem comparison data."""
        problems = set(r.problem_name for r in self.results)
        comparisons = []

        for problem in sorted(problems):
            problem_results = [r for r in self.results if r.problem_name == problem]
            solved_results = [r for r in problem_results if r.solved]

            if not solved_results:
                best_planner = "NONE"
                best_time = float('inf')
                best_expansions = 0
            else:
                # Best by time
                best_result = min(solved_results, key=lambda r: r.wall_clock_time)
                best_planner = best_result.planner_name
                best_time = best_result.wall_clock_time
                best_expansions = best_result.nodes_expanded

            comp = ProblemComparison(
                problem_name=problem,
                best_planner=best_planner,
                best_time=best_time,
                best_expansions=best_expansions
            )

            # Add all planner results for this problem
            for result in problem_results:
                comp.results_by_planner[result.planner_name] = {
                    'solved': result.solved,
                    'time': result.wall_clock_time,
                    'expansions': result.nodes_expanded,
                    'cost': result.plan_cost,
                    'h_preservation': result.h_star_preservation
                }

            comparisons.append(comp)

        return comparisons

    def get_speedup_analysis(self) -> Dict[str, Dict[str, Any]]:
        """Analyze speedup of GNN vs baselines."""
        speedup_analysis = {}

        if "GNN" not in self.by_planner:
            return {}

        gnn_results = self.by_planner["GNN"]
        problems = set(r.problem_name for r in gnn_results)

        speedups = []
        speedups_time = []
        speedups_expansions = []

        for problem in problems:
            gnn_result = next((r for r in gnn_results if r.problem_name == problem), None)
            if not gnn_result or not gnn_result.solved:
                continue

            # Compare against best baseline for this problem
            baseline_results = [r for r in self.results
                                if r.problem_name == problem
                                and r.planner_name != "GNN"
                                and r.solved]

            if not baseline_results:
                continue

            best_baseline = min(baseline_results, key=lambda r: r.wall_clock_time)

            # Speedup = baseline_time / gnn_time
            if gnn_result.wall_clock_time > 0:
                speedup_time = best_baseline.wall_clock_time / gnn_result.wall_clock_time
                speedups_time.append(speedup_time)

            if gnn_result.nodes_expanded > 0:
                speedup_exp = best_baseline.nodes_expanded / gnn_result.nodes_expanded
                speedups_expansions.append(speedup_exp)

            speedups.append({
                'problem': problem,
                'gnn_time': gnn_result.wall_clock_time,
                'baseline_time': best_baseline.wall_clock_time,
                'speedup_time': speedup_time if gnn_result.wall_clock_time > 0 else 0,
                'baseline_name': best_baseline.planner_name,
                'gnn_exp': gnn_result.nodes_expanded,
                'baseline_exp': best_baseline.nodes_expanded
            })

        speedup_analysis = {
            'per_problem': speedups,
            'mean_speedup_time': float(np.mean(speedups_time)) if speedups_time else 0,
            'mean_speedup_expansions': float(np.mean(speedups_expansions)) if speedups_expansions else 0,
            'geometric_mean_speedup': float(np.exp(np.mean(np.log(speedups_time)))) if speedups_time else 0,
        }

        return speedup_analysis

    def statistical_significance_test(
            self,
            planner1: str,
            planner2: str,
            metric: str = 'wall_clock_time'
    ) -> Dict[str, Any]:
        """Perform statistical significance test between two planners."""

        results1 = [getattr(r, metric) for r in self.by_planner[planner1]
                    if r.solved and getattr(r, metric) > 0]
        results2 = [getattr(r, metric) for r in self.by_planner[planner2]
                    if r.solved and getattr(r, metric) > 0]

        if not results1 or not results2:
            return {'significant': False, 'reason': 'insufficient_data'}

        # Mann-Whitney U test (non-parametric)
        statistic, p_value = stats.mannwhitneyu(results1, results2, alternative='two-sided')

        # Cohen's d (effect size)
        mean1, mean2 = np.mean(results1), np.mean(results2)
        std1, std2 = np.std(results1), np.std(results2)
        pooled_std = np.sqrt((std1**2 + std2**2) / 2)
        cohens_d = (mean1 - mean2) / pooled_std if pooled_std > 0 else 0

        # Confidence interval for difference
        diff_mean = mean1 - mean2
        se_diff = pooled_std * np.sqrt(1/len(results1) + 1/len(results2))
        ci_lower = diff_mean - 1.96 * se_diff
        ci_upper = diff_mean + 1.96 * se_diff

        return {
            'planner1': planner1,
            'planner2': planner2,
            'metric': metric,
            'mean1': float(mean1),
            'mean2': float(mean2),
            'std1': float(std1),
            'std2': float(std2),
            'p_value': float(p_value),
            'significant': p_value < EvaluationConfig.SIGNIFICANCE_LEVEL,
            'cohens_d': float(cohens_d),
            'ci_lower': float(ci_lower),
            'ci_upper': float(ci_upper),
            'n1': len(results1),
            'n2': len(results2),
        }

    def get_ranking_by_metric(self, metric: str = 'solve_rate_pct') -> List[Tuple[str, float]]:
        """Rank planners by given metric."""
        rankings = []

        for planner_name in self.by_planner.keys():
            results = self.by_planner[planner_name]
            if metric == 'solve_rate_pct':
                value = (sum(1 for r in results if r.solved) / len(results) * 100) if results else 0
            elif metric == 'mean_time':
                times = [r.wall_clock_time for r in results if r.solved and r.wall_clock_time > 0]
                value = np.mean(times) if times else float('inf')
            elif metric == 'mean_expansions':
                exps = [r.nodes_expanded for r in results if r.solved and r.nodes_expanded > 0]
                value = np.mean(exps) if exps else float('inf')
            elif metric == 'mean_h_preservation':
                h_pres = [r.h_star_preservation for r in results if r.solved]
                value = np.mean(h_pres) if h_pres else 0
            else:
                value = 0

            rankings.append((planner_name, value))

        # Sort: ascending for time/expansions, descending for rates/preservation
        if metric in ['mean_time', 'mean_expansions']:
            rankings.sort(key=lambda x: x[1])
        else:
            rankings.sort(key=lambda x: x[1], reverse=True)

        return rankings


# ============================================================================
# EVALUATION FRAMEWORK - ORCHESTRATOR
# ============================================================================

class EvaluationFramework:
    """Main evaluation orchestrator with rich baseline comparison."""

    def __init__(self, output_dir: str = "evaluation_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.results: List[DetailedMetrics] = []
        logger.info(f"[FRAMEWORK] Output directory: {self.output_dir}")

    def run_comprehensive_evaluation(
            self,
            domain_file: str,
            problem_pattern: str,
            model_path: str,
            timeout_sec: int = 300,
            include_baselines: bool = True,
            baseline_names: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """Run complete evaluation: baselines + GNN + analysis."""
        print_section("COMPREHENSIVE EVALUATION FRAMEWORK")

        # Load problems
        logger.info(f"Loading problems: {problem_pattern}")
        problems = sorted(glob.glob(problem_pattern))

        if not problems:
            logger.error("No problems found!")
            return {}

        logger.info(f"Found {len(problems)} problem(s)")

        # Run baselines
        if include_baselines:
            self._run_all_baselines(
                domain_file,
                problems,
                timeout_sec,
                baseline_names
            )

        # Run GNN
        self._run_gnn(domain_file, problems, model_path, timeout_sec)

        # Generate reports
        return self._generate_comprehensive_report()

    def _run_all_baselines(
            self,
            domain_file: str,
            problems: List[str],
            timeout_sec: int,
            baseline_names: Optional[List[str]] = None
    ):
        """Run all baseline configurations."""
        print_subsection("RUNNING BASELINE PLANNERS")

        baseline_runner = BaselineRunner(timeout_sec)

        # Select baselines to run
        configs = EvaluationConfig.BASELINE_CONFIGS
        if baseline_names:
            configs = [c for c in configs if c['name'] in baseline_names]

        for config in configs:
            logger.info(f"\n{config['name']}: {config['description']}")
            logger.info("-" * 70)

            for i, problem in enumerate(problems, 1):
                logger.info(f"  [{i}/{len(problems)}] {os.path.basename(problem)}")

                result = baseline_runner.run(
                    domain_file,
                    problem,
                    config['search'],
                    baseline_name=config['name']
                )
                self.results.append(result)

    def _run_gnn(
            self,
            domain_file: str,
            problems: List[str],
            model_path: str,
            timeout_sec: int
    ):
        """Run GNN policy."""
        print_subsection("RUNNING GNN POLICY")

        try:
            gnn_runner = GNNPolicyRunner(model_path, timeout_sec)
        except FileNotFoundError as e:
            logger.error(f"GNN evaluation skipped: {e}")
            return

        for i, problem in enumerate(problems, 1):
            logger.info(f"  [{i}/{len(problems)}] {os.path.basename(problem)}")

            result = gnn_runner.run(domain_file, problem)
            self.results.append(result)

    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive reports and statistics."""
        print_section("GENERATING COMPREHENSIVE REPORT")

        if not self.results:
            logger.error("No results to report!")
            return {}

        # Group by planner
        by_planner = defaultdict(list)
        for result in self.results:
            by_planner[result.planner_name].append(result)

        # Compute statistics for each planner
        summaries = {}
        for planner_name in sorted(by_planner.keys()):
            results_list = by_planner[planner_name]
            summary = self._compute_statistics(planner_name, results_list)
            summaries[planner_name] = summary

            logger.info(f"\n{planner_name}:")
            logger.info(f"  Solve rate: {summary.solve_rate_pct:.1f}%")
            logger.info(f"  Avg time: {summary.mean_time_sec:.2f}s")
            logger.info(f"  Avg expansions: {summary.mean_expansions:,}")
            if planner_name == "GNN":
                logger.info(f"  Avg h* preservation: {summary.mean_h_preservation:.3f}")

        # Detailed comparison analysis
        analyzer = ComparisonAnalyzer(self.results)

        # Per-problem comparisons
        per_problem_comps = analyzer.get_per_problem_comparisons()

        # Speedup analysis
        speedup_analysis = analyzer.get_speedup_analysis()

        # Statistical tests
        statistical_tests = self._perform_statistical_tests(analyzer, summaries)

        # Rankings
        rankings = {
            'by_solve_rate': analyzer.get_ranking_by_metric('solve_rate_pct'),
            'by_mean_time': analyzer.get_ranking_by_metric('mean_time'),
            'by_mean_expansions': analyzer.get_ranking_by_metric('mean_expansions'),
        }

        # Export results
        self._export_all_results(summaries, per_problem_comps, speedup_analysis,
                                 statistical_tests, rankings)

        return {
            "summaries": {name: summary.to_dict() for name, summary in summaries.items()},
            "per_problem_comparisons": [c.to_dict() for c in per_problem_comps],
            "speedup_analysis": speedup_analysis,
            "statistical_tests": statistical_tests,
            "rankings": rankings,
            "timestamp": datetime.now().isoformat(),
            "num_problems": len(set(r.problem_name for r in self.results)),
            "num_planners": len(summaries)
        }

    def _compute_statistics(
            self,
            planner_name: str,
            results_list: List[DetailedMetrics]
    ) -> AggregateStatistics:
        """Compute comprehensive statistics."""

        solved = [r for r in results_list if r.solved]
        num_solved = len(solved)
        num_total = len(results_list)

        times = [r.wall_clock_time for r in solved if r.wall_clock_time > 0]
        expansions = [r.nodes_expanded for r in solved if r.nodes_expanded > 0]
        costs = [r.plan_cost for r in solved if r.plan_cost > 0]
        h_preservations = [r.h_star_preservation for r in solved]
        depths = [r.search_depth for r in solved if r.search_depth > 0]
        efficiency_scores = [r.efficiency_score() for r in solved if r.solved]
        quality_scores = [r.quality_score() for r in solved]

        unsolved = [r for r in results_list if not r.solved]
        errors = [r for r in unsolved if r.error_type]
        timeouts = [r for r in unsolved if r.timeout_occurred]

        # Compute percentiles
        def safe_percentile(data, q):
            if not data:
                return 0.0
            return float(np.percentile(data, q))

        q1 = safe_percentile(times, 25)
        q3 = safe_percentile(times, 75)
        iqr = q3 - q1

        return AggregateStatistics(
            planner_name=planner_name,
            num_problems_total=num_total,
            num_problems_solved=num_solved,
            solve_rate_pct=(num_solved / max(num_total, 1)) * 100,
            mean_time_sec=float(np.mean(times)) if times else 0.0,
            median_time_sec=float(np.median(times)) if times else 0.0,
            std_time_sec=float(np.std(times)) if times else 0.0,
            min_time_sec=float(np.min(times)) if times else 0.0,
            max_time_sec=float(np.max(times)) if times else 0.0,
            q1_time_sec=q1,
            q3_time_sec=q3,
            iqr_time_sec=iqr,
            mean_expansions=int(np.mean(expansions)) if expansions else 0,
            median_expansions=int(np.median(expansions)) if expansions else 0,
            std_expansions=int(np.std(expansions)) if expansions else 0,
            min_expansions=int(np.min(expansions)) if expansions else 0,
            max_expansions=int(np.max(expansions)) if expansions else 0,
            mean_plan_cost=int(np.mean(costs)) if costs else 0,
            median_plan_cost=int(np.median(costs)) if costs else 0,
            std_plan_cost=int(np.std(costs)) if costs else 0,
            mean_h_preservation=float(np.mean(h_preservations)) if h_preservations else 1.0,
            median_h_preservation=float(np.median(h_preservations)) if h_preservations else 1.0,
            mean_efficiency_score=float(np.mean(efficiency_scores)) if efficiency_scores else float('inf'),
            mean_quality_score=float(np.mean(quality_scores)) if quality_scores else 0.0,
            unsolved_count=len(unsolved),
            timeout_count=len(timeouts),
            error_count=len(errors),
            total_wall_clock_time_sec=float(sum(times)) if times else 0.0,
            solved_per_time_unit=float(num_solved / sum(times)) if times and num_solved > 0 else 0.0,
            avg_depth_solved=float(np.mean(depths)) if depths else 0.0,
        )

    def _perform_statistical_tests(
            self,
            analyzer: ComparisonAnalyzer,
            summaries: Dict[str, AggregateStatistics]
    ) -> Dict[str, Any]:
        """Perform statistical significance tests."""
        test_results = {}

        planner_names = list(summaries.keys())

        # Compare all pairs
        for p1, p2 in combinations(planner_names, 2):
            key = f"{p1}_vs_{p2}"

            # Time comparison
            time_test = analyzer.statistical_significance_test(p1, p2, 'wall_clock_time')
            test_results[f"{key}_time"] = time_test

            # Expansion comparison
            exp_test = analyzer.statistical_significance_test(p1, p2, 'nodes_expanded')
            test_results[f"{key}_expansions"] = exp_test

        return test_results

    def _export_all_results(
            self,
            summaries: Dict[str, AggregateStatistics],
            per_problem_comps: List[ProblemComparison],
            speedup_analysis: Dict[str, Any],
            statistical_tests: Dict[str, Any],
            rankings: Dict[str, List[Tuple[str, float]]]
    ):
        """Export results in all formats."""

        # CSV: detailed results
        csv_path = self.output_dir / "evaluation_results.csv"
        if self.results:
            fieldnames = list(self.results[0].to_dict().keys())
            with open(csv_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                for result in self.results:
                    writer.writerow(result.to_dict())
            logger.info(f"✓ CSV: {csv_path}")

        # CSV: per-problem comparisons
        per_problem_csv = self.output_dir / "per_problem_comparison.csv"
        if per_problem_comps:
            with open(per_problem_csv, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=['problem_name', 'best_planner', 'best_time', 'best_expansions'])
                writer.writeheader()
                for comp in per_problem_comps:
                    writer.writerow({
                        'problem_name': comp.problem_name,
                        'best_planner': comp.best_planner,
                        'best_time': comp.best_time,
                        'best_expansions': comp.best_expansions
                    })
            logger.info(f"✓ Per-problem comparison CSV: {per_problem_csv}")

        # JSON: summary statistics
        json_path = self.output_dir / "evaluation_summary.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(
                {name: summary.to_dict() for name, summary in summaries.items()},
                f,
                indent=2
            )
        logger.info(f"✓ JSON: {json_path}")

        # JSON: speedup analysis
        speedup_json = self.output_dir / "speedup_analysis.json"
        with open(speedup_json, 'w', encoding='utf-8') as f:
            json.dump(speedup_analysis, f, indent=2, default=str)
        logger.info(f"✓ Speedup analysis: {speedup_json}")

        # JSON: statistical tests
        stats_json = self.output_dir / "statistical_tests.json"
        with open(stats_json, 'w', encoding='utf-8') as f:
            json.dump(statistical_tests, f, indent=2, default=str)
        logger.info(f"✓ Statistical tests: {stats_json}")

        # TXT: formatted report
        self._write_text_report(summaries, per_problem_comps, speedup_analysis,
                               statistical_tests, rankings)

    def _write_text_report(
            self,
            summaries: Dict[str, AggregateStatistics],
            per_problem_comps: List[ProblemComparison],
            speedup_analysis: Dict[str, Any],
            statistical_tests: Dict[str, Any],
            rankings: Dict[str, List[Tuple[str, float]]]
    ):
        """Write comprehensive text report."""
        report_path = self.output_dir / "comparison_report.txt"

        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("=" * 120 + "\n")
                f.write("COMPREHENSIVE EVALUATION REPORT - BASELINE COMPARISON\n")
                f.write("=" * 120 + "\n\n")

                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                f.write(f"Total problems: {len(set(r.problem_name for r in self.results))}\n")
                f.write(f"Planners evaluated: {len(summaries)}\n\n")

                # ============================================================
                # SECTION 1: SUMMARY TABLE
                # ============================================================
                f.write("1. PERFORMANCE SUMMARY\n")
                f.write("-" * 120 + "\n")
                f.write(f"{'Planner':<25} {'Solve Rate':<18} {'Avg Time':<15} {'Med Time':<15} "
                        f"{'Avg Exp':<15} {'H* Pres':<12}\n")
                f.write("-" * 120 + "\n")

                for planner_name in sorted(summaries.keys()):
                    summary = summaries[planner_name]
                    solve_str = f"{summary.num_problems_solved}/{summary.num_problems_total} ({summary.solve_rate_pct:.1f}%)"
                    avg_exp_str = f"{summary.mean_expansions:,}" if summary.num_problems_solved > 0 else "N/A"
                    h_pres_str = f"{summary.mean_h_preservation:.3f}" if planner_name == "GNN" else "N/A"

                    f.write(f"{planner_name:<25} {solve_str:<18} {summary.mean_time_sec:<15.2f} "
                            f"{summary.median_time_sec:<15.2f} {avg_exp_str:<15} {h_pres_str:<12}\n")

                f.write("-" * 120 + "\n\n")

                # ============================================================
                # SECTION 2: DETAILED STATISTICS
                # ============================================================
                f.write("2. DETAILED STATISTICS\n")
                f.write("-" * 120 + "\n\n")

                for planner_name in sorted(summaries.keys()):
                    summary = summaries[planner_name]
                    f.write(f"{planner_name}\n")
                    f.write(f"  Solved: {summary.num_problems_solved}/{summary.num_problems_total} ({summary.solve_rate_pct:.1f}%)\n")
                    f.write(f"  Time (mean ± std): {summary.mean_time_sec:.2f} ± {summary.std_time_sec:.2f}s\n")
                    f.write(f"  Time (median [Q1, Q3]): {summary.median_time_sec:.2f}s [{summary.q1_time_sec:.2f}, {summary.q3_time_sec:.2f}]\n")
                    f.write(f"  Time (IQR): {summary.iqr_time_sec:.2f}s\n")
                    f.write(f"  Expansions (mean ± std): {summary.mean_expansions:,} ± {summary.std_expansions:,}\n")
                    f.write(f"  Expansions (median): {summary.median_expansions:,}\n")
                    f.write(f"  Plan cost (mean ± std): {summary.mean_plan_cost} ± {summary.std_plan_cost}\n")
                    f.write(f"  Efficiency score: {summary.mean_efficiency_score:.4f}\n")
                    f.write(f"  Quality score: {summary.mean_quality_score:.4f}\n")
                    f.write(f"  Solved per second: {summary.solved_per_time_unit:.4f}\n")
                    if planner_name == "GNN":
                        f.write(f"  H* preservation (mean): {summary.mean_h_preservation:.3f}\n")
                        f.write(f"  H* preservation (median): {summary.median_h_preservation:.3f}\n")
                    f.write(f"  Errors: {summary.error_count}, Timeouts: {summary.timeout_count}\n")
                    f.write(f"  Total time: {summary.total_wall_clock_time_sec:.1f}s\n\n")

                # ============================================================
                # SECTION 3: RANKINGS
                # ============================================================
                f.write("3. RANKINGS BY METRIC\n")
                f.write("-" * 120 + "\n\n")

                f.write("Ranking by Solve Rate (%):\n")
                for rank, (name, value) in enumerate(rankings['by_solve_rate'], 1):
                    f.write(f"  {rank}. {name:<35} {value:.1f}%\n")
                f.write("\n")

                f.write("Ranking by Mean Time (seconds):\n")
                for rank, (name, value) in enumerate(rankings['by_mean_time'], 1):
                    if value == float('inf'):
                        f.write(f"  {rank}. {name:<35} N/A\n")
                    else:
                        f.write(f"  {rank}. {name:<35} {value:.2f}s\n")
                f.write("\n")

                f.write("Ranking by Mean Expansions:\n")
                for rank, (name, value) in enumerate(rankings['by_mean_expansions'], 1):
                    if value == float('inf'):
                        f.write(f"  {rank}. {name:<35} N/A\n")
                    else:
                        f.write(f"  {rank}. {name:<35} {int(value):,}\n")
                f.write("\n")

                # ============================================================
                # SECTION 4: PER-PROBLEM COMPARISON
                # ============================================================
                f.write("4. PER-PROBLEM BEST PLANNER\n")
                f.write("-" * 120 + "\n")
                f.write(f"{'Problem':<40} {'Best Planner':<25} {'Time (s)':<15} {'Expansions':<15}\n")
                f.write("-" * 120 + "\n")

                wins_by_planner = defaultdict(int)
                for comp in per_problem_comps:
                    f.write(f"{comp.problem_name:<40} {comp.best_planner:<25} {comp.best_time:<15.2f} {comp.best_expansions:<15}\n")
                    if comp.best_planner != "NONE":
                        wins_by_planner[comp.best_planner] += 1

                f.write("-" * 120 + "\n\n")

                f.write("Wins by Planner:\n")
                for planner in sorted(wins_by_planner.keys()):
                    f.write(f"  {planner:<35} {wins_by_planner[planner]} wins\n")
                f.write("\n")

                # ============================================================
                # SECTION 5: GNN SPEEDUP ANALYSIS
                # ============================================================
                if speedup_analysis and speedup_analysis.get('per_problem'):
                    f.write("5. GNN SPEEDUP ANALYSIS\n")
                    f.write("-" * 120 + "\n")
                    f.write(f"{'Problem':<40} {'GNN Time':<15} {'Best BL Time':<15} {'Speedup':<15} {'Baseline':<20}\n")
                    f.write("-" * 120 + "\n")

                    for item in speedup_analysis['per_problem']:
                        f.write(f"{item['problem']:<40} {item['gnn_time']:<15.2f} "
                               f"{item['baseline_time']:<15.2f} {item['speedup_time']:<15.2f}x "
                               f"{item['baseline_name']:<20}\n")

                    f.write("-" * 120 + "\n")
                    f.write(f"Mean Speedup (time):        {speedup_analysis['mean_speedup_time']:.2f}x\n")
                    f.write(f"Geometric Mean Speedup:     {speedup_analysis['geometric_mean_speedup']:.2f}x\n")
                    f.write(f"Mean Speedup (expansions):  {speedup_analysis['mean_speedup_expansions']:.2f}x\n\n")

                # ============================================================
                # SECTION 6: STATISTICAL SIGNIFICANCE
                # ============================================================
                if statistical_tests:
                    f.write("6. STATISTICAL SIGNIFICANCE TESTS\n")
                    f.write("-" * 120 + "\n")
                    f.write("Mann-Whitney U Test Results (p < 0.05 indicates significant difference)\n\n")

                    time_tests = {k: v for k, v in statistical_tests.items() if k.endswith('_time')}

                    for key, test in sorted(time_tests.items()):
                        if test.get('n1', 0) > 0 and test.get('n2', 0) > 0:
                            sig_str = "SIGNIFICANT ***" if test['significant'] else "not significant"
                            f.write(f"{test['planner1']} vs {test['planner2']} (Time):\n")
                            f.write(f"  Mean1: {test['mean1']:.2f}s, Mean2: {test['mean2']:.2f}s\n")
                            f.write(f"  p-value: {test['p_value']:.6f} ({sig_str})\n")
                            f.write(f"  Effect size (Cohen's d): {test['cohens_d']:.3f}\n")
                            f.write(f"  95% CI for difference: [{test['ci_lower']:.2f}, {test['ci_upper']:.2f}]\n")
                            f.write(f"  Sample sizes: n1={test['n1']}, n2={test['n2']}\n\n")

            logger.info(f"✓ Text report: {report_path}")

        except Exception as e:
            logger.error(f"Failed to write text report: {e}")


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def print_section(title: str, width: int = 120):
    """Print formatted section header."""
    logger.info("")
    logger.info("=" * width)
    logger.info(f"// {title.upper()}")
    logger.info("=" * width)
    logger.info("")


def print_subsection(title: str, width: int = 100):
    """Print formatted subsection header."""
    logger.info("")
    logger.info("-" * width)
    logger.info(f">>> {title}")
    logger.info("-" * width)
    logger.info("")


# ============================================================================
# COMMAND LINE INTERFACE
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Comprehensive Evaluation Framework with Baseline Comparison",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run complete evaluation (baselines + GNN)
  python evaluation_comprehensive.py \\
      --model mvp_output/gnn_model.zip \\
      --domain domain.pddl \\
      --problems "problem_*.pddl" \\
      --output evaluation_results/

  # Run GNN only (skip baselines for speed)
  python evaluation_comprehensive.py \\
      --model model.zip \\
      --domain domain.pddl \\
      --problems "problem_*.pddl" \\
      --skip-baselines

  # Run specific baselines only
  python evaluation_comprehensive.py \\
      --domain domain.pddl \\
      --problems "problem_*.pddl" \\
      --baselines FD_LM-Cut FD_Blind \\
      --skip-gnn
        """
    )

    parser.add_argument("--model", help="Path to trained GNN model (ZIP)")
    parser.add_argument("--domain", required=True, help="Path to domain PDDL")
    parser.add_argument("--problems", required=True, help="Glob pattern for problems")
    parser.add_argument("--output", default="evaluation_results", help="Output directory")
    parser.add_argument("--timeout", type=int, default=300, help="Timeout per problem (seconds)")
    parser.add_argument("--skip-baselines", action="store_true", help="Skip baseline evaluation")
    parser.add_argument("--skip-gnn", action="store_true", help="Skip GNN evaluation")
    parser.add_argument(
        "--baselines",
        nargs='+',
        help="Specific baselines to run (default: all)"
    )
    parser.add_argument("--downward-dir", help="Path to Fast Downward directory")

    args = parser.parse_args()

    # Validate inputs
    if not os.path.exists(args.domain):
        logger.error(f"Domain not found: {args.domain}")
        return 1

    if not args.skip_gnn and not args.model:
        logger.error("--model required unless --skip-gnn is specified")
        return 1

    # Run evaluation
    framework = EvaluationFramework(args.output)

    if args.skip_gnn:
        include_baselines = True
        model_path = None
    else:
        include_baselines = not args.skip_baselines
        model_path = args.model

    result = framework.run_comprehensive_evaluation(
        domain_file=args.domain,
        problem_pattern=args.problems,
        model_path=model_path,
        timeout_sec=args.timeout,
        include_baselines=include_baselines,
        baseline_names=args.baselines if hasattr(args, 'baselines') else None
    )

    print_section("EVALUATION COMPLETE")
    logger.info("✅ Evaluation pipeline finished!")
    logger.info(f"📁 Results: {os.path.abspath(args.output)}")
    logger.info(f"📊 Summary: {args.output}/evaluation_summary.json")
    logger.info(f"📊 Detailed Results: {args.output}/evaluation_results.csv")
    logger.info(f"📊 Per-Problem Comparison: {args.output}/per_problem_comparison.csv")
    logger.info(f"📊 Speedup Analysis: {args.output}/speedup_analysis.json")
    logger.info(f"📊 Statistical Tests: {args.output}/statistical_tests.json")
    logger.info(f"📋 Report: {args.output}/comparison_report.txt")

    return 0


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file run_full_evaluation.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
END-TO-END EVALUATION ORCHESTRATOR - FULLY COMPATIBLE VERSION
==============================================================
Master script for complete evaluation pipeline.

Features:
✓ Input validation (comprehensive checks)
✓ Baseline evaluation (all FD variants)
✓ GNN evaluation (ThinMergeEnv compatible)
✓ Statistical analysis
✓ Visualization generation
✓ Report compilation
✓ Error recovery

Usage:
  # Full evaluation (baseline + GNN)
  python run_full_evaluation.py \\
      --model mvp_output/gnn_model.zip \\
      --domain domain.pddl \\
      --problems "problem_*.pddl" \\
      --output evaluation_results/

  # GNN only (skip baselines for speed)
  python run_full_evaluation.py \\
      --model model.zip \\
      --domain domain.pddl \\
      --problems "problem_*.pddl" \\
      --skip-baselines

  # Analyze completed experiments
  python run_full_evaluation.py \\
      --analyze-experiments \\
      --experiments exp1_results exp2_results \\
      --output analysis_results/
"""

import sys
import os
import logging
import argparse
import subprocess
import json
from pathlib import Path
from datetime import datetime
import time

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("evaluation_orchestrator.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


# ============================================================================
# INPUT VALIDATION
# ============================================================================

class EvaluationValidator:
    """Comprehensive input validation."""

    @staticmethod
    def validate_model(model_path: str) -> bool:
        """Validate GNN model file."""
        if not model_path:
            logger.error("Model path not provided")
            return False

        if not os.path.exists(model_path):
            logger.error(f"Model file not found: {model_path}")
            return False

        if not model_path.endswith('.zip'):
            logger.warning(f"Model file should be ZIP: {model_path}")

        try:
            import zipfile
            with zipfile.ZipFile(model_path, 'r') as z:
                namelist = z.namelist()
                if 'data' not in namelist:
                    logger.warning(f"Model may not be valid PPO model")
        except Exception as e:
            logger.warning(f"Could not validate model ZIP: {e}")

        logger.info(f"✓ Model validated: {model_path}")
        return True

    @staticmethod
    def validate_domain(domain_path: str) -> bool:
        """Validate domain PDDL file."""
        if not domain_path:
            logger.error("Domain path not provided")
            return False

        if not os.path.exists(domain_path):
            logger.error(f"Domain file not found: {domain_path}")
            return False

        if not domain_path.endswith('.pddl'):
            logger.warning(f"Domain should be PDDL: {domain_path}")

        try:
            with open(domain_path, 'r', encoding='utf-8') as f:
                content = f.read()
                if '(define (domain' not in content:
                    logger.warning(f"Domain may not be valid PDDL")
        except Exception as e:
            logger.error(f"Could not read domain: {e}")
            return False

        logger.info(f"✓ Domain validated: {domain_path}")
        return True

    @staticmethod
    def validate_problems(problem_pattern: str) -> bool:
        """Validate problem files."""
        if not problem_pattern:
            logger.error("Problem pattern not provided")
            return False

        import glob
        problems = sorted(glob.glob(problem_pattern))

        if not problems:
            logger.error(f"No problems found: {problem_pattern}")
            return False

        logger.info(f"✓ Found {len(problems)} problem(s)")

        # Sample check first few
        for prob in problems[:min(3, len(problems))]:
            if not prob.endswith('.pddl'):
                logger.warning(f"Problem should be PDDL: {prob}")
            try:
                with open(prob, 'r', encoding='utf-8') as f:
                    content = f.read()
                    if '(define (problem' not in content:
                        logger.warning(f"May not be valid PDDL: {prob}")
            except Exception as e:
                logger.error(f"Could not read problem: {prob} - {e}")
                return False

        return True

    @staticmethod
    def validate_output_dir(output_dir: str) -> bool:
        """Validate output directory is writable."""
        try:
            os.makedirs(output_dir, exist_ok=True)

            test_file = os.path.join(output_dir, ".write_test")
            with open(test_file, 'w') as f:
                f.write("test")
            os.remove(test_file)

            logger.info(f"✓ Output directory ready: {output_dir}")
            return True

        except Exception as e:
            logger.error(f"Cannot write to output: {e}")
            return False

    @staticmethod
    def validate_experiment_dirs(experiment_dirs: list) -> bool:
        """Validate experiment directories."""
        if not experiment_dirs:
            logger.error("No experiment directories provided")
            return False

        for exp_dir in experiment_dirs:
            if not os.path.exists(exp_dir):
                logger.error(f"Experiment directory not found: {exp_dir}")
                return False

            results_file = os.path.join(exp_dir, "results.json")
            if not os.path.exists(results_file):
                logger.warning(f"Results file may not exist: {results_file}")

            logger.info(f"✓ Experiment validated: {exp_dir}")

        return True

    @staticmethod
    def validate_all_standalone(
            model_path: str,
            domain_path: str,
            problem_pattern: str,
            output_dir: str
    ) -> bool:
        """Run all validations for standalone mode."""
        print_section("INPUT VALIDATION (STANDALONE)")

        checks = [
            ("Model file", lambda: EvaluationValidator.validate_model(model_path)),
            ("Domain PDDL", lambda: EvaluationValidator.validate_domain(domain_path)),
            ("Problem files", lambda: EvaluationValidator.validate_problems(problem_pattern)),
            ("Output directory", lambda: EvaluationValidator.validate_output_dir(output_dir)),
        ]

        results = []
        for name, check in checks:
            try:
                result = check()
                results.append((name, result))
                if not result:
                    logger.error(f"✗ {name} validation failed")
            except Exception as e:
                logger.error(f"✗ {name} validation exception: {e}")
                results.append((name, False))

        passed = sum(1 for _, r in results if r)
        total = len(results)

        logger.info(f"\nValidation: {passed}/{total} passed")

        return all(r for _, r in results)

    @staticmethod
    def validate_all_experiment_analysis(
            experiment_dirs: list,
            output_dir: str
    ) -> bool:
        """Run validations for experiment analysis mode."""
        print_section("INPUT VALIDATION (EXPERIMENT ANALYSIS)")

        checks = [
            ("Experiment directories", lambda: EvaluationValidator.validate_experiment_dirs(experiment_dirs)),
            ("Output directory", lambda: EvaluationValidator.validate_output_dir(output_dir)),
        ]

        results = []
        for name, check in checks:
            try:
                result = check()
                results.append((name, result))
            except Exception as e:
                logger.error(f"✗ {name} exception: {e}")
                results.append((name, False))

        passed = sum(1 for _, r in results if r)
        logger.info(f"\nValidation: {passed}/{len(results)} passed")

        return all(r for _, r in results)


# ============================================================================
# PIPELINE STAGES
# ============================================================================

def run_comprehensive_evaluation(
        model_path: str,
        domain_path: str,
        problem_pattern: str,
        output_dir: str,
        timeout: int,
        skip_baselines: bool,
        baseline_names: list = None
) -> bool:
    """Run comprehensive evaluation."""
    print_section("STAGE 1: COMPREHENSIVE EVALUATION")

    try:
        cmd = [
            "python", "evaluation_comprehensive.py",
            "--domain", domain_path,
            "--problems", problem_pattern,
            "--output", output_dir,
            "--timeout", str(timeout)
        ]

        if model_path:
            cmd.extend(["--model", model_path])

        if skip_baselines:
            cmd.append("--skip-baselines")

        if baseline_names:
            cmd.extend(["--baselines"] + baseline_names)

        logger.info(f"Running: {' '.join(cmd)}")

        result = subprocess.run(cmd, text=True)

        if result.returncode != 0:
            logger.error(f"Evaluation failed (code {result.returncode})")
            return False

        logger.info("✅ Evaluation complete")
        return True

    except Exception as e:
        logger.error(f"Evaluation failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def run_analysis_and_visualization(
        results_csv: str,
        output_dir: str
) -> bool:
    """Run analysis and visualization."""
    print_section("STAGE 2: ANALYSIS & VISUALIZATION")

    if not os.path.exists(results_csv):
        logger.warning(f"Results CSV not found: {results_csv}")
        logger.warning("Skipping analysis")
        return False

    try:
        plots_dir = os.path.join(output_dir, "plots")

        cmd = [
            "python", "analysis_and_visualization.py",
            "--results", results_csv,
            "--output", plots_dir
        ]

        logger.info(f"Running: {' '.join(cmd)}")

        result = subprocess.run(cmd, text=True)

        if result.returncode != 0:
            logger.warning(f"Analysis exited with code {result.returncode}")
            return False

        logger.info("✅ Analysis complete")
        return True

    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        return False


def run_experiment_analysis(
        experiment_dirs: list,
        output_dir: str
) -> bool:
    """Analyze experiment results."""
    print_section("STAGE 1: EXPERIMENT ANALYSIS")

    try:
        plots_dir = os.path.join(output_dir, "plots")

        cmd = [
                  "python", "analysis_and_visualization.py",
                  "--experiments"] + experiment_dirs + [
                  "--output", plots_dir
              ]

        logger.info(f"Running: {' '.join(cmd)}")

        result = subprocess.run(cmd, text=True)

        if result.returncode != 0:
            logger.error(f"Analysis failed (code {result.returncode})")
            return False

        logger.info("✅ Analysis complete")
        return True

    except Exception as e:
        logger.error(f"Analysis failed: {e}")
        return False


def generate_final_report(output_dir: str) -> bool:
    """Generate final evaluation report."""
    print_section("FINAL REPORT GENERATION")

    try:
        report_path = os.path.join(output_dir, "EVALUATION_REPORT.txt")

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 100 + "\n")
            f.write("GNN MERGE STRATEGY - COMPREHENSIVE EVALUATION REPORT\n")
            f.write("=" * 100 + "\n\n")

            f.write(f"Timestamp: {datetime.now().isoformat()}\n\n")

            f.write("EVALUATION OVERVIEW\n")
            f.write("-" * 100 + "\n")
            f.write("Complete evaluation of GNN-guided merge-and-shrink planning.\n\n")

            f.write("KEY OUTPUT FILES\n")
            f.write("-" * 100 + "\n")
            f.write(f"  Results CSV:          {os.path.join(output_dir, 'evaluation_results.csv')}\n")
            f.write(f"  Summary JSON:         {os.path.join(output_dir, 'evaluation_summary.json')}\n")
            f.write(f"  Comparison Report:    {os.path.join(output_dir, 'comparison_report.txt')}\n")
            f.write(f"  Plots:                {os.path.join(output_dir, 'plots/')}\n\n")

            f.write("METRICS EXPLAINED\n")
            f.write("-" * 100 + "\n")
            f.write("  Solve Rate: % of problems solved\n")
            f.write("  Time: Wall clock time (seconds)\n")
            f.write("  Expansions: Nodes expanded during search\n")
            f.write("  H* Preservation: Quality of heuristic function (GNN-specific, 1.0 = perfect)\n")
            f.write("  Efficiency Score: Combined metric (lower = better)\n\n")

            f.write("INTERPRETATION\n")
            f.write("-" * 100 + "\n")
            f.write("  ✓ GOOD:    Solve rate >= 80% with reasonable time\n")
            f.write("  ⚠ FAIR:    Solve rate 60-80% or slow execution\n")
            f.write("  ✗ POOR:    Solve rate < 60%\n\n")

            f.write("=" * 100 + "\n")

        logger.info(f"✓ Report: {report_path}")

        logger.info("\n" + "=" * 100)
        logger.info("EVALUATION COMPLETE")
        logger.info("=" * 100)

        logger.info(f"\n📁 Results: {os.path.abspath(output_dir)}")
        logger.info(f"📊 View: {report_path}")

        return True

    except Exception as e:
        logger.error(f"Report generation failed: {e}")
        return False


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def print_section(title: str, width: int = 100):
    """Print section header."""
    logger.info("")
    logger.info("=" * width)
    logger.info(f"// {title}")
    logger.info("=" * width)
    logger.info("")


# ============================================================================
# MAIN ORCHESTRATOR
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="End-to-End Evaluation Orchestrator",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:

  # Complete evaluation (baseline + GNN)
  python run_full_evaluation.py \\
      --model mvp_output/gnn_model.zip \\
      --domain domain.pddl \\
      --problems "problem_*.pddl" \\
      --output evaluation_results/

  # GNN only (skip baselines)
  python run_full_evaluation.py \\
      --model model.zip \\
      --domain domain.pddl \\
      --problems "problem_*.pddl" \\
      --skip-baselines \\
      --output results/

  # Analyze experiments
  python run_full_evaluation.py \\
      --analyze-experiments \\
      --experiments exp1 exp2 exp3 \\
      --output analysis_results/
        """
    )

    parser.add_argument("--analyze-experiments", action="store_true",
                        help="Analyze completed experiments")

    parser.add_argument("--model", help="Path to GNN model (ZIP)")
    parser.add_argument("--domain", help="Path to domain PDDL")
    parser.add_argument("--problems", help="Glob pattern for problems")
    parser.add_argument("--experiments", nargs='+', help="Experiment directories")

    parser.add_argument("--output", default="evaluation_results", help="Output directory")
    parser.add_argument("--timeout", type=int, default=300, help="Timeout per problem")
    parser.add_argument("--skip-baselines", action="store_true", help="Skip baselines")
    parser.add_argument("--baselines", nargs='+', help="Specific baselines")
    parser.add_argument("--downward-dir", help="Fast Downward directory")

    args = parser.parse_args()

    logger.info("✅ Evaluation Orchestrator Started")

    # ========================================================================
    # EXPERIMENT ANALYSIS MODE
    # ========================================================================

    if args.analyze_experiments:
        if not args.experiments:
            logger.error("--experiments required with --analyze-experiments")
            return 1

        if not EvaluationValidator.validate_all_experiment_analysis(
                args.experiments,
                args.output
        ):
            logger.error("❌ Validation failed")
            return 1

        if not run_experiment_analysis(args.experiments, args.output):
            logger.error("❌ Experiment analysis failed")
            return 1

        if not generate_final_report(args.output):
            logger.error("❌ Report generation failed")
            return 1

        logger.info("✅ EXPERIMENT ANALYSIS PIPELINE COMPLETE")
        return 0

    # ========================================================================
    # STANDALONE EVALUATION MODE
    # ========================================================================

    if not args.domain or not args.problems:
        logger.error("--domain and --problems required for standalone evaluation")
        parser.print_help()
        return 1

    if not args.skip_baselines and not args.model:
        logger.error("--model required unless --skip-baselines")
        return 1

    # Validation
    if not EvaluationValidator.validate_all_standalone(
            args.model or "",
            args.domain,
            args.problems,
            args.output
    ):
        logger.error("❌ Validation failed")
        return 1

    # Run evaluation
    if not run_comprehensive_evaluation(
            args.model or "",
            args.domain,
            args.problems,
            args.output,
            args.timeout,
            args.skip_baselines,
            args.baselines
    ):
        logger.error("❌ Evaluation failed")
        return 1

    # Run analysis
    results_csv = os.path.join(args.output, "evaluation_results.csv")
    if not run_analysis_and_visualization(results_csv, args.output):
        logger.warning("⚠️ Analysis had issues (continuing)")

    # Generate report
    if not generate_final_report(args.output):
        logger.error("❌ Report generation failed")
        return 1

    logger.info("✅ EVALUATION PIPELINE COMPLETE")
    return 0


if __name__ == "__main__":
    sys.exit(main())




--------------------------------------------------------------------------------

