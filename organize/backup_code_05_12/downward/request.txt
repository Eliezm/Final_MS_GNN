do you think that my code is too complex for my project?
which files are the most essential to check the core functionality of my project? give a compact way
please analyze all the project pseudo code pipeline
analyze what are the required codes from my framework needed for the project
offer an extensive description of how to simplify my codebase, to a working, simple state, that the goal of the project will work, and i will easily run my experiments.
i will run the simplification later.

a few notes:
Please bias towards the longest answer you can give me. Include all the details i need, in a long answer.
If my solution, or user's code is too complex, please offer how to simplify it a little.
Please write a number of plans then make one grand plan out of all plans.
Please do not bias towards short answers.
Your response or answer quality doesn't have to be perfect, you just have to be simple and correct.
Try to offer novel perspectives and new ideas.
Ask yourself questions that invoke creativity, offering features, and offering more than is unsaid.
Please still use abstraction, but a little more advanced.
Please summarize in bullet-points or pseudo code my plan, or any other form of abstraction, to to point of implementation.
After you write outlines of my request, please review your outlines and check if you have covered everything relevant, or if I missed anything that helps my goal.
Try to write yourself outlines of my prompt.

The file test_one_step.py code is in the following block:
#!/usr/bin/env python3
"""
FILE: test_one_step_enhanced.py
Test one merge step with full diagnostics.
"""

import os
import sys
import time
from pathlib import Path

from merge_env import MergeEnv

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_ROOT)

# Fix unicode output on Windows
import io

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import logging

# Setup logging without unicode symbols
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("test_one_step_debug.log", encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


def read_fd_log():
    """Read the FD log file for diagnostics."""
    log_paths = [
        "downward/fd_output/log.txt",
        "downward/log.txt",
    ]

    for path in log_paths:
        if os.path.exists(path):
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                logger.info(f"\n=== FD LOG ({path}) ===")
                # Show last 100 lines
                lines = content.split('\n')
                for line in lines[-100:]:
                    logger.info(f"  {line}")
                logger.info("=== END FD LOG ===\n")
                return content
            except Exception as e:
                logger.error(f"Failed to read {path}: {e}")

    logger.warning("No FD log file found")
    return None


def list_output_files():
    """List all files in fd_output and gnn_output."""
    for dir_name in ["downward/fd_output", "downward/gnn_output"]:
        if os.path.isdir(dir_name):
            logger.info(f"\nFiles in {dir_name}:")
            for fname in sorted(os.listdir(dir_name)):
                fpath = os.path.join(dir_name, fname)
                if os.path.isfile(fpath):
                    size = os.path.getsize(fpath)
                    logger.info(f"  {fname:<40} {size:>8} bytes")


# FILE: test_one_step.py
# REPLACE main() function with this:

def main():
    logger.info("=" * 80)
    logger.info("TEST: VERIFY DIFFERENT PROBLEMS PRODUCE DIFFERENT RESULTS")
    logger.info("=" * 80)

    # Test multiple problems sequentially
    test_problems = [
        "benchmarks/small/problem_small_00.pddl",
        "benchmarks/small/problem_small_01.pddl",
        "benchmarks/small/problem_small_10.pddl",
    ]

    results = {}

    for problem_file in test_problems:
        problem_name = os.path.basename(problem_file)

        logger.info(f"\n{'=' * 80}")
        logger.info(f"TEST: {problem_name}")
        logger.info(f"{'=' * 80}\n")

        # Clean old files BEFORE creating environment
        logger.info("Cleaning old files...")
        for dir_name in ["downward/fd_output", "downward/gnn_output"]:
            if os.path.isdir(dir_name):
                for fname in os.listdir(dir_name):
                    fpath = os.path.join(dir_name, fname)
                    try:
                        os.remove(fpath)
                    except:
                        pass

        logger.info("✓ Old files cleaned\n")

        # Create environment
        logger.info(f"Creating MergeEnv for {problem_name}...")
        try:
            env = MergeEnv(
                domain_file="benchmarks/small/domain.pddl",
                problem_file=problem_file,
                max_merges=5,
                debug=False,
                reward_variant='astar_search',
                max_states=4000,
                threshold_before_merge=1
            )
            logger.info("✓ MergeEnv created\n")
        except Exception as e:
            logger.error(f"✗ Failed to create MergeEnv: {e}")
            import traceback
            logger.error(traceback.format_exc())
            continue

        # Reset
        logger.info("Calling reset()...")
        try:
            obs, info = env.reset()
            logger.info("✓ Reset successful\n")
        except Exception as e:
            logger.error(f"✗ Reset failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            env.close()
            continue

        initial_nodes = obs['num_nodes']
        initial_edges = obs['num_edges']

        logger.info(f"Initial state:")
        logger.info(f"  Nodes: {initial_nodes}")
        logger.info(f"  Edges: {initial_edges}\n")

        # Step
        logger.info("Calling step(0)...")
        try:
            obs, reward, done, truncated, info = env.step(0)
            logger.info("✓ Step successful\n")
        except Exception as e:
            logger.error(f"✗ Step failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            env.close()
            continue

        final_nodes = obs['num_nodes']
        delta_states = info.get('delta_states', 0)

        logger.info(f"After step(0):")
        logger.info(f"  Nodes: {final_nodes}")
        logger.info(f"  Reward: {reward:.6f}")
        logger.info(f"  Delta states: {delta_states}\n")

        # Store results
        results[problem_name] = {
            'initial_nodes': initial_nodes,
            'initial_edges': initial_edges,
            'final_nodes': final_nodes,
            'reward': reward,
            'delta_states': delta_states,
        }

        env.close()
        time.sleep(1)  # Wait between problems

    # ✅ VERIFICATION
    logger.info(f"\n{'=' * 80}")
    logger.info("VERIFICATION: PROBLEM-SPECIFIC DIFFERENTIATION")
    logger.info(f"{'=' * 80}\n")

    if not results:
        logger.error("❌ No results collected!")
        return 1

    logger.info("Results by problem:")
    for problem_name, data in sorted(results.items()):
        logger.info(f"\n{problem_name}:")
        logger.info(f"  Initial nodes: {data['initial_nodes']}")
        logger.info(f"  Initial edges: {data['initial_edges']}")
        logger.info(f"  Final nodes:   {data['final_nodes']}")
        logger.info(f"  Reward:        {data['reward']:+.6f}")
        logger.info(f"  Delta states:  {data['delta_states']:+d}")

    # ✅ CHECK FOR DIFFERENCES
    unique_initial_nodes = set(d['initial_nodes'] for d in results.values())
    unique_rewards = set(d['reward'] for d in results.values())

    logger.info(f"\nDifferentiation analysis:")
    logger.info(f"  Unique initial node counts: {sorted(unique_initial_nodes)}")
    logger.info(f"  Unique rewards: {len(unique_rewards)}")

    if len(unique_initial_nodes) == 1:
        logger.error(f"\n❌ FAILURE: All problems have SAME initial node count ({list(unique_initial_nodes)[0]})")
        logger.error("   This indicates different problems are NOT properly loaded")
        return 1

    if len(unique_rewards) == 1:
        logger.error(f"\n⚠️ WARNING: All problems have SAME reward ({list(unique_rewards)[0]:.6f})")
        logger.error("   Check if stale data is being reused")
        return 1

    logger.info(f"\n✅ SUCCESS: Different problems produce DIFFERENT results!")
    logger.info(f"   Initial nodes vary: {sorted(unique_initial_nodes)}")
    logger.info(f"   Rewards vary: {len(unique_rewards)} unique values")

    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

--------------------------------------------------------------------------------

The file logging_utils.py code is in the following block:
# FILE: logging_utils.py
import logging
import sys
from pathlib import Path


def setup_handshake_logging(log_file="handshake_debug.log"):
    """Setup detailed logging for handshake debugging."""

    # Create logger
    logger = logging.getLogger("HANDSHAKE")
    logger.setLevel(logging.DEBUG)

    # Clear existing handlers
    logger.handlers = []

    # Create formatter
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - [%(levelname)s] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    # Console handler (INFO level)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)

    # File handler (DEBUG level)
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    return logger


def log_phase(logger, phase_num, name, symbol="="):
    """Log a phase marker."""
    msg = f"PHASE {phase_num}: {name}"
    logger.info(f"\n{symbol * 80}")
    logger.info(msg)
    logger.info(f"{symbol * 80}\n")


def log_file_check(logger, file_path, operation="checking"):
    """Log file existence check with details."""
    import os
    path = Path(file_path)

    logger.debug(f"[FILE CHECK] {operation}: {path}")

    if path.exists():
        size = path.stat().st_size
        logger.debug(f"  [OK] EXISTS - Size: {size} bytes")
        return True
    else:
        logger.debug(f"  [MISSING] Path: {path.absolute()}")
        return False


def log_signal_export(logger, signal_name, file_path, success=True):
    """Log signal export status."""
    status = "[OK]" if success else "[FAIL]"
    logger.info(f"{status} Signal: {signal_name:<30} -> {Path(file_path).name}")

--------------------------------------------------------------------------------

The file merge_env.py code is in the following block:
# FILE: merge_env.py

import re
import tempfile
import glob
import os
import json
import time
import subprocess
import traceback
from json import JSONDecoder
from typing import Tuple, Dict, Optional
import datetime
import gymnasium as gym
import shutil
import numpy as np
from gymnasium import spaces
from torch.utils.tensorboard import SummaryWriter
import networkx as nx

# At top of merge_env.py, add import:
from communication_protocol import (
    CommConfig,
    send_merge_decision,
    wait_for_ack,
    wait_for_merge_signals,
    wait_for_initial_mapping,
    perform_handshake,
    ensure_communication_directories,
    check_for_error_signal, read_json_robust,
)

from graph_tracker import GraphTracker
from reward_info_extractor import RewardInfoExtractor, MergeInfo
from reward_function_variants import create_reward_function

import logging

from validate_merge_signals import validate_merge_signals
from shared_experiment_utils import print_section, print_subsection



logger = logging.getLogger(__name__)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

TB_LOGDIR = "tb_debug"
writer = SummaryWriter(log_dir=TB_LOGDIR)


def _safe_load_list(path: str, retries: int = 60, delay: float = 0.25) -> list:
    """Robustly load the *first* JSON value from `path` with retries."""
    dec = JSONDecoder()
    last_err = None
    for _ in range(retries):
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                s = f.read().lstrip()
            if not s:
                time.sleep(delay)
                continue
            obj, _ = dec.raw_decode(s)
            return obj if isinstance(obj, list) else []
        except (OSError, json.JSONDecodeError) as e:
            last_err = e
            time.sleep(delay)
    return []


def write_json_atomic(obj, final_path: str):
    """Write `obj` to JSON at `final_path` atomically."""
    dir_ = os.path.dirname(final_path) or "."
    fd, tmp_path = tempfile.mkstemp(dir=dir_, suffix=".tmp")
    with os.fdopen(fd, "w") as f:
        json.dump(obj, f)
        f.flush()
        os.fsync(f.fileno())
    os.replace(tmp_path, final_path)


def _wait_json_complete(path: str, timeout: float = 180.0, poll: float = 0.25) -> bool:
    """Wait until `path` exists and looks complete."""
    deadline = time.time() + timeout
    last_size = -1
    while time.time() < deadline:
        try:
            if not os.path.exists(path):
                time.sleep(poll)
                continue
            size = os.path.getsize(path)
            if size <= 0:
                time.sleep(poll)
                continue

            with open(path, "rb") as f:
                if size > 8192:
                    f.seek(-8192, os.SEEK_END)
                tail = f.read()

            tail = tail.rstrip(b"\r\n\t ")
            if not tail:
                time.sleep(poll)
                continue

            last_byte = tail[-1]
            looks_closed = last_byte in (ord("]"), ord("}"))

            if looks_closed and size == last_size:
                return True

            last_size = size
            time.sleep(poll)
        except OSError:
            time.sleep(poll)
    return False


class MergeEnv(gym.Env):
    """
    Gym environment wrapping Fast Downward's merge-and-shrink heuristic.
    ✅ FIXED: Now properly stores domain and problem file paths.
    """

    metadata = {"render.modes": []}

    def __init__(
            self,
            domain_file: str,
            problem_file: str,
            max_merges: int = 20,
            debug: bool = False,
            reward_variant: str = 'rich',
            max_states: int = 4000,
            threshold_before_merge: int = 1,
            **reward_kwargs
    ) -> None:
        """
        Initialize the merge-and-shrink learning environment.

        Args:
            domain_file: Absolute or relative path to domain.pddl
            problem_file: Absolute or relative path to problem.pddl
            max_merges: Maximum number of merges allowed per episode
            debug: If True, use in-memory debug mode (no real FD)
            reward_variant: Which reward function to use ('rich', 'astar_search', etc.)
            max_states: Max abstract states for M&S algorithm
            threshold_before_merge: Threshold for triggering shrinking
            **reward_kwargs: Additional kwargs for reward function (w_f_stability, etc.)
        """
        super().__init__()

        # ✅ NEW: Ensure communication directories exist IMMEDIATELY
        from communication_protocol import _ensure_directories_exist
        _ensure_directories_exist()

        logger.info(f"[MERGE_ENV INIT] Communication directories verified")

        # ✅ NEW: Verify paths are correct immediately
        self.domain_file = os.path.abspath(domain_file)
        self.problem_file = os.path.abspath(problem_file)

        from common_utils import DOWNWARD_DIR, FD_OUTPUT_DIR, GNN_OUTPUT_DIR

        self.fd_base_dir = DOWNWARD_DIR
        self.fd_output_dir = FD_OUTPUT_DIR
        self.gnn_output_dir = GNN_OUTPUT_DIR

        # ✅ NEW: Log the paths for debugging
        logger.info(f"[INIT::PATHS]")
        logger.info(f"  fd_base_dir:   {self.fd_base_dir}")
        logger.info(f"  fd_output_dir: {self.fd_output_dir}")
        logger.info(f"  gnn_output_dir: {self.gnn_output_dir}")

        # ✅ NEW: Verify directories exist
        for dir_path in [self.fd_output_dir, self.gnn_output_dir]:
            os.makedirs(dir_path, exist_ok=True)
            if not os.path.isdir(dir_path):
                raise RuntimeError(f"Cannot access directory: {dir_path}")

        logger.info(f"Environment initialized with:")
        logger.info(f"  Domain:  {self.domain_file}")
        logger.info(f"  Problem: {self.problem_file}")
        logger.info(f"  FD base: {self.fd_base_dir}")

        # ✅ Store M&S hyperparameters
        self.max_states = max_states
        self.threshold_before_merge = threshold_before_merge
        logger.info(f"  max_states: {self.max_states}")
        logger.info(f"  threshold_before_merge: {self.threshold_before_merge}")

        # ✅ Setup reward function with ALL parameters
        self.max_merges = max(1, max_merges)
        try:
            self.reward_function = create_reward_function(reward_variant, **reward_kwargs)
            logger.info(f"✓ Initialized reward function: {reward_variant}")
        except Exception as e:
            logger.error(f"Failed to initialize reward function '{reward_variant}': {e}")
            raise

        # ✅ Setup signal extraction
        self.reward_info_extractor = RewardInfoExtractor(fd_output_dir=os.path.join(self.fd_base_dir, "fd_output"))
        logger.info("✓ Initialized reward info extractor")

        # ✅ Initialize state variables
        self.current_merge_step = 0
        self.process = None  # FD subprocess
        self.fd_log_file = None
        self.graph_tracker: GraphTracker = None

        # ✅ Setup logging directory
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("logs", exist_ok=True)
        self.log_path = f"logs/run_{timestamp}.jsonl"

        # ✅ NEW: GNN METADATA COLLECTION (using common paths)
        self.gnn_metadata_dir = os.path.join(self.gnn_output_dir, "metadata")
        os.makedirs(self.gnn_metadata_dir, exist_ok=True)
        self.gnn_decisions_log = []


        # Verify output directories
        self._verify_output_directories()

        # ✅ Initialize observation tracking
        self.prev_total_states = 0
        self.max_vars = 1
        self.max_iter = 1
        self.centrality = {}

        # ✅ Define observation and action spaces (for gym.Env)
        self.feat_dim = 19  # Number of node features per node ✅ NEW: Expanded feature set
        # self.observation_space = spaces.Dict({
        #     "x": spaces.Box(0.0, 1.0, shape=(100, self.feat_dim), dtype=np.float32),
        #     "edge_index": spaces.Box(0, 100, shape=(2, 1000), dtype=np.int64),
        #     "num_nodes": spaces.Box(0, 100, shape=(), dtype=np.int32),
        #     "num_edges": spaces.Box(0, 1000, shape=(), dtype=np.int32),
        # })
        # ✅ UPDATED: Add edge features to observation space
        self.observation_space = spaces.Dict({
            "x": spaces.Box(0.0, 1.0, shape=(100, self.feat_dim), dtype=np.float32),
            "edge_index": spaces.Box(0, 100, shape=(2, 1000), dtype=np.int64),
            "edge_features": spaces.Box(  # ✅ NEW
                -1.0, 1.0,
                shape=(1000, 8),
                dtype=np.float32
            ),
            "num_nodes": spaces.Box(0, 100, shape=(), dtype=np.int32),
            "num_edges": spaces.Box(0, 1000, shape=(), dtype=np.int32),
        })
        self.action_space = spaces.Discrete(1000)  # Max 1000 possible merges

        # ✅ Store debug mode
        self.debug = debug

        # ✅ NEW: Initialize caching attributes
        self._obs_cache = None
        self._last_graph_hash = None
        self._last_num_nodes = 0
        self._last_num_edges = 0
        self._edge_features_cache = None
        self._edge_features_cache_hash = None

    def _verify_output_directories(self):
        """Verify all output directories are writable."""
        dirs_to_check = [
            self.fd_output_dir,
            self.gnn_output_dir,
        ]

        for dir_path in dirs_to_check:
            if not os.path.exists(dir_path):
                logger.error(f"❌ Directory does not exist: {dir_path}")
                raise RuntimeError(f"Required directory missing: {dir_path}")

            # Test write permission
            test_file = os.path.join(dir_path, ".write_test")
            try:
                with open(test_file, 'w') as f:
                    f.write("test")
                os.remove(test_file)
                logger.info(f"✓ Directory writable: {dir_path}")
            except Exception as e:
                logger.error(f"❌ Cannot write to directory: {dir_path}")
                raise RuntimeError(f"Cannot write to {dir_path}: {e}")

    # FILE: merge_env.py - REPLACE reset()

    # FILE: merge_env.py
    # REPLACE the entire reset() function with this:

    # FILE: merge_env.py
    # REPLACE THE ENTIRE reset() METHOD WITH THIS

    def reset(self, *, seed=None, options=None) -> Tuple[Dict, Dict]:
        """
        ✅ COMPLETE RESET WITH AGGRESSIVE CLEANUP AND FRESH DATA VALIDATION
        Ensures no stale data from previous problems is reused.
        """
        import time
        import glob
        from logging_utils import setup_handshake_logging, log_phase

        logger_main = setup_handshake_logging("reset_debug.log")
        logger_main.info(f"\n{'=' * 80}")
        logger_main.info(f"RESET STARTING - Problem: {os.path.basename(self.problem_file)}")
        logger_main.info(f"{'=' * 80}\n")

        # ========================================================================
        # PHASE 0: AGGRESSIVE CLEANUP - KILL PROCESS & DELETE ALL SIGNALS
        # ========================================================================
        log_phase(logger_main, 0, "AGGRESSIVE CLEANUP - PROCESS & SIGNALS")

        # Step 1: KILL FD process (not graceful)
        try:
            if self.process and self.process.poll() is None:
                logger_main.info("Killing old FD process...")
                self.process.kill()  # KILL, not terminate
                time.sleep(0.5)
                try:
                    self.process.wait(timeout=2.0)
                except subprocess.TimeoutExpired:
                    logger_main.warning("Process still running after kill, forcing...")
                    if os.name == 'nt':
                        os.system(f"taskkill /PID {self.process.pid} /F 2>nul")
                    else:
                        os.system(f"kill -9 {self.process.pid}")
                logger_main.info("✅ FD process killed")
        except Exception as e:
            logger_main.warning(f"Process kill error (continuing): {e}")
        finally:
            self.process = None

        # Step 2: Close log file
        try:
            if self.fd_log_file:
                self.fd_log_file.flush()
                self.fd_log_file.close()
        except:
            pass
        finally:
            self.fd_log_file = None

        # Step 3: Aggressively clean ALL signal files
        logger_main.info("Cleaning ALL signal files from previous runs...")

        signal_patterns = [
            "*.json",  # ✅ CRITICAL: Delete ALL JSON files to ensure fresh start
            "*.tmp",
        ]

        cleaned_count = 0
        for folder in [self.fd_output_dir, self.gnn_output_dir]:
            if not os.path.isdir(folder):
                continue

            for pattern in signal_patterns:
                for filepath in glob.glob(os.path.join(folder, pattern)):
                    # Skip log files
                    if filepath.endswith("debug.log"):
                        continue

                    try:
                        os.remove(filepath)
                        cleaned_count += 1
                        logger_main.debug(f"Cleaned: {os.path.basename(filepath)}")
                    except Exception as e:
                        logger_main.warning(f"Could not remove {filepath}: {e}")

        logger_main.info(f"✅ Cleaned {cleaned_count} files total")

        # Verify directories are truly empty (except logs)
        fd_files = [f for f in glob.glob(os.path.join(self.fd_output_dir, "*"))
                    if os.path.isfile(f) and not f.endswith("debug.log")]
        gnn_files = glob.glob(os.path.join(self.gnn_output_dir, "*"))

        if fd_files or gnn_files:
            logger_main.warning(f"⚠️ {len(fd_files)} FD files, {len(gnn_files)} GNN files remain")
        else:
            logger_main.info("✓ Signal directories verified empty")

        super().reset(seed=seed)

        if self.debug:
            logger_main.info("DEBUG MODE: Skipping real FD")
            return self._reset_debug()

        # ========================================================================
        # PHASE 1: DIRECTORY SETUP
        # ========================================================================
        log_phase(logger_main, 1, "DIRECTORY SETUP")

        for d in [self.fd_output_dir, self.gnn_output_dir]:
            os.makedirs(d, exist_ok=True)
            if not os.path.isdir(d):
                raise RuntimeError(f"Cannot create directory: {d}")
            logger_main.debug(f"✅ Directory ready: {d}")

        # ========================================================================
        # PHASE 2: LAUNCH FD PROCESS
        # ========================================================================
        log_phase(logger_main, 2, "LAUNCH FD PROCESS")

        fd_log_path = os.path.join(self.fd_base_dir, "fd_output", "log.txt")
        self.fd_log_file = open(fd_log_path, "a", buffering=1)

        # Run translator
        translator_script = os.path.join(
            self.fd_base_dir, "builds", "release", "bin", "translate", "translate.py"
        )

        if not os.path.exists(translator_script):
            raise RuntimeError(f"Translator not found: {translator_script}")

        import sys
        cmd = [
            sys.executable,
            translator_script,
            os.path.abspath(self.domain_file),
            os.path.abspath(self.problem_file),
            "--sas-file", "output.sas"
        ]

        logger_main.info(f"Running translator...")
        try:
            translate_result = subprocess.run(
                cmd,
                cwd=self.fd_base_dir,
                stdout=self.fd_log_file,
                stderr=self.fd_log_file,
                timeout=60
            )
            if translate_result.returncode != 0:
                raise RuntimeError(f"Translator failed: {translate_result.returncode}")
            logger_main.info("✅ Translator completed")
        except subprocess.TimeoutExpired:
            raise RuntimeError("Translator timeout")

        # Verify SAS file
        sas_path = os.path.join(self.fd_base_dir, "output.sas")
        if not os.path.exists(sas_path):
            raise RuntimeError("SAS file not created")

        logger_main.info(f"✅ SAS file ready ({os.path.getsize(sas_path)} bytes)")

        # Read SAS content
        with open(sas_path, "r") as f:
            sas_content = f.read()

        # Launch FD process
        downward_cmd = [
            os.path.join(self.fd_base_dir, "builds/release/bin/downward.exe"),
            "--search", (
                "astar(merge_and_shrink("
                "merge_strategy=merge_gnn(),"
                "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
                "label_reduction=exact(before_shrinking=true,before_merging=false),"
                f"max_states={self.max_states},"
                f"threshold_before_merge={self.threshold_before_merge}))"
            )
        ]

        logger_main.info(f"Launching FD process...")
        try:
            self.process = subprocess.Popen(
                downward_cmd,
                cwd=self.fd_base_dir,
                stdin=subprocess.PIPE,
                stdout=self.fd_log_file,
                stderr=self.fd_log_file,
                text=True,
                bufsize=1
            )
            logger_main.info(f"✅ FD process launched (PID: {self.process.pid})")

            # Write SAS to stdin
            self.process.stdin.write(sas_content)
            self.process.stdin.close()
            logger_main.info("✅ SAS written to FD stdin")
        except Exception as e:
            raise RuntimeError(f"FD launch failed: {e}")

        # ========================================================================
        # PHASE 3: HANDSHAKE WITH FRESH FILE VALIDATION
        # ========================================================================
        log_phase(logger_main, 3, "HANDSHAKE - VERIFY FRESH FILES")

        # ✅ CRITICAL: Record handshake start time
        handshake_start_time = time.time()
        logger_main.info(f"Handshake start timestamp: {handshake_start_time}")

        cg_path = os.path.join(self.fd_output_dir, "causal_graph.json")

        # Force delete any stale causal_graph.json
        logger_main.info("Deleting any stale causal_graph.json...")
        for attempt in range(3):
            try:
                if os.path.exists(cg_path):
                    os.remove(cg_path)
                    logger_main.info(f"✓ Deleted stale file (attempt {attempt + 1})")
                    break
            except PermissionError:
                time.sleep(0.2)
            except Exception as e:
                logger_main.warning(f"Delete attempt {attempt + 1} failed: {e}")

        time.sleep(0.2)

        # Wait for FRESH causal_graph.json
        logger_main.info("Waiting for fresh causal_graph.json...")
        deadline = time.time() + 120.0
        file_created_fresh = False

        while time.time() < deadline:
            if os.path.exists(cg_path):
                file_mtime = os.path.getmtime(cg_path)
                file_age = file_mtime - handshake_start_time

                logger_main.debug(f"File exists, mtime={file_mtime}, age={file_age:.2f}s")

                # ✅ CRITICAL: Verify file was created AFTER handshake started
                if file_mtime >= handshake_start_time - 1.0:
                    logger_main.info(f"✅ Fresh causal_graph.json detected (age: {file_age:.2f}s)")
                    file_created_fresh = True
                    break
                else:
                    # File is stale - re-delete and retry
                    logger_main.warning(f"Stale file detected (age: {file_age:.2f}s), re-deleting...")
                    try:
                        os.remove(cg_path)
                    except:
                        pass
                    time.sleep(0.5)
            else:
                time.sleep(0.2)

        if not file_created_fresh:
            elapsed = time.time() - handshake_start_time
            raise TimeoutError(f"FD failed to create fresh causal_graph.json after {elapsed:.1f}s")

        logger_main.info("✅ Handshake complete - fresh files verified")

        # ========================================================================
        # PHASE 4: INITIALIZE GRAPHTRACKER
        # ========================================================================
        log_phase(logger_main, 4, "INITIALIZE GRAPHTRACKER")

        ts_path = os.path.join(self.fd_output_dir, "merged_transition_systems.json")
        cg_path = os.path.join(self.fd_output_dir, "causal_graph.json")

        try:
            self.graph_tracker = GraphTracker(
                ts_json_path=ts_path,
                cg_json_path=cg_path
            )
            logger_main.info(f"✅ GraphTracker created")
            logger_main.info(f"   Nodes: {self.graph_tracker.graph.number_of_nodes()}")
            logger_main.info(f"   Edges: {self.graph_tracker.graph.number_of_edges()}")
        except Exception as e:
            logger_main.error(f"GraphTracker init failed: {e}")
            raise

        # ✅ CRITICAL: Invalidate ALL caches for new problem
        logger_main.info("Invalidating ALL caches for new problem...")
        self.graph_tracker._invalidate_caches()
        self._obs_cache = None
        self._last_graph_hash = None
        self._edge_features_cache = None
        self._edge_features_cache_hash = None
        logger_main.info("✅ All caches invalidated")

        # ========================================================================
        # PHASE 5: SYNC FD INDICES
        # ========================================================================
        log_phase(logger_main, 5, "SYNC FD INDICES")

        initial_mapping_path = os.path.join(
            self.fd_output_dir, "fd_index_mapping_-1.json"
        )

        max_wait = 30
        elapsed = 0
        while not os.path.exists(initial_mapping_path) and elapsed < max_wait:
            time.sleep(0.5)
            elapsed += 0.5

        if os.path.exists(initial_mapping_path):
            logger_main.info(f"Found initial mapping after {elapsed:.1f}s")
            if self.graph_tracker.sync_fd_indices_from_mapping(initial_mapping_path):
                logger_main.info("✅ FD indices synced")
            else:
                logger_main.warning("⚠️ FD index sync returned False")
        else:
            logger_main.warning(f"⚠️ Initial mapping not found after {elapsed:.1f}s")

        # ========================================================================
        # PHASE 6: BUILD INITIAL OBSERVATION
        # ========================================================================
        log_phase(logger_main, 6, "BUILD INITIAL OBSERVATION")

        self.current_merge_step = 0
        self.prev_total_states = 0
        self.gnn_decisions_log = []

        G = self.graph_tracker.graph
        self.max_vars = max(
            (len(d.get("incorporated_variables", [])) for _, d in G.nodes(data=True)),
            default=1
        ) or 1
        self.max_iter = max(
            (d.get("iteration", 0) for _, d in G.nodes(data=True)),
            default=0
        ) or 1

        try:
            self.centrality = self.graph_tracker.get_centrality(force_recompute=True)
            logger_main.info(f"Centrality computed for {len(self.centrality)} nodes")
        except Exception as e:
            logger_main.warning(f"Centrality computation failed: {e}")
            self.centrality = {}

        obs = self._get_observation()
        self.prev_total_states = self._count_total_states()
        self.state = obs

        logger_main.info(f"✅ Initial observation built")
        logger_main.info(f"   num_nodes: {obs.get('num_nodes')}")
        logger_main.info(f"   num_edges: {obs.get('num_edges')}")
        logger_main.info(f"   total_states: {self.prev_total_states}")

        logger_main.info(f"\n{'=' * 80}")
        logger_main.info("✅ RESET SUCCESSFUL")
        logger_main.info(f"{'=' * 80}\n")

        return obs, {}

    def _reset_debug(self) -> Tuple[Dict, Dict]:
        """
        ✅ DEBUG MODE: Reset using mock/toy data without launching real FD.
        """
        logger.info("[RESET DEBUG] Using debug mode with mock data")

        # Try to use TOY_TS environment variable, otherwise create minimal mock
        toy_dir = os.environ.get("TOY_TS", None)

        if toy_dir and os.path.isdir(toy_dir):
            ts_path = os.path.join(toy_dir, "merged_transition_systems.json")
            cg_path = os.path.join(toy_dir, "causal_graph.json")
            logger.info(f"[RESET DEBUG] Using TOY_TS directory: {toy_dir}")
        else:
            # Create minimal mock data in a temporary location
            import tempfile
            toy_dir = tempfile.mkdtemp(prefix="ms_debug_")

            ts_path = os.path.join(toy_dir, "merged_transition_systems.json")
            cg_path = os.path.join(toy_dir, "causal_graph.json")

            # Create minimal transition systems (2 atomic systems)
            ts_data = [
                {
                    "iteration": -1,
                    "num_states": 2,
                    "init_state": 0,
                    "goal_states": [1],
                    "incorporated_variables": [0],
                    "transitions": [],
                    "f_before": [0, 1]
                },
                {
                    "iteration": -1,
                    "num_states": 3,
                    "init_state": 0,
                    "goal_states": [2],
                    "incorporated_variables": [1],
                    "transitions": [],
                    "f_before": [0, 1, 2]
                }
            ]

            cg_data = {"edges": [{"from": 0, "to": 1}]}

            with open(ts_path, 'w') as f:
                json.dump(ts_data, f)
            with open(cg_path, 'w') as f:
                json.dump(cg_data, f)

            logger.info(f"[RESET DEBUG] Created mock data in: {toy_dir}")

        # Initialize timestamp and log path
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_path = f"logs/run_{timestamp}.jsonl"

        # Initialize graph tracker with mock data
        try:
            self.graph_tracker = GraphTracker(
                ts_json_path=ts_path,
                cg_json_path=cg_path,
                is_debug=True
            )
            logger.info("[RESET DEBUG] ✓ GraphTracker initialized with mock data")
        except Exception as e:
            logger.error(f"[RESET DEBUG] ✗ GraphTracker init failed: {e}")
            raise

        # Initialize metrics
        self.current_merge_step = 0
        self.prev_total_states = 0
        self.gnn_decisions_log = []
        self.process = None  # No real process in debug mode

        G = self.graph_tracker.graph
        self.max_vars = max(
            (len(d.get("incorporated_variables", [])) for _, d in G.nodes(data=True)),
            default=1
        ) or 1
        self.max_iter = max(
            (d.get("iteration", 0) for _, d in G.nodes(data=True)),
            default=0
        ) or 1

        try:
            self.centrality = self.graph_tracker.get_centrality(force_recompute=True)
        except Exception as e:
            logger.warning(f"Centrality computation failed: {e}")
            self.centrality = {}

        # Build initial observation
        obs = self._get_observation()
        self.prev_total_states = self._count_total_states()
        self.state = obs

        logger.info("[RESET DEBUG] ✅ Debug reset complete")
        logger.info(f"[RESET DEBUG] Graph has {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\n")

        return obs, {}

    # FILE: merge_env.py - ENHANCED step() for debugging

    # FILE: merge_env.py
    # IN THE step() METHOD, REPLACE THE SIGNAL WAITING SECTION

    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        """✅ COMPLETE STEP WITH FRESH SIGNAL VALIDATION"""
        import logging

        logger = logging.getLogger("HANDSHAKE")
        logger.info(f"\n{'=' * 80}")
        logger.info(f"STEP {self.current_merge_step}: MERGE EXECUTION")
        logger.info(f"{'=' * 80}\n")

        try:
            # ====================================================================
            # PHASE 1: VALIDATE ACTION
            # ====================================================================
            logger.info(f"[STEP] PHASE 1: Validate action")

            edges = list(self.graph_tracker.graph.edges())
            if not edges:
                logger.error("[STEP] ❌ No edges available")
                return self.state, 0.0, True, False, {"error": "no_edges"}

            action_idx = int(np.clip(action, 0, len(edges) - 1))
            src, tgt = edges[action_idx]
            logger.info(f"[STEP] Action {action} → edge ({src}, {tgt})")
            logger.debug(f"[STEP] Available edges: {len(edges)}")

            # ====================================================================
            # PHASE 2: GET FD INDICES
            # ====================================================================
            logger.info(f"[STEP] PHASE 2: Get FD indices")

            try:
                src_fd, tgt_fd = self.graph_tracker.get_fd_indices_for_merge(src, tgt)
                logger.info(f"[STEP] ✅ FD indices: ({src_fd}, {tgt_fd})")
            except KeyError as e:
                logger.error(f"[STEP] ❌ Cannot get FD indices: {e}")
                return self.state, 0.0, True, False, {"error": "no_fd_index"}

            # ====================================================================
            # PHASE 3: SEND MERGE DECISION
            # ====================================================================
            logger.info(f"[STEP] PHASE 3: Send merge decision")

            try:
                decision_path = os.path.join(
                    self.gnn_output_dir,
                    f"merge_{self.current_merge_step}.json"
                )
                logger.debug(f"[STEP] Writing to: {decision_path}")

                send_merge_decision(
                    iteration=self.current_merge_step,
                    merge_pair=(src_fd, tgt_fd),
                    problem=os.path.basename(self.problem_file)
                )

                logger.info(f"[STEP] ✅ Decision sent")

            except Exception as e:
                logger.error(f"[STEP] ❌ Failed to send decision: {e}")
                return self.state, 0.0, True, False, {"error": "send_failed"}

            # ====================================================================
            # PHASE 4: WAIT FOR ACK
            # ====================================================================
            logger.info(f"[STEP] PHASE 4: Wait for ACK")

            try:
                ack = wait_for_ack(
                    iteration=self.current_merge_step,
                    fd_process=self.process,
                    timeout_s=30.0
                )
                logger.info(f"[STEP] ✅ ACK received")

            except TimeoutError as e:
                logger.error(f"[STEP] ❌ ACK TIMEOUT: {e}")
                return self.state, 0.0, True, False, {"error": "ack_timeout"}
            except Exception as e:
                logger.error(f"[STEP] ❌ ACK ERROR: {e}")
                return self.state, 0.0, True, False, {"error": "ack_failed"}

            # ====================================================================
            # PHASE 5: WAIT FOR MERGE SIGNALS - ✅ WITH FRESH DATA VALIDATION
            # ====================================================================
            logger.info(f"[STEP] PHASE 5: Wait for merge signals")

            # ✅ CRITICAL: Record signal start time to validate freshness
            signal_start_time = time.time()

            try:
                merge_before, merge_after, ts_data, fd_mapping = wait_for_merge_signals(
                    iteration=self.current_merge_step,
                    fd_process=self.process
                )

                # ✅ CRITICAL: Verify signals are actually new (not stale)
                signal_age = time.time() - signal_start_time
                logger.info(f"[STEP] ✅ Merge signals received (retrieved in {signal_age:.2f}s)")

                # Validate that signals contain actual data
                if not ts_data or "iteration" not in ts_data:
                    logger.error("[STEP] ❌ Invalid TS data received")
                    return self.state, 0.0, True, False, {"error": "invalid_signals"}

                logger.debug(f"[STEP] merge_before keys: {list(merge_before.keys())}")
                logger.debug(f"[STEP] merge_after keys: {list(merge_after.keys())}")
                logger_main = logging.getLogger("HANDSHAKE")

            except Exception as e:
                logger.error(f"[STEP] ❌ Signals wait failed: {e}")
                return self.state, 0.0, True, False, {"error": "signals_failed"}

            # ====================================================================
            # PHASE 6: UPDATE GRAPH
            # ====================================================================
            logger.info(f"[STEP] PHASE 6: Update graph")

            try:
                if fd_mapping:
                    logger.debug("[STEP] Syncing FD indices from new mapping")
                    mapping_path = os.path.join(
                        self.fd_base_dir, "fd_output",
                        f"fd_index_mapping_{self.current_merge_step}.json"
                    )
                    self.graph_tracker.sync_fd_indices_from_mapping(mapping_path)

                # ✅ CRITICAL: merge_nodes now returns (success, reason)
                success, rejection_reason = self.graph_tracker.merge_nodes([src, tgt])

                if not success:
                    logger.error(f"[STEP] ❌ Graph merge rejected: {rejection_reason}")
                    return self.state, 0.0, True, False, {"error": "merge_rejected"}

                logger.info(f"[STEP] ✅ Graph updated")

            except Exception as e:
                logger.error(f"[STEP] ❌ Graph update failed: {e}")
                return self.state, 0.0, True, False, {"error": "graph_update_failed"}

            # ====================================================================
            # PHASE 7: EXTRACT REWARD
            # ====================================================================
            logger.info(f"[STEP] PHASE 7: Extract reward")

            reward = 0.0
            merge_info = None

            try:
                merge_info = self.reward_info_extractor.extract_merge_info(
                    iteration=self.current_merge_step,
                    timeout=10.0
                )

                if merge_info is None:
                    logger.warning("[STEP] ⚠️ Failed to extract merge info")
                    reward = 0.0
                else:
                    reward = self.reward_function.compute(
                        merge_info=merge_info,
                        search_expansions=merge_info.nodes_expanded,
                        plan_cost=merge_info.solution_cost,
                        is_terminal=False
                    )
                    logger.info(f"[STEP] ✅ Reward: {reward:.4f}")

            except Exception as e:
                logger.warning(f"[STEP] ⚠️ Reward computation failed: {e}")
                reward = 0.0

            # ====================================================================
            # PHASE 8: BUILD OBSERVATION
            # ====================================================================
            logger.info(f"[STEP] PHASE 8: Build observation")

            try:
                obs = self._get_observation()
                num_remaining = self.graph_tracker.graph.number_of_nodes()
                done = (num_remaining <= 1) or (self.current_merge_step >= self.max_merges - 1)

                logger.info(f"[STEP] ✅ Observation built")
                logger.debug(f"[STEP] Nodes remaining: {num_remaining}")
                logger.debug(f"[STEP] Done: {done}")

            except Exception as e:
                logger.error(f"[STEP] ❌ Observation build failed: {e}")
                return self.state, 0.0, True, False, {"error": "obs_build_failed"}

            # ====================================================================
            # PHASE 9: INCREMENT AND LOG
            # ====================================================================
            self.current_merge_step += 1
            self.state = obs

            info = {
                "merge_pair": [int(src), int(tgt)],
                "num_nodes": num_remaining,
                "step": self.current_merge_step - 1,
            }
            if merge_info:
                info.update(merge_info.to_dict())

            logger.info(f"[STEP] ✅ STEP COMPLETE")
            logger.info(f"[STEP] Next step: {self.current_merge_step}\n")

            return obs, float(reward), done, False, info

        except Exception as e:
            logger.error(f"[STEP] ❌ CRITICAL ERROR: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self.state, 0.0, True, False, {"error": str(e)[:100]}

    def _save_gnn_decision_metadata(
            self,
            merge_step: int,
            action: int,
            src: int,
            tgt: int,
            obs: Dict,
            reward: float,
            info: Dict
    ) -> None:
        """
        ✅ PHASE 2: Save GNN decision metadata for analysis.

        Format compatible with merge_metadata evaluation framework.
        """
        try:
            import json
            from datetime import datetime

            metadata = {
                'episode_step': self.current_merge_step,
                'merge_step': merge_step,
                'action_index': int(action),
                'chosen_edge': [int(src), int(tgt)],
                'observation_shape': {
                    'num_nodes': int(obs.get('num_nodes', 0)),
                    'num_edges': int(obs.get('num_edges', 0)),
                    'node_features_dim': int(obs['x'].shape[-1]) if obs['x'].ndim > 1 else 0,
                },
                'reward_received': float(reward),
                'merge_info': {
                    'plan_cost': info.get('solution_cost', 0),
                    'num_expansions': info.get('nodes_expanded', 0),
                    'delta_states': info.get('delta_states', 0),
                    'f_stability': info.get('f_value_stability', 0),
                },
                'timestamp': datetime.now().isoformat(),
                'problem': os.path.basename(self.problem_file),
            }

            self.gnn_decisions_log.append(metadata)

        except Exception as e:
            logger.debug(f"Could not save GNN metadata: {e}")

    def _export_episode_metadata(self) -> None:
        """
        ✅ PHASE 2: Export all GNN decisions from this episode.

        Creates episode_TIMESTAMP_DECISIONS.json in downward/gnn_metadata/
        """
        if not self.gnn_decisions_log:
            return

        try:
            import json
            from datetime import datetime

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            episode_metadata = {
                'problem': os.path.basename(self.problem_file),
                'num_decisions': len(self.gnn_decisions_log),
                'decisions': self.gnn_decisions_log,
                'export_timestamp': datetime.now().isoformat(),
            }

            metadata_file = os.path.join(
                self.gnn_metadata_dir,
                f"episode_{timestamp}_{len(self.gnn_decisions_log)}_decisions.json"
            )

            with open(metadata_file, 'w') as f:
                json.dump(episode_metadata, f, indent=2, default=str)

            logger.info(f"✓ Exported GNN episode metadata: {metadata_file}")

        except Exception as e:
            logger.warning(f"Failed to export episode metadata: {e}")

    def _handshake_with_fd(self) -> Tuple[str, str]:
        """Launch FD and wait for initial JSONs."""
        dw_dir = os.path.abspath("downward")
        if not os.path.isdir(dw_dir):
            raise RuntimeError(f"Downward folder not found: {dw_dir}")

        def _tail(path: str, n: int = 120) -> str:
            if not os.path.exists(path):
                return "(no log file yet)"
            try:
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    lines = f.readlines()
                return "".join(lines[-n:])
            except Exception as e:
                return f"(failed to read log tail: {e})"

        def _robust_copy(src: str, dst: str, tries: int = 80, delay: float = 0.1):
            for _ in range(tries):
                try:
                    with open(src, "rb") as fin, open(dst, "wb") as fout:
                        fout.write(fin.read())
                    return
                except OSError:
                    time.sleep(delay)
            raise RuntimeError(f"Could not copy {src} -> {dst}")

        def _wait_stable(path: str, grace: float = 0.15, timeout: float = 30.0) -> bool:
            end = time.time() + timeout
            last = -1
            while time.time() < end:
                if os.path.exists(path):
                    try:
                        sz = os.path.getsize(path)
                    except OSError:
                        sz = -1
                    if sz > 0 and sz == last:  # File exists and size hasn't changed
                        return True
                    last = sz
                time.sleep(grace)
            return False

        ## STEP 1: Clean up old files
        logger.info("[Handshake] Cleaning up old files...")
        for fname in ("causal_graph.json", "merged_transition_systems.json", "output.sas"):
            p = os.path.join(dw_dir, fname)
            if os.path.exists(p):
                try:
                    os.remove(p)
                except Exception as e:
                    logger.warning(f"⚠️ Could not delete {p}: {e}")

        for subdir_path in [self.gnn_output_dir, self.fd_output_dir]:
            # subdir_path = os.path.join(dw_dir, subdir_name)
            os.makedirs(subdir_path, exist_ok=True)
            for f in os.listdir(subdir_path):
                try:
                    os.remove(os.path.join(subdir_path, f))
                except Exception:
                    pass

        ## STEP 2: Safely terminate any previous FD process
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                self.process.wait(timeout=3.0)
        except (subprocess.TimeoutExpired, AttributeError):
            if hasattr(self, 'process') and self.process:
                self.process.kill()
        except Exception:
            pass

        ## STEP 3: Build the FD command with GNN merge strategy
        # ✅ NEW: Build FD command from scratch using stored file paths
        fd_translate_bin = os.path.join(dw_dir, "builds/release/bin/translate/translate.py")
        fd_downward_exe = os.path.join(dw_dir, "builds/release/bin/downward.exe")

        # ✅ Use stored paths (absolute)
        domain_path = self.domain_file
        problem_path = self.problem_file

        logger.info(f"[Handshake] Building FD command:")
        logger.info(f"  Domain:  {domain_path}")
        logger.info(f"  Problem: {problem_path}")

        # ✅ Build complete FD command with merge_gnn strategy
        cmd = (
            f'python "{fd_translate_bin}" "{domain_path}" "{problem_path}" '
            f'--sas-file output.sas && '
            f'"{fd_downward_exe}" '
            r'--search "astar(merge_and_shrink('
            r'merge_strategy=merge_gnn(),'  # ✅ Uses GNN for merge decisions
            r'shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),'
            r'label_reduction=exact(before_shrinking=true,before_merging=false),'
            f'max_states={self.max_states},'  # ✅ Use stored parameter
            f'threshold_before_merge={self.threshold_before_merge}'  # ✅ Use stored parameter
            r'))" < output.sas'
        )

        logger.info(f"[Handshake] FD Command: {cmd[:150]}...")

        ## STEP 4: Launch FD process
        fd_dir = os.path.join(dw_dir, "fd_output")
        log_path = os.path.join(fd_dir, "log.txt")
        self.fd_log_file = open(log_path, "w", buffering=1, encoding="utf-8")

        self.process = subprocess.Popen(
            cmd,
            shell=True,
            cwd=dw_dir,
            stdout=self.fd_log_file,
            stderr=self.fd_log_file,
        )

        ## STEP 5: Wait for translator
        logger.info("[Handshake] Waiting for translator to produce 'output.sas'...")
        sas_path = os.path.join(dw_dir, "output.sas")
        if not _wait_stable(sas_path, timeout=60.0):
            tail = _tail(log_path)
            raise RuntimeError(f"Translator failed to produce output.sas.\nLog tail:\n{tail}")
        logger.info("[Handshake] 'output.sas' is ready.")

        ## STEP 6: Wait for initial JSONs
        logger.info("[Handshake] Waiting for planner to produce initial JSONs...")
        cg_root = os.path.join(dw_dir, "causal_graph.json")
        ts_root = os.path.join(dw_dir, "merged_transition_systems.json")
        start, timeout_s = time.time(), 240.0

        while not (os.path.exists(cg_root) and os.path.exists(ts_root)):
            if self.process and (self.process.poll() is not None):
                tail = _tail(log_path)
                raise RuntimeError(
                    f"FD exited early (code {self.process.returncode}) while waiting for JSONs.\nLog tail:\n{tail}")
            if time.time() - start > timeout_s:
                tail = _tail(log_path)
                raise RuntimeError(f"Timeout waiting for initial JSONs after {timeout_s:.0f}s.\nLog tail:\n{tail}")
            time.sleep(0.1)

        ## STEP 7: Copy and verify
        logger.info("[Handshake] Initial JSONs found. Verifying and copying...")
        if not (_wait_stable(cg_root) and _wait_stable(ts_root)):
            tail = _tail(log_path)
            raise RuntimeError(f"Root JSONs never stabilized.\nLog tail:\n{tail}")

        fd_cg = os.path.join(fd_dir, "causal_graph.json")
        fd_ts = os.path.join(fd_dir, "merged_transition_systems.json")

        dec = JSONDecoder()
        for _ in range(80):
            _robust_copy(cg_root, fd_cg)
            _robust_copy(ts_root, fd_ts)
            try:
                with open(fd_ts, "r", encoding="utf-8", errors="ignore") as f:
                    s = f.read().lstrip()
                if not s:
                    time.sleep(0.1)
                    continue
                obj, _ = dec.raw_decode(s)
                break
            except json.JSONDecodeError:
                time.sleep(0.1)
        else:
            tail = _tail(log_path)
            raise RuntimeError("Copied TS JSON is not parseable.\n" + tail)

        logger.info("[Handshake] Handshake complete.")
        return fd_ts, fd_cg

    def _log_step(self, src: int, tgt: int, info: dict, reward: float, done: bool):
        """Append merge-step metrics to log file."""
        total_states = self._count_total_states()

        entry = {
            "step": int(self.current_merge_step),
            "merge_choice": [int(src), int(tgt)],
            "plan_cost": int(info.get("plan_cost", 0)),
            "num_expansions": int(info.get("num_expansions", 0)),
            "num_significant_f_changes": int(info.get("num_significant_f_changes", 0)),
            "delta_states": int(info.get("delta_states", 0)),
            "total_states": int(total_states),
            "reward": float(reward),
            "done": bool(done),
            "timestamp": time.time(),
        }

        with open(self.log_path, "a") as f:
            json.dump(entry, f)
            f.write("\n")

    # ============================================================================
    # MODIFY step() METHOD - ADD COMMUNICATION
    # ============================================================================

    def _cleanup_process(self) -> None:
        """Clean up FD process safely."""
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                try:
                    self.process.wait(timeout=5.0)
                except subprocess.TimeoutExpired:
                    self.process.kill()
        except Exception as e:
            logger.warning(f"Error cleaning up process: {e}")

    def _step_debug(self, src: int, tgt: int):
        """
        ✅ ENHANCED: In-memory merge (debug mode) with detailed logging.

        Shows every step of the merge process with:
        - State before and after
        - Computation details
        - Graph metrics
        - Reward calculation
        """
        logger.info("\n" + "=" * 90)
        logger.info(f"STEP {self.current_merge_step}: DEBUG MODE MERGE")
        logger.info("=" * 90)

        try:
            # ====================================================================
            # PHASE 1: ENTRY LOGGING & VALIDATION
            # ====================================================================

            logger.info("\n[PHASE 1] ENTRY VALIDATION")
            logger.info(f"  Merge indices: ({src}, {tgt})")
            logger.info(f"  Current merge step: {self.current_merge_step}")
            logger.info(f"  Max merges allowed: {self.max_merges}")

            # ====================================================================
            # PHASE 2: PRE-MERGE STATE SNAPSHOT
            # ====================================================================

            logger.info("\n[PHASE 2] PRE-MERGE STATE SNAPSHOT")

            # Count total states before merge
            old_total = self._count_total_states()
            logger.info(f"  Total states before merge: {old_total}")

            # Log node details
            G = self.graph_tracker.graph
            logger.info(f"  Num nodes before: {len(G.nodes)}")
            logger.info(f"  Num edges before: {len(G.edges)}")

            # Log specific nodes being merged
            if src in G.nodes and tgt in G.nodes:
                src_data = G.nodes[src]
                tgt_data = G.nodes[tgt]
                logger.info(f"\n  Node {src} (source):")
                logger.info(f"    - num_states: {src_data.get('num_states', 'N/A')}")
                logger.info(f"    - iteration: {src_data.get('iteration', 'N/A')}")
                logger.info(f"    - incorporated_variables: {src_data.get('incorporated_variables', 'N/A')}")

                logger.info(f"\n  Node {tgt} (target):")
                logger.info(f"    - num_states: {tgt_data.get('num_states', 'N/A')}")
                logger.info(f"    - iteration: {tgt_data.get('iteration', 'N/A')}")
                logger.info(f"    - incorporated_variables: {tgt_data.get('incorporated_variables', 'N/A')}")

            # ====================================================================
            # PHASE 3: PERFORM MERGE
            # ====================================================================

            logger.info("\n[PHASE 3] PERFORMING MERGE")
            logger.info(f"  Calling graph_tracker.merge_nodes([{src}, {tgt}])...")

            self.graph_tracker.merge_nodes([src, tgt])
            self.current_merge_step += 1

            logger.info(f"  ✓ Merge completed, current_merge_step = {self.current_merge_step}")

            # ====================================================================
            # PHASE 4: POST-MERGE STATE SNAPSHOT
            # ====================================================================

            logger.info("\n[PHASE 4] POST-MERGE STATE SNAPSHOT")

            new_total = self._count_total_states()
            delta_states = new_total - old_total

            logger.info(f"  Total states after merge: {new_total}")
            logger.info(f"  Delta states (new - old): {delta_states}")
            logger.info(f"  State explosion ratio: {delta_states / max(old_total, 1):.4f}")

            # Log updated graph
            G = self.graph_tracker.graph
            logger.info(f"  Num nodes after: {len(G.nodes)}")
            logger.info(f"  Num edges after: {len(G.edges)}")

            # ====================================================================
            # PHASE 5: UPDATE GRAPH METRICS
            # ====================================================================

            logger.info("\n[PHASE 5] UPDATE GRAPH METRICS")

            self.max_vars = max(
                (len(d.get("incorporated_variables", [])) for _, d in G.nodes(data=True)),
                default=1
            ) or 1
            logger.info(f"  max_vars: {self.max_vars}")

            self.max_iter = max(
                (d.get("iteration", 0) for _, d in G.nodes(data=True)),
                default=0
            ) or 1
            logger.info(f"  max_iter: {self.max_iter}")

            self.centrality = nx.closeness_centrality(G)
            logger.info(f"  Centrality computed for {len(self.centrality)} nodes")

            # ====================================================================
            # PHASE 6: COMPUTE REWARD
            # ====================================================================

            logger.info("\n[PHASE 6] REWARD COMPUTATION")

            reward = -max(abs(delta_states), 0.1)
            logger.info(f"  Reward formula: -max(|delta_states|, 0.1)")
            logger.info(f"  Computed reward: {reward:.4f}")

            # ====================================================================
            # PHASE 7: TENSORBOARD LOGGING
            # ====================================================================

            logger.info("\n[PHASE 7] TENSORBOARD LOGGING")

            try:
                writer.add_scalar("env/num_nodes", len(self.graph_tracker.graph.nodes),
                                  self.current_merge_step)
                logger.info(f"  ✓ Logged num_nodes: {len(self.graph_tracker.graph.nodes)}")

                writer.add_scalar("env/num_edges", len(self.graph_tracker.graph.edges),
                                  self.current_merge_step)
                logger.info(f"  ✓ Logged num_edges: {len(self.graph_tracker.graph.edges)}")

                writer.add_scalar("env/total_states", new_total, self.current_merge_step)
                logger.info(f"  ✓ Logged total_states: {new_total}")

                writer.add_scalar("env/delta_states", delta_states, self.current_merge_step)
                logger.info(f"  ✓ Logged delta_states: {delta_states}")

                writer.add_scalar("env/reward", reward, self.current_merge_step)
                logger.info(f"  ✓ Logged reward: {reward:.4f}")
            except Exception as e:
                logger.warning(f"  ⚠️ TensorBoard logging error: {e}")

            # ====================================================================
            # PHASE 8: BUILD OBSERVATION
            # ====================================================================

            logger.info("\n[PHASE 8] BUILD OBSERVATION")

            obs = self._get_observation()
            logger.info(f"  Observation built successfully")
            logger.info(f"  - x shape: {obs['x'].shape}")
            logger.info(f"  - edge_index shape: {obs['edge_index'].shape}")
            logger.info(f"  - num_nodes: {obs['num_nodes']}")
            logger.info(f"  - num_edges: {obs['num_edges']}")

            # ====================================================================
            # PHASE 9: TERMINATION CHECK
            # ====================================================================

            logger.info("\n[PHASE 9] TERMINATION CHECK")

            num_remaining = len(G.nodes)
            done = (num_remaining <= 1) or (self.current_merge_step >= self.max_merges)

            logger.info(f"  Num remaining nodes: {num_remaining}")
            logger.info(f"  Current step: {self.current_merge_step} / {self.max_merges}")
            logger.info(f"  Done: {done}")

            if done:
                if num_remaining <= 1:
                    logger.info(f"  Reason: Only {num_remaining} node(s) left")
                else:
                    logger.info(f"  Reason: Max merges reached")

            # ====================================================================
            # PHASE 10: BUILD INFO DICT
            # ====================================================================

            logger.info("\n[PHASE 10] BUILD INFO DICT")

            info = {
                "num_nodes": len(self.graph_tracker.graph.nodes),
                "num_edges": len(self.graph_tracker.graph.edges),
                "total_states": new_total,
                "delta_states": delta_states,
                "plan_cost": 0,
                "num_expansions": 0,
                "error": None,
            }
            logger.info(f"  Info dict built with {len(info)} keys")

            # ====================================================================
            # PHASE 11: LOG STEP & UPDATE STATE
            # ====================================================================

            logger.info("\n[PHASE 11] LOGGING & STATE UPDATE")

            self._log_step(src, tgt, info, reward, done)
            logger.info(f"  ✓ Step logged to file")

            self.state = obs
            logger.info(f"  ✓ State updated")

            # ====================================================================
            # PHASE 12: RETURN
            # ====================================================================

            logger.info("\n[PHASE 12] RETURN")
            logger.info(f"  Returning: obs, reward={reward:.4f}, done={done}, info")
            logger.info("=" * 90 + "\n")



            return obs, reward, done, False, info

        except Exception as e:
            logger.error(f"\n❌ ERROR in _step_debug: {e}")
            logger.error(f"Traceback:\n{traceback.format_exc()}")
            logger.info("=" * 90 + "\n")
            return self.state, 0.0, True, False, {"error": str(e)}



    def _step_real(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        """
        ✅ ENHANCED: Uses fingerprint-based FD index synchronization
        """
        logger.info("\n" + "=" * 90)
        logger.info(f"STEP {self.current_merge_step}: REAL MODE")
        logger.info("=" * 90)
        logger.info(f"FD output dir: {self.fd_output_dir}")
        logger.info(f"GNN output dir: {self.gnn_output_dir}")

        # Verify directories exist
        import os
        assert os.path.isdir(self.fd_output_dir), f"Missing: {self.fd_output_dir}"
        assert os.path.isdir(self.gnn_output_dir), f"Missing: {self.gnn_output_dir}"

        try:
            # ====================================================================
            # PHASE 0: SYNC FD INDICES FROM LATEST MAPPING ✅ FIXED: EARLIER SYNC
            # ====================================================================

            logger.info("\n[PHASE 0] SYNCHRONIZING FD INDICES")

            # ✅ FIX: Get INITIAL mapping BEFORE any merges started
            initial_mapping_path = os.path.join(
                self.fd_base_dir,
                "fd_output",
                "fd_index_mapping_-1.json"  # ✅ Initial state mapping
            )

            # Get the latest mapping file (from previous iterations)
            latest_mapping = initial_mapping_path
            for i in range(self.current_merge_step - 1, -2, -1):  # ✅ Check -1 first
                mapping_path = os.path.join(
                    self.fd_base_dir,
                    "fd_output",
                    f"fd_index_mapping_{i}.json"
                )
                if os.path.exists(mapping_path):
                    latest_mapping = mapping_path
                    logger.info(f"  Found mapping: iteration {i}")
                    break

            if os.path.exists(latest_mapping):
                if not self.graph_tracker.sync_fd_indices_from_mapping(latest_mapping):
                    logger.error("Failed to sync FD indices!")
                    return self.state, 0.0, True, False, {"error": "sync_failed"}
                logger.info(f"  ✓ Successfully synced from: {latest_mapping}")
            else:
                logger.warning(f"  ⚠️ No initial mapping found, may cause index errors")

            # ====================================================================
            # PHASE 1: VALIDATE INPUT & EXTRACT MERGE PAIR
            # ====================================================================

            logger.info("\n[PHASE 1] INPUT VALIDATION")

            edges = list(self.graph_tracker.graph.edges())
            if not edges:
                logger.warning("No edges available")
                return self.state, 0.0, True, False, {"error": "no_edges"}

            action = int(action)
            num_valid_edges = len(edges)
            action_idx = max(0, min(action % max(num_valid_edges, 1), num_valid_edges - 1))

            node_a, node_b = edges[action_idx]
            logger.info(f"  Graph nodes: {node_a}, {node_b}")

            # ✅ GET FD INDICES FROM SYNCHRONIZED STATE
            try:
                src_fd, tgt_fd = self.graph_tracker.get_fd_indices_for_merge(node_a, node_b)
                logger.info(f"  FD indices: ({src_fd}, {tgt_fd})")
            except KeyError as e:
                logger.error(f"  ❌ Cannot get FD indices: {e}")
                return self.state, 0.0, True, False, {"error": "no_fd_index"}

            # ====================================================================
            # PHASE 2: PRE-MERGE STATE ANALYSIS
            # ====================================================================

            logger.info(f"\n[PHASE 2] PRE-MERGE STATE ANALYSIS")

            # ✅ NEW: Store node IDs for later use in logging
            src = node_a
            tgt = node_b

            src_data = self.graph_tracker.graph.nodes[src]
            tgt_data = self.graph_tracker.graph.nodes[tgt]
            src_size = src_data.get('num_states', 1)
            tgt_size = tgt_data.get('num_states', 1)
            expected_product = src_size * tgt_size

            pre_merge_total = sum(d.get('num_states', 0) for _, d in self.graph_tracker.graph.nodes(data=True))
            logger.info(f"    - Total states before merge: {pre_merge_total}")
            logger.info(f"    - Num nodes: {len(self.graph_tracker.graph.nodes)}")
            logger.info(f"    - Num edges: {len(self.graph_tracker.graph.edges)}")
            logger.info(f"\n    Nodes being merged:")
            logger.info(f"      - Node {src}: {src_size} states")
            logger.info(f"      - Node {tgt}: {tgt_size} states")
            logger.info(f"      - Expected product: {expected_product} states")

            # ====================================================================
            # PHASE 3: WRITE MERGE DECISION (using FD indices)
            # ====================================================================

            logger.info(f"\n[PHASE 3] WRITE MERGE DECISION")

            merge_decision = {
                "iteration": self.current_merge_step,
                "merge_pair": [int(src_fd), int(tgt_fd)],  # ✅ Use FD indices, not node IDs
                "timestamp": time.time()
            }

            # gnn_output_dir = os.path.abspath(os.path.join(self.fd_base_dir, "gnn_output"))
            # os.makedirs(gnn_output_dir, exist_ok=True)
            #
            # merge_decision_path = os.path.join(gnn_output_dir, f"merge_{self.current_merge_step}.json")

            # Use the instance variable (already initialized in __init__)
            merge_decision_path = os.path.join(self.gnn_output_dir, f"merge_{self.current_merge_step}.json")

            # ✅ Write atomically
            try:
                fd, temp_path = tempfile.mkstemp(
                    dir=self.gnn_output_dir,
                    suffix=".json",
                    prefix=f"merge_{self.current_merge_step}_"
                )
                with os.fdopen(fd, 'w') as f:
                    json.dump(merge_decision, f, indent=2)
                    f.flush()
                    os.fsync(f.fileno())

                os.replace(temp_path, merge_decision_path)
                logger.info(f"  ✓ Wrote merge decision: {merge_decision_path}")

            except Exception as e:
                logger.error(f"  ❌ Failed to write merge decision: {e}")
                raise


            # ====================================================================
            # PHASE 4: WAIT FOR FD ACKNOWLEDGMENT
            # ====================================================================

            logger.info(f"\n[PHASE 4] WAIT FOR FD ACKNOWLEDGMENT")

            ack_path = os.path.join(
                self.fd_base_dir,
                "fd_output",
                f"gnn_ack_{self.current_merge_step}.json"
            )

            logger.info(f"  Expected ACK file: {ack_path}")

            start_time = time.time()
            ACK_TIMEOUT = 30.0

            while time.time() - start_time < ACK_TIMEOUT:
                elapsed = time.time() - start_time

                if os.path.exists(ack_path):
                    try:
                        with open(ack_path) as f:
                            ack_data = json.load(f)
                        elapsed = time.time() - start_time
                        logger.info(f"  ✓ ACK received after {elapsed:.2f}s")
                        break
                    except json.JSONDecodeError:
                        time.sleep(0.1)
                        continue

                if int(elapsed) > 0 and int(elapsed) % 5 == 0:
                    logger.debug(f"  Still waiting... ({elapsed:.0f}s)")

                time.sleep(0.1)
            else:
                elapsed = time.time() - start_time
                raise TimeoutError(
                    f"FD did not acknowledge merge within {ACK_TIMEOUT}s (waited {elapsed:.1f}s)")

            # ====================================================================
            # PHASE 5: WAIT FOR UPDATED TS FILE (ENHANCED WITH DIAGNOSTICS)
            # ====================================================================

            logger.info(f"\n[PHASE 5] WAIT FOR UPDATED TS FILE")

            # ✅ CRITICAL: Use the NEXT iteration number, not current
            # (because we incremented after writing the merge decision)
            expected_ts_iteration = self.current_merge_step

            ts_path = os.path.join(
                self.fd_output_dir,
                f"ts_{expected_ts_iteration}.json"
            )

            logger.info(f"  Expected TS file: {ts_path}")
            logger.info(f"  (iteration counter: {expected_ts_iteration})")

            start_time = time.time()
            TS_TIMEOUT = 60.0

            while time.time() - start_time < TS_TIMEOUT:
                elapsed = time.time() - start_time

                if os.path.exists(ts_path):
                    try:
                        with open(ts_path) as f:
                            ts_data = json.load(f)
                        elapsed = time.time() - start_time
                        logger.info(f"  ✓ TS file received after {elapsed:.2f}s")
                        break
                    except (json.JSONDecodeError, IOError) as e:
                        logger.debug(f"  JSON not ready yet: {e}")
                        time.sleep(0.2)
                        continue

                # ✅ NEW: Check if FD crashed
                if self.process and self.process.poll() is not None:
                    rc = self.process.returncode
                    logger.error(f"  ❌ FD PROCESS DIED with code {rc}")
                    logger.error(f"  Cannot get TS file: {ts_path}")
                    self._diagnose_fd_output(expected_ts_iteration)
                    return self.state, 0.0, True, False, {"error": "fd_crashed"}

                if int(elapsed) > 0 and int(elapsed) % 10 == 0:
                    logger.debug(f"  Still waiting... ({elapsed:.0f}s)")

                time.sleep(0.2)
            else:
                elapsed = time.time() - start_time
                logger.error(f"  ❌ TIMEOUT waiting for TS file after {elapsed:.1f}s")
                self._diagnose_fd_output(expected_ts_iteration)
                raise TimeoutError(
                    f"TS file not produced within {TS_TIMEOUT}s: {ts_path}")

            # ====================================================================
            # PHASE 6: UPDATE GRAPH TRACKER
            # ====================================================================

            # ====================================================================
            # PHASE 6: UPDATE GRAPH TRACKER - ✅ FIXED: Handle return value
            # ====================================================================

            logger.info(f"\n[PHASE 6] UPDATE GRAPH TRACKER")
            logger.info(f"  Calling graph_tracker.merge_nodes([{src}, {tgt}])...")

            try:
                # ✅ FIX: merge_nodes now returns (success, reason)
                success, rejection_reason = self.graph_tracker.merge_nodes([src, tgt])

                if not success:
                    # ✅ Merge was rejected - treat as failed step
                    logger.error(f"  ❌ Merge rejected: {rejection_reason}")
                    reward = -0.5  # Penalize invalid merge
                    done = True
                    info = {"error": "merge_rejected", "message": rejection_reason}
                    obs = self._get_observation()
                    self._log_step(src, tgt, info, reward, done)
                    return obs, reward, done, False, info

                logger.info(f"  ✓ Merge succeeded in graph tracker")

            except Exception as e:
                logger.error(f"  ❌ Graph merge failed: {e}")
                raise

            # ====================================================================
            # PHASE 7: EXTRACT SIGNALS & COMPUTE REWARD
            # ====================================================================

            # ✅ Initialize merge_info before try block
            merge_info = None
            reward = 0.0

            logger.info(f"\n[PHASE 7] EXTRACT SIGNALS & COMPUTE REWARD")
            logger.info(f"  Calling reward_info_extractor.extract_merge_info(iteration={self.current_merge_step})...")

            try:
                merge_info = self.reward_info_extractor.extract_merge_info(
                    iteration=self.current_merge_step,
                    timeout=10.0
                )

                if merge_info is None:
                    logger.warning(f"  ⚠️ Failed to extract merge info, using defaults")
                    reward = 0.0
                else:
                    logger.info(f"  ✓ Merge info extracted successfully")

                    # ✅ ENHANCED: Log extracted signals with bad merge context
                    logger.info(f"\n  Extracted signals:")
                    logger.info(f"    - iteration: {merge_info.iteration}")
                    logger.info(f"    - states_before: {merge_info.states_before}")
                    logger.info(f"    - states_after: {merge_info.states_after}")
                    logger.info(f"    - delta_states: {merge_info.delta_states}")
                    logger.info(f"    - f_value_stability: {merge_info.f_value_stability:.4f}")
                    logger.info(f"    - num_significant_f_changes: {merge_info.num_significant_f_changes}")
                    logger.info(f"    - state_explosion_penalty: {merge_info.state_explosion_penalty:.4f}")
                    logger.info(f"    - nodes_expanded: {merge_info.nodes_expanded}")
                    logger.info(f"    - search_depth: {merge_info.search_depth}")
                    logger.info(f"    - solution_cost: {merge_info.solution_cost}")
                    logger.info(f"    - branching_factor: {merge_info.branching_factor:.4f}")
                    logger.info(f"    - solution_found: {merge_info.solution_found}")

                    # ✅ NEW: Pre-reward bad merge diagnostics
                    logger.info(f"\n  [BAD MERGE DIAGNOSTIC CHECKS]:")

                    # Check 1: State explosion
                    if merge_info.states_after > expected_product * 1.5:
                        logger.warning(
                            f"    ⚠️  State explosion detected: {merge_info.states_after} > {expected_product * 1.5:.0f}")

                    # Check 2: Goal reachability
                    goal_reachable = any(f != float('inf') and f < 1_000_000_000 for f in merge_info.f_after)
                    if not goal_reachable:
                        logger.error(f"    ❌ CRITICAL: Goal unreachable after merge!")

                    # Check 3: F-stability
                    if merge_info.f_value_stability < 0.3:
                        logger.warning(f"    ⚠️  Poor F-stability: {merge_info.f_value_stability:.4f}")

                    # Check 4: Unreachable states
                    unreachable_count = sum(1 for f in merge_info.f_after if f == float('inf') or f >= 1_000_000_000)
                    unreachable_ratio = unreachable_count / max(merge_info.states_after, 1)
                    if unreachable_ratio > 0.7:
                        logger.warning(f"    ⚠️  High unreachability: {unreachable_ratio * 100:.1f}%")

                    # Check 5: Branching factor
                    if merge_info.branching_factor > 8.0:
                        logger.warning(f"    ⚠️  High branching factor: {merge_info.branching_factor:.4f}")

                    logger.info(f"\n  Computing reward:")
                    logger.info(f"    - Reward function: {self.reward_function.name}")
                    logger.info(f"    - Calling compute() with merge_info and signals...")

                    reward = self.reward_function.compute(
                        merge_info=merge_info,
                        search_expansions=merge_info.nodes_expanded,
                        plan_cost=merge_info.solution_cost,
                        is_terminal=False
                    )

                    logger.info(f"    ✓ Reward computed: {reward:.4f}")

                    # Log component breakdown
                    components = self.reward_function.get_components_dict()
                    logger.info(f"\n  Reward component breakdown:")
                    for key, val in components.items():
                        logger.info(f"    - {key:<30} {val:.4f}")

            except Exception as e:
                logger.warning(f"  ⚠️ Reward computation failed: {e}")
                logger.warning(f"  Using reward = 0.0")
                reward = 0.0

            # ====================================================================
            # PHASE 8: CHECK EPISODE TERMINATION
            # ====================================================================

            logger.info(f"\n[PHASE 8] TERMINATION CHECK")

            num_remaining = self.graph_tracker.graph.number_of_nodes()
            logger.info(f"  Num remaining nodes: {num_remaining}")
            logger.info(f"  Current merge step: {self.current_merge_step} / {self.max_merges}")

            done = (num_remaining <= 1) or (self.current_merge_step >= self.max_merges - 1)
            logger.info(f"  Done: {done}")

            if done:
                if num_remaining <= 1:
                    logger.info(f"  Reason: Only {num_remaining} node(s) remaining")
                if self.current_merge_step >= self.max_merges - 1:
                    logger.info(f"  Reason: Max merges reached")

            # ====================================================================
            # PHASE 9: BUILD OBSERVATION
            # ====================================================================

            logger.info(f"\n[PHASE 9] BUILD OBSERVATION")

            try:
                obs = self._get_observation()
                logger.info(f"  ✓ Observation built successfully")
                logger.info(f"    - x shape: {obs['x'].shape}")
                logger.info(f"    - edge_index shape: {obs['edge_index'].shape}")
                logger.info(f"    - num_nodes: {obs['num_nodes']}")
                logger.info(f"    - num_edges: {obs['num_edges']}")
            except Exception as e:
                logger.error(f"  ❌ Failed to build observation: {e}")
                obs = self._get_observation()

                # ====================================================================
                # PHASE 10: INCREMENT STEP COUNTER (BEFORE SYNC - CRITICAL ORDER)
                # ====================================================================

                logger.info(f"\n[PHASE 10] INCREMENT STEP COUNTER")
                logger.info(f"  Before: current_merge_step = {self.current_merge_step}")
                self.current_merge_step += 1
                logger.info(f"  After: current_merge_step = {self.current_merge_step}")

                # ✅ CRITICAL: Sync AFTER incrementing
                latest_mapping = os.path.join(
                    self.fd_base_dir,
                    "fd_output",
                    f"fd_index_mapping_{self.current_merge_step - 1}.json"
                )
                if os.path.exists(latest_mapping):
                    if not self.graph_tracker.sync_fd_indices_from_mapping(latest_mapping):
                        logger.warning(f"  ⚠️ Could not sync FD indices after merge")

            # ====================================================================
            # PHASE 11: BUILD INFO DICT & LOG
            # ====================================================================

            logger.info(f"\n[PHASE 11] BUILD INFO DICT & LOG STEP")

            info = {
                "merge_pair": [int(src), int(tgt)],
                "num_nodes": num_remaining,
                "step": self.current_merge_step - 1,
            }

            if merge_info is not None:
                is_valid, issues = validate_merge_signals(merge_info)
                if not is_valid:
                    logger.error(f"[SIGNAL VALIDATION] ✗ Merge signals invalid: {issues}")
                    merge_info = None

                info.update(merge_info.to_dict())
                logger.info(f"  ✓ Added merge_info to info dict")

            self._log_step(src, tgt, info, reward, done)
            logger.info(f"  ✓ Step logged")

            # ====================================================================
            # PHASE 12: RETURN
            # ====================================================================

            # In MergeEnv.step(), after computing reward:

            # ✅ ADD THIS BEFORE RETURNING
            self._save_gnn_decision_metadata(
                merge_step=self.current_merge_step,
                action=action,
                src=src,
                tgt=tgt,
                obs=obs,
                reward=reward,
                info=info
            )

            logger.info(f"\n[PHASE 12] RETURN RESULT")
            logger.info(f"  Returning:")
            logger.info(f"    - obs: shape {obs['x'].shape}")
            logger.info(f"    - reward: {reward:.4f}")
            logger.info(f"    - done: {done}")
            logger.info(f"    - truncated: False")
            logger.info(f"    - info: {len(info)} keys")
            logger.info("=" * 90 + "\n")

            return obs, float(reward), done, False, info

        except Exception as e:
            logger.error(f"\n❌ STEP FAILED: {e}")
            logger.error(f"Traceback:\n{traceback.format_exc()}")
            logger.info("=" * 90 + "\n")
            return self.state, 0.0, True, False, {"error": str(e)}

    def _wait_for_json_file_safe(self, path: str, step: int, timeout_seconds: float = 60.0) -> Optional[Dict]:
        """
        ✅ ROBUST: Wait for JSON file with proper validation and diagnostics.

        NOW WITH PHASE TRACKING to identify exactly where handshake breaks down.

        Returns: Parsed JSON dict, or None if timeout/error occurs
        """
        start_time = time.time()
        last_size = -1
        last_modified = -1
        consecutive_parse_errors = 0
        max_consecutive_errors = 3

        # ✅ Track which phase of handshake we're in
        file_basename = os.path.basename(path)

        if "gnn_ack" in file_basename:
            phase_name = "ACK"
            phase_desc = "FD acknowledging merge decision"
        elif "ts_" in file_basename:
            phase_name = "TS"
            phase_desc = "Updated transition system"
        elif "merge_before" in file_basename:
            phase_name = "BEFORE"
            phase_desc = "Pre-merge metrics"
        elif "merge_after" in file_basename:
            phase_name = "AFTER"
            phase_desc = "Post-merge metrics"
        else:
            phase_name = "DATA"
            phase_desc = "Data file"

        logger.info(f"[Step {step}] [PHASE: {phase_name}] Starting wait...")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Waiting for: {path}")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Description: {phase_desc}")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Timeout: {timeout_seconds}s")

        while time.time() - start_time < timeout_seconds:
            elapsed = time.time() - start_time

            if not os.path.exists(path):
                if int(elapsed) > 0 and int(elapsed) % 15 == 0:  # Every 15 seconds
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] Still waiting... ({elapsed:.0f}s)")
                time.sleep(0.5)
                continue

            try:
                # ✅ Check file size and modification time
                current_size = os.path.getsize(path)
                current_mtime = os.path.getmtime(path)

                if current_size == 0:
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] File exists but EMPTY (0 bytes)")
                    consecutive_parse_errors += 1
                    if consecutive_parse_errors >= max_consecutive_errors:
                        logger.error(
                            f"[Step {step}] [PHASE: {phase_name}] File empty for {consecutive_parse_errors} checks - giving up")
                        return None
                    time.sleep(1.0)
                    continue

                # ✅ Wait for file to stabilize (size and mtime haven't changed)
                if current_size == last_size and current_mtime == last_modified:
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] File stable at {current_size} bytes, parsing...")

                    try:
                        with open(path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        if not content.strip():
                            logger.warning(f"[Step {step}] [PHASE: {phase_name}] File content is empty/whitespace")
                            consecutive_parse_errors += 1
                            time.sleep(1.0)
                            continue

                        # ✅ Validate JSON structure
                        content_stripped = content.strip()
                        if not (content_stripped.startswith('{') or content_stripped.startswith('[')):
                            logger.error(
                                f"[Step {step}] [PHASE: {phase_name}] Invalid JSON: doesn't start with {{ or [")
                            logger.error(f"[Step {step}] [PHASE: {phase_name}] First 100 chars: {content[:100]}")
                            consecutive_parse_errors += 1
                            time.sleep(1.0)
                            continue

                        # ✅ Try to parse
                        data = json.loads(content)

                        logger.info(
                            f"[Step {step}] [PHASE: {phase_name}] ✅ Successfully parsed JSON ({current_size} bytes)")
                        logger.info(f"[Step {step}] [PHASE: {phase_name}] Elapsed time: {elapsed:.1f}s")
                        consecutive_parse_errors = 0  # Reset on success
                        return data

                    except json.JSONDecodeError as e:
                        logger.warning(
                            f"[Step {step}] [PHASE: {phase_name}] JSON parse error (line {e.lineno}, col {e.colno}): {e.msg}")
                        logger.debug(f"[Step {step}] [PHASE: {phase_name}] Content preview: {content[:200]}")
                        consecutive_parse_errors += 1

                        if consecutive_parse_errors >= max_consecutive_errors:
                            logger.error(
                                f"[Step {step}] [PHASE: {phase_name}] JSON parsing failed {max_consecutive_errors} times - giving up")
                            return None

                        time.sleep(2.0)
                        continue

                else:
                    # File size or mtime changed - still writing
                    logger.debug(
                        f"[Step {step}] [PHASE: {phase_name}] File size changing: {last_size} → {current_size} bytes (still writing)")
                    last_size = current_size
                    last_modified = current_mtime
                    consecutive_parse_errors = 0
                    time.sleep(0.5)
                    continue

            except (OSError, IOError) as e:
                logger.debug(f"[Step {step}] [PHASE: {phase_name}] File I/O error: {e}")
                consecutive_parse_errors += 1
                time.sleep(1.0)
                continue

            except Exception as e:
                logger.error(f"[Step {step}] [PHASE: {phase_name}] Unexpected error: {e}", exc_info=True)
                return None

        # Check if FD died
        if self.process and self.process.poll() is not None:
            rc = self.process.returncode
            logger.error(f"[Step {step}] [PHASE: {phase_name}] ❌ FD process DIED with code {rc}")
            logger.error(
                f"[Step {step}] [PHASE: {phase_name}] FD crashed before producing {os.path.basename(path)}")
            return None

        # TIMEOUT
        logger.error(
            f"[Step {step}] [PHASE: {phase_name}] ❌ TIMEOUT after {timeout_seconds}s waiting for {os.path.basename(path)}")
        logger.error(f"[Step {step}] [PHASE: {phase_name}] This indicates:")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   1. FD process crashed or hung")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   2. JSON file not being written by FD")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   3. File I/O or permission problem")

        # Diagnostic: list what files DO exist
        fd_output_dir = "downward/fd_output"
        if os.path.exists(fd_output_dir):
            logger.error(f"[Step {step}] [PHASE: {phase_name}] Files in fd_output/:")
            try:
                for fname in sorted(os.listdir(fd_output_dir)):
                    fpath = os.path.join(fd_output_dir, fname)
                    if os.path.isfile(fpath):
                        size = os.path.getsize(fpath)
                        logger.error(f"[Step {step}] [PHASE: {phase_name}]   - {fname} ({size} bytes)")
            except:
                pass

        return None

    def _diagnose_fd_output(self, step: int) -> None:
        """✅ NEW: Detailed diagnostics when FD output fails."""
        fd_output_dir = "downward/fd_output"

        logger.info(f"\n[DIAGNOSIS] Checking FD output state for step {step}...")

        # Check directory
        if not os.path.exists(fd_output_dir):
            logger.error(f"[DIAGNOSIS] fd_output directory DOES NOT EXIST")
            return

        logger.info(f"[DIAGNOSIS] Files in {fd_output_dir}:")
        try:
            for fname in sorted(os.listdir(fd_output_dir)):
                fpath = os.path.join(fd_output_dir, fname)
                if os.path.isfile(fpath):
                    size = os.path.getsize(fpath)
                    mtime = os.path.getmtime(fpath)
                    age_sec = time.time() - mtime
                    logger.info(f"  - {fname:<30} {size:>10} bytes (age: {age_sec:.1f}s)")
        except Exception as e:
            logger.error(f"[DIAGNOSIS] Error listing files: {e}")

        # Check for expected files
        expected_ts = os.path.join(fd_output_dir, f"ts_{step}.json")
        expected_before = os.path.join(fd_output_dir, f"merge_before_{step}.json")
        expected_after = os.path.join(fd_output_dir, f"merge_after_{step}.json")

        logger.info(f"[DIAGNOSIS] Expected files for step {step}:")
        for expected_path in [expected_ts, expected_before, expected_after]:
            exists = os.path.exists(expected_path)
            status = "✓ EXISTS" if exists else "✗ MISSING"
            logger.info(f"  {status}: {os.path.basename(expected_path)}")

            if exists:
                try:
                    size = os.path.getsize(expected_path)
                    with open(expected_path, 'r') as f:
                        content = f.read()
                    logger.info(f"    Size: {size} bytes")
                    logger.info(f"    Preview: {content[:100]}...")
                except Exception as e:
                    logger.warning(f"    Error reading: {e}")

        # Check FD process
        if self.process:
            retcode = self.process.poll()
            if retcode is not None:
                logger.error(f"[DIAGNOSIS] FD process has EXITED with code {retcode}")
            else:
                logger.info(f"[DIAGNOSIS] FD process is RUNNING (PID: {self.process.pid})")

    def _diagnose_signals(self, iteration: int):
        """✅ NEW: Verify signal correctness."""
        before_path = os.path.join("downward", "fd_output", f"merge_before_{iteration}.json")
        after_path = os.path.join("downward", "fd_output", f"merge_after_{iteration}.json")

        if not os.path.exists(before_path) or not os.path.exists(after_path):
            logger.error(f"Signal files missing for iteration {iteration}")
            logger.error(f"  Before path: {before_path} (exists: {os.path.exists(before_path)})")
            logger.error(f"  After path: {after_path} (exists: {os.path.exists(after_path)})")
            return False

        try:
            with open(before_path) as f:
                before = json.load(f)
            with open(after_path) as f:
                after = json.load(f)
        except Exception as e:
            logger.error(f"Failed to load signal files: {e}")
            return False

        # ✅ VERIFICATION 1: State count consistency
        ts1_size = before.get("ts1_size", 0)
        ts2_size = before.get("ts2_size", 0)
        expected_merged_size = ts1_size * ts2_size
        actual_merged_size = after.get("num_states", 0)

        logger.info(f"[VERIFY] Iteration {iteration}:")
        logger.info(f"  TS1 size: {ts1_size}, TS2 size: {ts2_size}")
        logger.info(f"  Expected merged size: {expected_merged_size}")
        logger.info(f"  Actual merged size: {actual_merged_size}")

        if actual_merged_size > expected_merged_size * 1.1:
            logger.warning(f"  ⚠️ Merged size larger than expected (not properly shrunk?)")

        # ✅ VERIFICATION 2: F-value list lengths
        f1_len = len(before.get("ts1_f_values", []))
        f2_len = len(before.get("ts2_f_values", []))
        f_after_len = len(after.get("f_values", []))

        logger.info(f"  F-values: |f1|={f1_len}, |f2|={f2_len}, |f_after|={f_after_len}")

        if f1_len != ts1_size or f2_len != ts2_size:
            logger.error(f"  ❌ F-value list size mismatch!")
            return False

        # ✅ VERIFICATION 3: A* signals present and valid
        signals = after.get("search_signals", {})
        logger.info(f"  A* signals: {signals}")

        if not signals:
            logger.warning(f"  ⚠️ No A* signals exported")
        else:
            for key in ["nodes_expanded", "branching_factor", "solution_found"]:
                if key not in signals:
                    logger.error(f"  ❌ Missing signal: {key}")
                    return False

        return True
    def _parse_fd_log(self) -> Tuple[int, int]:
        """Read FD log and extract plan cost and expansions."""
        if self.fd_log_file:
            try:
                self.fd_log_file.flush()
            except Exception:
                pass

        log_path = os.path.join("downward", "fd_output", "log.txt")
        plan_cost, expansions = 0, 0
        if os.path.exists(log_path):
            try:
                text = open(log_path, "r", encoding="utf-8", errors="ignore").read()
            except Exception:
                text = ""
            if text:
                m1 = list(re.finditer(r"Plan length:\s*(\d+)", text))
                if m1:
                    plan_cost = int(m1[-1].group(1))
                m2 = list(re.finditer(r"[Ee]xpanded\s+(\d+)\s+state(?:s)?(?:\(\w*\))?", text))
                if m2:
                    expansions = int(m2[-1].group(1))
        return plan_cost, expansions

    # ============================================================================
    # ✅ NEW: Extract rich edge features for GNN learning
    # ============================================================================

    def _extract_edge_features(self) -> np.ndarray:
        """
        ✅ NEW: Extract rich features about merge candidates.

        For each edge (u, v), compute:
        1. Relative size difference
        2. Expected merge size ratio
        3. Shared variable count
        4. Reachability metrics
        5. Iteration difference
        6. Centrality similarity
        7. F-value consistency
        8. Merge risk indicator

        Returns:
            [E, 8] edge features
        """
        G = self.graph_tracker.graph
        edges = list(G.edges())

        if not edges:
            return np.zeros((0, 8), dtype=np.float32)

        edge_features = []

        for u, v in edges:
            u_data = G.nodes[u]
            v_data = G.nodes[v]

            # Feature 1: Relative size difference (normalized)
            u_size = u_data.get("num_states", 1)
            v_size = v_data.get("num_states", 1)
            max_size_in_graph = max(
                d.get("num_states", 1) for _, d in G.nodes(data=True)
            )
            size_diff = float(abs(u_size - v_size)) / max(max_size_in_graph, 1)

            # Feature 2: Expected merged size ratio (% of reachable states)
            # Heuristic: product of sizes as fraction of typical max
            product_size = (u_size * v_size) / max(max_size_in_graph * max_size_in_graph, 1)
            merge_size_ratio = np.clip(product_size, 0.0, 1.0)

            # Feature 3: Shared variables (normalized)
            u_vars = set(u_data.get("incorporated_variables", []))
            v_vars = set(v_data.get("incorporated_variables", []))
            shared_vars = len(u_vars & v_vars)
            total_vars = len(u_vars | v_vars)
            shared_ratio = shared_vars / max(total_vars, 1)

            # Feature 4: Reachability consistency
            u_f = u_data.get("f_before", [])
            v_f = v_data.get("f_before", [])
            u_reachable = sum(1 for f in u_f if f != float('inf') and f < 1_000_000_000) / max(len(u_f), 1)
            v_reachable = sum(1 for f in v_f if f != float('inf') and f < 1_000_000_000) / max(len(v_f), 1)
            reachability_similarity = 1.0 - abs(u_reachable - v_reachable)

            # Feature 5: Iteration difference (normalized)
            u_iter = u_data.get("iteration", 0)
            v_iter = v_data.get("iteration", 0)
            max_iter = max((d.get("iteration", 0) for _, d in G.nodes(data=True)), default=1)
            iter_diff = float(abs(u_iter - v_iter)) / max(max_iter, 1)

            # Feature 6: Centrality similarity
            u_centrality = self.centrality.get(u, 0.0)
            v_centrality = self.centrality.get(v, 0.0)
            centrality_similarity = 1.0 - abs(u_centrality - v_centrality)

            # Feature 7: F-value consistency (std of combined distributions)
            f_combined = u_f + v_f
            if f_combined:
                # Filter to valid values
                f_valid = [f for f in f_combined if f != float('inf') and f < 1_000_000_000]
                if f_valid:
                    f_std = float(np.std(f_valid)) / (1.0 + float(np.mean(f_valid)))
                    f_consistency = np.clip(1.0 - f_std, 0.0, 1.0)
                else:
                    f_consistency = 0.0
            else:
                f_consistency = 0.5

            # Feature 8: Merge risk indicator
            # Combines: degree, transition density, reachability
            u_degree = G.degree(u) / max(G.number_of_nodes(), 1)
            v_degree = G.degree(v) / max(G.number_of_nodes(), 1)
            u_trans = u_data.get("num_transitions", 0) / max(u_size, 1)
            v_trans = v_data.get("num_transitions", 0) / max(v_size, 1)

            merge_risk = np.clip(
                (u_degree + v_degree) * 0.5 + (u_trans + v_trans) * 0.25,
                0.0, 1.0
            )

            edge_features.append([
                size_diff,
                merge_size_ratio,
                shared_ratio,
                reachability_similarity,
                iter_diff,
                centrality_similarity,
                f_consistency,
                merge_risk
            ])

        return np.array(edge_features, dtype=np.float32)

    def _get_observation(self) -> Dict:
        max_nodes, max_edges = 100, 1000

        G = self.graph_tracker.graph

        # ✅ OPTIMIZATION 1: Pre-allocate observation arrays ONCE (not per call)
        # ✅ FIXED: Check if None instead of hasattr (None is already initialized in __init__)
        if self._obs_cache is None:
            self._obs_cache = {
                'x': np.zeros((max_nodes, self.feat_dim), dtype=np.float32),
                'edge_index': np.zeros((2, max_edges), dtype=np.int64),
                'edge_features': np.zeros((max_edges, 8), dtype=np.float32),
            }
            self._last_graph_hash = None

        # ✅ OPTIMIZATION 2: Robust cache check - ✅ FIXED
        # Include more than just edges in hash to catch all changes
        current_num_nodes = self.graph_tracker.graph.number_of_nodes()
        current_num_edges = self.graph_tracker.graph.number_of_edges()

        # ✅ Create composite hash including node/edge counts
        current_hash = (
            self.graph_tracker._get_graph_hash(),
            current_num_nodes,
            current_num_edges
        )

        # ✅ FIX: Also check that cache is populated before returning cached obs
        if (current_hash == self._last_graph_hash and
                self._last_graph_hash is not None and
                self._obs_cache is not None):
            # Graph hasn't changed - return cached observation
            return {
                "x": self._obs_cache['x'].copy(),
                "edge_index": self._obs_cache['edge_index'].copy(),
                "edge_features": self._obs_cache['edge_features'].copy(),
                "num_nodes": np.int32(self._last_num_nodes),
                "num_edges": np.int32(self._last_num_edges),
            }
        self._last_graph_hash = current_hash

        # ✅ REUSE arrays (clear instead of reallocate)
        x = self._obs_cache['x']
        x.fill(0)

        ei = self._obs_cache['edge_index']
        ei.fill(0)

        ef = self._obs_cache['edge_features']
        ef.fill(0)

        # ✅ OPTIMIZATION 3: Get cached metrics (don't recompute)
        degs = dict(G.degree()).values()
        max_deg = max(max(degs, default=0), 1)
        max_states_node = max((d.get("num_states", 0) for _, d in G.nodes(data=True)), default=1) or 1

        # ✅ OPTIMIZATION 4: Compute F-scale ONCE (vectorized)
        f_values_all = []
        for _, d in G.nodes(data=True):
            f_vals = d.get("f_before", [])
            if f_vals:
                f_values_all.extend(f_vals)

        f_scale = float(max(f_values_all)) if f_values_all else 1.0
        f_scale = float(max(f_scale, 1.0))

        # ✅ OPTIMIZATION 5: Use cached centrality (not recomputed every step)
        centrality = self.graph_tracker.get_centrality()
        max_vars = self.graph_tracker.get_max_vars()
        max_iter = self.graph_tracker.get_max_iter()

        # ✅ OPTIMIZATION 6: Vectorized node feature computation
        node_features_list = []
        idx = {}

        for i, (nid, data) in enumerate(G.nodes(data=True)):
            if i >= max_nodes:
                break

            # Batch compute all features for this node at once
            ns_raw = float(data.get("num_states", 0))
            num_states = ns_raw / float(max_states_node)
            is_atomic = 1.0 if data.get("iteration", -1) == -1 else 0.0
            d_norm = G.degree(nid) / max_deg
            od_norm = G.out_degree(nid) / max_deg

            # ✅ CACHED F-stats (memoized in graph_tracker)
            f_min_norm, f_mean_norm, f_max_norm, f_std_norm = self.graph_tracker.f_stats(nid)

            if f_scale > 0:
                f_min_norm = max(0.0, min(f_min_norm / f_scale, 1.0))
                f_mean_norm = max(0.0, min(f_mean_norm / f_scale, 1.0))
                f_max_norm = max(0.0, min(f_max_norm / f_scale, 1.0))
                f_std_norm = max(0.0, min(f_std_norm / f_scale, 1.0))
            else:
                f_min_norm = f_mean_norm = f_max_norm = f_std_norm = 0.0

            # Heuristic quality features
            f_vals = np.array(data.get("f_before", []), dtype=np.float32)
            if len(f_vals) > 0 and f_scale > 0:
                valid_f = f_vals[(f_vals != np.inf) & (f_vals >= 0) & (f_vals < 1e9)]
                if len(valid_f) > 0:
                    avg_f_norm = float(np.mean(valid_f)) / f_scale
                    max_f_norm_heur = float(np.max(valid_f)) / f_scale
                    f_median = np.median(valid_f)
                    heuristic_concentration = float(np.std(valid_f)) / (1.0 + f_median)
                    heuristic_concentration = float(np.clip(heuristic_concentration, 0.0, 1.0))
                else:
                    avg_f_norm = 0.0
                    max_f_norm_heur = 0.0
                    heuristic_concentration = 0.0
            else:
                avg_f_norm = 0.0
                max_f_norm_heur = 0.0
                heuristic_concentration = 0.0

            reachable_ratio = 1.0
            if len(f_vals) > 0:
                unreachable = np.sum((f_vals == np.inf) | (f_vals >= 1e9))
                reachable_ratio = float(1.0 - unreachable / len(f_vals))

            num_vars_norm = len(data.get("incorporated_variables", [])) / float(max_vars)
            iter_idx_norm = data.get("iteration", 0) / float(max_iter)
            centrality_norm = float(centrality.get(nid, 0.0))

            num_neighbors = len(list(G.neighbors(nid)))
            neighbor_risk = float(num_neighbors) / max(len(G.nodes), 1)

            # ✅ SINGLE ASSIGNMENT (faster than individual assignments)
            x[i, :] = [
                num_states, is_atomic, d_norm, od_norm,
                avg_f_norm, max_f_norm_heur, heuristic_concentration, reachable_ratio,
                num_vars_norm, iter_idx_norm, centrality_norm,
                f_min_norm, f_mean_norm, f_max_norm, f_std_norm,
                neighbor_risk, np.clip(ns_raw / 10000.0, 0.0, 1.0),
                0.0, 0.0
            ]

            idx[nid] = i

        num_nodes_feat = len(idx)

        # ✅ OPTIMIZATION 7: Vectorized edge processing
        edges = [
            (idx[u], idx[v])
            for u, v in G.edges()
            if u in idx and v in idx
        ]
        ne = len(edges)

        for j, (u, v) in enumerate(edges[:max_edges]):
            ei[0, j] = u
            ei[1, j] = v

        # ✅ OPTIMIZATION 8: Extract edge features (only if needed)
        if ne > 0:
            edge_features = self._extract_edge_features_cached()
            ef[:ne, :] = edge_features[:ne]

        # ✅ Store cache for next call (use in-place copy to preserve pre-allocated arrays)
        self._last_num_nodes = num_nodes_feat
        self._last_num_edges = ne

        # Update the cache with new values (numpy in-place operations preserve memory)
        self._obs_cache['x'] = x
        self._obs_cache['edge_index'] = ei
        self._obs_cache['edge_features'] = ef

        return {
            "x": x,
            "edge_index": ei,
            "edge_features": ef,
            "num_nodes": np.int32(num_nodes_feat),
            "num_edges": np.int32(ne),
        }

    def _extract_edge_features_cached(self) -> np.ndarray:
        """✅ NEW: Cached edge feature extraction."""
        G = self.graph_tracker.graph
        edges = list(G.edges())

        if not edges:
            return np.zeros((0, 8), dtype=np.float32)

        # ✅ Quick check: if graph hasn't changed, return cached
        current_hash = self.graph_tracker._get_graph_hash()
        if (self._edge_features_cache is not None and
                self._edge_features_cache_hash == current_hash):
            return self._edge_features_cache

        # Compute edge features (this is the expensive part)
        edge_features = []

        # Get graph-level metrics ONCE (not per edge)
        max_size_in_graph = max(
            d.get("num_states", 1) for _, d in G.nodes(data=True)
        ) or 1
        max_iter_global = max((d.get("iteration", 0) for _, d in G.nodes(data=True)), default=1) or 1

        for u, v in edges:
            u_data = G.nodes[u]
            v_data = G.nodes[v]

            u_size = u_data.get("num_states", 1)
            v_size = v_data.get("num_states", 1)
            size_diff = float(abs(u_size - v_size)) / max(max_size_in_graph, 1)

            product_size = (u_size * v_size) / max(max_size_in_graph * max_size_in_graph, 1)
            merge_size_ratio = np.clip(product_size, 0.0, 1.0)

            u_vars = set(u_data.get("incorporated_variables", []))
            v_vars = set(v_data.get("incorporated_variables", []))
            shared_vars = len(u_vars & v_vars)
            total_vars = len(u_vars | v_vars)
            shared_ratio = shared_vars / max(total_vars, 1)

            u_f = u_data.get("f_before", [])
            v_f = v_data.get("f_before", [])
            u_reachable = sum(1 for f in u_f if f != float('inf') and f < 1_000_000_000) / max(len(u_f), 1)
            v_reachable = sum(1 for f in v_f if f != float('inf') and f < 1_000_000_000) / max(len(v_f), 1)
            reachability_similarity = 1.0 - abs(u_reachable - v_reachable)

            u_iter = u_data.get("iteration", 0)
            v_iter = v_data.get("iteration", 0)
            iter_diff = float(abs(u_iter - v_iter)) / max(max_iter_global, 1)

            # Use CACHED centrality
            centrality = self.graph_tracker.get_centrality()
            u_centrality = centrality.get(u, 0.0)
            v_centrality = centrality.get(v, 0.0)
            centrality_similarity = 1.0 - abs(u_centrality - v_centrality)

            f_combined = u_f + v_f
            if f_combined:
                f_valid = [f for f in f_combined if f != float('inf') and f < 1_000_000_000]
                if f_valid:
                    f_std = float(np.std(f_valid)) / (1.0 + float(np.mean(f_valid)))
                    f_consistency = np.clip(1.0 - f_std, 0.0, 1.0)
                else:
                    f_consistency = 0.0
            else:
                f_consistency = 0.5

            u_degree = G.degree(u) / max(G.number_of_nodes(), 1)
            v_degree = G.degree(v) / max(G.number_of_nodes(), 1)
            u_trans = u_data.get("num_transitions", 0) / max(u_size, 1)
            v_trans = v_data.get("num_transitions", 0) / max(v_size, 1)

            merge_risk = np.clip(
                (u_degree + v_degree) * 0.5 + (u_trans + v_trans) * 0.25,
                0.0, 1.0
            )

            edge_features.append([
                size_diff,
                merge_size_ratio,
                shared_ratio,
                reachability_similarity,
                iter_diff,
                centrality_similarity,
                f_consistency,
                merge_risk
            ])

        result = np.array(edge_features, dtype=np.float32)

        # Cache the result
        self._edge_features_cache = result
        self._edge_features_cache_hash = current_hash

        return result

    def _save_gnn_decision_metadata(self, merge_step: int, action: int,
                                    src: int, tgt: int, obs: Dict,
                                    reward: float, info: Dict) -> None:
        """✅ NEW: Save GNN decision metadata for analysis."""

        try:
            import json
            from datetime import datetime

            metadata = {
                'episode_step': self.current_merge_step,
                'merge_step': merge_step,
                'action_index': int(action),
                'chosen_edge': [int(src), int(tgt)],
                'observation_shape': {
                    'num_nodes': int(obs.get('num_nodes', 0)),
                    'num_edges': int(obs.get('num_edges', 0)),
                    'node_features_dim': int(obs['x'].shape[-1]) if obs['x'].ndim > 1 else 0,
                },
                'reward_received': float(reward),
                'merge_info': {
                    'plan_cost': info.get('plan_cost', 0),
                    'num_expansions': info.get('num_expansions', 0),
                    'delta_states': info.get('delta_states', 0),
                },
                'timestamp': datetime.now().isoformat(),
                'problem': os.path.basename(self.problem_file),
            }

            self.gnn_decisions_log.append(metadata)

        except Exception as e:
            logger.debug(f"Could not save GNN metadata: {e}")

    def _export_episode_metadata(self) -> None:
        """✅ NEW: Export all GNN decisions from this episode."""

        if not self.gnn_decisions_log:
            return

        try:
            import json
            from datetime import datetime

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            episode_metadata = {
                'problem': os.path.basename(self.problem_file),
                'num_decisions': len(self.gnn_decisions_log),
                'decisions': self.gnn_decisions_log,
                'export_timestamp': datetime.now().isoformat(),
            }

            metadata_file = os.path.join(
                self.gnn_metadata_dir,
                f"episode_{timestamp}_{len(self.gnn_decisions_log)}_decisions.json"
            )

            with open(metadata_file, 'w') as f:
                json.dump(episode_metadata, f, indent=2, default=str)

            logger.info(f"✓ Exported GNN episode metadata: {metadata_file}")

        except Exception as e:
            logger.warning(f"Failed to export episode metadata: {e}")

    def _count_total_states(self) -> int:
        return sum(d["num_states"] for _, d in self.graph_tracker.graph.nodes(data=True))

    def close(self):
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                try:
                    self.process.wait(timeout=3.0)
                except subprocess.TimeoutExpired:
                    self.process.kill()
        except Exception:
            pass
        finally:
            self.process = None

        try:
            if self.fd_log_file:
                self.fd_log_file.flush()
                self.fd_log_file.close()
        except Exception:
            pass
        finally:
            self.fd_log_file = None

--------------------------------------------------------------------------------

The file graph_tracker.py code is in the following block:
# -*- coding: utf-8 -*-
"""
This module provides the GraphTracker class, a data structure for managing the
state of the merge-and-shrink heuristic construction process.

It represents the set of transition systems (TS) as nodes in a directed graph,
where edges represent causal dependencies from the Fast Downward planner. The class
is responsible for loading the initial state from planner output, performing
merge operations on nodes, and updating the graph based on new information from
the planner. It serves as the core state management component for the MergeEnv.
"""

# ------------------------------------------------------------------------------
#  Imports
# ------------------------------------------------------------------------------
import json
import logging
import time
from json import JSONDecoder
from typing import List, Union, Dict, Tuple, Any, FrozenSet, Optional

import os


import networkx as nx
import numpy as np

# matplotlib is an optional dependency for visualization
try:
    import matplotlib.pyplot as plt
except ImportError:
    plt = None

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
#  Configuration and Constants
# ------------------------------------------------------------------------------
# --- Setup basic logging ---
# Consistent logging configuration with merge_env.py
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)

# --- Constants for file I/O ---
FILE_RETRY_COUNT = 60
FILE_RETRY_DELAY_S = 0.2


# ------------------------------------------------------------------------------
#  Helper Functions
# ------------------------------------------------------------------------------

def _load_json_robustly(path: str, retries: int = FILE_RETRY_COUNT, delay: float = FILE_RETRY_DELAY_S) -> Any:
    """
    Parses the first complete JSON object from a file path with retries.

    This function is designed to handle cases where a file might be read while
    another process is writing to it. It ensures the file is not empty and that
    the content appears to be a complete JSON object (ends with '}' or ']')
    before attempting to parse it.

    Method of Action:
    1. Loop for a specified number of `retries`.
    2. Read the file content, ignoring UTF-8 errors.
    3. If the file is empty or doesn't end with a closing brace/bracket,
       it's considered incomplete. Wait and retry.
    4. Use `JSONDecoder.raw_decode` to parse only the *first* valid JSON
       object, which avoids errors from trailing, partially-written data.
    5. If any error occurs (`OSError`, `JSONDecodeError`), wait and retry.
    6. If all retries fail, raise a `RuntimeError` with the last known error.
    """
    decoder = JSONDecoder()
    last_error = None
    for _ in range(retries):
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read().lstrip()

            if not content:
                # File is empty, wait for content to be written.
                raise json.JSONDecodeError("File is empty", content, 0)

            # Heuristic check for completeness to avoid parsing mid-write.
            tail = content.rstrip()
            if not tail or tail[-1] not in ("]", "}"):
                raise json.JSONDecodeError("JSON appears incomplete (no closing bracket/brace)", content, len(content))

            # Decode the first object, ignoring any trailing garbage.
            obj, _ = decoder.raw_decode(content)
            return obj

        except (OSError, json.JSONDecodeError) as e:
            last_error = e
            time.sleep(delay)

    raise RuntimeError(f"Failed to load valid JSON from '{path}' after {retries} retries. Last error: {last_error}")


def product_state_index(s1: int, s2: int, n2: int) -> int:
    """
    Maps a pair of local states to a single index in their Cartesian product.

    This is a standard row-major order mapping. Given two state spaces of sizes
    `n1` and `n2`, a state `s1` from the first space and `s2` from the second
    are mapped to a unique index in the combined space of size `n1 * n2`.

    Args:
        s1 (int): The index of the state in the first transition system.
        s2 (int): The index of the state in the second transition system.
        n2 (int): The total number of states in the second transition system.

    Returns:
        int: The unique index in the product state space.
    """
    return s1 * n2 + s2


# FILE: graph_tracker.py
# REPLACE THIS FUNCTION (around line 143)

def merge_transition_systems(ts1: Dict[str, Any], ts2: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    ✅ OPTIMIZED: Early detection of problematic merges.

    Returns None if merge is obviously bad (saves 90% of computation time).
    """
    n1, n2 = ts1["num_states"], ts2["num_states"]

    # ✅ OPTIMIZATION 1: Reject merges that will exceed reasonable size
    # This prevents attempting trillion-state merges
    SAFE_MERGE_LIMIT = 50_000_000  # 50M states (tunable)

    try:
        # Check if product would overflow
        if n1 > SAFE_MERGE_LIMIT or n2 > SAFE_MERGE_LIMIT:
            product = n1 * n2
        else:
            product = n1 * n2

    except OverflowError:
        logger.error(f"[EARLY REJECT] Merge would overflow: {n1} × {n2}")
        return None

    # Reject if product is unreasonably large
    if product > 10_000_000_000:  # 10 billion
        logger.warning(f"[EARLY REJECT] Merge product too large: {product} (limit: 10B)")
        logger.warning(f"  TS1: {n1} states, TS2: {n2} states")
        return None

    # ✅ OPTIMIZATION 2: Check reachability before creating goal states
    # Only create goal_states if both TS have reachable goals
    reachable_goals_1 = len([f for f in ts1.get("f_before", [])
                             if f != float('inf') and f < 1_000_000_000])
    reachable_goals_2 = len([f for f in ts2.get("f_before", [])
                             if f != float('inf') and f < 1_000_000_000])

    # if reachable_goals_1 == 0 or reachable_goals_2 == 0:
    #     logger.warning(f"[EARLY REJECT] Unreachable goals in merge")
    #     logger.warning(f"  TS1 reachable goals: {reachable_goals_1}, TS2: {reachable_goals_2}")
    #     return None

    # ✅ OPTIMIZATION 3: Lazy goal state computation
    # Don't pre-compute the full list; just store metadata
    goal_states_1 = ts1["goal_states"]
    goal_states_2 = ts2["goal_states"]

    # Only materialize if reasonable size
    num_product_goals = len(goal_states_1) * len(goal_states_2)

    if num_product_goals > 1_000_000:
        # Too many goals; don't enumerate, store as mapping function
        logger.warning(f"[LAZY GOALS] {num_product_goals} product goals; using lazy computation")

        merged_ts = {
            "num_states": n1 * n2,
            "init_state": product_state_index(ts1["init_state"], ts2["init_state"], n2),
            "goal_states": None,  # ✅ Mark as lazy
            "_goal_state_mapping": {
                "ts1_goals": goal_states_1,
                "ts2_goals": goal_states_2,
                "n2": n2,
            },
            "incorporated_variables": ts1["incorporated_variables"] + ts2["incorporated_variables"],
            "iteration": max(ts1.get("iteration", -1), ts2.get("iteration", -1)) + 1,
        }

        logger.info(f"[MERGE] Created lazy goal state mapping (would have {num_product_goals} entries)")
        return merged_ts

    # Otherwise, safe to materialize
    merged_ts = {
        "num_states": n1 * n2,
        "init_state": product_state_index(ts1["init_state"], ts2["init_state"], n2),
        "goal_states": [
            product_state_index(g1, g2, n2)
            for g1 in goal_states_1
            for g2 in goal_states_2
        ],
        "incorporated_variables": ts1["incorporated_variables"] + ts2["incorporated_variables"],
        "iteration": max(ts1.get("iteration", -1), ts2.get("iteration", -1)) + 1,
    }

    return merged_ts


# ✅ ADD THIS NEW HELPER FUNCTION to handle lazy goal states
def is_goal_state_lazy(state_index: int, ts_merged: Dict[str, Any]) -> bool:
    """Check if a state is a goal using lazy mapping (no list materialization)."""
    if ts_merged.get("goal_states") is not None:
        # Already materialized
        return state_index in ts_merged["goal_states"]

    # Use lazy mapping
    mapping = ts_merged.get("_goal_state_mapping")
    if not mapping:
        return False

    ts1_goals = mapping["ts1_goals"]
    ts2_goals = mapping["ts2_goals"]
    n2 = mapping["n2"]

    s1 = state_index // n2
    s2 = state_index % n2

    return s1 in ts1_goals and s2 in ts2_goals

# ------------------------------------------------------------------------------
#  GraphTracker Class
# ------------------------------------------------------------------------------

class GraphTracker:
    """
    Manages the graph of transition systems for the merge-and-shrink process.

    This class holds a `networkx.DiGraph` where each node represents a transition
    system (TS). Initially, nodes correspond to atomic TSs for individual problem
    variables. The class provides methods to merge nodes (creating a new composite
    TS node) and update node properties based on new data from the planner.

    Attributes:
        graph (nx.DiGraph): The graph of transition systems.
        varset_to_node (Dict): A mapping from a frozenset of variable IDs to the
                                corresponding node ID in the graph. This allows for
                                efficient lookups.
        next_node_id (int): A counter for allocating unique IDs to new merged nodes.
    """

    def __init__(self, ts_json_path: str, cg_json_path: str, is_debug: bool = False):
        """Initialize with caching infrastructure."""
        self.graph = nx.DiGraph()
        self.varset_to_node: Dict[FrozenSet, Union[int, str]] = {}
        self.next_node_id: int = 0
        self.is_debug = is_debug
        # ✅ NEW: Persistent caches (survive across observations)
        self._centrality_cache: Optional[Dict] = None
        self._centrality_cache_valid = False
        self._max_vars_cache: Optional[int] = None
        self._max_iter_cache: Optional[int] = None
        self._f_stats_cache: Dict[int, Tuple[float, float, float, float]] = {}  # Cache for f_stats
        self._graph_hash_last = None
        # Note: _edge_features_cache and _node_features_cache were mentioned
        # in the prompt but not used in the provided methods, so omitting them for now.
        # Add them here if needed later:
        # self._edge_features_cache: Optional[np.ndarray] = None
        # self._node_features_cache: Dict[int, np.ndarray] = {}

        logging.info("Initializing GraphTracker...")
        try:
            self._load_atomic_systems(ts_json_path)
            self._load_causal_edges(cg_json_path)
        except Exception as e:
            logging.error(f"Failed during initial graph loading: {e}")
            if not self.is_debug:
                raise
            else:
                logging.warning("Proceeding with an empty graph in debug mode.")

        # ✅ NEW: FD index tracking
        self._last_fd_mapping_iteration = -1

    # --- END OF REPLACEMENT FOR __init__ ---

    # ============================================================================
    # ✅ NEW: FD Index Synchronization
    # ============================================================================

    # ============================================================================
    # ✅ NEW: FD Index Synchronization Methods
    # ============================================================================

    # In GraphTracker class, ADD THESE METHODS:

    # FILE: graph_tracker.py
    # REPLACE the sync_fd_indices_from_mapping method

    def sync_fd_indices_from_mapping(self, mapping_file: str) -> bool:
        """
        Sync Python node IDs with C++ FD indices.

        After a merge, some nodes will no longer exist in the FD mapping.
        This is expected and not an error.
        """
        if not os.path.exists(mapping_file):
            logger.warning(f"[SYNC] Mapping file not found: {mapping_file}")
            return False

        try:
            with open(mapping_file, 'r') as f:
                mapping_data = json.load(f)
        except Exception as e:
            logger.error(f"[SYNC] Failed to load mapping: {e}")
            return False

        systems = mapping_data.get("systems", [])

        # Build: incorporated_variables → fd_index
        fingerprint_to_fd_index = {}
        for system_entry in systems:
            fd_idx = system_entry.get("fd_index")
            inc_vars = tuple(sorted(system_entry.get("incorporated_variables", [])))
            fingerprint_to_fd_index[inc_vars] = fd_idx
            logger.debug(f"[SYNC] Mapping {inc_vars} -> FD index {fd_idx}")

        # Update all graph nodes
        updated_count = 0
        missing_count = 0

        for node_id, node_data in self.graph.nodes(data=True):
            inc_vars = tuple(sorted(node_data.get("incorporated_variables", [])))

            if inc_vars in fingerprint_to_fd_index:
                old_idx = node_data.get("fd_index", None)
                new_idx = fingerprint_to_fd_index[inc_vars]

                node_data["fd_index"] = new_idx
                updated_count += 1

                if old_idx is not None and old_idx != new_idx:
                    logger.info(f"[SYNC] Updated node {node_id}: fd_index {old_idx} -> {new_idx}")
            else:
                # This is EXPECTED after merges - the merged nodes no longer exist
                # Use DEBUG level, not WARNING
                missing_count += 1
                logger.debug(f"[SYNC] Node {node_id} not in FD mapping (vars: {inc_vars}) - likely merged")

        logger.info(f"[SYNC] Synchronized {updated_count} nodes with FD indices")
        if missing_count > 0:
            logger.debug(f"[SYNC] {missing_count} nodes not in mapping (expected after merges)")

        return True

    def get_fd_indices_for_merge(self, node_a_id: int, node_b_id: int) -> Tuple[int, int]:
        """
        ✅ CRITICAL: Get current FD indices for merge pair.

        RETURNS:
            (fd_index_a, fd_index_b)

        RAISES:
            KeyError if nodes don't have FD indices (sync not called)

        USED BY: merge_env.step() before sending decision
        """
        node_a = self.graph.nodes[node_a_id]
        node_b = self.graph.nodes[node_b_id]

        fd_idx_a = node_a.get("fd_index")
        fd_idx_b = node_b.get("fd_index")

        if fd_idx_a is None or fd_idx_b is None:
            raise KeyError(
                f"Node missing FD index: A.fd_index={fd_idx_a}, B.fd_index={fd_idx_b}. "
                f"Must call sync_fd_indices_from_mapping() first!"
            )

        logger.debug(f"[INDICES] Merge {node_a_id}→{node_b_id}: FD indices ({fd_idx_a}, {fd_idx_b})")
        return (fd_idx_a, fd_idx_b)

    # --- ADD THESE NEW METHODS INSIDE THE GraphTracker CLASS ---

    def _get_graph_hash(self) -> str:
        """✅ Robust graph hash including node data."""
        edges_tuple = tuple(sorted(self.graph.edges()))
        node_count = self.graph.number_of_nodes()
        # Include max node ID to catch renumbering
        max_node_id = max(self.graph.nodes()) if self.graph.nodes() else -1
        return str(hash((edges_tuple, node_count, max_node_id)))

    def _invalidate_all_caches(self):
        """✅ COMPLETE: Invalidate ALL caches after any modification"""
        logging.debug("🧹 INVALIDATING ALL GRAPHTRACKER CACHES")

        # Core computation caches
        self._centrality_cache = None
        self._centrality_cache_valid = False
        self._f_stats_cache.clear()
        self._max_vars_cache = None
        self._max_iter_cache = None
        self._graph_hash_last = None

        logging.debug("✓ All caches cleared")

    def _invalidate_caches(self):
        """Alias for consistency"""
        self._invalidate_all_caches()

    def get_centrality(self, force_recompute: bool = False) -> Dict:
        """✅ GUARANTEED FRESH: Compute or return cached centrality"""
        if force_recompute:
            logging.debug("🔄 Force recomputing centrality...")
            self._centrality_cache_valid = False

        if not self._centrality_cache_valid:
            logging.debug("📊 Computing centrality (not in cache)...")

            try:
                if self.graph.number_of_nodes() > 0:
                    self._centrality_cache = nx.closeness_centrality(self.graph)
                    logging.debug(f"  ✓ Computed for {len(self._centrality_cache)} nodes")
                else:
                    self._centrality_cache = {}
                    logging.debug("  (Empty graph)")
            except nx.NetworkXError as e:
                logging.warning(f"  Centrality failed: {e}, using empty dict")
                self._centrality_cache = {}

            self._centrality_cache_valid = True

        return self._centrality_cache if self._centrality_cache else {}

    def get_max_vars(self) -> int:
        """✅ CACHED: Return max incorporated variables."""
        # Compute only if cache is empty
        if self._max_vars_cache is None:
            logging.debug("Computing max_vars (cached)...")
            self._max_vars_cache = max(
                (len(d.get("incorporated_variables", [])) for _, d in self.graph.nodes(data=True)),
                default=1  # Default if graph is empty
            ) or 1  # Ensure it's at least 1 if max returns 0
        return self._max_vars_cache

    def get_max_iter(self) -> int:
        """✅ CACHED: Return max iteration level."""
        # Compute only if cache is empty
        if self._max_iter_cache is None:
            logging.debug("Computing max_iter (cached)...")
            self._max_iter_cache = max(
                (d.get("iteration", 0) for _, d in self.graph.nodes(data=True)),
                default=0  # Default if graph is empty
            ) or 1  # Ensure it's at least 1 if max returns 0
        return self._max_iter_cache

    def update_graph(self, ts_json_path: str) -> None:
        """
        Updates the graph with new transition system data from a JSON file.

        This method reads a TS list from the given path and updates the properties
        of existing nodes. This is typically used after a merge operation, where
        the planner provides an updated TS file for the newly created node.

        Args:
            ts_json_path (str): The path to the JSON file with TS data.
        """
        logging.info(f"Updating graph from '{ts_json_path}'...")
        try:
            data = _load_json_robustly(ts_json_path)
            ts_list = data if isinstance(data, list) else [data]

            for ts in ts_list:
                if not isinstance(ts, dict):
                    continue
                self._add_or_update_node(ts)

        except Exception as e:
            logging.warning(f"Could not parse or process TS JSON from '{ts_json_path}': {e}")

    # FILE: graph_tracker.py
    # REPLACE THE EXISTING merge_nodes METHOD WITH THIS

    def f_stats(self, node_id: int) -> Tuple[float, float, float, float]:
        """✅ GUARANTEED FRESH: Memoized f-statistics"""
        # Check cache FIRST
        if node_id in self._f_stats_cache:
            cached = self._f_stats_cache[node_id]
            logging.debug(f"  [CACHE HIT] f_stats({node_id})")
            return cached

        logging.debug(f"  [CACHE MISS] Computing f_stats({node_id})...")

        if node_id not in self.graph.nodes:
            logging.warning(f"    Node {node_id} not in graph!")
            result = (0.0, 0.0, 0.0, 0.0)
        else:
            f_values_raw = self.graph.nodes[node_id].get("f_before", [])

            # ✅ Filter invalid values
            f_values = [f for f in f_values_raw
                        if f != float('inf') and f >= 0 and f < 1_000_000_000]

            if not f_values:
                result = (0.0, 0.0, 0.0, 0.0)
                logging.debug(f"    No valid f-values")
            else:
                arr = np.array(f_values, dtype=np.float32)
                result = (
                    float(np.min(arr)),
                    float(np.mean(arr)),
                    float(np.max(arr)),
                    float(np.std(arr))
                )
                logging.debug(f"    min={result[0]:.1f}, mean={result[1]:.1f}, "
                              f"max={result[2]:.1f}, std={result[3]:.1f}")

        # ✅ Store in cache
        self._f_stats_cache[node_id] = result
        return result

    def merge_nodes(self, node_ids: List[int]) -> Tuple[bool, Optional[str]]:
        """✅ FIXED: Returns (success, reason) and invalidates caches"""
        if len(node_ids) != 2:
            raise KeyError(f"merge_nodes requires 2 IDs, got {len(node_ids)}")

        a, b = node_ids

        logging.info(f"Attempting merge: {a} + {b}")

        if a not in self.graph or b not in self.graph:
            return False, "Node not in graph"

        ts1 = self.graph.nodes[a]
        ts2 = self.graph.nodes[b]

        merged_ts = merge_transition_systems(ts1, ts2)

        if merged_ts is None:
            reason = "Merge rejected: product too large"
            logging.warning(f"  ⚠️ {reason}")
            return False, reason

        # Perform merge
        new_id = self.next_node_id
        self.next_node_id += 1

        self.graph.add_node(new_id, **merged_ts)
        var_key = frozenset(merged_ts["incorporated_variables"])
        self.varset_to_node[var_key] = new_id

        self._rewire_edges(a, new_id)
        self._rewire_edges(b, new_id)

        self.graph.remove_nodes_from([a, b])

        # ✅ CRITICAL: Invalidate ALL caches IMMEDIATELY after modification
        self._invalidate_all_caches()

        logging.info(f"✓ Merged into new node {new_id}")
        return True, None

    # --- END OF REPLACEMENT FOR f_stats ---

    def _load_atomic_systems(self, ts_json_path: str) -> None:
        """
        Loads the initial set of atomic transition systems from a JSON file.

        These form the initial nodes of the graph. Atomic systems are identified
        by having `iteration == -1`.
        """
        logging.info(f"Loading atomic systems from '{ts_json_path}'...")
        data = _load_json_robustly(ts_json_path)
        ts_list = data if isinstance(data, list) else [data]

        num_loaded = 0
        for ts in ts_list:
            if isinstance(ts, dict) and ts.get("iteration", -1) == -1:
                self._add_or_update_node(ts)
                num_loaded += 1

        # Set the next node ID to be higher than any existing integer ID to avoid collisions.
        int_ids = [n for n in self.graph.nodes if isinstance(n, int)]
        self.next_node_id = max(int_ids, default=-1) + 1
        logging.info(f"Loaded {num_loaded} atomic systems. Next node ID set to {self.next_node_id}.")

    # REPLACE THE OLD METHOD WITH THIS NEW ONE
    def _load_causal_edges(self, cg_json_path: str) -> None:
        """Loads the causal graph edges from a JSON file into the graph."""
        logging.info(f"Loading causal edges from '{cg_json_path}'...")
        try:
            with open(cg_json_path, "r") as f:
                data = json.load(f)

            edges = data.get("edges", [])
            if not edges:
                logging.warning("Causal graph file contains no 'edges' key or the list is empty.")
                return

            # Debug: log what nodes exist before trying to add edges
            logging.info(f"Current graph nodes before adding edges: {list(self.graph.nodes())}")

            num_added = 0
            for edge in edges:
                src = edge.get("from")
                tgt = edge.get("to")

                # This check is crucial and now more explicit
                if src is not None and tgt is not None:
                    if self.graph.has_node(src) and self.graph.has_node(tgt):
                        self.graph.add_edge(src, tgt)
                        num_added += 1
                        # logging.info(f"Added edge ({src}, {tgt})")
                    else:
                        logging.warning(
                            f"Skipping edge ({src}, {tgt}) because one or both nodes do not exist in the graph. "
                            f"Current nodes: {list(self.graph.nodes())}"
                        )
                else:
                    logging.warning(f"Edge has None values: from={src}, to={tgt}")

            logging.info(f"Loaded {num_added} causal edges. Final edge count: {len(list(self.graph.edges()))}")

        except FileNotFoundError:
            logging.warning(f"Causal graph file '{cg_json_path}' not found. No edges loaded.")
            if not self.is_debug:
                raise
        except Exception as e:
            logging.error(f"An unexpected error occurred while loading causal edges: {e}", exc_info=True)
            if not self.is_debug:
                raise

    def _add_or_update_node(self, ts: Dict[str, Any]) -> None:
        """
        Adds a new node or updates an existing node's data based on a TS dict.

        The identity of a node is determined by its set of "incorporated_variables".
        If a node representing a given set of variables already exists, its
        attributes are updated. Otherwise, a new node is created.

        This method also removes the large 'transitions' list from the node data
        to save memory.

        ✅ NEW: Validates that TS has meaningful data before adding.
        """
        # Validate input
        if not ts or not isinstance(ts, dict):
            logging.warning("Skipping invalid TS: not a dict or empty")
            return

        ts_data = ts.copy()

        # ✅ MEMORY: Don't store full transitions - just count them
        transitions = ts_data.pop("transitions", [])
        ts_data["num_transitions"] = len(transitions)

        ts_data.pop("transitions", None)

        # ✅ NEW: Comprehensive validation
        inc_vars = ts_data.get("incorporated_variables", [])

        if not inc_vars:
            logging.warning("❌ Skipping TS with NO INCORPORATED VARIABLES")
            logging.warning(f"   TS keys: {list(ts.keys())}")
            logging.warning(f"   TS content: {ts}")
            return

        # ✅ NEW: Validate other critical fields
        num_states = ts_data.get("num_states", 0)
        if num_states <= 0:
            logging.warning(f"⚠️  Skipping TS with invalid num_states: {num_states}")
            return

        # ✅ NEW: Check if TS looks complete
        required_fields = ["num_states", "init_state", "goal_states"]
        missing = [k for k in required_fields if k not in ts_data]
        if missing:
            logging.warning(f"⚠️  TS missing fields: {missing}")
            logging.warning(f"   Available fields: {list(ts_data.keys())}")
            # Don't skip - continue with defaults
            for field in missing:
                if field == "init_state":
                    ts_data[field] = 0
                elif field == "goal_states":
                    # ✅ FIX: Default should be empty, not [0]
                    ts_data[field] = []
                    logger.warning(f"⚠️ TS {ts_data.get('iteration')} has no goal_states, using empty list")

        # Now proceed with normal logic
        var_key = frozenset(inc_vars)
        existing_node_id = self.varset_to_node.get(var_key)

        if existing_node_id is not None and existing_node_id in self.graph:
            logging.info(f"✓ Updating existing node {existing_node_id} with {num_states} states")
            self.graph.nodes[existing_node_id].update(ts_data)
        else:
            # New node
            is_atomic = ts_data.get("iteration", -1) == -1
            if is_atomic:
                node_id = inc_vars[0]
                # logging.info(f"✓ Adding atomic node {node_id} for variable {inc_vars[0]}")
            else:
                node_id = self.next_node_id
                self.next_node_id += 1
                # logging.info(f"✓ Adding merged node {node_id} for variables {inc_vars}")

            self.graph.add_node(node_id, **ts_data)
            self.varset_to_node[var_key] = node_id

            # Update counter
            if isinstance(node_id, int):
                self.next_node_id = max(self.next_node_id, node_id + 1)

            # logging.info(f"   → Node {node_id} has {num_states} states")


    def _rewire_edges(self, old_id: Union[int, str], new_id: Union[int, str]) -> None:
        """
        Moves all incoming and outgoing edges from an old node to a new node.
        """
        # Rewire incoming edges: for every predecessor `p` of `old_id`, add edge `(p, new_id)`.
        if old_id in self.graph:
            for predecessor in list(self.graph.predecessors(old_id)):
                if predecessor != new_id:  # Avoid self-loops with the other merged node
                    self.graph.add_edge(predecessor, new_id)
            # Rewire outgoing edges: for every successor `s` of `old_id`, add edge `(new_id, s)`.
            for successor in list(self.graph.successors(old_id)):
                if successor != new_id:
                    self.graph.add_edge(new_id, successor)

    def display(self) -> None:
        """
        Renders and displays the current state of the graph using matplotlib.

        Note: This is intended for interactive debugging and requires the
        `matplotlib` library to be installed.
        """
        if plt is None:
            logging.warning("matplotlib is not installed. Cannot display graph.")
            return

        plt.figure(figsize=(12, 9))
        pos = nx.spring_layout(self.graph, seed=42)

        labels = {
            n: f"ID: {n}\n|S|={d.get('num_states', '?')}\nIter: {d.get('iteration', '?')}"
            for n, d in self.graph.nodes(data=True)
        }

        nx.draw_networkx(
            self.graph,
            pos,
            labels=labels,
            node_size=1500,
            node_color="lightblue",
            font_size=8,
            arrows=True,
            arrowstyle="-|>",
            arrowsize=15,
        )
        plt.title("Transition System Causal Graph", fontsize=16)
        plt.axis("off")
        plt.tight_layout()
        plt.show()


if __name__ == "__main__":
    # This block serves as a "smoke test" to verify basic functionality.
    # It requires dummy JSON files to be present in the same directory.
    logging.info("--- Running GraphTracker Smoke Test ---")

    # Create dummy files for testing purposes
    DUMMY_CG_FILE = "causal_graph_test.json"
    DUMMY_TS_FILE = "ts_test.json"

    cg_data = {"edges": [{"from": 0, "to": 1}, {"from": 1, "to": 2}]}
    ts_data = [
        {"num_states": 2, "init_state": 0, "goal_states": [1], "incorporated_variables": [0], "iteration": -1},
        {"num_states": 3, "init_state": 1, "goal_states": [2], "incorporated_variables": [1], "iteration": -1},
        {"num_states": 4, "init_state": 2, "goal_states": [0], "incorporated_variables": [2], "iteration": -1},
    ]

    with open(DUMMY_CG_FILE, "w") as f:
        json.dump(cg_data, f)
    with open(DUMMY_TS_FILE, "w") as f:
        json.dump(ts_data, f)

    try:
        # 1. Test initialization
        tracker = GraphTracker(ts_json_path=DUMMY_TS_FILE, cg_json_path=DUMMY_CG_FILE, is_debug=True)
        print("\nInitial Graph Nodes:", list(tracker.graph.nodes()))
        print("Initial Graph Edges:", list(tracker.graph.edges()))
        # tracker.display() # Uncomment for visual inspection

        # 2. Test merging
        if len(tracker.graph.nodes) >= 2:
            nodes_to_merge = [0, 1]
            tracker.merge_nodes(nodes_to_merge)
            print(f"\nGraph Nodes after merging {nodes_to_merge}:", list(tracker.graph.nodes()))
            print("Graph Edges after merging:", list(tracker.graph.edges()))
            # tracker.display()

            # 3. Test f_stats on the new node
            new_node_id = list(tracker.graph.nodes)[-1]
            # Add some dummy f-values to test f_stats
            tracker.graph.nodes[new_node_id]['f_before'] = [10, 20, 30, 40]
            stats = tracker.f_stats(new_node_id)
            print(
                f"\nF-stats for new node {new_node_id}: min={stats[0]}, mean={stats[1]}, max={stats[2]}, std={stats[3]}")

        else:
            print("\nNot enough nodes to test merge.")

        logging.info("--- Smoke Test Completed Successfully ---")

    except Exception as e:
        logging.error(f"--- Smoke Test FAILED: {e} ---")

    finally:
        # Clean up dummy files
        import os

        if os.path.exists(DUMMY_CG_FILE): os.remove(DUMMY_CG_FILE)
        if os.path.exists(DUMMY_TS_FILE): os.remove(DUMMY_TS_FILE)

--------------------------------------------------------------------------------

The file reward_function_variants.py code is in the following block:
# -*- coding: utf-8 -*-
"""
REWARD FUNCTION VARIANTS WITH BAD MERGE DETECTION
==================================================

This file contains multiple reward function implementations for merge strategy learning.
Each variant emphasizes different aspects of merge quality, with enhanced bad merge detection.
"""

import numpy as np
import logging
from typing import Dict, Tuple, Optional, List
from reward_info_extractor import MergeInfo

logger = logging.getLogger(__name__)


class RewardFunctionBase:
    """Base class for all reward functions with bad merge detection."""

    def __init__(self, name: str):
        self.name = name
        self.component_values = {}
        self.bad_merge_reasons: List[str] = []

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """
        Computes reward for a merge.

        Args:
            merge_info: Extracted merge information
            search_expansions: Number of states expanded so far
            plan_cost: Current plan cost (if terminal)
            is_terminal: Whether this is the final merge

        Returns:
            Scalar reward value (may be heavily penalized if bad merge detected)
        """
        raise NotImplementedError

    def get_components_dict(self) -> Dict[str, float]:
        """Returns the constituent components of the reward for logging."""
        return self.component_values.copy()

    def _log_bad_merge_detected(self, reason: str) -> None:
        """Log a bad merge detection event."""
        self.bad_merge_reasons.append(reason)
        logger.warning(f"  ⚠️  BAD MERGE DETECTED: {reason}")


class SimpleStabilityReward(RewardFunctionBase):
    """VARIANT 1: Simple reward - penalizes F-value changes and state explosion."""

    def __init__(self, alpha: float = 1.0, beta: float = 0.1, lambda_shrink: float = 0.02,
                 f_threshold: float = 5.0):
        super().__init__("SimpleStabilityReward")
        self.alpha = alpha
        self.beta = beta
        self.lambda_shrink = lambda_shrink
        self.f_threshold = f_threshold

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Compute reward with bad merge detection."""

        logger.info(f"\n[REWARD] Computing {self.name}")

        # Component 1: F-value stability
        f_stability_term = self.alpha * merge_info.f_value_stability
        logger.info(f"  [1] F-stability: {f_stability_term:.4f}")

        # Component 2: State explosion
        state_explosion = merge_info.state_explosion_penalty
        state_term = -self.lambda_shrink * state_explosion
        logger.info(f"  [2] State explosion penalty: {state_term:.4f}")

        # Component 3: Search effort
        if search_expansions > 0:
            exp_norm = min(search_expansions / 200_000.0, 1.0)
            exp_term = -self.beta * exp_norm
        else:
            exp_term = 0.0
        logger.info(f"  [3] Search effort: {exp_term:.4f}")

        # Combine
        reward = f_stability_term + state_term + exp_term
        logger.info(f"  Base reward: {reward:.4f}")

        # ✅ BAD MERGE DETECTION
        bad_merge_penalty = self._detect_bad_merges(merge_info)
        reward += bad_merge_penalty

        if bad_merge_penalty < 0:
            logger.warning(f"  ⚠️  Applied bad merge penalty: {bad_merge_penalty:.4f}")
            logger.warning(f"  Final reward: {reward:.4f}")

        self.component_values = {
            'f_stability': f_stability_term,
            'state_explosion': state_term,
            'search_effort': exp_term,
            'bad_merge_penalty': bad_merge_penalty,
            'total': reward
        }

        return reward

    def _detect_bad_merges(self, merge_info: MergeInfo) -> float:
        """✅ NEW: Detect bad merges and apply penalties."""
        penalty = 0.0

        # CHECK 1: State explosion (TS1_size × TS2_size explosion not controlled)
        expected_merged_size = merge_info.ts1_size * merge_info.ts2_size
        if merge_info.states_after > expected_merged_size * 1.2:
            # Shrinking didn't work as expected
            explosion_ratio = (merge_info.states_after - expected_merged_size) / max(expected_merged_size, 1)
            explosion_penalty = -0.5 * min(explosion_ratio, 2.0)
            penalty += explosion_penalty
            self._log_bad_merge_detected(f"State explosion not controlled: {merge_info.states_after} vs {expected_merged_size}")

        # CHECK 2: F-stability degradation (low preservation)
        if merge_info.f_value_stability < 0.3:
            penalty -= 0.8
            self._log_bad_merge_detected(f"Critical F-value degradation: {merge_info.f_value_stability:.4f}")

        # CHECK 3: Many unreachable states
        reachable_count = sum(1 for f in merge_info.f_after if f != float('inf') and f < 1_000_000_000)
        unreachable_ratio = 1.0 - (reachable_count / max(merge_info.states_after, 1))
        if unreachable_ratio > 0.7:
            penalty -= 1.0
            self._log_bad_merge_detected(f"High unreachability: {unreachable_ratio*100:.1f}% unreachable")

        # CHECK 4: Significant F-value changes (unstable)
        if merge_info.num_significant_f_changes > merge_info.states_after * 0.5:
            penalty -= 0.3
            self._log_bad_merge_detected(f"Significant F-changes: {merge_info.num_significant_f_changes} changes")

        return penalty


class InformationPreservationReward(RewardFunctionBase):
    """VARIANT 2: Information preservation - minimizes heuristic quality loss."""

    def __init__(self, alpha: float = 2.0, beta: float = 0.05, lambda_density: float = 0.1):
        super().__init__("InformationPreservationReward")
        self.alpha = alpha
        self.beta = beta
        self.lambda_density = lambda_density

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Reward information preservation with bad merge detection."""

        info_preserve = self.alpha * merge_info.f_value_stability
        state_penalty = -self.beta * (merge_info.delta_states / max(merge_info.states_before, 10))
        transition_penalty = -self.lambda_density * abs(merge_info.transition_density_change)

        reward = info_preserve + state_penalty + transition_penalty

        # ✅ BAD MERGE DETECTION
        bad_merge_penalty = self._detect_bad_merges(merge_info)
        reward += bad_merge_penalty

        self.component_values = {
            'info_preservation': info_preserve,
            'state_penalty': state_penalty,
            'transition_penalty': transition_penalty,
            'bad_merge_penalty': bad_merge_penalty,
            'total': reward
        }

        return reward

    def _detect_bad_merges(self, merge_info: MergeInfo) -> float:
        """Detect bad merges in information preservation context."""
        penalty = 0.0

        # Very low F-stability is catastrophic for information preservation
        if merge_info.f_value_stability < 0.2:
            penalty = -1.5
            self._log_bad_merge_detected(f"Critical info loss: f_stability={merge_info.f_value_stability:.4f}")

        # Explosive state growth
        if merge_info.delta_states > merge_info.states_before * 2.0:
            penalty -= 0.7
            self._log_bad_merge_detected(f"State count tripled: {merge_info.states_before} → {merge_info.states_after}")

        # Transition density explosion
        if merge_info.transition_density_change > 0.5:
            penalty -= 0.4
            self._log_bad_merge_detected(f"Transition density explosion: +{merge_info.transition_density_change:.2f}")

        return penalty


class HybridMergeQualityReward(RewardFunctionBase):
    """VARIANT 3: Hybrid - balances multiple quality metrics."""

    def __init__(self, w_f_stability: float = 0.4, w_state_control: float = 0.3,
                 w_transition: float = 0.1, w_search: float = 0.2):
        super().__init__("HybridMergeQualityReward")

        total_w = w_f_stability + w_state_control + w_transition + w_search
        self.w_f_stability = w_f_stability / total_w
        self.w_state_control = w_state_control / total_w
        self.w_transition = w_transition / total_w
        self.w_search = w_search / total_w

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Weighted combination with bad merge detection."""

        f_score = merge_info.f_value_stability

        if merge_info.states_before > 0:
            explosion_ratio = merge_info.delta_states / merge_info.states_before
            state_score = max(0, 1.0 - abs(explosion_ratio))
        else:
            state_score = 0.5

        trans_change = merge_info.transition_density_change
        transition_score = 0.5 - min(0.5, trans_change / 2.0) if trans_change >= 0 else 0.5
        transition_score = float(np.clip(transition_score, 0.0, 1.0))

        total_states_after = merge_info.states_after
        if total_states_after > 0:
            valid_f_count = sum(1 for f in merge_info.f_after if f != float('inf'))
            reachability_score = valid_f_count / total_states_after
        else:
            reachability_score = 0.5

        composite = (
            self.w_f_stability * f_score +
            self.w_state_control * state_score +
            self.w_transition * transition_score +
            self.w_search * (1.0 - min(search_expansions / 100_000.0, 1.0))
        )

        reward = 2.0 * composite - 1.0

        # ✅ BAD MERGE DETECTION
        bad_merge_penalty = self._detect_bad_merges(merge_info)
        reward += bad_merge_penalty

        self.component_values = {
            'f_stability': f_score,
            'state_control': state_score,
            'transition': transition_score,
            'composite': composite,
            'bad_merge_penalty': bad_merge_penalty,
            'total': float(reward)
        }

        return float(reward)

    def _detect_bad_merges(self, merge_info: MergeInfo) -> float:
        """Detect bad merges in hybrid context."""
        penalty = 0.0

        # Multiple bad indicators simultaneously
        issues_count = 0

        if merge_info.f_value_stability < 0.35:
            issues_count += 1
        if merge_info.delta_states > merge_info.states_before * 1.5:
            issues_count += 1
        if merge_info.transition_density_change > 0.3:
            issues_count += 1

        # Cumulative penalty for multiple issues
        if issues_count >= 2:
            penalty = -0.5 * issues_count
            self._log_bad_merge_detected(f"Multiple quality issues detected: {issues_count} indicators")

        return penalty


class ConservativeReward(RewardFunctionBase):
    """VARIANT 4: Conservative - heavily penalizes risky merges."""

    def __init__(self, stability_threshold: float = 0.7):
        super().__init__("ConservativeReward")
        self.stability_threshold = stability_threshold

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Conservative approach with strict bad merge detection."""

        if merge_info.f_value_stability < self.stability_threshold:
            base_reward = -2.0
            self._log_bad_merge_detected(f"Below stability threshold: {merge_info.f_value_stability:.4f}")
        elif merge_info.delta_states > 0:
            explosion_ratio = merge_info.delta_states / max(merge_info.states_before, 1)
            base_reward = -0.5 * explosion_ratio
            if explosion_ratio > 0.5:
                self._log_bad_merge_detected(f"State explosion: {explosion_ratio*100:.1f}%")
        else:
            base_reward = merge_info.f_value_stability * 0.5

        reward = base_reward

        self.component_values = {
            'stability_check': base_reward,
            'total': reward
        }

        return reward


class ProgressiveReward(RewardFunctionBase):
    """VARIANT 5: Progressive - adapts based on episode progress."""

    def __init__(self):
        super().__init__("ProgressiveReward")
        self.merge_count = 0
        self._episode_initialized = False

    def reset_episode(self):
        """Call at episode start."""
        self.merge_count = 0
        self._episode_initialized = True

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Adapts reward based on merge count."""
        self.merge_count += 1
        progress = min(self.merge_count / 50.0, 1.0)

        if progress < 0.3:
            conservatism = 1.0
        elif progress < 0.7:
            conservatism = 0.5
        else:
            conservatism = 0.1

        stability_reward = merge_info.f_value_stability * 1.0
        state_penalty = -(1.0 - conservatism) * (
            merge_info.delta_states / max(merge_info.states_before, 1)
        )
        reward = stability_reward + state_penalty

        self.component_values = {
            'progress': progress,
            'conservatism': conservatism,
            'stability_reward': stability_reward,
            'state_penalty': state_penalty,
            'total': reward
        }

        return reward


class RichMergeQualityReward(RewardFunctionBase):
    """VARIANT 6: Rich - combines multiple quality signals."""

    def __init__(self,
                 w_f_stability: float = 0.35,
                 w_state_efficiency: float = 0.30,
                 w_transition_quality: float = 0.20,
                 w_reachability: float = 0.15):
        super().__init__("RichMergeQualityReward")

        total_w = w_f_stability + w_state_efficiency + w_transition_quality + w_reachability
        self.w_f_stability = w_f_stability / total_w
        self.w_state_efficiency = w_state_efficiency / total_w
        self.w_transition_quality = w_transition_quality / total_w
        self.w_reachability = w_reachability / total_w

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Rich reward with comprehensive bad merge detection."""

        f_stability_score = merge_info.f_value_stability

        if merge_info.states_before > 0:
            explosion_ratio = merge_info.delta_states / merge_info.states_before
            state_efficiency = max(-1.0, 1.0 - explosion_ratio)
        else:
            state_efficiency = 0.5

        trans_change = merge_info.transition_density_change
        if trans_change < 0:
            transition_quality = 0.5
        else:
            transition_quality = 0.5 - min(0.5, trans_change / 2.0)
        transition_quality = float(np.clip(transition_quality, 0.0, 1.0))

        total_states_after = merge_info.states_after
        if total_states_after > 0:
            valid_f_count = sum(1 for f in merge_info.f_after if f != float('inf'))
            reachability_score = valid_f_count / total_states_after
        else:
            reachability_score = 0.5

        composite = (
            self.w_f_stability * f_stability_score +
            self.w_state_efficiency * state_efficiency +
            self.w_transition_quality * transition_quality +
            self.w_reachability * reachability_score
        )

        reward = 2.0 * composite - 1.0

        # ✅ BAD MERGE DETECTION - Enhanced for rich variant
        bad_merge_penalty = self._detect_bad_merges(merge_info, composite)
        reward += bad_merge_penalty

        if is_terminal and merge_info.f_value_stability > 0.8:
            reward += 0.5

        self.component_values = {
            'f_stability': f_stability_score,
            'state_efficiency': state_efficiency,
            'transition_quality': transition_quality,
            'reachability': reachability_score,
            'composite': composite,
            'bad_merge_penalty': bad_merge_penalty,
            'total': float(reward)
        }

        return float(reward)

    def _detect_bad_merges(self, merge_info: MergeInfo, composite: float) -> float:
        """✅ COMPREHENSIVE: Detect bad merges across all dimensions."""
        penalty = 0.0

        # CHECK 1: F-stability catastrophic failure
        if merge_info.f_value_stability < 0.25:
            penalty -= 1.2
            self._log_bad_merge_detected(
                f"F-stability catastrophic: {merge_info.f_value_stability:.4f} (threshold: 0.25)")

        # CHECK 2: State explosion uncontrolled
        expected_product = merge_info.ts1_size * merge_info.ts2_size
        if merge_info.states_after > expected_product and merge_info.states_after > merge_info.states_before * 3:
            penalty -= 1.0
            self._log_bad_merge_detected(
                f"Uncontrolled state explosion: {merge_info.states_before} → {merge_info.states_after}")

        # CHECK 3: Most states became unreachable
        unreachable_count = sum(1 for f in merge_info.f_after if f == float('inf') or f >= 1_000_000_000)
        unreachable_ratio = unreachable_count / max(merge_info.states_after, 1)
        if unreachable_ratio > 0.8:
            penalty -= 1.5
            self._log_bad_merge_detected(
                f"Critical unreachability: {unreachable_ratio*100:.1f}% of states unreachable")

        # CHECK 4: Goal unreachability
        goal_reachable = any(
            f != float('inf') and f < 1_000_000_000
            for f in merge_info.f_after
        )
        if not goal_reachable:
            penalty -= 2.0
            self._log_bad_merge_detected("CRITICAL: Goal unreachable after merge!")

        # CHECK 5: Transition density explosion
        if merge_info.transition_density_change > 0.5:
            penalty -= 0.6
            self._log_bad_merge_detected(
                f"Transition density explosion: +{merge_info.transition_density_change:.2f}")

        # CHECK 6: Significant F-value instability
        if merge_info.num_significant_f_changes > merge_info.states_after * 0.6:
            penalty -= 0.8
            self._log_bad_merge_detected(
                f"High F-value instability: {merge_info.num_significant_f_changes} changes")

        # CHECK 7: Composite score tells us overall quality
        if composite < 0.2:
            penalty -= 0.4
            self._log_bad_merge_detected(f"Poor composite score: {composite:.4f}")

        return penalty

class AStarSearchReward(RewardFunctionBase):
    """
    ✅ ENHANCED: A*-informed reward with robust signal for long-term learning.

    Key improvements:
    - Robust F-value stability measurement
    - Softer bad merge penalties that preserve learning signal
    - Better A* signal normalization
    - Reward shaping for smoother learning curves
    - Success bonuses for terminal states
    """

    def __init__(self,
                 w_search_efficiency: float = 0.25,
                 w_solution_quality: float = 0.20,
                 w_f_stability: float = 0.40,
                 w_state_control: float = 0.15):
        super().__init__("EnhancedAStarSearchReward")

        # Normalize weights
        total_w = w_search_efficiency + w_solution_quality + w_f_stability + w_state_control
        self.w_search_efficiency = w_search_efficiency / total_w
        self.w_solution_quality = w_solution_quality / total_w
        self.w_f_stability = w_f_stability / total_w
        self.w_state_control = w_state_control / total_w

        # ✅ NEW: Calibration parameters for robust normalization
        self.bf_comfort_zone = 3.0  # BF values 1-3 are good
        self.depth_comfort_zone = 50  # Typical depth for many problems
        self.stability_threshold = 0.3  # Threshold for acceptable stability

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """✅ ENHANCED: Robust A*-informed reward with better learning signal."""

        logger.info(f"\n[REWARD] Computing {self.name}")

        # ====================================================================
        # COMPONENT 1: SEARCH EFFICIENCY (Branching Factor)
        # ====================================================================

        # ✅ IMPROVED: More nuanced branching factor scoring
        bf = merge_info.branching_factor

        if bf < 1.0 or np.isnan(bf) or np.isinf(bf):
            bf_score = 0.5
            bf_reason = "invalid BF, using neutral"
        elif bf <= 1.1:
            bf_score = 1.0  # Optimal: nearly linear branching
            bf_reason = "optimal"
        elif bf <= self.bf_comfort_zone:
            # Smooth interpolation in comfort zone
            bf_score = 1.0 - (bf - 1.0) / (self.bf_comfort_zone - 1.0) * 0.3
            bf_reason = f"good (in comfort zone)"
        elif bf <= 6.0:
            # Tolerable range with increasing penalty
            bf_score = 0.7 - (bf - self.bf_comfort_zone) / (6.0 - self.bf_comfort_zone) * 0.4
            bf_reason = f"tolerable"
        else:
            # Beyond 6: still provide some credit for trying
            bf_score = max(0.1, 0.3 - (bf - 6.0) / 10.0)
            bf_reason = "high BF, minimal credit"

        bf_score = float(np.clip(bf_score, 0.0, 1.0))
        logger.info(f"  [1] BRANCHING FACTOR: {bf:.3f} → score {bf_score:.3f} ({bf_reason})")

        # ====================================================================
        # COMPONENT 2: SOLUTION QUALITY (Search Depth)
        # ====================================================================

        # ✅ IMPROVED: Depth scoring with solution bonus
        depth_score = 0.5  # Default
        solution_bonus = 0.0

        if merge_info.solution_found:
            depth = merge_info.search_depth

            if depth < 1:
                depth_score = 0.5
            elif depth <= self.depth_comfort_zone:
                # Good depths: log scale to avoid over-penalizing
                depth_score = 1.0 - np.log(depth + 1) / np.log(self.depth_comfort_zone + 1) * 0.5
                depth_score = float(np.clip(depth_score, 0.5, 1.0))
            else:
                # Beyond comfort zone: still positive credit
                depth_score = max(0.2, 0.5 - (np.log(depth + 1) - np.log(self.depth_comfort_zone + 1)) / 2.0)
                depth_score = float(np.clip(depth_score, 0.0, 0.5))

            # ✅ NEW: Solution found is genuinely good
            solution_bonus = 0.3
            logger.info(f"  [2] SOLUTION QUALITY: depth={depth} → score {depth_score:.3f} + bonus {solution_bonus:.3f}")
        else:
            depth_score = 0.2  # Penalty for no solution
            logger.info(f"  [2] SOLUTION QUALITY: NO SOLUTION → score {depth_score:.3f}")

        # ====================================================================
        # COMPONENT 3: F-VALUE STABILITY (CRITICAL FOR LEARNING)
        # ====================================================================

        # ✅ ENHANCED: Better F-stability measurement
        f_stability = self._compute_robust_f_stability(merge_info)


        logger.info(f"  [3] F-VALUE STABILITY: {f_stability:.3f}")

        # ✅ NEW: Stability-based learning signal
        if f_stability > 0.7:
            stability_bonus = 0.2
            stability_reason = "excellent stability"
        elif f_stability > 0.5:
            stability_bonus = 0.1
            stability_reason = "good stability"
        elif f_stability > 0.3:
            stability_bonus = 0.0
            stability_reason = "acceptable stability"
        else:
            stability_bonus = -0.1
            stability_reason = "poor stability (but not penalized heavily)"

        logger.info(f"      → {stability_reason}, bonus: {stability_bonus:.3f}")

        # ====================================================================
        # COMPONENT 4: STATE CONTROL
        # ====================================================================

        # ✅ IMPROVED: More sophisticated state explosion handling
        if merge_info.states_before > 0:
            explosion_ratio = merge_info.delta_states / merge_info.states_before

            if explosion_ratio < -0.5:
                # State reduction is excellent
                state_score = 1.0
                state_reason = "excellent reduction"
            elif explosion_ratio < 0:
                # Some reduction
                state_score = 0.8
                state_reason = "reduction"
            elif explosion_ratio < 0.3:
                # Moderate increase is acceptable
                state_score = 0.8 - explosion_ratio * 0.3
                state_reason = "minor increase (acceptable)"
            elif explosion_ratio < 1.0:
                # Noticeable increase
                state_score = 0.65 - (explosion_ratio - 0.3) * 0.2
                state_reason = "significant increase"
            else:
                # Major explosion: still provide small credit
                state_score = max(0.1, 0.4 - np.log(explosion_ratio + 1) * 0.2)
                state_reason = "major explosion (penalized)"
        else:
            state_score = 0.5
            state_reason = "no baseline"

        state_score = float(np.clip(state_score, 0.0, 1.0))
        logger.info(
            f"  [4] STATE CONTROL: ratio={merge_info.delta_states / max(merge_info.states_before, 1):.3f} → score {state_score:.3f} ({state_reason})")

        # ====================================================================
        # WEIGHTED COMBINATION
        # ====================================================================

        composite = (
                self.w_search_efficiency * bf_score +
                self.w_solution_quality * depth_score +
                self.w_f_stability * f_stability +
                self.w_state_control * state_score
        )

        logger.info(f"\n  [COMPOSITE]: {composite:.3f}")
        logger.info(f"    = {self.w_search_efficiency:.3f}*{bf_score:.3f} (bf)")
        logger.info(f"    + {self.w_solution_quality:.3f}*{depth_score:.3f} (depth)")
        logger.info(f"    + {self.w_f_stability:.3f}*{f_stability:.3f} (stability)")
        logger.info(f"    + {self.w_state_control:.3f}*{state_score:.3f} (state)")

        # Scale to [-1, 1]
        reward = 2.0 * composite - 1.0

        # ====================================================================
        # BONUSES & PENALTIES (MILD - Preserve Learning Signal)
        # ====================================================================

        # ✅ Solution bonus
        reward += solution_bonus
        logger.info(f"\n  [BONUSES]:")
        logger.info(f"    + Solution bonus: {solution_bonus:.3f}")

        # ✅ Stability bonus
        reward += stability_bonus
        logger.info(f"    + Stability bonus: {stability_bonus:.3f}")

        # ✅ BAD MERGE DETECTION (SOFT PENALTIES)
        bad_merge_penalty = self._detect_bad_merges_soft(merge_info)
        reward += bad_merge_penalty

        if bad_merge_penalty != 0.0:
            logger.info(f"    + Bad merge penalty: {bad_merge_penalty:.3f}")

        # ✅ TERMINAL BONUS
        if is_terminal and merge_info.solution_found:
            terminal_bonus = 0.3
            reward += terminal_bonus
            logger.info(f"    + Terminal bonus: {terminal_bonus:.3f}")

        # ====================================================================
        # FINAL CLIPPING & LOGGING
        # ====================================================================

        reward = float(np.clip(reward, -1.0, 1.0))

        logger.info(f"\n  [FINAL REWARD]: {reward:.4f}")
        logger.info(f"  [RANGE]: [-1.0, +1.0] (clipped)\n")

        # Store components
        self.component_values = {
            'bf_score': float(bf_score),
            'depth_score': float(depth_score),
            'solution_bonus': float(solution_bonus),
            'f_stability': float(f_stability),
            'stability_bonus': float(stability_bonus),
            'state_score': float(state_score),
            'bad_merge_penalty': float(bad_merge_penalty),
            'composite': float(composite),
            'total': float(reward)
        }

        return reward

    def _compute_robust_f_stability(self, merge_info: MergeInfo) -> float:
        """
        ✅ ENHANCED: Compute F-stability with robust handling of edge cases.

        This is the MOST IMPORTANT metric for learning quality.
        """

        # ✅ NEW: Better handling of valid vs invalid values
        f_before_valid = [
            f for f in (merge_info.f_before if merge_info.f_before else [])
            if f != float('inf') and f >= 0 and f < 1_000_000_000
        ]

        f_after_valid = [
            f for f in (merge_info.f_after if merge_info.f_after else [])
            if f != float('inf') and f >= 0 and f < 1_000_000_000
        ]

        # Edge case 1: No valid data
        if not f_before_valid or not f_after_valid:
            logger.debug(
                f"      [F-STABILITY] Insufficient valid data: {len(f_before_valid)} before, {len(f_after_valid)} after")
            return 0.5  # Neutral score

        # Edge case 2: Too few samples
        if len(f_after_valid) < 2:
            return 0.5

        # ✅ IMPROVED: Use median-based stability (robust to outliers)
        before_median = float(np.median(f_before_valid))
        after_median = float(np.median(f_after_valid))

        before_std = float(np.std(f_before_valid)) if len(f_before_valid) > 1 else 0.0
        after_std = float(np.std(f_after_valid)) if len(f_after_valid) > 1 else 0.0

        # Normalized change in median
        if before_median > 0:
            median_change = abs(after_median - before_median) / before_median
        else:
            median_change = 0.0

        # Normalized change in variance
        if before_std > 0:
            std_change = abs(after_std - before_std) / before_std
        else:
            std_change = 0.0 if after_std == 0 else 1.0

        # ✅ IMPROVED: Weighted combination (median is more important than std)
        change_metric = 0.7 * np.clip(median_change, 0, 1) + 0.3 * np.clip(std_change, 0, 1)

        # Convert to stability score: lower change = higher stability
        f_stability = max(0.0, 1.0 - change_metric)

        logger.debug(
            f"      [F-STABILITY] median: {before_median:.1f}→{after_median:.1f}, std: {before_std:.1f}→{after_std:.1f}")
        logger.debug(f"      [F-STABILITY] change_metric: {change_metric:.3f}, stability: {f_stability:.3f}")

        return float(np.clip(f_stability, 0.0, 1.0))

    def _detect_bad_merges_soft(self, merge_info: MergeInfo) -> float:
        """
        ✅ ENHANCED: Detect bad merges with SOFT penalties that preserve learning signal.

        Key: Don't over-penalize—let the main reward components handle it.
        Only penalize CRITICAL failures.
        """
        penalty = 0.0
        self.bad_merge_reasons = []

        # CRITICAL CHECK 1: Goal becomes unreachable
        goal_reachable = any(
            f != float('inf') and f < 1_000_000_000
            for f in (merge_info.f_after if merge_info.f_after else [])
        )

        if not goal_reachable:
            penalty -= 0.5  # ✅ MILD penalty for critical failure
            self._log_bad_merge_detected("Goal unreachable (CRITICAL)")
            return penalty  # Return early—this is catastrophic

        # CHECK 2: Very poor F-stability (only if REALLY bad)
        if merge_info.f_value_stability < 0.15:
            penalty -= 0.15  # ✅ VERY MILD
            self._log_bad_merge_detected(f"F-stability extremely poor: {merge_info.f_value_stability:.3f}")

        # CHECK 3: Unreachable states > 90% (indicates broken abstraction)
        unreachable_count = sum(
            1 for f in (merge_info.f_after if merge_info.f_after else [])
            if f == float('inf') or f >= 1_000_000_000
        )

        total_states = len(merge_info.f_after) if merge_info.f_after else 1
        unreachability_ratio = unreachable_count / max(total_states, 1)

        if unreachability_ratio > 0.9:
            penalty -= 0.1  # ✅ MILD: most states unreachable
            self._log_bad_merge_detected(f"High unreachability: {unreachability_ratio * 100:.1f}%")

        # ✅ REMOVED: Other penalties (let composite score handle them)

        return penalty


def create_reward_function(variant: str, **kwargs) -> RewardFunctionBase:
    """
    Creates a reward function instance by name.

    Supported variants:
    - 'simple_stability': Basic stability-focused reward
    - 'information_preservation': Preserve heuristic information
    - 'hybrid': Balance multiple metrics
    - 'conservative': Risk-averse approach
    - 'progressive': Adaptive based on progress
    - 'rich': Comprehensive multi-signal reward
    - 'astar_search': A*-informed with bad merge detection (RECOMMENDED)
    """
    variants = {
        'simple_stability': SimpleStabilityReward,
        'information_preservation': InformationPreservationReward,
        'hybrid': HybridMergeQualityReward,
        'conservative': ConservativeReward,
        'progressive': ProgressiveReward,
        'rich': RichMergeQualityReward,
        'astar_search': AStarSearchReward,
    }

    if variant not in variants:
        raise ValueError(f"Unknown variant: {variant}. Supported: {list(variants.keys())}")

    return variants[variant](**kwargs)

--------------------------------------------------------------------------------

The file reward_info_extractor.py code is in the following block:
# FILE: reward_info_extractor.py (PHASE 2 - ENHANCED)
import os
import json
import numpy as np
import logging
import time
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import traceback

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONSTANTS - MUST MATCH C++ DEFINITIONS
# ============================================================================

# Fast Downward's INF constant
LARGE_VALUE_THRESHOLD = 1_000_000_000
FD_INF_CANDIDATES = [2 ** 31 - 1, 2 ** 31, 2 ** 32 - 1, 999999999, 1_000_000_000, float('inf')]


def is_unreachable_value(value: Any) -> bool:
    """
    ✅ CRITICAL: Properly detect unreachable states.

    A value is unreachable if:
    - It's NaN
    - It's infinity (float or large int)
    - It's negative (error code)
    """
    try:
        if isinstance(value, float):
            if np.isnan(value) or np.isinf(value):
                return True

        if isinstance(value, int):
            if value < 0:
                return True
            if value in FD_INF_CANDIDATES:
                return True
            if value > LARGE_VALUE_THRESHOLD:
                return True

        return False
    except (TypeError, ValueError):
        return True


def is_valid_fvalue(value: Any) -> bool:
    """Validate that an F-value is actually valid."""
    if is_unreachable_value(value):
        return False

    try:
        num_value = float(value)
        if num_value < 0 or num_value > LARGE_VALUE_THRESHOLD:
            return False
        return True
    except (TypeError, ValueError):
        return False


# ============================================================================
# MERGE INFO DATACLASS - COMPLETE
# ============================================================================

@dataclass
class MergeInfo:
    """Complete container for merge information with ALL necessary fields."""

    # ✅ Iteration & IDs
    iteration: int
    ts1_id: int
    ts2_id: int

    # ✅ Sizes (BEFORE merge)
    states_before: int
    ts1_size: int
    ts2_size: int

    # ✅ Sizes (AFTER merge)
    states_after: int

    # ✅ F-values (RAW)
    f_before: List[int]
    f_after: List[int]

    # ✅ F-value statistics
    f_value_stability: float  # [0, 1] how stable F-values are
    f_preservation_score: float  # [0, 1] preservation of heuristic info

    # ✅ State changes
    delta_states: int
    state_explosion_penalty: float

    # ✅ Transition properties
    ts1_transitions: int = 0
    ts2_transitions: int = 0
    merged_transitions: int = 0
    transition_density_change: float = 0.0

    # ✅ Change detection
    num_significant_f_changes: int = 0
    avg_f_change: float = 0.0
    max_f_change: float = 0.0

    # ✅ A* Search signals
    nodes_expanded: int = 0
    search_depth: int = 0
    solution_cost: int = 0
    branching_factor: float = 1.0
    solution_found: bool = False

    # ✅ Reachability
    reachable_states: int = 0
    unreachable_states: int = 0
    reachability_ratio: float = 0.0

    # ✅ Goal states
    merged_goal_states: int = 0

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'iteration': self.iteration,
            'ts1_id': self.ts1_id,
            'ts2_id': self.ts2_id,
            'states_before': self.states_before,
            'states_after': self.states_after,
            'delta_states': self.delta_states,
            'f_value_stability': float(self.f_value_stability),
            'f_preservation_score': float(self.f_preservation_score),
            'state_explosion_penalty': float(self.state_explosion_penalty),
            'transition_density_change': float(self.transition_density_change),
            'num_significant_f_changes': self.num_significant_f_changes,
            'avg_f_change': float(self.avg_f_change),
            'max_f_change': float(self.max_f_change),
            'nodes_expanded': self.nodes_expanded,
            'search_depth': self.search_depth,
            'solution_cost': self.solution_cost,
            'branching_factor': float(self.branching_factor),
            'solution_found': self.solution_found,
            'reachable_states': self.reachable_states,
            'unreachable_states': self.unreachable_states,
            'reachability_ratio': float(self.reachability_ratio),
        }

    def validate(self) -> Tuple[bool, List[str]]:
        """Comprehensive validation."""
        issues = []

        if not (0 <= self.f_value_stability <= 1):
            issues.append(f"f_value_stability out of range: {self.f_value_stability}")
            self.f_value_stability = np.clip(self.f_value_stability, 0, 1)

        if self.states_before <= 0 or self.states_after <= 0:
            issues.append(f"Invalid state counts: {self.states_before} → {self.states_after}")

        if self.branching_factor < 1.0:
            self.branching_factor = 1.0
            issues.append("Corrected branching_factor to 1.0")

        for field in ['f_value_stability', 'avg_f_change', 'max_f_change',
                      'state_explosion_penalty', 'transition_density_change']:
            val = getattr(self, field)
            if isinstance(val, float):
                if np.isnan(val):
                    setattr(self, field, 0.0)
                    issues.append(f"Replaced NaN in {field}")
                elif np.isinf(val):
                    setattr(self, field, 1.0 if 'stability' in field or 'preservation' in field else 0.0)
                    issues.append(f"Replaced Inf in {field}")

        return len(issues) == 0, issues


# ============================================================================
# SIGNAL EXTRACTION - CORE LOGIC
# ============================================================================

class SignalParser:
    """Parses C++ exported JSON signals."""

    @staticmethod
    def safe_compute_average(values: List[Any], context: str = "") -> Tuple[float, int]:
        """Safely compute average with validation."""
        if not values:
            logger.warning(f"[{context}] Empty value list for averaging")
            return 0.0, 0

        valid_values = [v for v in values if is_valid_fvalue(v)]
        removed_count = len(values) - len(valid_values)

        if removed_count > 0:
            logger.debug(f"[{context}] Removed {removed_count}/{len(values)} invalid F-values")

        if not valid_values:
            logger.error(f"[{context}] ALL values were invalid! Using 0.5 as default")
            return 0.5, 0

        avg = float(np.mean(np.array(valid_values, dtype=np.float32)))

        if np.isnan(avg) or np.isinf(avg):
            logger.error(f"[{context}] Computed average is NaN/Inf! Defaulting to 0.5")
            return 0.5, len(valid_values)

        return avg, len(valid_values)

    @staticmethod
    def compute_f_stability(
            f_before_ts1: List[int],
            f_before_ts2: List[int],
            f_after: List[int],
            ts1_size: int,
            ts2_size: int
    ) -> float:
        """
        ✅ CRITICAL: Compute F-stability with robust handling.

        This is the MOST IMPORTANT metric for reward computation.
        """
        # Filter to valid values
        f_before_ts1_valid = [f for f in f_before_ts1 if is_valid_fvalue(f)]
        f_before_ts2_valid = [f for f in f_before_ts2 if is_valid_fvalue(f)]
        f_after_valid = [f for f in f_after if is_valid_fvalue(f)]

        logger.debug(f"F-value validation: {len(f_before_ts1_valid)}/{len(f_before_ts1)} valid in TS1, "
                     f"{len(f_before_ts2_valid)}/{len(f_before_ts2)} valid in TS2, "
                     f"{len(f_after_valid)}/{len(f_after)} valid after")

        if not f_before_ts1_valid or not f_before_ts2_valid or not f_after_valid:
            logger.warning("Insufficient valid F-values for stability computation, using neutral 0.5")
            return 0.5

        # Compute medians (robust to outliers)
        before_median = float(np.median(f_before_ts1_valid + f_before_ts2_valid))
        after_median = float(np.median(f_after_valid))

        before_std = float(np.std(f_before_ts1_valid + f_before_ts2_valid)) if len(
            f_before_ts1_valid + f_before_ts2_valid) > 1 else 0.0
        after_std = float(np.std(f_after_valid)) if len(f_after_valid) > 1 else 0.0

        # Normalized change in median
        if before_median > 0:
            median_change = abs(after_median - before_median) / before_median
        else:
            median_change = 0.0

        # Normalized change in variance
        if before_std > 0:
            std_change = abs(after_std - before_std) / before_std
        else:
            std_change = 0.0 if after_std == 0 else 1.0

        # Weighted combination (median is more important than std)
        change_metric = 0.7 * np.clip(median_change, 0, 1) + 0.3 * np.clip(std_change, 0, 1)

        # Convert to stability score: lower change = higher stability
        f_stability = max(0.0, 1.0 - change_metric)

        logger.debug(f"F-stability: median {before_median:.1f}→{after_median:.1f}, "
                     f"std {before_std:.1f}→{after_std:.1f}, stability={f_stability:.3f}")

        return float(np.clip(f_stability, 0.0, 1.0))

    @staticmethod
    def compute_significant_changes(
            f_before: List[int],
            f_after: List[int],
            threshold: float = 5.0
    ) -> Tuple[int, float, float]:
        """Count significant F-value changes."""
        f_before_valid = [f for f in f_before if is_valid_fvalue(f)]
        f_after_valid = [f for f in f_after if is_valid_fvalue(f)]

        if not f_before_valid or not f_after_valid:
            return 0, 0.0, 0.0

        # Compare first min(len) values
        min_len = min(len(f_before_valid), len(f_after_valid))

        try:
            abs_changes = np.abs(
                np.array(f_before_valid[:min_len], dtype=np.float32) -
                np.array(f_after_valid[:min_len], dtype=np.float32)
            )
            num_significant = int(np.sum(abs_changes > threshold))
            avg_change = float(np.mean(abs_changes))
            max_change = float(np.max(abs_changes))

            return num_significant, avg_change, max_change
        except Exception as e:
            logger.warning(f"Error computing changes: {e}")
            return 0, 0.0, 0.0


# ============================================================================
# MAIN EXTRACTOR CLASS
# ============================================================================

class RewardInfoExtractor:
    """Extracts and validates merge information from C++ signals."""

    def __init__(self, fd_output_dir: str = "downward/fd_output"):
        self.fd_output_dir = fd_output_dir
        self.parser = SignalParser()

    def extract_merge_info(
            self,
            iteration: int,
            timeout: float = 30.0
    ) -> Optional[MergeInfo]:
        """
        ✅ COMPLETE: Extract merge information from C++ signals.

        Waits for both merge_before and merge_after files.
        """
        logger.info(f"\n[EXTRACT] Iteration {iteration}: Extracting merge info")

        try:
            before_path = os.path.join(self.fd_output_dir, f"merge_before_{iteration}.json")
            after_path = os.path.join(self.fd_output_dir, f"merge_after_{iteration}.json")

            # ✅ WAIT WITH VALIDATION
            logger.info(f"[LOAD] Waiting for signal files (timeout: {timeout}s)...")

            start_time = time.time()

            # Wait for files with poll
            while time.time() - start_time < timeout:
                before_exists = os.path.exists(before_path)
                after_exists = os.path.exists(after_path)

                if before_exists and after_exists:
                    # ✅ Files exist, try to load them
                    try:
                        before_data = self._load_json_with_retry(before_path, timeout=5.0)
                        after_data = self._load_json_with_retry(after_path, timeout=5.0)

                        if before_data is not None and after_data is not None:
                            logger.info(f"[LOAD] ✓ Both files loaded successfully")
                            break
                        else:
                            logger.debug("[LOAD] Files exist but not yet readable, retrying...")
                            time.sleep(0.2)
                            continue

                    except Exception as e:
                        logger.debug(f"[LOAD] Parse error, retrying: {e}")
                        time.sleep(0.2)
                        continue
                else:
                    logger.debug(f"[LOAD] Waiting... before={before_exists}, after={after_exists}")
                    time.sleep(0.2)
                    continue

            elapsed = time.time() - start_time

            if before_data is None or after_data is None:
                logger.error(f"[LOAD] ✗ Timeout after {elapsed:.1f}s")
                logger.error(f"[LOAD] before exists: {os.path.exists(before_path)}")
                logger.error(f"[LOAD] after exists: {os.path.exists(after_path)}")
                return None

            logger.info(f"[LOAD] ✓ Both files loaded successfully")

            # ====================================================================
            # EXTRACT F-VALUES
            # ====================================================================
            logger.info(f"\n[F-VALUES] Extracting F-values...")

            ts1_f = before_data.get("ts1_f_values", [])
            ts2_f = before_data.get("ts2_f_values", [])
            f_after_raw = after_data.get("f_values", [])

            ts1_f_valid = [f for f in ts1_f if is_valid_fvalue(f)]
            ts2_f_valid = [f for f in ts2_f if is_valid_fvalue(f)]
            f_after_valid = [f for f in f_after_raw if is_valid_fvalue(f)]

            logger.info(f"  - ts1_valid: {len(ts1_f_valid)} / {len(ts1_f)}")
            logger.info(f"  - ts2_valid: {len(ts2_f_valid)} / {len(ts2_f)}")
            logger.info(f"  - after_valid: {len(f_after_valid)} / {len(f_after_raw)}")

            # ====================================================================
            # EXTRACT IDS & SIZES
            # ====================================================================
            logger.info(f"\n[IDS] Extracting IDs and sizes...")

            ts1_id = before_data.get("ts1_id", -1)
            ts2_id = before_data.get("ts2_id", -1)
            ts1_size = before_data.get("ts1_size", 0)
            ts2_size = before_data.get("ts2_size", 0)

            logger.info(f"  - ts1_id: {ts1_id}, size: {ts1_size}")
            logger.info(f"  - ts2_id: {ts2_id}, size: {ts2_size}")

            # ====================================================================
            # COMPUTE F-STABILITY
            # ====================================================================
            logger.info(f"\n[STABILITY] Computing F-stability...")

            f_stability = self.parser.compute_f_stability(
                ts1_f, ts2_f, f_after_raw, ts1_size, ts2_size
            )
            logger.info(f"  - f_stability: {f_stability:.3f}")

            # ====================================================================
            # COMPUTE SIGNIFICANT CHANGES
            # ====================================================================
            logger.info(f"\n[CHANGES] Computing F-value changes...")

            num_changes, avg_change, max_change = self.parser.compute_significant_changes(
                ts1_f + ts2_f, f_after_raw, threshold=5.0
            )
            logger.info(f"  - significant changes: {num_changes}")
            logger.info(f"  - avg change: {avg_change:.2f}")
            logger.info(f"  - max change: {max_change:.2f}")

            # ====================================================================
            # EXTRACT SIZES & DELTAS
            # ====================================================================
            logger.info(f"\n[SIZES] Extracting state counts...")

            states_before = max(1, ts1_size * ts2_size)
            states_after = max(1, len(f_after_raw))
            delta_states = states_after - states_before

            state_explosion = self._compute_state_explosion(delta_states, states_before)

            logger.info(f"  - states_before: {states_before}")
            logger.info(f"  - states_after: {states_after}")
            logger.info(f"  - delta_states: {delta_states}")
            logger.info(f"  - explosion_penalty: {state_explosion:.4f}")

            # ====================================================================
            # EXTRACT REACHABILITY
            # ====================================================================
            logger.info(f"\n[REACHABILITY] Computing reachability...")

            reachable_states = after_data.get("reachable_states", 0)
            unreachable_states = after_data.get("unreachable_states", 0)
            reachability_ratio = after_data.get("reachability_ratio", 0.0)

            logger.info(f"  - reachable: {reachable_states}")
            logger.info(f"  - unreachable: {unreachable_states}")
            logger.info(f"  - ratio: {reachability_ratio:.2%}")

            # ====================================================================
            # EXTRACT A* SIGNALS
            # ====================================================================
            logger.info(f"\n[A*] Extracting search signals...")

            search_signals = after_data.get("search_signals", {})

            nodes_expanded = int(search_signals.get("nodes_expanded", 0))
            search_depth = int(search_signals.get("search_depth", 0))
            solution_cost = int(search_signals.get("solution_cost", 0))
            branching_factor = float(search_signals.get("branching_factor", 1.0))
            solution_found = bool(search_signals.get("solution_found", False))

            # Safety checks
            if nodes_expanded < 0:
                nodes_expanded = 0
            if search_depth < 0:
                search_depth = 0
            if solution_cost < 0:
                solution_cost = 0
            if branching_factor < 1.0 or np.isnan(branching_factor) or np.isinf(branching_factor):
                branching_factor = 1.0

            logger.info(f"  - nodes_expanded: {nodes_expanded}")
            logger.info(f"  - search_depth: {search_depth}")
            logger.info(f"  - solution_cost: {solution_cost}")
            logger.info(f"  - branching_factor: {branching_factor:.3f}")
            logger.info(f"  - solution_found: {solution_found}")

            # ====================================================================
            # CREATE MERGE INFO
            # ====================================================================
            logger.info(f"\n[CREATE] Creating MergeInfo object...")

            merge_info = MergeInfo(
                iteration=iteration,
                ts1_id=ts1_id,
                ts2_id=ts2_id,
                states_before=states_before,
                ts1_size=ts1_size,
                ts2_size=ts2_size,
                states_after=states_after,
                f_before=ts1_f + ts2_f,
                f_after=f_after_raw,
                f_value_stability=f_stability,
                f_preservation_score=f_stability,
                delta_states=delta_states,
                state_explosion_penalty=state_explosion,
                ts1_transitions=before_data.get("ts1_num_transitions", 0),
                ts2_transitions=before_data.get("ts2_num_transitions", 0),
                merged_transitions=after_data.get("num_transitions", 0),
                transition_density_change=self._compute_transition_density_change(
                    before_data.get("ts1_num_transitions", 0),
                    before_data.get("ts2_num_transitions", 0),
                    after_data.get("num_transitions", 0),
                    ts1_size, ts2_size, states_after
                ),
                num_significant_f_changes=num_changes,
                avg_f_change=avg_change,
                max_f_change=max_change,
                nodes_expanded=nodes_expanded,
                search_depth=search_depth,
                solution_cost=solution_cost,
                branching_factor=branching_factor,
                solution_found=solution_found,
                reachable_states=reachable_states,
                unreachable_states=unreachable_states,
                reachability_ratio=reachability_ratio,
                merged_goal_states=after_data.get("num_goal_states", 0),
            )

            logger.info(f"[CREATE] ✓ MergeInfo created")

            # Validate
            is_valid, issues = merge_info.validate()
            if issues:
                for issue in issues:
                    logger.info(f"  [VALIDATE] {issue}")

            logger.info(f"\n[SUMMARY] Iteration {iteration}:")
            logger.info(f"  - TS{ts1_id}({ts1_size}) + TS{ts2_id}({ts2_size}) → {states_after} states")
            logger.info(f"  - f_stability={f_stability:.3f}")
            logger.info(f"  - A* nodes_expanded={nodes_expanded}")
            logger.info(f"  - solution_found={solution_found}")

            return merge_info

        except Exception as e:
            logger.error(f"\n[ERROR] Failed to extract merge info: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return None

    # ========================================================================
    # HELPER METHODS
    # ========================================================================

    def _load_json_with_retry(
            self,
            path: str,
            timeout: float = 30.0
    ) -> Optional[Dict]:
        """Load JSON with retries and validation."""
        start_time = time.time()
        last_size = -1

        while time.time() - start_time < timeout:
            try:
                if not os.path.exists(path):
                    time.sleep(0.1)
                    continue

                with open(path, 'r') as f:
                    content = f.read()

                if not content.strip():
                    time.sleep(0.1)
                    continue

                data = json.loads(content)
                return data

            except json.JSONDecodeError:
                time.sleep(0.1)
                continue
            except IOError:
                time.sleep(0.1)
                continue

        logger.error(f"Timeout loading {path}")
        return None

    def _compute_state_explosion(self, delta_states: int, states_before: int) -> float:
        """Compute state explosion penalty."""
        if states_before <= 0:
            return 0.0
        pct_increase = delta_states / float(max(states_before, 1))
        penalty = min(1.0, max(0.0, pct_increase / 0.5))
        return float(penalty)

    def _compute_transition_density_change(
            self,
            ts1_trans: int,
            ts2_trans: int,
            merged_trans: int,
            ts1_size: int,
            ts2_size: int,
            merged_size: int
    ) -> float:
        """Change in transition density."""
        if ts1_size <= 0 or ts2_size <= 0 or merged_size <= 0:
            return 0.0
        try:
            density_before_ts1 = ts1_trans / float(ts1_size)
            density_before_ts2 = ts2_trans / float(ts2_size)
            density_after = merged_trans / float(merged_size)
            expected_density = (ts1_trans + ts2_trans) / float(max(merged_size, 1))
            density_change = density_after - expected_density
            return float(density_change)
        except Exception:
            return 0.0

--------------------------------------------------------------------------------

The file validate_merge_signals.py code is in the following block:
# FILE: validate_merge_signals.py (COMPLETE)
# -*- coding: utf-8 -*-
"""
✅ FIXED: Validate merge signals with comprehensive checks
"""

import numpy as np
import logging
from typing import Tuple, List

logger = logging.getLogger(__name__)


def validate_merge_signals(merge_info) -> Tuple[bool, List[str]]:
    """
    Validate that extracted merge signals are physically meaningful.

    Returns:
        (is_valid, issues_found)
    """
    if merge_info is None:
        return False, ["merge_info is None"]

    issues = []

    # CHECK 1: State counts are positive
    if merge_info.states_before <= 0:
        issues.append(f"states_before <= 0: {merge_info.states_before}")
        return False, issues

    if merge_info.states_after <= 0:
        issues.append(f"states_after <= 0: {merge_info.states_after}")
        return False, issues

    # CHECK 2: F-stability in valid range
    if not (0.0 <= merge_info.f_value_stability <= 1.0):
        issues.append(f"f_value_stability out of range: {merge_info.f_value_stability}")
        merge_info.f_value_stability = np.clip(merge_info.f_value_stability, 0.0, 1.0)

    # CHECK 3: Branching factor >= 1
    if merge_info.branching_factor < 1.0:
        issues.append(f"branching_factor < 1.0: {merge_info.branching_factor}")
        merge_info.branching_factor = 1.0

    if np.isnan(merge_info.branching_factor) or np.isinf(merge_info.branching_factor):
        issues.append(f"branching_factor is NaN/Inf: {merge_info.branching_factor}")
        merge_info.branching_factor = 1.0

    # CHECK 4: Explosion penalty is reasonable
    if merge_info.state_explosion_penalty < 0.0 or merge_info.state_explosion_penalty > 1.0:
        issues.append(f"explosion_penalty out of range: {merge_info.state_explosion_penalty}")
        merge_info.state_explosion_penalty = np.clip(merge_info.state_explosion_penalty, 0.0, 1.0)

    # CHECK 5: F-value lists have data
    if len(merge_info.f_after) == 0:
        issues.append("No f_after values")
        return False, issues

    # CHECK 6: No NaN/Inf in critical fields
    critical_fields = [
        'f_value_stability', 'branching_factor', 'state_explosion_penalty',
        'transition_density_change', 'avg_f_change', 'max_f_change'
    ]

    for field in critical_fields:
        if not hasattr(merge_info, field):
            continue

        val = getattr(merge_info, field)
        if isinstance(val, float):
            if np.isnan(val):
                issues.append(f"{field} is NaN")
                setattr(merge_info, field, 0.0)
            elif np.isinf(val):
                issues.append(f"{field} is Inf")
                setattr(merge_info, field, 0.0)

    # CHECK 7: TS sizes reasonable
    if merge_info.ts1_size <= 0 or merge_info.ts2_size <= 0:
        issues.append(f"TS sizes invalid: {merge_info.ts1_size} x {merge_info.ts2_size}")
        return False, issues

    # Allow up to 3 minor issues
    has_critical_issues = any(issue in issues for issue in [
        "merge_info is None",
        "states_before <= 0",
        "states_after <= 0",
        "No f_after values",
        "TS sizes invalid",
    ])

    if has_critical_issues:
        logger.error(f"[VALIDATE] CRITICAL ISSUES: {issues}")
        return False, issues

    if len(issues) > 3:
        logger.warning(f"[VALIDATE] Multiple issues ({len(issues)}): {issues[:3]}")

    if issues:
        logger.info(f"[VALIDATE] Minor issues detected and corrected: {issues}")
        return True, issues  # Valid with warnings

    logger.debug("[VALIDATE] ✅ All signals valid")
    return True, []

--------------------------------------------------------------------------------

The file communication_protocol.py code is in the following block:
# FILE: communication_protocol.py (COMPLETE REWRITE - TOP SECTION)

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COMMUNICATION PROTOCOL LAYER - COMPLETE IMPLEMENTATION (FIXED)
"""

import os
import json
import time
import tempfile
import logging
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
import numpy as np

logger = logging.getLogger(__name__)

# ============================================================================
# ✅ CRITICAL FIX: USE ABSOLUTE PATHS FROM common_utils
# ============================================================================

def _get_absolute_paths():
    """Get absolute paths from common_utils or compute them."""
    try:
        from common_utils import DOWNWARD_DIR, FD_OUTPUT_DIR, GNN_OUTPUT_DIR
        return DOWNWARD_DIR, FD_OUTPUT_DIR, GNN_OUTPUT_DIR
    except ImportError:
        # Fallback: compute from project root
        project_root = Path(__file__).parent.absolute()
        downward_dir = project_root / "downward"
        fd_output_dir = downward_dir / "fd_output"
        gnn_output_dir = downward_dir / "gnn_output"
        return str(downward_dir), str(fd_output_dir), str(gnn_output_dir)

DOWNWARD_DIR, FD_OUTPUT_DIR, GNN_OUTPUT_DIR = _get_absolute_paths()

class CommConfig:
    """All tuning parameters in one place."""

    POLL_INTERVAL_MS = 50
    POLL_LOG_INTERVAL_S = 10

    # ✅ FIXED: Use absolute paths
    TIMEOUT_ACK_S = 30  # Increased from 20
    TIMEOUT_SIGNALS_S = 120
    TIMEOUT_MAPPING_S = 30
    TOTAL_STEP_TIMEOUT_S = 180

    # ✅ FIXED: These are now absolute paths
    GNN_OUTPUT_DIR = GNN_OUTPUT_DIR
    FD_OUTPUT_DIR = FD_OUTPUT_DIR

    # File patterns (relative to above dirs)
    MERGE_DECISION_PATTERN = "merge_{}.json"
    GNN_ACK_PATTERN = "gnn_ack_{}.json"
    MERGE_BEFORE_PATTERN = "merge_before_{}.json"
    MERGE_AFTER_PATTERN = "merge_after_{}.json"
    TS_PATTERN = "ts_{}.json"
    FD_INDEX_MAPPING_PATTERN = "fd_index_mapping_{}.json"
    ERROR_PATTERN = "gnn_error_{}.json"

    INITIAL_ITERATION = -1

# ============================================================================
# ✅ NEW: Ensure directories exist at module load time
# ============================================================================

def _ensure_directories_exist():
    """Create all necessary directories."""
    for dir_path in [CommConfig.GNN_OUTPUT_DIR, CommConfig.FD_OUTPUT_DIR]:
        try:
            os.makedirs(dir_path, exist_ok=True)
            logger.info(f"[COMM] Directory ready: {dir_path}")
        except Exception as e:
            logger.error(f"[COMM] Failed to create {dir_path}: {e}")
            raise

_ensure_directories_exist()

# Rest of the file continues with your existing code, but now using
# CommConfig.GNN_OUTPUT_DIR and CommConfig.FD_OUTPUT_DIR which are absolute paths

# ============================================================================
# MESSAGE DEFINITIONS - DATACLASSES WITH SERIALIZATION
# ============================================================================

@dataclass
class MergeDecision:
    """Python → C++: Merge decision"""
    iteration: int
    merge_pair: Tuple[int, int]  # (fd_index_1, fd_index_2)
    timestamp: float
    problem: Optional[str] = None

    def to_json(self) -> Dict[str, Any]:
        return {
            "iteration": self.iteration,
            "merge_pair": list(self.merge_pair),
            "timestamp": self.timestamp,
            "problem": self.problem,
        }

    @staticmethod
    def from_json(obj: Dict[str, Any]) -> 'MergeDecision':
        return MergeDecision(
            iteration=obj["iteration"],
            merge_pair=tuple(obj["merge_pair"]),
            timestamp=obj.get("timestamp", time.time()),
            problem=obj.get("problem"),
        )


@dataclass
class MergeAckMessage:
    """C++ → Python: ACK of merge decision"""
    iteration: int
    merge_pair: Tuple[int, int]
    received: bool
    timestamp: float

    def to_json(self) -> Dict[str, Any]:
        return {
            "iteration": self.iteration,
            "merge_pair": list(self.merge_pair),
            "received": self.received,
            "timestamp": self.timestamp,
        }

    @staticmethod
    def from_json(obj: Dict[str, Any]) -> 'MergeAckMessage':
        return MergeAckMessage(
            iteration=obj["iteration"],
            merge_pair=tuple(obj["merge_pair"]),
            received=obj.get("received", True),
            timestamp=obj.get("timestamp", time.time()),
        )


@dataclass
class ErrorMessage:
    """C++ → Python: Error during merge"""
    iteration: int
    error: bool
    message: str
    timestamp: float

    def to_json(self) -> Dict[str, Any]:
        return {
            "iteration": self.iteration,
            "error": self.error,
            "message": self.message,
            "timestamp": self.timestamp,
        }


# ============================================================================
# ATOMIC FILE I/O - CORE RELIABILITY
# ============================================================================

def write_json_atomic(obj: Any, final_path: str) -> None:
    """
    Write JSON file atomically using temp file + rename.

    ✅ GUARANTEES:
    - Complete write to disk BEFORE rename
    - Atomic rename (no partial files visible)
    - Works on Windows, Linux, macOS

    ⚠️ CALL THIS FOR ALL WRITES TO ensure consistency
    """
    final_path = str(final_path)
    dir_path = os.path.dirname(final_path) or "."

    os.makedirs(dir_path, exist_ok=True)

    # Create temp file in same directory (same filesystem = atomic rename)
    fd, temp_path = tempfile.mkstemp(
        dir=dir_path,
        suffix='.tmp',
        prefix=os.path.basename(final_path) + '.'
    )

    try:
        with os.fdopen(fd, 'w') as f:
            json.dump(obj, f, indent=2, default=str)
            f.flush()
            # CRITICAL: Force to disk
            os.fsync(f.fileno())

        # Atomic rename
        os.replace(temp_path, final_path)
        logger.debug(f"[IO] ✅ Wrote atomically: {os.path.basename(final_path)}")

    except Exception as e:
        try:
            os.remove(temp_path)
        except:
            pass
        raise


def read_json_robust(
        file_path: str,
        timeout_s: float = 30.0,
        allow_missing: bool = False
) -> Optional[Dict[str, Any]]:
    """
    Read JSON file with retries.

    ✅ HANDLES:
    - File not yet created
    - JSON parse errors (incomplete writes)
    - Up to timeout_s seconds

    🔑 Used by: wait_for_* functions
    """
    file_path = str(file_path)
    start_time = time.time()
    last_error = None

    while time.time() - start_time < timeout_s:
        try:
            if not os.path.exists(file_path):
                if allow_missing:
                    return None
                time.sleep(CommConfig.POLL_INTERVAL_MS / 1000.0)
                continue

            # Read file content
            with open(file_path, 'r') as f:
                content = f.read()

            if not content.strip():
                time.sleep(CommConfig.POLL_INTERVAL_MS / 1000.0)
                continue

            # Parse JSON
            obj = json.loads(content)
            return obj

        except json.JSONDecodeError as e:
            last_error = e
            time.sleep(CommConfig.POLL_INTERVAL_MS / 1000.0)
            continue
        except IOError as e:
            last_error = e
            time.sleep(CommConfig.POLL_INTERVAL_MS / 1000.0)
            continue

    raise TimeoutError(
        f"Timeout reading {file_path} after {timeout_s}s. "
        f"Last error: {last_error}"
    )


# ============================================================================
# POLLING WITH DIAGNOSTICS - PRODUCTION QUALITY
# ============================================================================

def wait_for_file_with_diagnostics(
        file_path: str,
        timeout_s: float = 30.0,
        fd_process: Optional[subprocess.Popen] = None,
        phase_name: str = "unknown",
        iteration: Optional[int] = None
) -> bool:
    """
    Wait for file with diagnostic logging.

    ✅ FEATURES:
    - Detailed progress logging every 10 seconds
    - Checks if FD process crashed
    - Diagnoses missing files on timeout
    - Clear error messages

    RETURNS:
        True if file exists
        False if timeout or process died
    """
    file_path = str(file_path)
    start_time = time.time()
    last_log_time = start_time
    file_basename = os.path.basename(file_path)

    iter_str = f"[Iter {iteration}]" if iteration is not None else ""

    logger.debug(f"{iter_str} [WAIT] Waiting for: {file_basename}")

    while time.time() - start_time < timeout_s:
        elapsed = time.time() - start_time

        # Check if FD process died
        if fd_process and fd_process.poll() is not None:
            rc = fd_process.returncode
            logger.error(f"{iter_str} [WAIT] ❌ FD process died with code {rc}")
            logger.error(f"{iter_str} [WAIT] Expected: {file_basename}")
            return False

        # Progress logging every 10 seconds
        if elapsed - (start_time - last_log_time) > CommConfig.POLL_LOG_INTERVAL_S:
            logger.debug(
                f"{iter_str} [WAIT] Still waiting ({elapsed:.0f}s)... {file_basename}"
            )
            last_log_time = time.time()

        if os.path.exists(file_path):
            logger.debug(f"{iter_str} [WAIT] ✅ File found")
            return True

        time.sleep(CommConfig.POLL_INTERVAL_MS / 1000.0)

    # TIMEOUT - Log diagnostics
    elapsed = time.time() - start_time
    logger.error(f"{iter_str} [WAIT] ❌ TIMEOUT after {elapsed:.1f}s")
    logger.error(f"{iter_str} [WAIT] Expected: {file_basename}")
    logger.error(f"{iter_str} [WAIT] Phase: {phase_name}")

    # List available files
    dir_path = os.path.dirname(file_path)
    if os.path.exists(dir_path):
        logger.error(f"{iter_str} [WAIT] Files in {os.path.basename(dir_path)}:")
        try:
            for fname in sorted(os.listdir(dir_path)):
                fpath = os.path.join(dir_path, fname)
                if os.path.isfile(fpath):
                    size = os.path.getsize(fpath)
                    logger.error(f"{iter_str} [WAIT]   - {fname} ({size} bytes)")
        except Exception as e:
            logger.error(f"{iter_str} [WAIT] Error listing: {e}")

    return False


# ============================================================================
# HIGH-LEVEL COMMUNICATION FUNCTIONS
# ============================================================================

def send_merge_decision(
        iteration: int,
        merge_pair: Tuple[int, int],
        problem: Optional[str] = None,
) -> None:
    """Send merge decision to C++ with path verification."""
    decision = MergeDecision(
        iteration=iteration,
        merge_pair=merge_pair,
        timestamp=time.time(),
        problem=problem,
    )

    path = os.path.join(
        CommConfig.GNN_OUTPUT_DIR,
        CommConfig.MERGE_DECISION_PATTERN.format(iteration)
    )

    # ✅ NEW: Verify directory exists before writing
    dir_path = os.path.dirname(path)
    os.makedirs(dir_path, exist_ok=True)

    write_json_atomic(decision.to_json(), path)

    # ✅ NEW: Verify file was actually written
    if not os.path.exists(path):
        raise RuntimeError(f"Decision file not created: {path}")

    logger.debug(f"[COMM] Sent merge decision: iteration={iteration}, pair={merge_pair}, path={path}")


# FILE: communication_protocol.py - REPLACE wait_for_ack

import logging

logger = logging.getLogger("HANDSHAKE")


def wait_for_ack(
        iteration: int,
        fd_process: Optional[subprocess.Popen] = None,
        timeout_s: float = 30.0
) -> MergeAckMessage:
    """
    ✅ ENHANCED: Wait for C++ to ACK merge decision with detailed logging.
    """
    path = os.path.join(
        CommConfig.FD_OUTPUT_DIR,
        CommConfig.GNN_ACK_PATTERN.format(iteration)
    )

    logger.info(f"\n[ACK] Iteration {iteration}: Waiting for ACK...")
    logger.debug(f"[ACK] Expected file: {path}")
    logger.debug(f"[ACK] Absolute path: {os.path.abspath(path)}")

    start_time = time.time()
    last_log = start_time
    file_created = False

    while time.time() - start_time < timeout_s:
        elapsed = time.time() - start_time

        # Log every 5 seconds
        if elapsed - (last_log - start_time) > 5.0:
            logger.debug(f"[ACK] Still waiting... ({elapsed:.1f}s)")
            last_log = time.time()

        if os.path.exists(path):
            file_created = True
            logger.debug(f"[ACK] File exists at {elapsed:.2f}s")
            break

        # Check if FD died
        if fd_process and fd_process.poll() is not None:
            rc = fd_process.returncode
            logger.error(f"[ACK] ❌ FD process died with code {rc}")
            raise RuntimeError(f"FD crashed before ACK (code {rc})")

        time.sleep(0.2)

    if not file_created:
        elapsed = time.time() - start_time
        logger.error(f"[ACK] ❌ TIMEOUT after {elapsed:.1f}s")
        logger.error(f"[ACK] Waiting for: {path}")

        # Diagnostic
        fd_dir = os.path.dirname(path)
        if os.path.exists(fd_dir):
            logger.error(f"[ACK] Files in {fd_dir}:")
            try:
                for f in sorted(os.listdir(fd_dir)):
                    logger.error(f"[ACK]   - {f}")
            except Exception as e:
                logger.error(f"[ACK]   Error listing: {e}")

        raise TimeoutError(f"ACK not received after {elapsed:.1f}s")

    # Load and validate ACK
    try:
        logger.debug(f"[ACK] Loading JSON from {path}")
        obj = read_json_robust(path, timeout_s=5.0)
        logger.debug(f"[ACK] JSON loaded: {obj}")

        ack = MergeAckMessage.from_json(obj)

        if ack.iteration != iteration:
            logger.error(f"[ACK] ❌ Iteration mismatch: expected {iteration}, got {ack.iteration}")
            raise ValueError(f"ACK iteration mismatch")

        logger.info(f"[ACK] ✅ ACK received and validated")
        return ack

    except Exception as e:
        logger.error(f"[ACK] ❌ Failed to parse ACK: {e}")
        raise


# FILE: communication_protocol.py
# ADD this function and update wait_for_merge_signals

def read_fd_log_tail(fd_output_dir: str, lines: int = 100) -> str:
    """Read the last N lines of FD log file for diagnostics."""
    log_path = os.path.join(fd_output_dir, "..", "fd_output", "log.txt")

    # Try multiple possible locations
    possible_paths = [
        os.path.join(fd_output_dir, "log.txt"),
        os.path.join(os.path.dirname(fd_output_dir), "fd_output", "log.txt"),
        os.path.join(fd_output_dir, "..", "log.txt"),
    ]

    for path in possible_paths:
        try:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    all_lines = f.readlines()
                    tail = all_lines[-lines:] if len(all_lines) > lines else all_lines
                    return ''.join(tail)
        except Exception as e:
            continue

    return "(Could not read FD log file)"


# FILE: communication_protocol.py
# REPLACE THE wait_for_merge_signals FUNCTION

def wait_for_merge_signals(
        iteration: int,
        fd_process: Optional[subprocess.Popen] = None
) -> Tuple[Dict[str, Any], Dict[str, Any], Dict[str, Any], Dict[str, Any]]:
    """
    ✅ ENHANCED: Wait for C++ to export merge signals with diagnostic logging.
    """
    iteration_str = f"[Iter {iteration}]"

    ts_path = os.path.join(
        CommConfig.FD_OUTPUT_DIR,
        CommConfig.TS_PATTERN.format(iteration)
    )

    logger.info(f"{iteration_str} Waiting for merge signals...")
    logger.debug(f"{iteration_str} Expected: {ts_path}")

    # ✅ CRITICAL: Record when we START waiting
    wait_start_time = time.time()

    if not wait_for_file_with_diagnostics(
            ts_path,
            timeout_s=CommConfig.TIMEOUT_SIGNALS_S,
            fd_process=fd_process,
            phase_name="MERGE_SIGNALS",
            iteration=iteration
    ):
        # ✅ NEW: Read and display FD log on failure
        if fd_process and fd_process.poll() is not None:
            rc = fd_process.returncode
            logger.error(f"{iteration_str} FD process CRASHED with exit code {rc}")

            # Read FD log for diagnostics
            log_tail = read_fd_log_tail(CommConfig.FD_OUTPUT_DIR)
            logger.error(f"{iteration_str} === FD LOG TAIL ===")
            for line in log_tail.split('\n')[-50:]:
                logger.error(f"  {line.rstrip()}")
            logger.error(f"{iteration_str} === END FD LOG ===")

            raise RuntimeError(f"FD crashed with code {rc}")

        raise TimeoutError(f"Merge signals timeout for iteration {iteration}")

    wait_elapsed = time.time() - wait_start_time
    logger.debug(f"{iteration_str} Signals retrieved in {wait_elapsed:.2f}s")

    # Load all signals
    ts_data = read_json_robust(ts_path, timeout_s=5.0)

    # ✅ CRITICAL: Validate iteration in loaded data
    if "iteration" in ts_data and ts_data["iteration"] != iteration:
        logger.error(f"{iteration_str} ❌ Iteration mismatch: expected {iteration}, "
                     f"got {ts_data['iteration']}")
        raise ValueError("Signal iteration mismatch - stale data!")

    merge_before = read_json_robust(
        os.path.join(
            CommConfig.FD_OUTPUT_DIR,
            CommConfig.MERGE_BEFORE_PATTERN.format(iteration)
        ),
        timeout_s=5.0,
        allow_missing=True
    ) or {}

    merge_after = read_json_robust(
        os.path.join(
            CommConfig.FD_OUTPUT_DIR,
            CommConfig.MERGE_AFTER_PATTERN.format(iteration)
        ),
        timeout_s=5.0,
        allow_missing=True
    ) or {}

    fd_mapping = read_json_robust(
        os.path.join(
            CommConfig.FD_OUTPUT_DIR,
            CommConfig.FD_INDEX_MAPPING_PATTERN.format(iteration)
        ),
        timeout_s=CommConfig.TIMEOUT_MAPPING_S,
        allow_missing=True
    ) or {}

    logger.info(f"{iteration_str} All merge signals received")

    return merge_before, merge_after, ts_data, fd_mapping


def wait_for_initial_mapping(
        fd_process: Optional[subprocess.Popen] = None
) -> Dict[str, Any]:
    """
    Wait for C++ to export initial FD index mapping.

    ⏱️ TIMEOUT: 30s

    USED BY: merge_env.reset() to sync initial indices
    """
    path = os.path.join(
        CommConfig.FD_OUTPUT_DIR,
        CommConfig.FD_INDEX_MAPPING_PATTERN.format(CommConfig.INITIAL_ITERATION)
    )

    logger.info("[INIT] Waiting for initial FD index mapping...")

    if not wait_for_file_with_diagnostics(
            path,
            timeout_s=CommConfig.TIMEOUT_MAPPING_S,
            fd_process=fd_process,
            phase_name="INITIAL_MAPPING"
    ):
        if fd_process and fd_process.poll() is not None:
            raise RuntimeError("FD crashed before initial mapping export")
        raise TimeoutError("Initial mapping not exported in time")

    obj = read_json_robust(path, timeout_s=5.0)
    logger.info("[INIT] ✅ Initial FD index mapping received")

    return obj


def check_for_error_signal(
        iteration: int,
) -> Optional[ErrorMessage]:
    """
    Check if C++ wrote an error file.

    USED BY: Error handling to get context
    """
    path = os.path.join(
        CommConfig.FD_OUTPUT_DIR,
        CommConfig.ERROR_PATTERN.format(iteration)
    )

    if os.path.exists(path):
        try:
            with open(path, 'r') as f:
                obj = json.load(f)
            return ErrorMessage(
                iteration=obj.get("iteration"),
                error=obj.get("error", True),
                message=obj.get("message", "Unknown error"),
                timestamp=obj.get("timestamp", time.time()),
            )
        except Exception as e:
            logger.error(f"Error reading error signal: {e}")
            return None

    return None


# ============================================================================
# DIRECTORY MANAGEMENT
# ============================================================================

def ensure_communication_directories() -> None:
    """Ensure both gnn_output and fd_output directories exist."""
    for directory in [CommConfig.GNN_OUTPUT_DIR, CommConfig.FD_OUTPUT_DIR]:
        os.makedirs(directory, exist_ok=True)
        if not os.path.isdir(directory):
            raise RuntimeError(f"Cannot create directory: {directory}")


def clear_iteration_files(iteration: int) -> None:
    """
    Clean up files from a specific iteration.

    USED BY: Debugging / restart scenarios
    """
    patterns = [
        CommConfig.MERGE_DECISION_PATTERN,
        CommConfig.GNN_ACK_PATTERN,
        CommConfig.MERGE_BEFORE_PATTERN,
        CommConfig.MERGE_AFTER_PATTERN,
        CommConfig.TS_PATTERN,
        CommConfig.FD_INDEX_MAPPING_PATTERN,
        CommConfig.ERROR_PATTERN,
    ]

    for pattern in patterns:
        path = os.path.join(CommConfig.FD_OUTPUT_DIR, pattern.format(iteration))
        if os.path.exists(path):
            try:
                os.remove(path)
            except Exception as e:
                logger.warning(f"Could not delete {path}: {e}")


# ============================================================================
# HANDSHAKE PROTOCOL - INITIALIZATION
# ============================================================================

def perform_handshake(
        fd_process: subprocess.Popen,
        timeout_s: float = 120.0
) -> Tuple[Dict[str, Any], Dict[str, Any]]:
    """
    Perform initial handshake with FD.

    ✅ SEQUENCE:
    1. Wait for causal_graph.json
    2. Wait for merged_transition_systems.json
    3. Wait for fd_index_mapping_-1.json (initial)
    4. Parse all three

    RETURNS:
        (causal_graph, fd_index_mapping)

    RAISES:
        TimeoutError: If handshake times out
        ProcessCrashedException: If FD crashed

    USED BY: merge_env.reset()
    """
    logger.info("[HANDSHAKE] Starting FD handshake...")
    start_time = time.time()

    # Wait for causal graph
    cg_path = os.path.join(CommConfig.FD_OUTPUT_DIR, "causal_graph.json")
    logger.info("[HANDSHAKE] Waiting for causal_graph.json...")

    if not wait_for_file_with_diagnostics(
            cg_path,
            timeout_s=timeout_s,
            fd_process=fd_process,
            phase_name="CAUSAL_GRAPH"
    ):
        raise TimeoutError("Causal graph not created by FD")

    cg_data = read_json_robust(cg_path, timeout_s=5.0)
    logger.info("[HANDSHAKE] ✅ Causal graph loaded")

    # Wait for initial mapping
    mapping_path = os.path.join(
        CommConfig.FD_OUTPUT_DIR,
        CommConfig.FD_INDEX_MAPPING_PATTERN.format(CommConfig.INITIAL_ITERATION)
    )
    logger.info("[HANDSHAKE] Waiting for fd_index_mapping_-1.json...")

    if not wait_for_file_with_diagnostics(
            mapping_path,
            timeout_s=timeout_s,
            fd_process=fd_process,
            phase_name="INITIAL_MAPPING"
    ):
        raise TimeoutError("Initial FD index mapping not created")

    mapping_data = read_json_robust(mapping_path, timeout_s=5.0)
    logger.info("[HANDSHAKE] ✅ FD index mapping loaded")

    elapsed = time.time() - start_time
    logger.info(f"[HANDSHAKE] ✅ Handshake complete in {elapsed:.1f}s")

    return cg_data, mapping_data

--------------------------------------------------------------------------------

The file merge_metadata_collector.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MERGE METADATA COLLECTOR - FIXED
================================
Collects merge metadata from C++ JSON exports.
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict, field
from datetime import datetime
import numpy as np

logger = logging.getLogger(__name__)

# ✅ FIXED: Define INF constant
INF = 1000000000


@dataclass
class MergeDecision:
    """Metadata for a single merge decision."""
    iteration: int
    ts1_id: int
    ts2_id: int
    ts1_size: int
    ts2_size: int
    expected_product_size: int
    ts1_transitions: int
    ts2_transitions: int
    ts1_density: float
    ts2_density: float
    ts1_goal_states: int
    ts2_goal_states: int
    ts1_reachable_fraction: float
    ts2_reachable_fraction: float
    ts1_variables: List[int] = field(default_factory=list)
    ts2_variables: List[int] = field(default_factory=list)
    shrunk: bool = False
    reduced: bool = False
    merged_size: int = 0
    merged_goal_states: int = 0
    merged_transitions: int = 0
    merged_density: float = 0.0
    reachable_states: int = 0
    unreachable_states: int = 0
    reachability_ratio: float = 0.0
    shrinking_ratio: float = 0.0
    ts1_f_min: int = 0
    ts1_f_max: int = 0
    ts1_f_mean: float = 0.0
    ts1_f_std: float = 0.0
    ts2_f_min: int = 0
    ts2_f_max: int = 0
    ts2_f_mean: float = 0.0
    ts2_f_std: float = 0.0
    merged_f_min: int = 0
    merged_f_max: int = 0
    merged_f_mean: float = 0.0
    merged_f_std: float = 0.0
    branching_factor: float = 1.0
    search_depth: int = 0
    solution_found: bool = False
    solution_cost: int = 0
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return asdict(self)

    def quality_score(self) -> float:
        """Compute quality score for this merge (0-1, higher is better)."""
        # Component 1: Compression efficiency
        compression = self.shrinking_ratio
        compression_score = min(compression, 1.0)

        # Component 2: F-value stability
        if self.merged_f_mean > 0 and self.ts1_f_mean > 0 and self.ts2_f_mean > 0:
            avg_before = (self.ts1_f_mean + self.ts2_f_mean) / 2.0
            f_stability = 1.0 - abs(self.merged_f_mean - avg_before) / (avg_before + 1e-6)
            f_stability_score = max(0.0, min(f_stability, 1.0))
        else:
            f_stability_score = 0.5

        # Component 3: Reachability
        reachability_score = self.reachability_ratio

        # Component 4: Search efficiency
        if self.branching_factor >= 1.0:
            bf_score = 1.0 / (1.0 + (self.branching_factor - 1.0) / 3.0)
            bf_score = max(0.0, min(bf_score, 1.0))
        else:
            bf_score = 0.5

        # Weighted combination
        overall = (
            0.35 * compression_score +
            0.35 * f_stability_score +
            0.20 * reachability_score +
            0.10 * bf_score
        )

        return max(0.0, min(overall, 1.0))


class MergeMetadataCollector:
    """Collects merge metadata from C++ JSON exports."""

    def __init__(self, fd_output_dir: str = "downward/fd_output"):
        """Initialize collector."""
        self.fd_output_dir = Path(fd_output_dir)
        self.merges: List[MergeDecision] = []

    def load_all_metadata(self) -> int:
        """Load all merge metadata from JSON files."""
        logger.info(f"Loading merge metadata from: {self.fd_output_dir}")

        if not self.fd_output_dir.exists():
            logger.warning(f"fd_output directory not found: {self.fd_output_dir}")
            return 0

        iteration = 0
        loaded_count = 0

        while True:
            before_file = self.fd_output_dir / f"merge_before_{iteration}.json"
            after_file = self.fd_output_dir / f"merge_after_{iteration}.json"

            if not before_file.exists() or not after_file.exists():
                break

            try:
                merge_data = self._load_merge_pair(iteration, before_file, after_file)
                if merge_data:
                    self.merges.append(merge_data)
                    loaded_count += 1
                    logger.info(f"  ✓ Loaded merge {iteration}")
            except Exception as e:
                logger.warning(f"  Failed to load merge {iteration}: {e}")

            iteration += 1

        logger.info(f"✓ Loaded {loaded_count} merge decisions")
        return loaded_count

    def _load_merge_pair(
        self,
        iteration: int,
        before_file: Path,
        after_file: Path
    ) -> Optional[MergeDecision]:
        """Load a single merge pair."""
        with open(before_file) as f:
            before_data = json.load(f)

        with open(after_file) as f:
            after_data = json.load(f)

        # Extract F-value statistics
        ts1_f_stats = before_data.get("ts1_f_stats", {})
        ts2_f_stats = before_data.get("ts2_f_stats", {})
        merged_f_stats = after_data.get("f_stats", {})

        # Count reachable states
        ts1_f = before_data.get("ts1_f_values", [])
        ts2_f = before_data.get("ts2_f_values", [])
        ts1_reachable = sum(1 for f in ts1_f if f != INF and f >= 0) / max(len(ts1_f), 1)
        ts2_reachable = sum(1 for f in ts2_f if f != INF and f >= 0) / max(len(ts2_f), 1)

        merge_data = MergeDecision(
            iteration=iteration,
            ts1_id=before_data.get("ts1_id", -1),
            ts2_id=before_data.get("ts2_id", -1),
            ts1_size=before_data.get("ts1_size", 0),
            ts2_size=before_data.get("ts2_size", 0),
            expected_product_size=before_data.get("expected_product_size", 0),
            ts1_transitions=before_data.get("ts1_transitions", 0),
            ts2_transitions=before_data.get("ts2_transitions", 0),
            ts1_density=before_data.get("ts1_density", 0.0),
            ts2_density=before_data.get("ts2_density", 0.0),
            ts1_goal_states=before_data.get("ts1_goal_states", 0),
            ts2_goal_states=before_data.get("ts2_goal_states", 0),
            ts1_reachable_fraction=ts1_reachable,
            ts2_reachable_fraction=ts2_reachable,
            ts1_variables=before_data.get("ts1_variables", []),
            ts2_variables=before_data.get("ts2_variables", []),
            shrunk=before_data.get("shrunk", False),
            reduced=before_data.get("reduced", False),
            merged_size=after_data.get("merged_size", 0),
            merged_goal_states=after_data.get("merged_goal_states", 0),
            merged_transitions=after_data.get("merged_transitions", 0),
            merged_density=after_data.get("merged_density", 0.0),
            reachable_states=after_data.get("reachable_states", 0),
            unreachable_states=after_data.get("unreachable_states", 0),
            reachability_ratio=after_data.get("reachability_ratio", 0.0),
            shrinking_ratio=after_data.get("shrinking_ratio", 0.0),
            ts1_f_min=ts1_f_stats.get("min", 0),
            ts1_f_max=ts1_f_stats.get("max", 0),
            ts1_f_mean=ts1_f_stats.get("mean", 0.0),
            ts1_f_std=ts1_f_stats.get("std", 0.0),
            ts2_f_min=ts2_f_stats.get("min", 0),
            ts2_f_max=ts2_f_stats.get("max", 0),
            ts2_f_mean=ts2_f_stats.get("mean", 0.0),
            ts2_f_std=ts2_f_stats.get("std", 0.0),
            merged_f_min=merged_f_stats.get("min", 0),
            merged_f_max=merged_f_stats.get("max", 0),
            merged_f_mean=merged_f_stats.get("mean", 0.0),
            merged_f_std=merged_f_stats.get("std", 0.0),
            branching_factor=after_data.get("search_signals", {}).get("branching_factor", 1.0),
            search_depth=after_data.get("search_signals", {}).get("search_depth", 0),
            solution_found=after_data.get("search_signals", {}).get("solution_found", False),
            solution_cost=after_data.get("search_signals", {}).get("solution_cost", 0),
        )

        return merge_data

    def get_statistics(self) -> Dict[str, Any]:
        """Get summary statistics of all merges."""
        if not self.merges:
            return {}

        quality_scores = [m.quality_score() for m in self.merges]
        compression_ratios = [m.shrinking_ratio for m in self.merges]
        branching_factors = [m.branching_factor for m in self.merges]

        return {
            'total_merges': len(self.merges),
            'avg_quality_score': float(np.mean(quality_scores)),
            'min_quality_score': float(np.min(quality_scores)),
            'max_quality_score': float(np.max(quality_scores)),
            'avg_compression_ratio': float(np.mean(compression_ratios)),
            'avg_branching_factor': float(np.mean(branching_factors)),
            'avg_reachability': float(np.mean([m.reachability_ratio for m in self.merges])),
            'solution_found_rate': sum(1 for m in self.merges if m.solution_found) / len(self.merges) if self.merges else 0,
        }

    def export_to_json(self, output_path: str) -> None:
        """Export all metadata to JSON."""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        data = {
            'metadata': [m.to_dict() for m in self.merges],
            'statistics': self.get_statistics(),
            'timestamp': datetime.now().isoformat(),
        }

        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2, default=str)

        logger.info(f"✓ Exported metadata to: {output_path}")

--------------------------------------------------------------------------------

The file merge_explainability.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MERGE EXPLAINABILITY - ENHANCED
===============================
Comprehensive analysis of merge decisions from metadata.
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict, field
from datetime import datetime
import numpy as np

from merge_metadata_collector import MergeMetadataCollector, MergeDecision

logger = logging.getLogger(__name__)


@dataclass
class MergeExplanation:
    """Complete explanation for a merge decision."""
    iteration: int
    ts1_id: int
    ts2_id: int

    # Why were these chosen?
    rationale: str  # Human-readable explanation

    # Pre-merge properties that motivated selection
    ts1_properties: Dict[str, Any] = field(default_factory=dict)
    ts2_properties: Dict[str, Any] = field(default_factory=dict)
    relative_properties: Dict[str, Any] = field(default_factory=dict)

    # Outcome
    outcome: Dict[str, Any] = field(default_factory=dict)

    # Quality assessment
    quality_score: float = 0.0
    success_indicators: List[str] = field(default_factory=list)
    risk_indicators: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class MergeExplainabilityAnalyzer:
    """Analyzes and explains merge decisions."""

    def __init__(self, metadata_collector: Optional[MergeMetadataCollector] = None):
        """Initialize analyzer."""
        self.collector = metadata_collector or MergeMetadataCollector()
        self.explanations: List[MergeExplanation] = []
        self._analyze_all_merges()

    def _analyze_all_merges(self) -> None:
        """Generate explanations for all merges."""
        for merge in self.collector.merges:
            explanation = self._explain_merge(merge)
            self.explanations.append(explanation)

    def _explain_merge(self, merge: MergeDecision) -> MergeExplanation:
        """Generate explanation for a single merge."""

        # Extract properties
        ts1_props = {
            'size': merge.ts1_size,
            'transitions': merge.ts1_transitions,
            'density': merge.ts1_density,
            'goal_states': merge.ts1_goal_states,
            'variables': len(merge.ts1_variables),
            'f_value_quality': {
                'min': merge.ts1_f_min,
                'mean': merge.ts1_f_mean,
                'max': merge.ts1_f_max,
                'std': merge.ts1_f_std,
            }
        }

        ts2_props = {
            'size': merge.ts2_size,
            'transitions': merge.ts2_transitions,
            'density': merge.ts2_density,
            'goal_states': merge.ts2_goal_states,
            'variables': len(merge.ts2_variables),
            'f_value_quality': {
                'min': merge.ts2_f_min,
                'mean': merge.ts2_f_mean,
                'max': merge.ts2_f_max,
                'std': merge.ts2_f_std,
            }
        }

        relative_props = {
            'size_ratio': merge.ts2_size / max(merge.ts1_size, 1),
            'transition_ratio': merge.ts2_transitions / max(merge.ts1_transitions, 1),
            'density_similarity': 1.0 - abs(merge.ts1_density - merge.ts2_density),
        }

        outcome = {
            'merged_size': merge.merged_size,
            'expected_size': merge.expected_product_size,
            'compression_ratio': merge.shrinking_ratio,
            'reachability': merge.reachability_ratio,
            'branching_factor': merge.branching_factor,
            'solution_found': merge.solution_found,
        }

        # Generate rationale
        rationale = self._generate_rationale(merge, ts1_props, ts2_props, relative_props, outcome)

        # Identify success/risk indicators
        success, risks = self._identify_indicators(merge, outcome)

        return MergeExplanation(
            iteration=merge.iteration,
            ts1_id=merge.ts1_id,
            ts2_id=merge.ts2_id,
            rationale=rationale,
            ts1_properties=ts1_props,
            ts2_properties=ts2_props,
            relative_properties=relative_props,
            outcome=outcome,
            quality_score=merge.quality_score(),
            success_indicators=success,
            risk_indicators=risks,
        )

    def _generate_rationale(
            self,
            merge: MergeDecision,
            ts1_props: Dict,
            ts2_props: Dict,
            relative_props: Dict,
            outcome: Dict
    ) -> str:
        """Generate human-readable explanation."""

        points = []

        # Size analysis
        if merge.ts1_size > merge.ts2_size:
            points.append(
                f"Merging smaller TS{merge.ts2_id} ({merge.ts2_size} states) into larger TS{merge.ts1_id} ({merge.ts1_size} states)")
        elif merge.ts2_size > merge.ts1_size:
            points.append(
                f"Merging smaller TS{merge.ts1_id} ({merge.ts1_size} states) into larger TS{merge.ts2_id} ({merge.ts2_size} states)")
        else:
            points.append(f"Merging equally-sized transition systems ({merge.ts1_size} states each)")

        # Density analysis
        if merge.ts1_density > merge.ts2_density:
            points.append(
                f"TS{merge.ts1_id} is denser ({merge.ts1_density:.2f} edges/state) than TS{merge.ts2_id} ({merge.ts2_density:.2f})")
        else:
            points.append(
                f"TS{merge.ts2_id} is denser ({merge.ts2_density:.2f} edges/state) than TS{merge.ts1_id} ({merge.ts1_density:.2f})")

        # Outcome analysis
        if outcome['compression_ratio'] < 0.5:
            points.append(
                f"Successfully compressed merged product to {outcome['compression_ratio'] * 100:.1f}% of theoretical maximum")
        elif outcome['compression_ratio'] < 1.0:
            points.append(
                f"Moderate compression achieved ({outcome['compression_ratio'] * 100:.1f}% of theoretical maximum)")
        else:
            points.append(
                f"WARNING: Merged size ({outcome['merged_size']}) exceeds theoretical maximum ({outcome['expected_size']})")

        # Reachability
        if outcome['reachability'] > 0.9:
            points.append("Excellent reachability preserved")
        elif outcome['reachability'] > 0.7:
            points.append(f"Good reachability ({outcome['reachability'] * 100:.1f}%)")
        else:
            points.append(f"Warning: Low reachability ({outcome['reachability'] * 100:.1f}%)")

        # Search efficiency
        if outcome['branching_factor'] < 2.0:
            points.append(f"Excellent branching factor ({outcome['branching_factor']:.2f})")
        else:
            points.append(f"High branching factor ({outcome['branching_factor']:.2f}) may reduce search efficiency")

        return " | ".join(points)

    def _identify_indicators(self, merge: MergeDecision, outcome: Dict) -> tuple:
        """Identify success and risk indicators."""
        success = []
        risks = []

        # Success indicators
        if merge.shrinking_ratio < 0.5:
            success.append("Excellent compression (< 50%)")
        if merge.reachability_ratio > 0.85:
            success.append("High reachability preserved (> 85%)")
        if merge.branching_factor < 1.5:
            success.append("Low branching factor (< 1.5)")
        if merge.solution_found:
            success.append("Solution found")
        if merge.quality_score() > 0.7:
            success.append("High quality merge")

        # Risk indicators
        if merge.shrinking_ratio > 1.0:
            risks.append("State explosion after merge")
        if merge.reachability_ratio < 0.5:
            risks.append("Low reachability (> 50% unreachable)")
        if merge.branching_factor > 5.0:
            risks.append("Very high branching factor")
        if merge.unreachable_states > merge.merged_size * 0.5:
            risks.append("Most states became unreachable")
        if merge.quality_score() < 0.3:
            risks.append("Low quality merge")

        return success, risks

    def get_best_merges(self, n: int = 5) -> List[MergeExplanation]:
        """Get top N best merges."""
        sorted_explanations = sorted(
            self.explanations,
            key=lambda e: e.quality_score,
            reverse=True
        )
        return sorted_explanations[:n]

    def get_worst_merges(self, n: int = 5) -> List[MergeExplanation]:
        """Get top N worst merges."""
        sorted_explanations = sorted(
            self.explanations,
            key=lambda e: e.quality_score
        )
        return sorted_explanations[:n]

    def export_explanations(self, output_path: str) -> None:
        """Export all explanations to JSON."""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        data = {
            'explanations': [e.to_dict() for e in self.explanations],
            'summary': {
                'total': len(self.explanations),
                'avg_quality': float(np.mean([e.quality_score for e in self.explanations])),
                'best_quality': float(np.max([e.quality_score for e in self.explanations])),
                'worst_quality': float(np.min([e.quality_score for e in self.explanations])),
            },
            'timestamp': datetime.now().isoformat(),
        }

        with open(output_path, 'w') as f:
            json.dump(data, f, indent=2, default=str)

        logger.info(f"Exported explanations to: {output_path}")

    def generate_human_report(self, output_path: str) -> None:
        """Generate human-readable report."""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w') as f:
            f.write("=" * 100 + "\n")
            f.write("MERGE EXPLAINABILITY REPORT\n")
            f.write("=" * 100 + "\n\n")

            f.write(f"Total merges analyzed: {len(self.explanations)}\n")
            f.write(f"Average quality: {np.mean([e.quality_score for e in self.explanations]):.3f}\n\n")

            # Best merges
            f.write("\n" + "=" * 100 + "\n")
            f.write("TOP 5 BEST MERGES\n")
            f.write("=" * 100 + "\n\n")

            for i, exp in enumerate(self.get_best_merges(5), 1):
                f.write(f"[{i}] Iteration {exp.iteration}: TS{exp.ts1_id} + TS{exp.ts2_id}\n")
                f.write(f"    Quality: {exp.quality_score:.3f}\n")
                f.write(f"    {exp.rationale}\n")
                if exp.success_indicators:
                    f.write(f"    ✓ {', '.join(exp.success_indicators)}\n")
                f.write("\n")

            # Worst merges
            f.write("\n" + "=" * 100 + "\n")
            f.write("TOP 5 WORST MERGES\n")
            f.write("=" * 100 + "\n\n")

            for i, exp in enumerate(self.get_worst_merges(5), 1):
                f.write(f"[{i}] Iteration {exp.iteration}: TS{exp.ts1_id} + TS{exp.ts2_id}\n")
                f.write(f"    Quality: {exp.quality_score:.3f}\n")
                f.write(f"    {exp.rationale}\n")
                if exp.risk_indicators:
                    f.write(f"    ⚠️  {', '.join(exp.risk_indicators)}\n")
                f.write("\n")

        logger.info(f"Generated human report: {output_path}")


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    analyzer = MergeExplainabilityAnalyzer()
    analyzer.export_explanations("merge_explanations.json")
    analyzer.generate_human_report("merge_explainability_report.txt")

    print("\n✅ Explainability analysis complete!")

--------------------------------------------------------------------------------

The file merge_choice_analysis.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MERGE CHOICE ANALYSIS - ENHANCED
================================
Learn patterns from good and bad merge choices.
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
import numpy as np
from collections import defaultdict

from merge_metadata_collector import MergeMetadataCollector, MergeDecision

logger = logging.getLogger(__name__)


class MergeChoiceAnalyzer:
    """Analyzes patterns in merge decisions to learn optimal strategies."""

    def __init__(self, metadata_collector: Optional[MergeMetadataCollector] = None):
        """Initialize analyzer."""
        self.collector = metadata_collector or MergeMetadataCollector()

    def extract_features(self, merge: MergeDecision) -> Dict[str, float]:
        """Extract numerical features from a merge."""
        return {
            'ts1_size': float(merge.ts1_size),
            'ts2_size': float(merge.ts2_size),
            'size_ratio': float(merge.ts2_size / max(merge.ts1_size, 1)),
            'size_product': float(merge.ts1_size * merge.ts2_size),
            'ts1_density': float(merge.ts1_density),
            'ts2_density': float(merge.ts2_density),
            'density_diff': float(abs(merge.ts1_density - merge.ts2_density)),
            'ts1_reachable': float(merge.ts1_reachable_fraction),
            'ts2_reachable': float(merge.ts2_reachable_fraction),
            'ts1_goal_ratio': float(merge.ts1_goal_states / max(merge.ts1_size, 1)),
            'ts2_goal_ratio': float(merge.ts2_goal_states / max(merge.ts2_size, 1)),
            'ts1_f_mean': float(merge.ts1_f_mean),
            'ts2_f_mean': float(merge.ts2_f_mean),
            'f_stability_delta': float(abs(merge.ts1_f_mean - merge.ts2_f_mean)),
        }

    def analyze_good_vs_bad(self, threshold: float = 0.6) -> Dict[str, Any]:
        """Compare features of good merges vs bad merges."""

        good_merges = [m for m in self.collector.merges if m.quality_score() >= threshold]
        bad_merges = [m for m in self.collector.merges if m.quality_score() < threshold]

        if not good_merges or not bad_merges:
            logger.warning("Insufficient data for good/bad comparison")
            return {}

        good_features = [self.extract_features(m) for m in good_merges]
        bad_features = [self.extract_features(m) for m in bad_merges]

        analysis = {}
        all_keys = set(good_features[0].keys())

        for key in all_keys:
            good_values = np.array([f[key] for f in good_features])
            bad_values = np.array([f[key] for f in bad_features])

            analysis[key] = {
                'good': {
                    'mean': float(np.mean(good_values)),
                    'std': float(np.std(good_values)),
                    'min': float(np.min(good_values)),
                    'max': float(np.max(good_values)),
                },
                'bad': {
                    'mean': float(np.mean(bad_values)),
                    'std': float(np.std(bad_values)),
                    'min': float(np.min(bad_values)),
                    'max': float(np.max(bad_values)),
                },
                'delta': float(np.mean(good_values) - np.mean(bad_values)),
            }

        return analysis

    def find_patterns_in_good_merges(self) -> Dict[str, Any]:
        """Identify common patterns in high-quality merges."""

        good_merges = [m for m in self.collector.merges if m.quality_score() >= 0.7]

        if not good_merges:
            return {}

        patterns = {
            'compression': [],
            'reachability': [],
            'branching': [],
            'f_stability': [],
        }

        # Compression pattern
        compressions = [m.shrinking_ratio for m in good_merges]
        patterns['compression'] = {
            'mean': float(np.mean(compressions)),
            'std': float(np.std(compressions)),
            'ideal_range': [float(np.percentile(compressions, 25)),
                            float(np.percentile(compressions, 75))],
        }

        # Reachability pattern
        reachabilities = [m.reachability_ratio for m in good_merges]
        patterns['reachability'] = {
            'mean': float(np.mean(reachabilities)),
            'std': float(np.std(reachabilities)),
            'ideal_range': [float(np.percentile(reachabilities, 25)),
                            float(np.percentile(reachabilities, 75))],
        }

        # Branching pattern
        branching = [m.branching_factor for m in good_merges]
        patterns['branching'] = {
            'mean': float(np.mean(branching)),
            'std': float(np.std(branching)),
            'ideal_range': [float(np.percentile(branching, 25)),
                            float(np.percentile(branching, 75))],
        }

        # F-stability pattern
        f_stability = [m.merged_f_std / max(m.merged_f_mean, 1) for m in good_merges]
        patterns['f_stability'] = {
            'mean': float(np.mean(f_stability)),
            'std': float(np.std(f_stability)),
            'ideal_range': [float(np.percentile(f_stability, 25)),
                            float(np.percentile(f_stability, 75))],
        }

        return patterns

    def generate_recommendations(self) -> List[str]:
        """Generate recommendations based on learned patterns."""

        recommendations = []

        good_merges = [m for m in self.collector.merges if m.quality_score() >= 0.7]
        bad_merges = [m for m in self.collector.merges if m.quality_score() < 0.3]

        if not good_merges or not bad_merges:
            return recommendations

        # Recommendation 1: Size balance
        avg_good_ratio = np.mean([m.ts2_size / max(m.ts1_size, 1) for m in good_merges])
        avg_bad_ratio = np.mean([m.ts2_size / max(m.ts1_size, 1) for m in bad_merges])

        if avg_good_ratio < 0.3 or avg_good_ratio > 3.0:
            recommendations.append(
                f"Prefer merging similarly-sized transition systems (ratio ~1.0) rather than "
                f"highly imbalanced pairs (ratio {avg_good_ratio:.2f})"
            )

        # Recommendation 2: Density
        avg_good_density_diff = np.mean([abs(m.ts1_density - m.ts2_density) for m in good_merges])
        avg_bad_density_diff = np.mean([abs(m.ts1_density - m.ts2_density) for m in bad_merges])

        if avg_good_density_diff < avg_bad_density_diff:
            recommendations.append(
                f"Prefer merging TS with similar transition density "
                f"(good merges differ by {avg_good_density_diff:.3f}, bad by {avg_bad_density_diff:.3f})"
            )

        # Recommendation 3: Reachability
        avg_good_reachability = np.mean([m.reachability_ratio for m in good_merges])
        avg_bad_reachability = np.mean([m.reachability_ratio for m in bad_merges])

        if avg_good_reachability > avg_bad_reachability + 0.2:
            recommendations.append(
                f"Prioritize merges that preserve reachability "
                f"(good merges: {avg_good_reachability:.1%}, bad merges: {avg_bad_reachability:.1%})"
            )

        # Recommendation 4: Compression
        avg_good_compression = np.mean([m.shrinking_ratio for m in good_merges])
        avg_bad_compression = np.mean([m.shrinking_ratio for m in bad_merges])

        if avg_good_compression < 0.7:
            recommendations.append(
                f"Target merges with good compression ratios "
                f"(good: {avg_good_compression:.1%}, bad: {avg_bad_compression:.1%})"
            )

        return recommendations

    def export_analysis(self, output_dir: str) -> None:
        """Export full analysis."""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        analysis_data = {
            'good_vs_bad': self.analyze_good_vs_bad(),
            'good_merge_patterns': self.find_patterns_in_good_merges(),
            'recommendations': self.generate_recommendations(),
            'timestamp': str(datetime.now()),
        }

        with open(output_dir / "merge_choice_analysis.json", 'w') as f:
            json.dump(analysis_data, f, indent=2, default=str)

        logger.info(f"Exported analysis to: {output_dir}/merge_choice_analysis.json")

    def generate_report(self, output_path: str) -> None:
        """Generate human-readable report."""
        from datetime import datetime

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w') as f:
            f.write("=" * 100 + "\n")
            f.write("MERGE CHOICE ANALYSIS REPORT\n")
            f.write("=" * 100 + "\n\n")

            f.write(f"Generated: {datetime.now().isoformat()}\n")
            f.write(f"Merges analyzed: {len(self.collector.merges)}\n\n")

            # Patterns
            f.write("\n" + "=" * 100 + "\n")
            f.write("PATTERNS IN GOOD MERGES\n")
            f.write("=" * 100 + "\n\n")

            patterns = self.find_patterns_in_good_merges()
            for key, data in patterns.items():
                f.write(f"{key.upper()}:\n")
                f.write(f"  Mean: {data['mean']:.3f}\n")
                f.write(f"  Range: {data['ideal_range'][0]:.3f} - {data['ideal_range'][1]:.3f}\n\n")

            # Recommendations
            f.write("\n" + "=" * 100 + "\n")
            f.write("RECOMMENDATIONS\n")
            f.write("=" * 100 + "\n\n")

            for i, rec in enumerate(self.generate_recommendations(), 1):
                f.write(f"[{i}] {rec}\n\n")

        logger.info(f"Generated report: {output_path}")


# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    from datetime import datetime

    analyzer = MergeChoiceAnalyzer()
    analyzer.export_analysis("merge_choice_results/")
    analyzer.generate_report("merge_choice_report.txt")

    print("\n✅ Merge choice analysis complete!")

--------------------------------------------------------------------------------

The file analyze_merge_metadata.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MASTER MERGE METADATA ANALYSIS
=============================
Orchestrates complete analysis pipeline including:
  - FD merge metadata (C++ exports)
  - GNN decision metadata (Python collection)
  - Unified explainability and choice analysis
"""

import sys
import os
import logging
from pathlib import Path
from datetime import datetime

sys.path.insert(0, os.getcwd())

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """Complete analysis pipeline."""

    logger.info("\n" + "=" * 100)
    logger.info("COMPLETE MERGE METADATA ANALYSIS PIPELINE")
    logger.info("=" * 100 + "\n")

    output_root = Path("merge_metadata_analysis_results")
    output_root.mkdir(parents=True, exist_ok=True)

    # ========================================================================
    # PHASE 1: AGGREGATE FD MERGE METADATA (from C++)
    # ========================================================================

    logger.info("\n[PHASE 1] Aggregating Fast Downward merge metadata...")
    logger.info("-" * 100 + "\n")

    try:
        from merge_metadata_collector import MergeMetadataCollector

        fd_collector = MergeMetadataCollector()
        fd_stats = fd_collector.get_statistics()

        logger.info(f"✓ FD merges collected: {fd_stats.get('total_merges', 0)}")

        # Export FD analysis
        fd_collector.export_to_json(output_root / "fd_metadata.json")
        fd_collector.export_good_bad_analysis(output_root / "fd_analysis")

    except Exception as e:
        logger.warning(f"⚠️ FD metadata aggregation failed: {e}")
        fd_collector = None

    # ========================================================================
    # PHASE 2: AGGREGATE GNN DECISION METADATA (from Python)
    # ========================================================================

    logger.info("\n[PHASE 2] Aggregating GNN decision metadata...")
    logger.info("-" * 100 + "\n")

    try:
        # Import the new aggregator
        sys.path.insert(0, os.getcwd())
        from gnn_metadata_aggregator import GNNMetadataAggregator

        gnn_aggregator = GNNMetadataAggregator()
        episodes_loaded = gnn_aggregator.load_all_episodes()

        if episodes_loaded > 0:
            logger.info(f"✓ GNN episodes loaded: {episodes_loaded}")
            gnn_aggregator.export_aggregated_report(output_root / "gnn_metadata")
        else:
            logger.warning("⚠️ No GNN metadata episodes found")
            gnn_aggregator = None

    except Exception as e:
        logger.warning(f"⚠️ GNN metadata aggregation failed: {e}")
        gnn_aggregator = None

    # ========================================================================
    # PHASE 3: EXPLAINABILITY ANALYSIS (FD merges)
    # ========================================================================

    if fd_collector and fd_collector.merges:
        logger.info("\n[PHASE 3] Generating merge explainability...")
        logger.info("-" * 100 + "\n")

        try:
            from merge_explainability import MergeExplainabilityAnalyzer

            explainer = MergeExplainabilityAnalyzer(fd_collector)
            explainer.export_explanations(output_root / "fd_explanations.json")
            explainer.generate_human_report(output_root / "fd_explainability_report.txt")

            logger.info("✓ Explainability analysis complete")

        except Exception as e:
            logger.warning(f"⚠️ Explainability analysis failed: {e}")

    # ========================================================================
    # PHASE 4: CHOICE PATTERN ANALYSIS (FD merges)
    # ========================================================================

    if fd_collector and fd_collector.merges:
        logger.info("\n[PHASE 4] Analyzing merge choice patterns...")
        logger.info("-" * 100 + "\n")

        try:
            from merge_choice_analysis import MergeChoiceAnalyzer

            choice_analyzer = MergeChoiceAnalyzer(fd_collector)
            choice_analyzer.export_analysis(output_root / "fd_choice_analysis")
            choice_analyzer.generate_report(output_root / "fd_choice_report.txt")

            logger.info("✓ Choice analysis complete")

        except Exception as e:
            logger.warning(f"⚠️ Choice analysis failed: {e}")

    # ========================================================================
    # PHASE 5: GENERATE MASTER SUMMARY
    # ========================================================================

    logger.info("\n[PHASE 5] Generating master summary...")
    logger.info("-" * 100 + "\n")

    _generate_master_summary(output_root, fd_collector, gnn_aggregator)

    # ========================================================================
    # COMPLETION
    # ========================================================================

    logger.info("\n" + "=" * 100)
    logger.info("✅ COMPLETE MERGE METADATA ANALYSIS FINISHED")
    logger.info("=" * 100 + "\n")

    logger.info(f"Results saved to: {output_root.absolute()}/\n")
    logger.info("Key outputs:")
    logger.info(f"  - FD metadata:          {output_root / 'fd_metadata.json'}")
    logger.info(f"  - GNN metadata:         {output_root / 'gnn_metadata' / 'gnn_metadata_report.json'}")
    logger.info(f"  - Explanations:         {output_root / 'fd_explanations.json'}")
    logger.info(f"  - Choice patterns:      {output_root / 'fd_choice_analysis'}")
    logger.info(f"  - Master summary:       {output_root / 'METADATA_ANALYSIS_SUMMARY.txt'}\n")

    return 0


def _generate_master_summary(output_root: Path, fd_collector, gnn_aggregator) -> None:
    """Generate comprehensive master summary."""

    summary_file = output_root / "METADATA_ANALYSIS_SUMMARY.txt"

    with open(summary_file, 'w') as f:
        f.write("=" * 100 + "\n")
        f.write("MASTER MERGE METADATA ANALYSIS SUMMARY\n")
        f.write("=" * 100 + "\n\n")

        f.write(f"Analysis timestamp: {datetime.now().isoformat()}\n\n")

        # FD Summary
        if fd_collector:
            f.write("FAST DOWNWARD MERGE ANALYSIS\n")
            f.write("-" * 100 + "\n")

            stats = fd_collector.get_statistics()
            f.write(f"  Total merges:                 {stats.get('total_merges', 0)}\n")
            f.write(f"  Average quality score:        {stats.get('avg_quality_score', 0):.3f}\n")
            f.write(f"  Average compression ratio:    {stats.get('avg_compression_ratio', 0):.3f}\n")
            f.write(f"  Average branching factor:     {stats.get('avg_branching_factor', 0):.3f}\n")
            f.write(f"  Average reachability:         {stats.get('avg_reachability', 0):.1%}\n\n")

        # GNN Summary
        if gnn_aggregator:
            f.write("GNN DECISION ANALYSIS\n")
            f.write("-" * 100 + "\n")

            f.write(f"  Total GNN decisions:          {len(gnn_aggregator.decisions)}\n")
            f.write(f"  Episodes processed:           {len(gnn_aggregator.episodes)}\n")
            f.write(f"  Unique problems:              {len(gnn_aggregator.problems_processed)}\n\n")

        f.write("OUTPUT FILES\n")
        f.write("-" * 100 + "\n")
        f.write("  See individual files for detailed analysis:\n")
        f.write(f"    - fd_metadata.json (FD merge metadata)\n")
        f.write(f"    - gnn_metadata/ (GNN decision statistics)\n")
        f.write(f"    - fd_explanations.json (merge explanations)\n")
        f.write(f"    - fd_choice_analysis/ (pattern analysis)\n\n")

        f.write("=" * 100 + "\n")

    logger.info(f"✓ Master summary: {summary_file}")


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file gnn_metadata_aggregator.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GNN METADATA AGGREGATOR
=======================
Aggregates all GNN decision metadata from multiple episodes/problems.
Must be run AFTER training/evaluation is complete.
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, List, Any
from collections import defaultdict
import numpy as np
from datetime import datetime


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GNNMetadataAggregator:
    """Aggregates GNN decision metadata from all episodes."""

    def __init__(self, gnn_metadata_dir: str = "downward/gnn_metadata"):
        self.gnn_metadata_dir = Path(gnn_metadata_dir)
        self.episodes = []
        self.decisions = []
        self.problems_processed = set()

    def load_all_episodes(self) -> int:
        """Load all episode metadata files."""
        if not self.gnn_metadata_dir.exists():
            logger.warning(f"Metadata directory not found: {self.gnn_metadata_dir}")
            return 0

        logger.info(f"Loading episodes from: {self.gnn_metadata_dir}")

        count = 0
        for episode_file in sorted(self.gnn_metadata_dir.glob("episode_*.json")):
            try:
                with open(episode_file, 'r') as f:
                    episode_data = json.load(f)

                self.episodes.append(episode_data)

                # Flatten decisions
                if 'decisions' in episode_data:
                    for decision in episode_data['decisions']:
                        self.decisions.append({
                            **decision,
                            'episode_file': episode_file.name,
                        })

                    self.problems_processed.add(episode_data.get('problem', 'unknown'))

                count += 1

            except Exception as e:
                logger.warning(f"Failed to load {episode_file}: {e}")

        logger.info(f"✓ Loaded {count} episodes with {len(self.decisions)} total decisions")
        return count

    def compute_decision_statistics(self) -> Dict[str, Any]:
        """Compute statistics on GNN decisions."""

        if not self.decisions:
            logger.warning("No decisions loaded")
            return {}

        logger.info("\nComputing decision statistics...")

        rewards = [d['reward_received'] for d in self.decisions]
        deltas = [d['merge_info'].get('delta_states', 0) for d in self.decisions]
        expansions = [d['merge_info'].get('num_expansions', 0) for d in self.decisions]

        stats = {
            'total_decisions': len(self.decisions),
            'problems_processed': len(self.problems_processed),
            'problems': sorted(list(self.problems_processed)),

            'reward_statistics': {
                'mean': float(np.mean(rewards)),
                'median': float(np.median(rewards)),
                'std': float(np.std(rewards)),
                'min': float(np.min(rewards)),
                'max': float(np.max(rewards)),
                'q1': float(np.percentile(rewards, 25)),
                'q3': float(np.percentile(rewards, 75)),
            },

            'state_dynamics': {
                'mean_delta_states': float(np.mean(deltas)),
                'mean_expansions': float(np.mean(expansions)),
                'problems_with_expansion': sum(1 for d in deltas if d > 0),
                'problems_with_reduction': sum(1 for d in deltas if d < 0),
            },

            'decision_distribution': self._compute_decision_distribution(),
        }

        logger.info(f"\nDecision Statistics:")
        logger.info(f"  Total decisions: {stats['total_decisions']}")
        logger.info(f"  Avg reward: {stats['reward_statistics']['mean']:.4f}")
        logger.info(f"  Avg delta states: {stats['state_dynamics']['mean_delta_states']:.2f}")

        return stats

    def _compute_decision_distribution(self) -> Dict[str, Any]:
        """Analyze distribution of decisions."""

        decisions_per_episode = defaultdict(int)
        rewards_per_episode = defaultdict(list)

        for decision in self.decisions:
            episode = decision['episode_file']
            decisions_per_episode[episode] += 1
            rewards_per_episode[episode].append(decision['reward_received'])

        return {
            'avg_decisions_per_episode': float(np.mean(list(decisions_per_episode.values()))),
            'max_decisions_in_episode': max(decisions_per_episode.values()),
            'min_decisions_in_episode': min(decisions_per_episode.values()),
        }

    def identify_patterns(self) -> Dict[str, Any]:
        """Identify patterns in GNN decisions."""

        logger.info("\nIdentifying decision patterns...")

        high_reward_decisions = [d for d in self.decisions if d['reward_received'] > 0.1]
        low_reward_decisions = [d for d in self.decisions if d['reward_received'] < -0.5]

        patterns = {
            'high_reward_decisions': {
                'count': len(high_reward_decisions),
                'avg_delta_states': float(np.mean([
                    d['merge_info'].get('delta_states', 0)
                    for d in high_reward_decisions
                ])) if high_reward_decisions else 0,
                'avg_node_count': float(np.mean([
                    d['observation_shape']['num_nodes']
                    for d in high_reward_decisions
                ])) if high_reward_decisions else 0,
            },

            'low_reward_decisions': {
                'count': len(low_reward_decisions),
                'avg_delta_states': float(np.mean([
                    d['merge_info'].get('delta_states', 0)
                    for d in low_reward_decisions
                ])) if low_reward_decisions else 0,
                'avg_node_count': float(np.mean([
                    d['observation_shape']['num_nodes']
                    for d in low_reward_decisions
                ])) if low_reward_decisions else 0,
            },
        }

        logger.info(f"  High reward decisions: {patterns['high_reward_decisions']['count']}")
        logger.info(f"  Low reward decisions: {patterns['low_reward_decisions']['count']}")

        return patterns

    def export_aggregated_report(self, output_dir: str = "gnn_metadata_results") -> str:
        """Export comprehensive aggregated report."""

        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"\nExporting results to: {output_dir}")

        stats = self.compute_decision_statistics()
        patterns = self.identify_patterns()

        # Main report
        report_data = {
            'metadata_timestamp': datetime.now().isoformat(),
            'summary': {
                'total_episodes': len(self.episodes),
                'total_decisions': len(self.decisions),
                'unique_problems': len(self.problems_processed),
            },
            'statistics': stats,
            'patterns': patterns,
            'episodes': self.episodes,
            'all_decisions': self.decisions,
        }

        report_path = output_dir / "gnn_metadata_report.json"
        with open(report_path, 'w') as f:
            json.dump(report_data, f, indent=2, default=str)

        logger.info(f"✓ Saved report: {report_path}")

        # Summary TXT
        self._write_summary_report(output_dir, stats, patterns)

        return str(output_dir)

    def _write_summary_report(self, output_dir: Path, stats: Dict, patterns: Dict) -> None:
        """Write human-readable summary."""

        report_path = output_dir / "gnn_metadata_summary.txt"

        with open(report_path, 'w') as f:
            f.write("=" * 90 + "\n")
            f.write("GNN METADATA AGGREGATION REPORT\n")
            f.write("=" * 90 + "\n\n")

            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
            f.write(f"Total episodes: {len(self.episodes)}\n")
            f.write(f"Total decisions: {len(self.decisions)}\n")
            f.write(f"Unique problems: {len(self.problems_processed)}\n\n")

            f.write("REWARD STATISTICS\n")
            f.write("-" * 90 + "\n")
            f.write(f"  Mean reward:     {stats['reward_statistics']['mean']:>8.4f}\n")
            f.write(f"  Median reward:   {stats['reward_statistics']['median']:>8.4f}\n")
            f.write(f"  Std reward:      {stats['reward_statistics']['std']:>8.4f}\n")
            f.write(f"  Reward range:    [{stats['reward_statistics']['min']:.4f}, "
                    f"{stats['reward_statistics']['max']:.4f}]\n\n")

            f.write("STATE DYNAMICS\n")
            f.write("-" * 90 + "\n")
            f.write(f"  Mean delta states:        {stats['state_dynamics']['mean_delta_states']:>8.2f}\n")
            f.write(f"  Mean expansions:          {stats['state_dynamics']['mean_expansions']:>8.0f}\n")
            f.write(f"  Problems with expansion:  {stats['state_dynamics']['problems_with_expansion']}\n")
            f.write(f"  Problems with reduction:  {stats['state_dynamics']['problems_with_reduction']}\n\n")

            f.write("DECISION PATTERNS\n")
            f.write("-" * 90 + "\n")
            f.write(f"  High reward decisions:  {patterns['high_reward_decisions']['count']}\n")
            f.write(f"    Avg delta states:     {patterns['high_reward_decisions']['avg_delta_states']:.2f}\n")
            f.write(f"    Avg node count:       {patterns['high_reward_decisions']['avg_node_count']:.1f}\n\n")
            f.write(f"  Low reward decisions:   {patterns['low_reward_decisions']['count']}\n")
            f.write(f"    Avg delta states:     {patterns['low_reward_decisions']['avg_delta_states']:.2f}\n")
            f.write(f"    Avg node count:       {patterns['low_reward_decisions']['avg_node_count']:.1f}\n\n")

            f.write("=" * 90 + "\n")

        logger.info(f"✓ Saved summary: {report_path}")


def main():
    """Main execution."""

    import argparse

    parser = argparse.ArgumentParser(description="GNN Metadata Aggregator")
    parser.add_argument("--metadata-dir", default="downward/gnn_metadata",
                        help="GNN metadata directory")
    parser.add_argument("--output", default="gnn_metadata_results",
                        help="Output directory")

    args = parser.parse_args()

    aggregator = GNNMetadataAggregator(args.metadata_dir)

    if aggregator.load_all_episodes() == 0:
        logger.error("No episodes loaded!")
        return 1

    aggregator.export_aggregated_report(args.output)

    logger.info(f"\n✅ Complete! Results in: {args.output}/")
    return 0


if __name__ == "__main__":
    import sys

    sys.exit(main())

--------------------------------------------------------------------------------

The file run_merge_analysis.py code is in the following block:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RUN MERGE ANALYSIS - Post-Evaluation Metadata Analysis
======================================================
Called AFTER evaluation to analyze collected metadata.
"""

import sys
import os
import logging
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_merge_analysis(evaluation_output_dir: str) -> bool:
    """
    Run complete merge metadata analysis pipeline.

    This should be called AFTER evaluation is complete.
    By then, the following files should exist:
      - downward/fd_output/merge_before_*.json
      - downward/fd_output/merge_after_*.json
      - downward/gnn_metadata/episode_*.json

    Args:
        evaluation_output_dir: Directory where evaluation results are saved

    Returns:
        True if analysis successful
    """

    logger.info("\n" + "=" * 90)
    logger.info("STAGE 1: MERGE METADATA ANALYSIS")
    logger.info("=" * 90 + "\n")

    try:
        # Import analysis modules
        from merge_metadata_collector import MergeMetadataCollector
        from merge_explainability import MergeExplainabilityAnalyzer
        from merge_choice_analysis import MergeChoiceAnalyzer
        from gnn_metadata_aggregator import GNNMetadataAggregator

        output_dir = Path(evaluation_output_dir) / "merge_analysis"
        output_dir.mkdir(parents=True, exist_ok=True)

        # ====================================================================
        # PHASE 1: FD MERGE METADATA COLLECTION
        # ====================================================================

        logger.info("[PHASE 1] Collecting FD merge metadata...")

        try:
            fd_collector = MergeMetadataCollector("downward/fd_output")
            num_merges = fd_collector.load_all_metadata()

            if num_merges == 0:
                logger.warning("⚠️ No FD merge metadata found")
                fd_collector = None
            else:
                logger.info(f"✓ Collected {num_merges} merge decisions from FD")

                # Export FD analysis
                fd_collector.export_to_json(output_dir / "fd_metadata.json")

        except Exception as e:
            logger.warning(f"⚠️ FD metadata collection failed: {e}")
            fd_collector = None

        # ====================================================================
        # PHASE 2: GNN METADATA AGGREGATION
        # ====================================================================

        logger.info("[PHASE 2] Aggregating GNN decision metadata...")

        try:
            gnn_aggregator = GNNMetadataAggregator("downward/gnn_metadata")
            episodes_loaded = gnn_aggregator.load_all_episodes()

            if episodes_loaded == 0:
                logger.warning("⚠️ No GNN metadata found")
                gnn_aggregator = None
            else:
                logger.info(f"✓ Loaded {episodes_loaded} GNN episodes")
                gnn_aggregator.export_aggregated_report(output_dir / "gnn_metadata")

        except Exception as e:
            logger.warning(f"⚠️ GNN metadata aggregation failed: {e}")
            gnn_aggregator = None

        # ====================================================================
        # PHASE 3: MERGE EXPLAINABILITY ANALYSIS (FD)
        # ====================================================================

        if fd_collector and fd_collector.merges:
            logger.info("[PHASE 3] Generating merge explainability...")

            try:
                explainer = MergeExplainabilityAnalyzer(fd_collector)
                explainer.export_explanations(output_dir / "fd_explanations.json")
                explainer.generate_human_report(output_dir / "fd_explainability_report.txt")
                logger.info("✓ Explainability analysis complete")

            except Exception as e:
                logger.warning(f"⚠️ Explainability analysis failed: {e}")

        # ====================================================================
        # PHASE 4: MERGE CHOICE PATTERN ANALYSIS (FD)
        # ====================================================================

        if fd_collector and fd_collector.merges:
            logger.info("[PHASE 4] Analyzing merge choice patterns...")

            try:
                choice_analyzer = MergeChoiceAnalyzer(fd_collector)
                choice_analyzer.export_analysis(output_dir / "fd_choice_analysis")
                choice_analyzer.generate_report(output_dir / "fd_choice_report.txt")
                logger.info("✓ Choice pattern analysis complete")

            except Exception as e:
                logger.warning(f"⚠️ Choice pattern analysis failed: {e}")

        # ====================================================================
        # PHASE 5: GENERATE SUMMARY
        # ====================================================================

        logger.info("[PHASE 5] Generating analysis summary...")

        summary_file = output_dir / "MERGE_ANALYSIS_SUMMARY.txt"

        with open(summary_file, 'w') as f:
            f.write("=" * 90 + "\n")
            f.write("MERGE METADATA ANALYSIS SUMMARY\n")
            f.write("=" * 90 + "\n\n")

            # FD Summary
            if fd_collector:
                f.write("FAST DOWNWARD MERGE ANALYSIS\n")
                f.write("-" * 90 + "\n")

                stats = fd_collector.get_statistics()
                f.write(f"  Total merges:              {stats.get('total_merges', 0)}\n")
                f.write(f"  Avg quality score:         {stats.get('avg_quality_score', 0):.3f}\n")
                f.write(f"  Avg compression ratio:     {stats.get('avg_compression_ratio', 0):.3f}\n")
                f.write(f"  Avg branching factor:      {stats.get('avg_branching_factor', 0):.3f}\n")
                f.write(f"  Avg reachability:          {stats.get('avg_reachability', 0):.1%}\n\n")

            # GNN Summary
            if gnn_aggregator:
                f.write("GNN DECISION ANALYSIS\n")
                f.write("-" * 90 + "\n")

                f.write(f"  Total GNN decisions:       {len(gnn_aggregator.decisions)}\n")
                f.write(f"  Episodes processed:        {len(gnn_aggregator.episodes)}\n")
                f.write(f"  Unique problems:           {len(gnn_aggregator.problems_processed)}\n\n")

            f.write("OUTPUT FILES\n")
            f.write("-" * 90 + "\n")
            f.write("  FD metadata:               fd_metadata.json\n")
            f.write("  FD explanations:           fd_explanations.json\n")
            f.write("  FD choice analysis:        fd_choice_analysis/\n")
            f.write("  GNN metadata:              gnn_metadata/\n")
            f.write("  Explainability report:     fd_explainability_report.txt\n")
            f.write("  Choice patterns report:    fd_choice_report.txt\n\n")

            f.write("=" * 90 + "\n")

        logger.info(f"✓ Summary written to: {summary_file}")

        logger.info("\n" + "=" * 90)
        logger.info("✅ MERGE ANALYSIS PIPELINE COMPLETE")
        logger.info("=" * 90 + "\n")

        logger.info(f"Results saved to: {output_dir.absolute()}/\n")

        return True

    except Exception as e:
        logger.error(f"❌ MERGE ANALYSIS FAILED: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Run merge metadata analysis")
    parser.add_argument("--eval-dir", default="evaluation_results",
                        help="Evaluation output directory")

    args = parser.parse_args()

    success = run_merge_analysis(args.eval_dir)
    return 0 if success else 1


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file gnn_model.py code is in the following block:
# FILE: gnn_model.py (COMPLETE REWRITE WITH ATTENTION & EDGE FEATURES)
import torch
from torch import Tensor, nn
from torch_geometric.nn import GCNConv, GATConv
import torch.nn as nn
from typing import Tuple, Optional
import numpy as np

import logging
logger = logging.getLogger(__name__)


# ============================================================================
# ATTENTION BACKBONE: Improved GCN with Multi-Head Attention
# ============================================================================

class GCNWithAttention(nn.Module):
    """✅ NEW: GCN backbone with multi-head attention for focusing on key relationships."""

    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int = 3, n_heads: int = 4):
        super().__init__()

        # GCN layers
        layers = []
        dims = [input_dim] + [hidden_dim] * (n_layers - 1) + [hidden_dim]
        for i in range(n_layers):
            layers.append(GCNConv(dims[i], dims[i + 1]))
        self.convs = nn.ModuleList(layers)


        # ✅ NEW: Graph Attention Network layer for learning which relationships matter
        self.attention = GATConv(
            in_channels=hidden_dim,
            out_channels=hidden_dim,
            heads=n_heads,
            concat=True,
            dropout=0.1,
            add_self_loops=False
        )

        # Post-attention projection (if concat=True, attention outputs n_heads*out_channels)
        self.attention_proj = nn.Linear(hidden_dim * n_heads, hidden_dim)

        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:
        """Forward pass with attention mechanism."""
        device = x.device
        dtype = x.dtype
        edge_index = edge_index.to(device, dtype=torch.long)

        # Standard GCN pass
        for conv in self.convs:
            x = self.activation(conv(x, edge_index))
            x = self.dropout(x)

        # ✅ NEW: Apply attention on top of GCN embeddings
        # This learns which node pairs are important for merge decisions
        if edge_index.numel() > 0:
            try:
                attn_out = self.attention(x, edge_index)
                # Project back to hidden_dim
                attn_out = self.activation(self.attention_proj(attn_out))
                # Residual connection: blend GCN output with attention output
                x = x + attn_out * 0.3  # Small weight to preserve GCN learning
            except Exception as e:
                # Fallback if attention fails
                logger.warning(f"Attention layer failed: {e}, skipping")

        return x


# ============================================================================
# EDGE FEATURE ENCODER: Encode merge candidate properties
# ============================================================================

class EdgeFeatureEncoder(nn.Module):
    """✅ NEW: Encodes rich features about merge candidates."""

    def __init__(self, num_edge_features: int = 8, output_dim: int = 16):
        super().__init__()

        # Map edge features through neural net
        self.encoder = nn.Sequential(
            nn.Linear(num_edge_features, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, output_dim),
            nn.ReLU()
        )

        self.output_dim = output_dim

    def forward(self, edge_features: Tensor) -> Tensor:
        """
        Encode edge features into embeddings.

        Args:
            edge_features: [E, num_edge_features] raw edge features

        Returns:
            [E, output_dim] encoded edge features
        """
        if edge_features.numel() == 0:
            return torch.zeros(0, self.output_dim, device=edge_features.device)

        return self.encoder(edge_features)


# ============================================================================
# ATTENTION-WEIGHTED EDGE SCORER
# ============================================================================

class AttentionWeightedEdgeScorer(nn.Module):
    """✅ NEW: Score edges using attention + edge features + node embeddings."""

    def __init__(self, hidden_dim: int, edge_feature_dim: int = 16):
        super().__init__()

        # Combine node embeddings with edge features
        # Input: [src_embedding, tgt_embedding, edge_features]
        total_dim = 2 * hidden_dim + edge_feature_dim

        self.mlp = nn.Sequential(
            nn.Linear(total_dim, 2 * total_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2 * total_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 1),
        )

        # ✅ NEW: Attention weights for edge features
        # Learn which edge features are most important
        self.edge_attention = nn.Sequential(
            nn.Linear(edge_feature_dim, 16),
            nn.Tanh(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(
            self,
            node_embs: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tensor:
        """
        Score edges using attention-weighted combination of features.

        Args:
            node_embs: [N, H] node embeddings
            edge_index: [2, E] edge list in COO format
            edge_features: [E, D] edge features (optional)

        Returns:
            [E] edge scores
        """
        # ✅ SAFETY: Handle empty edge lists
        if edge_index.numel() == 0 or edge_index.shape[1] == 0:
            return torch.zeros(0, device=node_embs.device, dtype=torch.float32)

        src_idx, tgt_idx = edge_index
        num_nodes = node_embs.shape[0]

        # ✅ SAFETY: Validate indices
        max_idx = max(src_idx.max().item(), tgt_idx.max().item()) if len(src_idx) > 0 else -1
        if max_idx >= num_nodes:
            print(f"WARNING: Edge index contains invalid node ID {max_idx} >= {num_nodes}")
            return torch.zeros(len(src_idx), device=node_embs.device, dtype=torch.float32)

        src_emb = node_embs[src_idx]  # [E, H]
        tgt_emb = node_embs[tgt_idx]  # [E, H]

        # ✅ NEW: Handle edge features with attention
        if edge_features is not None and edge_features.numel() > 0:
            # Attention weights for edge features
            edge_attn_weights = self.edge_attention(edge_features)  # [E, 1]

            # Scale edge features by attention weights
            edge_feats_weighted = edge_features * edge_attn_weights  # [E, D]

            # Concatenate node embeddings with weighted edge features
            edge_feat = torch.cat([src_emb, tgt_emb, edge_feats_weighted], dim=1)  # [E, 2H+D]
        else:
            # Fallback: just use node embeddings
            edge_feat = torch.cat([src_emb, tgt_emb], dim=1)  # [E, 2H]

        score = self.mlp(edge_feat).squeeze(-1)  # [E]

        # ✅ SAFETY: Clamp to avoid explosion
        score = torch.clamp(score, min=-1e6, max=1e6)

        # ✅ SAFETY: Replace NaN/Inf with safe defaults
        score = torch.nan_to_num(score, nan=0.0, posinf=1e6, neginf=-1e6)

        return score


# ============================================================================
# UNIFIED GNN MODEL WITH ATTENTION & EDGE FEATURES
# ============================================================================

class GNNModel(nn.Module):
    """✅ COMPLETE: Full GNN with attention, edge features, and robust validation."""

    def __init__(
            self,
            input_dim: int,
            hidden_dim: int,
            n_layers: int = 3,
            n_heads: int = 4,
            edge_feature_dim: int = 8
    ):
        super().__init__()

        # ✅ NEW: GCN with attention backbone
        self.backbone = GCNWithAttention(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            n_layers=n_layers,
            n_heads=n_heads
        )

        # ✅ NEW: Edge feature encoder
        self.edge_encoder = EdgeFeatureEncoder(
            num_edge_features=edge_feature_dim,
            output_dim=16
        )

        # ✅ NEW: Attention-weighted edge scorer
        self.scorer = AttentionWeightedEdgeScorer(
            hidden_dim=hidden_dim,
            edge_feature_dim=16
        )

    def forward(
            self,
            x: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        """Forward pass with validation."""
        # ✅ INPUT VALIDATION
        if x.dim() != 2:
            raise ValueError(f"Node features must be 2D, got {x.dim()}D")
        if edge_index.dim() != 2 or edge_index.shape[0] != 2:
            raise ValueError(f"Edge index must be [2, E], got {edge_index.shape}")

        device = x.device
        edge_index = edge_index.to(device, dtype=torch.long)

        # ✅ EDGE INDEX VALIDATION - CRITICAL!
        num_nodes = x.shape[0]
        if edge_index.numel() > 0:
            max_idx = edge_index.max().item()
            if max_idx >= num_nodes:
                logger.warning(f"Edge index {max_idx} >= num_nodes {num_nodes}. Clamping...")
                edge_index = torch.clamp(edge_index, 0, num_nodes - 1)

            # ✅ NEW: Check for negative indices
            min_idx = edge_index.min().item()
            if min_idx < 0:
                logger.warning(f"Negative edge index {min_idx} found. Clamping...")
                edge_index = torch.clamp(edge_index, 0, num_nodes - 1)

        # Rest of forward pass...
        node_embs = self.backbone(x, edge_index)

        if torch.isnan(node_embs).any():
            logger.warning("NaN in node embeddings, replacing with 0")
            node_embs = torch.nan_to_num(node_embs, nan=0.0)

        encoded_edge_features = None
        if edge_features is not None and edge_features.numel() > 0:
            try:
                encoded_edge_features = self.edge_encoder(edge_features.float())
            except Exception as e:
                logger.warning(f"Could not encode edge features: {e}")

        edge_logits = self.scorer(node_embs, edge_index, encoded_edge_features)

        return edge_logits, node_embs

--------------------------------------------------------------------------------

The file gnn_policy.py code is in the following block:
# FILE: gnn_policy.py (CRITICAL FIXES)
import traceback

import numpy as np
import torch
from torch import nn, Tensor
from torch.distributions import Categorical
from stable_baselines3.common.policies import ActorCriticPolicy
from typing import Tuple, Dict, Any, Optional
from gnn_model import GNNModel

import logging
logger = logging.getLogger(__name__)


class GNNExtractor(nn.Module):
    """✅ UPDATED: Wraps GNNModel with edge feature support."""

    def __init__(self, input_dim: int, hidden_dim: int, edge_feature_dim: int = 8):
        super().__init__()
        self.gnn = GNNModel(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            n_layers=3,
            n_heads=4,  # ✅ NEW: Attention heads
            edge_feature_dim=edge_feature_dim  # ✅ NEW: Edge feature support
        )
        self.edge_feature_dim = edge_feature_dim

    def forward(
        self,
        x: Tensor,
        edge_index: Tensor,
        edge_features: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        """✅ UPDATED: Pass edge features to GNN."""
        edge_logits, node_embs = self.gnn(x, edge_index, edge_features)
        return edge_logits, node_embs


class GNNPolicy(ActorCriticPolicy):
    """✅ FIXED: Robust policy with action validation."""

    def __init__(
            self,
            observation_space,
            action_space,
            lr_schedule,
            net_arch=None,
            activation_fn=nn.ReLU,
            hidden_dim: int = 128,
            **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            net_arch=[],
            activation_fn=activation_fn,
            **kwargs
        )

        self.node_feat_dim = observation_space["x"].shape[-1]
        self.hidden_dim = hidden_dim

        self.extractor = GNNExtractor(input_dim=self.node_feat_dim, hidden_dim=self.hidden_dim)
        self.value_net = nn.Linear(self.hidden_dim, 1)
        self.action_net = nn.Identity()

        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1))

    def _mask_invalid_edges(self, logits: Tensor, num_edges: int) -> Tensor:
        """
        ✅ FIXED: Mask invalid edges without breaking everything.

        GUARANTEE: At least one edge remains unmasked.
        """
        E = logits.size(0)

        # ✅ SAFETY: Clamp num_edges to valid range [1, E]
        num_edges_clamped = max(1, min(int(num_edges), E))

        mask = torch.arange(E, device=logits.device) < num_edges_clamped

        # ✅ GUARANTEE: At least one edge is available
        if not mask.any():
            mask[0] = True

        masked = logits.clone()
        masked[~mask] = -1e9

        return masked

    def _sample_or_argmax(self, logits: Tensor, deterministic: bool) -> Tuple[Tensor, Tensor]:
        """
        ✅ FIXED: Sample action safely with fallback.

        Returns (action, log_prob)
        """
        try:
            logits = torch.clamp(logits, min=-100, max=100)
            probs = torch.softmax(logits, dim=0)

            # ✅ SAFETY: Check if distribution is valid
            if torch.isnan(probs).any() or torch.isinf(probs).any():
                print("WARNING: Invalid probabilities, using uniform")
                probs = torch.ones_like(logits) / len(logits)

            dist = Categorical(probs=probs)

            if deterministic:
                action = probs.argmax(dim=0)
            else:
                action = dist.sample()

            logp = dist.log_prob(action)

            # ✅ SAFETY: Check log_prob
            if torch.isnan(logp) or torch.isinf(logp):
                logp = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)

            return action, logp

        except Exception as e:
            print(f"WARNING: Sampling failed: {e}, using argmax fallback")
            action = logits.argmax(dim=0)
            logp = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
            return action, logp

    @torch.no_grad()
    def predict(
            self,
            observation: Dict[str, Any],
            state=None,
            mask=None,
            deterministic=False
    ):
        """✅ FIXED: Properly handle observation batching from Stable-Baselines3."""
        self.eval()
        device = self.device

        # ================================================================
        # PHASE 1: EXTRACT OBSERVATIONS WITH PROPER BATCHING DETECTION
        # ================================================================

        x = torch.as_tensor(observation["x"], dtype=torch.float32, device=device)
        ei = torch.as_tensor(observation["edge_index"], dtype=torch.long, device=device)

        # ✅ FIX: Extract edge features with EXPLICIT validation
        edge_features = None
        if "edge_features" in observation:
            ef_raw = observation["edge_features"]
            edge_features = torch.as_tensor(ef_raw, dtype=torch.float32, device=device)

            # ✅ SAFETY: Validate edge features shape
            if edge_features.ndim not in [2, 3]:
                logger.error(f"Invalid edge_features shape: {edge_features.shape}")
                edge_features = None

        # ================================================================
        # PHASE 2: DETECT BATCH DIMENSION - ✅ FIXED
        # ================================================================

        # Determine if observations are already batched
        if x.ndim == 2:
            # Unbatched: [N, feat_dim]
            is_batched = False
            B = 1
            x = x.unsqueeze(0)  # [1, N, feat_dim]

            # ✅ FIX: Handle edge_index unsqueezing correctly
            if ei.ndim == 2 and ei.shape[0] == 2:
                ei = ei.unsqueeze(0)  # [1, 2, E]
            elif ei.ndim == 1:
                # Malformed - fix it
                ei = ei.reshape(2, -1).unsqueeze(0)

            # ✅ FIX: Handle edge_features unsqueezing correctly
            if edge_features is not None:
                if edge_features.ndim == 2:
                    # [E, 8] → [1, E, 8]
                    edge_features = edge_features.unsqueeze(0)
                # else already [B, E, 8]

        elif x.ndim == 3:
            # Already batched: [batch, N, feat_dim]
            is_batched = True
            B = x.shape[0]

            # ✅ CRITICAL FIX: Ensure edge_features is also batched
            if edge_features is not None:
                if edge_features.ndim == 2:
                    # edge_features is [E, 8], need to broadcast to [batch, E, 8]
                    edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)
                # else: already [batch, E, 8], keep as is
        else:
            raise ValueError(f"Invalid observation dimension: {x.ndim}")

        # ================================================================
        # PHASE 3: EXTRACT DIMENSIONS
        # ================================================================

        # Handle num_nodes/num_edges which might be scalars or arrays
        ne = observation.get("num_edges", None)
        num_nodes_obs = observation.get("num_nodes", None)

        # Parse num_edges
        if ne is None:
            ne = [ei.shape[-1]] * B
        elif isinstance(ne, np.ndarray):
            if ne.ndim == 0:
                ne = [int(ne)] * B
            else:
                ne = ne.reshape(-1).tolist()
        elif isinstance(ne, (int, float)):
            ne = [int(ne)] * B
        elif isinstance(ne, torch.Tensor):
            ne = ne.cpu().numpy().reshape(-1).tolist()
        else:
            ne = [int(n) for n in ne] if hasattr(ne, '__iter__') else [int(ne)] * B

        # Ensure ne list has correct length
        if len(ne) == 1 and B > 1:
            ne = ne * B
        elif len(ne) != B:
            logger.warning(f"num_edges length {len(ne)} != batch size {B}, padding")
            ne = (ne + [ei.shape[-1]] * B)[:B]

        # ================================================================
        # PHASE 4: INPUT VALIDATION
        # ================================================================

        try:
            if x.dim() < 2:
                raise ValueError(f"x must be at least 2D, got {x.dim()}D: {x.shape}")
            if ei.dim() < 2:
                raise ValueError(f"edge_index must be at least 2D, got {ei.dim()}D: {ei.shape}")

            num_nodes = x.shape[-2]

            if ei.numel() > 0:
                max_idx = ei.max().item()
                if max_idx >= num_nodes:
                    logger.warning(f"Edge index {max_idx} >= num_nodes {num_nodes}. Clamping.")
                    ei = torch.clamp(ei, max=num_nodes - 1)

        except Exception as e:
            logger.error(f"Input validation failed: {e}")
            return np.zeros(B, dtype=int), None

        # ================================================================
        # PHASE 5: PROCESS EACH SAMPLE
        # ================================================================

        actions = []

        for i in range(B):
            try:
                # Extract sample from batch
                x_i = x[i]  # [N, feat_dim]
                ei_i = ei[i]  # [2, E]

                # ✅ FIX: Properly extract edge features for this sample
                if edge_features is not None:
                    edge_feats_i = edge_features[i]  # [E, 8]
                else:
                    edge_feats_i = None

                # ✅ VALIDATE SHAPES before passing to extractor
                if x_i.ndim != 2:
                    logger.error(f"Sample {i}: x_i has wrong shape {x_i.shape}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue
                if ei_i.numel() > 0 and ei_i.ndim != 2:
                    logger.error(f"Sample {i}: ei_i has wrong shape {ei_i.ndim}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue
                if edge_feats_i is not None and edge_feats_i.ndim != 2:
                    logger.error(f"Sample {i}: edge_feats_i has wrong shape {edge_feats_i.shape}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue

                # Forward through GNN
                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                # Validate logits
                if torch.isnan(logits_i).any() or torch.isinf(logits_i).any():
                    logger.warning(f"Sample {i}: Invalid logits, using uniform")
                    logits_i = torch.ones_like(logits_i)

                num_edges = int(ne[i]) if i < len(ne) else logits_i.shape[0]

                if num_edges <= 0 or logits_i.shape[0] == 0:
                    a_i = torch.tensor(0, dtype=torch.long, device=device)
                else:
                    masked = self._mask_invalid_edges(logits_i, num_edges)
                    a_i, _ = self._sample_or_argmax(masked, deterministic)

                # Validate action
                if a_i < 0 or (logits_i.shape[0] > 0 and a_i >= logits_i.shape[0]):
                    logger.warning(f"Sample {i}: Invalid action {a_i}, clamping to 0")
                    a_i = torch.tensor(0, dtype=torch.long, device=device)

                actions.append(a_i)

            except Exception as e:
                logger.error(f"Sample {i} processing failed: {e}")
                logger.error(traceback.format_exc())
                actions.append(torch.tensor(0, dtype=torch.long, device=device))

        # ================================================================
        # PHASE 6: RETURN RESULT
        # ================================================================

        actions_tensor = torch.stack(actions) if actions else torch.zeros(B, dtype=torch.long, device=device)
        return actions_tensor.cpu().numpy(), None

    def forward(self, obs: Dict[str, Any], deterministic: bool = False
                ) -> Tuple[Tensor, Tensor, Tensor]:
        """✅ FIXED: Properly handle batched observations and initialize edge_features."""
        device = self.device

        # ================================================================
        # EXTRACT AND VALIDATE OBSERVATIONS
        # ================================================================

        x = torch.as_tensor(obs["x"], dtype=torch.float32, device=device)
        ei = torch.as_tensor(obs["edge_index"], dtype=torch.long, device=device)

        # Extract edge_features
        edge_features = None
        if "edge_features" in obs:
            edge_features = torch.as_tensor(obs["edge_features"], dtype=torch.float32, device=device)

        # ================================================================
        # DETECT AND HANDLE BATCHING (✅ APPLIED FIX HERE)
        # ================================================================

        # 1. Standardize x and ei dimensions first
        if x.ndim == 2:
            # Unbatched input
            x = x.unsqueeze(0)  # [1, N, feat]
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei

        # 2. Robust Edge Feature Initialization
        # We use x.shape[0] to determine Batch size B reliably
        B = x.shape[0]

        if edge_features is None:
            # ✅ FIXED: Create dummy edge features if missing
            # Shape becomes: [Batch_Size, 0_Edges, 8_Features]
            edge_features = torch.zeros((B, 0, 8), dtype=torch.float32, device=device)
        elif edge_features.ndim == 2:
            # [E, 8] -> [1, E, 8]
            edge_features = edge_features.unsqueeze(0)
            # If input was batched (B > 1) but features were shared/single, expand them
            if B > 1:
                edge_features = edge_features.expand(B, -1, -1)
        elif edge_features.ndim == 3:
            pass  # Already batched [B, E, 8]
        else:
            raise ValueError(f"Invalid edge_features shape: {edge_features.shape}")

        # ================================================================
        # EXTRACT ACTUAL DIMENSIONS
        # ================================================================

        num_nodes_obs = obs.get("num_nodes", None)
        if num_nodes_obs is not None:
            if isinstance(num_nodes_obs, torch.Tensor):
                actual_num_nodes = int(num_nodes_obs.item()) if num_nodes_obs.dim() == 0 else int(
                    num_nodes_obs[0].item())
            else:
                actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0])
        else:
            actual_num_nodes = x.shape[-2]

        ne_obs = obs.get("num_edges", None)
        if ne_obs is not None:
            if isinstance(ne_obs, torch.Tensor):
                actual_num_edges = int(ne_obs.item()) if ne_obs.dim() == 0 else int(ne_obs[0].item())
            else:
                actual_num_edges = int(np.asarray(ne_obs).flat[0])
        else:
            actual_num_edges = ei.shape[-1]

        # ================================================================
        # TRIM TO ACTUAL SIZES
        # ================================================================

        x_trimmed = x[..., :actual_num_nodes, :]

        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        # Trim edge_features accordingly
        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # ================================================================
        # PROCESS BATCH
        # ================================================================

        # B is already defined above
        actions, values, logps = [], [], []

        for i in range(B):
            try:
                # Extract sample
                x_i = x_trimmed[i]  # [N, feat]
                ei_i = ei_trimmed[i]  # [2, E]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None  # [E, 8]

                # Forward through GNN
                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                # Compute value
                if node_embs_i.dim() == 1:
                    node_embs_i = node_embs_i.unsqueeze(0)

                if node_embs_i.shape[0] > 0:
                    v_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True)).squeeze(-1)
                else:
                    v_i = torch.zeros((), dtype=torch.float32, device=device)

                # Sample action
                ne_i = actual_num_edges

                if ne_i <= 0 or logits_i.shape[0] == 0:
                    a_i = torch.zeros((), dtype=torch.long, device=device)
                    lp_i = torch.zeros((), dtype=torch.float32, device=device)
                else:
                    masked = self._mask_invalid_edges(logits_i, ne_i)
                    a_i, lp_i = self._sample_or_argmax(masked, deterministic)

                actions.append(a_i)
                values.append(v_i)
                logps.append(lp_i)

            except Exception as e:
                logger.error(f"Batch {i} forward failed: {e}")
                logger.error(traceback.format_exc())
                actions.append(torch.zeros((), dtype=torch.long, device=device))
                values.append(torch.zeros((), device=device))
                logps.append(torch.zeros((), device=device))

        actions = torch.stack(actions)
        values = torch.stack(values).unsqueeze(-1)
        logps = torch.stack(logps)

        return actions, values, logps

    def evaluate_actions(self, obs: Dict[str, Tensor], actions: Tensor
                         ) -> Tuple[Tensor, Tensor, Tensor]:
        """✅ FIXED: Evaluate actions with proper batching."""
        device = self.device

        x = obs["x"].to(device, dtype=torch.float32)
        ei = obs["edge_index"].to(device, dtype=torch.long)

        # Extract edge_features with batching awareness
        edge_features = None
        if "edge_features" in obs:
            edge_features = obs["edge_features"].to(device, dtype=torch.float32)

        # ✅ Handle batching
        if x.ndim == 2:
            x = x.unsqueeze(0)
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)
        elif x.ndim == 3:
            B = x.shape[0]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)

        # Extract dimensions
        num_nodes_obs = obs.get("num_nodes", None)
        actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0]) if num_nodes_obs is not None else x.shape[-2]

        ne = obs.get("num_edges", None)
        actual_num_edges = int(np.asarray(ne).flat[0]) if ne is not None else ei.shape[-1]

        # Trim
        x_trimmed = x[..., :actual_num_nodes, :]
        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # Process batch
        B = x_trimmed.shape[0]
        values, logps, ents = [], [], []

        for i in range(B):
            try:
                x_i = x_trimmed[i]
                ei_i = ei_trimmed[i]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None

                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                if node_embs_i.shape[0] > 0:
                    v_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True)).squeeze(-1)
                else:
                    v_i = torch.zeros((), device=device)

                ne_i = actual_num_edges

                if ne_i <= 0 or logits_i.shape[0] == 0:
                    logp_i = torch.zeros((), device=device, requires_grad=True)
                    ent_i = torch.zeros((), device=device, requires_grad=True)
                else:
                    masked = self._mask_invalid_edges(logits_i, ne_i)
                    dist = Categorical(logits=masked)
                    action_clamped = torch.clamp(actions[i], 0, logits_i.shape[0] - 1)
                    logp_i = dist.log_prob(action_clamped)
                    ent_i = dist.entropy()

                values.append(v_i)
                logps.append(logp_i)
                ents.append(ent_i)

            except Exception as e:
                logger.error(f"Evaluate batch {i} failed: {e}")
                values.append(torch.zeros((), device=device, requires_grad=True))
                logps.append(torch.zeros((), device=device, requires_grad=True))
                ents.append(torch.zeros((), device=device, requires_grad=True))

        return torch.stack(values), torch.stack(logps), torch.stack(ents)

    def predict_values(self, obs: Dict[str, Tensor]) -> Tensor:
        """✅ FIXED: Predict values with proper batching."""
        device = self.device

        x = obs["x"].to(device, dtype=torch.float32)
        ei = obs["edge_index"].to(device, dtype=torch.long)

        # Extract edge_features with batching awareness
        edge_features = None
        if "edge_features" in obs:
            edge_features = obs["edge_features"].to(device, dtype=torch.float32)

        # Handle batching
        if x.ndim == 2:
            x = x.unsqueeze(0)
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)
        elif x.ndim == 3:
            B = x.shape[0]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)

        # Extract dimensions
        num_nodes_obs = obs.get("num_nodes", None)
        actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0]) if num_nodes_obs is not None else x.shape[-2]

        ne = obs.get("num_edges", None)
        actual_num_edges = int(np.asarray(ne).flat[0]) if ne is not None else ei.shape[-1]

        # Trim
        x_trimmed = x[..., :actual_num_nodes, :]
        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # Process batch
        B = x_trimmed.shape[0]
        vals = []

        for i in range(B):
            try:
                x_i = x_trimmed[i]
                ei_i = ei_trimmed[i]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None

                _, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                if node_embs_i.shape[0] > 0:
                    val_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True))
                else:
                    val_i = torch.zeros((1, 1), device=device)

                vals.append(val_i)

            except Exception as e:
                logger.error(f"Predict values batch {i} failed: {e}")
                vals.append(torch.zeros((1, 1), device=device))

        return torch.cat(vals, dim=0)

--------------------------------------------------------------------------------

The file common_utils.py code is in the following block:
# -*- coding: utf-8 -*-
"""
COMPREHENSIVE CENTRAL UTILITIES FILE
Single source of truth for ALL shared code across the project.
"""

import glob
import logging
import json
import tempfile
from typing import List, Dict, Any, Optional, Tuple

from tqdm import tqdm
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import BaseCallback

import os

import gymnasium as gym

logger = logging.getLogger(__name__)

# ============================================================================
# ✅ SINGLE SOURCE OF TRUTH FOR ALL PATHS
# ============================================================================

# Get the project root (where this script lives and where downward/ folder exists)
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
DOWNWARD_DIR = os.path.join(PROJECT_ROOT, "downward")
FD_OUTPUT_DIR = os.path.join(DOWNWARD_DIR, "fd_output")
GNN_OUTPUT_DIR = os.path.join(DOWNWARD_DIR, "gnn_output")

# Ensure directories exist
os.makedirs(FD_OUTPUT_DIR, exist_ok=True)
os.makedirs(GNN_OUTPUT_DIR, exist_ok=True)

logger = logging.getLogger(__name__)

logger.info(f"[PATH CONFIG]")
logger.info(f"  PROJECT_ROOT: {PROJECT_ROOT}")
logger.info(f"  DOWNWARD_DIR: {DOWNWARD_DIR}")
logger.info(f"  FD_OUTPUT_DIR: {FD_OUTPUT_DIR}")
logger.info(f"  GNN_OUTPUT_DIR: {GNN_OUTPUT_DIR}")

# ============================================================================
# 1. FAST DOWNWARD COMMAND TEMPLATE (CENTRALIZED)
# ============================================================================

# DOWNWARD_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "downward"))
#
# # Windows version
# FD_COMMAND_TEMPLATE = (
#     f'python "{DOWNWARD_DIR}\\builds\\release\\bin\\translate\\translate.py" '
#     r'"{domain}" "{problem}" --sas-file output.sas && '
#     f'"{DOWNWARD_DIR}\\builds\\release\\bin\\downward.exe" '
#     r'--search "astar(merge_and_shrink('
#     r'merge_strategy=merge_gnn(),'
#     r'shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),'
#     r'label_reduction=exact(before_shrinking=true,before_merging=false),'
#     r'max_states=4000,threshold_before_merge=1'
#     r'))"'
# )

# FILE: common_utils.py (UPDATE THIS)

import os

# Get absolute path to downward folder
DOWNWARD_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "downward"))

# ✅ FIXED: Run translate and downward from the downward/ directory
# The key is to:
# 1. Use absolute paths for domain/problem (so they're found from downward/ cwd)
# 2. Run translate FIRST, verify output.sas exists
# 3. Then run downward to read that file
FD_COMMAND_TEMPLATE = (
    f'python "{DOWNWARD_DIR}\\builds\\release\\bin\\translate\\translate.py" '
    r'"{domain}" "{problem}" --sas-file output.sas && '
    f'"{DOWNWARD_DIR}\\builds\\release\\bin\\downward.exe" '
    r'--search "astar(merge_and_shrink('
    r'merge_strategy=merge_gnn(),'
    r'shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),'
    r'label_reduction=exact(before_shrinking=true,before_merging=false),'
    r'max_states=4000,threshold_before_merge=1'
    r'))" < output.sas'
)


# ============================================================================
# 2. SIMPLE SINGLE-PROBLEM ENVIRONMENT (NO MULTIENV)
# ============================================================================

# ============================================================================
# 2. SIMPLE SINGLE-PROBLEM ENVIRONMENT (FIXED)
# ============================================================================

class SimpleTrainingEnv(gym.Env):
    def __init__(self, domain_file: str, problem_file: str,
                 reward_variant: str = 'rich', debug: bool = False,
                 **reward_kwargs):
        super().__init__()

        from merge_env import MergeEnv

        # ✅ FIXED: Store environment as self.merge_env
        self.merge_env = MergeEnv(
            domain_file=domain_file,
            problem_file=problem_file,
            max_merges=50,
            debug=debug,
            reward_variant=reward_variant,
            **reward_kwargs
        )

        # ✅ CRITICAL: These MUST be set before training
        self.observation_space = self.merge_env.observation_space
        self.action_space = self.merge_env.action_space

        # ✅ ADD: Store reference to metadata
        self.metadata = {"render_modes": []}

    def reset(self, **kwargs):
        return self.merge_env.reset(**kwargs)
    def step(self, action):
        return self.merge_env.step(action)

    def close(self):
        try:
            self.merge_env.close()
        except:
            pass

    def render(self, mode='human'):
        pass


# ============================================================================
# 3. CALLBACKS (UNIFIED)
# ============================================================================

class SimpleProgressCallback(BaseCallback):
    """✅ FIXED: Simple progress tracking without issues."""

    def __init__(self, total_steps: int):
        super().__init__()
        self.total_steps = total_steps
        self.pbar = None

    def _on_training_start(self):
        """Initialize progress bar."""
        self.pbar = tqdm(total=self.total_steps, desc="Training", unit="steps")

    def _on_step(self) -> bool:
        """Update progress bar."""
        if self.pbar:
            self.pbar.update(1)
        return True

    def _on_training_end(self):
        """Close progress bar."""
        if self.pbar:
            self.pbar.close()


# ============================================================================
# 4. CHECKPOINT UTILITIES
# ============================================================================

def find_latest_checkpoint(checkpoint_dir: str) -> Optional[str]:
    """Finds the latest checkpoint by step count."""
    if not os.path.isdir(checkpoint_dir):
        return None

    checkpoints = glob.glob(os.path.join(checkpoint_dir, "*_steps.zip"))
    if not checkpoints:
        return None

    try:
        latest = max(checkpoints, key=lambda f: int(f.split('_')[-2]))
        logger.info(f"Found checkpoint: {latest}")
        return latest
    except (ValueError, IndexError):
        logger.warning(f"Could not parse checkpoint names in {checkpoint_dir}")
        return None


# ============================================================================
# 5. TRAINING WORKFLOW (COMPLETE & FIXED)
# ============================================================================

def train_model(
        model_save_path: str,
        benchmarks: List[Tuple[str, str]],
        hyperparams: Dict[str, Any],
        total_timesteps: int = 500,
        tb_log_dir: str = "tb_logs/",
        tb_log_name: str = "MVP_Training",
        debug_mode: bool = True,
        max_states: int = 4000,
        threshold_before_merge: int = 1,
        reward_variant: str = 'astar_search',
) -> Optional[PPO]:
    """
    Train a GNN policy using RL with REAL Fast Downward feedback.

    Args:
        model_save_path: Path to save the trained model
        benchmarks: List of (domain_file, problem_file) tuples
        hyperparams: Dictionary of PPO and reward function hyperparameters
        total_timesteps: Total training timesteps
        tb_log_dir: TensorBoard log directory
        tb_log_name: TensorBoard run name
        debug_mode: If True, use debug mode (no real FD)
        max_states: M&S max_states parameter
        threshold_before_merge: M&S threshold parameter
        reward_variant: Which reward function to use

    Returns:
        Trained PPO model, or None if training failed
    """

    # ✅ STEP 1: Validate and extract reward parameters
    valid_variants = [
        'simple_stability',
        'information_preservation',
        'hybrid',
        'conservative',
        'progressive',
        'rich',
        'astar_search'
    ]

    if reward_variant not in valid_variants:
        logger.error(f"Invalid reward variant: {reward_variant}")
        logger.error(f"Valid options: {', '.join(valid_variants)}")
        return None

    # ✅ STEP 2: Extract reward-specific kwargs from hyperparams
    reward_kwargs = {}

    # Define all possible keys for each variant
    reward_param_map = {
        'rich': ['w_f_stability', 'w_state_efficiency', 'w_transition_quality', 'w_reachability'],
        'astar_search': ['w_search_efficiency', 'w_solution_quality', 'w_f_stability', 'w_state_control'],
        'hybrid': ['w_f_stability', 'w_state_control', 'w_transition', 'w_search'],
        'simple_stability': ['alpha', 'beta', 'lambda_shrink', 'f_threshold'],
        'information_preservation': ['alpha', 'beta', 'lambda_density'],
        'conservative': ['stability_threshold'],
        'progressive': [],  # No special params, uses defaults
    }

    # Extract parameters for this variant
    if reward_variant in reward_param_map:
        for key in reward_param_map[reward_variant]:
            if key in hyperparams:
                reward_kwargs[key] = hyperparams[key]

    logger.info(f"\n{'=' * 80}")
    logger.info(f"REWARD VARIANT: {reward_variant}")
    logger.info(f"{'=' * 80}")
    if reward_kwargs:
        logger.info("Reward function parameters:")
        for k, v in reward_kwargs.items():
            logger.info(f"  {k:<30} = {v}")
    else:
        logger.info("(Using default parameters for reward function)")
    logger.info(f"{'=' * 80}\n")

    # ✅ STEP 3: Create environment with reward variant
    from merge_env import MergeEnv

    if not benchmarks or len(benchmarks) == 0:
        logger.error("No benchmarks provided!")
        return None

    domain_file, problem_file = benchmarks[0]

    logger.info(f"Creating environment with reward_variant={reward_variant}...")
    logger.info(f"  Domain:  {domain_file}")
    logger.info(f"  Problem: {problem_file}")

    env = MergeEnv(
        domain_file=domain_file,
        problem_file=problem_file,
        max_merges=50,
        debug=debug_mode,
        reward_variant=reward_variant,
        max_states=max_states,
        threshold_before_merge=threshold_before_merge,
        **reward_kwargs
    )

    env = Monitor(env)
    logger.info("✓ Environment created and wrapped with Monitor")

    # ✅ STEP 4: Create and train model
    from gnn_policy import GNNPolicy

    logger.info("Creating PPO model with GNN policy...")

    model = PPO(
        policy=GNNPolicy,
        env=env,
        learning_rate=hyperparams.get('learning_rate', 0.0003),
        n_steps=hyperparams.get('n_steps', 64),
        batch_size=hyperparams.get('batch_size', 32),
        ent_coef=hyperparams.get('ent_coef', 0.01),
        verbose=1,
        tensorboard_log=tb_log_dir,
        policy_kwargs={"hidden_dim": 64},
    )

    logger.info("✓ PPO model created")
    logger.info(f"\nStarting training for {total_timesteps} timesteps...")
    logger.info(f"Reward variant: {reward_variant}\n")

    try:
        model.learn(
            total_timesteps=total_timesteps,
            tb_log_name=tb_log_name,
            reset_num_timesteps=True,
        )
        logger.info(f"✓ Training complete")
    except KeyboardInterrupt:
        logger.warning("⚠️ Training interrupted by user")
    except Exception as e:
        logger.error(f"❌ Training failed with error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        env.close()
        return None

    # Save model
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    model.save(model_save_path)
    logger.info(f"✓ Model saved: {model_save_path}")

    env.close()
    return model


# ============================================================================
# 6. BENCHMARK LOADING
# ============================================================================

def load_benchmarks_from_pattern(
        domain_file: str,
        problem_pattern: str,
        set_name: str = "Unknown"
) -> List[Tuple[str, str]]:
    """Loads benchmark problems matching a glob pattern."""
    if not os.path.exists(domain_file):
        logger.warning(f"Domain file not found: {domain_file}")
        return []

    problems = sorted(glob.glob(problem_pattern))
    if not problems:
        logger.warning(f"No problems found matching: {problem_pattern}")
        return []

    benchmarks = [(domain_file, p) for p in problems]
    logger.info(f"{set_name}: Loaded {len(benchmarks)} problems")
    return benchmarks


# ============================================================================
# 7. JSON UTILITIES
# ============================================================================

def write_json_atomic(obj: Any, path: str) -> None:
    """Atomically writes JSON to a file."""
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(
        dir=os.path.dirname(path) or ".",
        suffix=".tmp"
    )
    try:
        with os.fdopen(fd, "w") as f:
            json.dump(obj, f)
            f.flush()
            os.fsync(f.fileno())
        os.replace(tmp_path, path)
    except:
        try:
            os.remove(tmp_path)
        except:
            pass
        raise

--------------------------------------------------------------------------------

The file train_real_working.py code is in the following block:
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MULTI-PROBLEM REAL TRAINING - GNN Merge Strategy with Dataset Support
======================================================================

Trains a GNN policy using actual Fast Downward with multiple problems across
difficulty levels (small, medium, hard).

Features:
  ✓ Load problems from benchmarks/small|medium|hard/ structure
  ✓ Train on single or multiple difficulty levels
  ✓ Curriculum learning (easy→medium→hard)
  ✓ Mixed training (random difficulty per episode)
  ✓ Full compatibility with existing framework
  ✓ Comprehensive logging and diagnostics

Run with:
    python train_real_working.py

Environment Variables:
    REWARD_VARIANT: Which reward function to use
      Options: simple_stability, information_preservation, hybrid, conservative,
               progressive, rich, astar_search (default: astar_search)

    DIFFICULTY: Which difficulty to train on
      Options: small, medium, hard, mixed (default: mixed)

    MAX_PROBLEMS_PER_DIFFICULTY: Max problems per difficulty (default: 5)

    CURRICULUM_LEARNING: Enable curriculum learning (default: false)
      When true: train on small→medium→hard sequentially
      When false: random sampling from selected difficulties

Example:
    REWARD_VARIANT=astar_search DIFFICULTY=mixed python train_real_working.py
    CURRICULUM_LEARNING=true DIFFICULTY=mixed python train_real_working.py
"""

import sys
import os
import logging
import glob
import traceback
from typing import List, Dict, Tuple, Optional
import random


# Add project root to path
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, PROJECT_ROOT)

# Import common utils to initialize paths
from common_utils import PROJECT_ROOT, DOWNWARD_DIR, FD_OUTPUT_DIR, GNN_OUTPUT_DIR


# Setup paths
sys.path.insert(0, os.getcwd())
os.makedirs("downward/gnn_output", exist_ok=True)
os.makedirs("downward/fd_output", exist_ok=True)
os.makedirs("logs", exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("training_multi_problem.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)

logger.info(f"Initialized with PROJECT_ROOT: {PROJECT_ROOT}")


def print_section(title: str, symbol: str = "=", width: int = 90):
    """Print a formatted section header."""
    logger.info("\n" + symbol * width)
    logger.info(f"// {title.upper()}")
    logger.info(symbol * width + "\n")


def print_subsection(title: str):
    """Print a formatted subsection header."""
    logger.info("\n" + "-" * 80)
    logger.info(f">>> {title}")
    logger.info("-" * 80)


# ============================================================================
# PHASE 0: BENCHMARK LOADING
# ============================================================================

def load_benchmarks_from_folders() -> Dict[str, List[Tuple[str, str]]]:
    """
    Load benchmarks from benchmarks/small|medium|hard/ directory structure.

    Expected structure:
        benchmarks/
        ├── small/
        │   ├── domain.pddl
        │   ├── problem_small_00.pddl
        │   └── ...
        ├── medium/
        │   ├── domain.pddl
        │   ├── problem_medium_00.pddl
        │   └── ...
        └── hard/
            ├── domain.pddl
            ├── problem_hard_00.pddl
            └── ...

    Returns:
        Dict mapping difficulty → list of (domain_file, problem_file) tuples
    """
    print_section("PHASE 0: LOAD BENCHMARKS FROM FOLDER STRUCTURE")

    benchmarks_dir = os.path.abspath("benchmarks")

    if not os.path.isdir(benchmarks_dir):
        logger.error(f"Benchmarks directory not found: {benchmarks_dir}")
        logger.error("Expected structure:")
        logger.error("  benchmarks/")
        logger.error("  ├── small/")
        logger.error("  ├── medium/")
        logger.error("  └── hard/")
        return {}

    logger.info(f"Benchmarks directory: {benchmarks_dir}\n")

    difficulties = ["small", "medium", "large"]
    all_benchmarks = {}

    for difficulty in difficulties:
        difficulty_dir = os.path.join(benchmarks_dir, difficulty)

        logger.info(f"Loading {difficulty.upper()} difficulty problems...")

        if not os.path.isdir(difficulty_dir):
            logger.warning(f"  ⚠️ Directory not found: {difficulty_dir}")
            all_benchmarks[difficulty] = []
            continue

        # Find domain file
        domain_file = os.path.join(difficulty_dir, "domain.pddl")
        if not os.path.exists(domain_file):
            logger.warning(f"  ⚠️ Domain file not found: {domain_file}")
            all_benchmarks[difficulty] = []
            continue

        logger.info(f"  ✓ Domain: {domain_file}")

        # Find problem files (look for problem_*.pddl patterns)
        problem_patterns = [
            f"problem_{difficulty}_*.pddl",
            f"problem_*.pddl"
        ]

        problems = []
        for pattern in problem_patterns:
            problems = sorted(glob.glob(os.path.join(difficulty_dir, pattern)))
            if problems:
                break

        if not problems:
            logger.warning(f"  ⚠️ No problem files found in {difficulty_dir}")
            all_benchmarks[difficulty] = []
            continue

        logger.info(f"  ✓ Found {len(problems)} problem(s)")
        for i, prob in enumerate(problems[:3], 1):  # Show first 3
            logger.info(f"    {i}. {os.path.basename(prob)}")
        if len(problems) > 3:
            logger.info(f"    ... and {len(problems) - 3} more")

        # Create benchmark list (absolute paths)
        benchmarks_list = [
            (os.path.abspath(domain_file), os.path.abspath(prob))
            for prob in problems
        ]

        all_benchmarks[difficulty] = benchmarks_list
        logger.info(f"  ✓ Loaded {len(benchmarks_list)} benchmark(s) for {difficulty}\n")

    # Summary
    total = sum(len(b) for b in all_benchmarks.values())
    if total == 0:
        logger.error("No benchmarks loaded!")
        return {}

    logger.info(f"✅ Loaded {total} total benchmarks across all difficulties:")
    for difficulty in difficulties:
        count = len(all_benchmarks[difficulty])
        logger.info(f"   {difficulty:<10} {count:>3} problems")

    return all_benchmarks


def get_benchmark_sequence(
        all_benchmarks: Dict[str, List[Tuple[str, str]]],
        difficulty: str = "mixed",
        max_problems_per_difficulty: int = 5,
        curriculum_learning: bool = False
) -> List[Tuple[str, str]]:
    """
    Create a sequence of benchmark problems for training.

    Args:
        all_benchmarks: All loaded benchmarks by difficulty
        difficulty: "small", "medium", "hard", or "mixed"
        max_problems_per_difficulty: Max problems to use per difficulty
        curriculum_learning: If True, train small→medium→hard sequentially

    Returns:
        List of (domain_file, problem_file) tuples for training
    """
    print_subsection("Create Benchmark Sequence")

    sequence = []

    if difficulty == "mixed":
        difficulties = ["small", "medium", "hard"]
    else:
        difficulties = [difficulty]

    logger.info(f"Difficulty mode: {difficulty}")
    logger.info(f"Max problems per difficulty: {max_problems_per_difficulty}")
    logger.info(f"Curriculum learning: {curriculum_learning}\n")

    if curriculum_learning and difficulty == "mixed":
        # Curriculum: small → medium → hard
        logger.info("Using CURRICULUM LEARNING: small → medium → hard\n")

        for diff in difficulties:
            if diff not in all_benchmarks or not all_benchmarks[diff]:
                continue

            problems = all_benchmarks[diff][:max_problems_per_difficulty]
            sequence.extend(problems)
            logger.info(f"  Added {len(problems)} {diff} problems (total: {len(sequence)})")

    else:
        # Random mixing or single difficulty
        for diff in difficulties:
            if diff not in all_benchmarks or not all_benchmarks[diff]:
                continue

            problems = all_benchmarks[diff][:max_problems_per_difficulty]
            sequence.extend(problems)
            logger.info(f"  Added {len(problems)} {diff} problems")

        # Shuffle if mixed
        if difficulty == "mixed":
            random.shuffle(sequence)
            logger.info(f"\nShuffled sequence randomly")

    logger.info(f"\n✅ Total problems in sequence: {len(sequence)}")

    return sequence


def create_problem_iterator(
        benchmark_sequence: List[Tuple[str, str]],
        epochs: int = 1
):
    """
    Create an iterator that cycles through problems for multiple epochs.

    Args:
        benchmark_sequence: List of (domain, problem) tuples
        epochs: Number of times to cycle through the sequence

    Yields:
        (domain_file, problem_file, problem_name, epoch, step)
    """
    for epoch in range(epochs):
        for step, (domain_file, problem_file) in enumerate(benchmark_sequence):
            problem_name = os.path.basename(problem_file)
            yield domain_file, problem_file, problem_name, epoch + 1, step + 1


# ============================================================================
# PHASE 1: ENVIRONMENT INITIALIZATION
# ============================================================================

# FILE: train_real_working.py
# REPLACE THIS FUNCTION

def init_training_environment(
        domain_file: str,
        problem_file: str,
        reward_variant: str = "astar_search",
        **reward_kwargs  # ✅ ADD THIS
) -> Optional[Tuple]:
    """Initialize a training environment for a single problem."""
    try:
        from merge_env import MergeEnv

        domain_path = os.path.abspath(domain_file)
        problem_path = os.path.abspath(problem_file)

        logger.info(f"Creating environment...")
        logger.info(f"  Domain:  {domain_path}")
        logger.info(f"  Problem: {problem_path}")

        # ✅ FIX: Pass the reward_kwargs dictionary instead of hard-coded values
        env = MergeEnv(
            domain_file=domain_path,
            problem_file=problem_path,
            max_merges=50,
            debug=False,  # REAL FD
            reward_variant=reward_variant,
            **reward_kwargs # ✅ USE THE PASSED ARGUMENTS
        )

        logger.info("✓ Environment created\n")
        return env, domain_path, problem_path

    except Exception as e:
        logger.error(f"✗ Environment creation failed: {e}")
        logger.error(traceback.format_exc())
        return None


# ============================================================================
# PHASE 2: MULTI-PROBLEM TRAINING
# ============================================================================

# FILE: train_real_working.py
# REPLACE THIS FUNCTION

def run_multi_problem_training(
        benchmark_sequence: List[Tuple[str, str]],
        reward_variant: str = "astar_search",
        total_timesteps: int = 10000,
        timesteps_per_problem: int = 500,
        **reward_kwargs  # ✅ This was already here and is correct
) -> bool:
    """
    Run training on multiple problems with real FD.

    Args:
        benchmark_sequence: List of (domain, problem) tuples to train on
        reward_variant: Reward function variant
        total_timesteps: Total training timesteps (if 0, use problems-based limit)
        timesteps_per_problem: Timesteps to train on each problem
        **reward_kwargs: Reward function parameters

    Returns:
        True if training succeeded, False otherwise
    """
    print_section("PHASE 2: MULTI-PROBLEM TRAINING")

    try:
        from merge_env import MergeEnv
        from gnn_policy import GNNPolicy
        from stable_baselines3 import PPO
        from stable_baselines3.common.monitor import Monitor

        # ✅ NEW: Load or create model once for all problems
        model_path = "misc/mvp_output/gnn_model.zip"
        model = None

        if os.path.exists(model_path):
            logger.info(f"Loading existing model: {model_path}")
            try:
                model = PPO.load(model_path)
                logger.info("✓ Model loaded, will continue training\n")
            except Exception as e:
                logger.warning(f"Could not load model: {e}")
                logger.warning("Will create new model\n")

        # Variables for tracking
        total_steps = 0
        problems_trained = 0

        # Iterate through problems
        for domain_file, problem_file, problem_name, epoch, problem_step in \
                create_problem_iterator(benchmark_sequence, epochs=1):

            print_subsection(f"Problem {problem_step}/{len(benchmark_sequence)}: {problem_name}")

            logger.info(f"Epoch {epoch}, Step {problem_step}")
            logger.info(f"Training timesteps for this problem: {timesteps_per_problem}\n")

            # ✅ FIX: Pass reward_kwargs to the init function
            result = init_training_environment(
                domain_file,
                problem_file,
                reward_variant,
                **reward_kwargs
            )

            if result is None:
                logger.error(f"Failed to initialize environment for {problem_name}")
                logger.error("Skipping this problem\n")
                continue

            env, domain_path, problem_path = result
            env = Monitor(env)

            # Create model if not loaded
            if model is None:
                logger.info("Creating new PPO model with GNN policy...")
                model = PPO(
                    policy=GNNPolicy,
                    env=env,
                    learning_rate=0.0003,
                    n_steps=64,
                    batch_size=32,
                    ent_coef=0.01,
                    verbose=0,
                    tensorboard_log="tb_logs/",
                    policy_kwargs={"hidden_dim": 64},
                )
                logger.info("✓ New model created\n")
            else:
                # Update model's environment
                model.set_env(env)

            # Train on this problem
            logger.info(f"Training for {timesteps_per_problem} timesteps...")
            try:
                model.learn(
                    total_timesteps=timesteps_per_problem,
                    tb_log_name=f"multi_problem_training_p{problem_step}",
                    reset_num_timesteps=False,
                )
                logger.info("✓ Training completed for this problem\n")

                total_steps += timesteps_per_problem
                problems_trained += 1

            except KeyboardInterrupt:
                logger.warning("\n⚠️ Training interrupted by user")
                break
            except Exception as e:
                logger.error(f"✗ Training failed for {problem_name}: {e}")
                logger.error(traceback.format_exc())
                continue
            finally:
                env.close()

            # Check if we've reached total timesteps limit
            if total_timesteps > 0 and total_steps >= total_timesteps:
                logger.info(f"\n✓ Reached total timesteps limit: {total_steps}/{total_timesteps}")
                break

        # Save final model
        if model is not None:
            model_filename = f"gnn_model_multi_problem_{reward_variant}.zip"
            model_path_out = f"misc/mvp_output/{model_filename}"
            model.save(model_path_out)
            logger.info(f"\n✓ Final model saved to: {model_path_out}")
            logger.info(f"  Problems trained on: {problems_trained}")
            logger.info(f"  Total timesteps: {total_steps}")

        return True

    except Exception as e:
        logger.error(f"\n❌ Multi-problem training failed: {e}")
        logger.error(traceback.format_exc())
        return False


# ============================================================================
# PHASE 3: VALIDATION (OPTIONAL)
# ============================================================================

def validate_on_benchmark_sample(model, benchmarks_dict: Dict, n_problems: int = 3) -> bool:
    """
    Quick validation: test trained model on a sample of problems.

    Args:
        model: Trained PPO model
        benchmarks_dict: All loaded benchmarks
        n_problems: Number of problems to test on

    Returns:
        True if validation succeeded
    """
    print_section("PHASE 3: VALIDATION ON SAMPLE")

    try:
        from merge_env import MergeEnv

        if model is None:
            logger.warning("No model to validate")
            return False

        # Sample problems from each difficulty
        test_problems = []
        for difficulty in ["small", "medium", "hard"]:
            if difficulty in benchmarks_dict and benchmarks_dict[difficulty]:
                # Take first problem from each difficulty
                test_problems.append(benchmarks_dict[difficulty][0])

        logger.info(f"Testing on {len(test_problems)} sampled problems:\n")

        total_reward = 0.0
        episodes_completed = 0

        for i, (domain_file, problem_file) in enumerate(test_problems, 1):
            problem_name = os.path.basename(problem_file)
            logger.info(f"  [{i}] {problem_name}")

            try:
                env = MergeEnv(
                    domain_file=os.path.abspath(domain_file),
                    problem_file=os.path.abspath(problem_file),
                    max_merges=10,
                    debug=False,
                    reward_variant="astar_search",
                )

                obs, _ = env.reset()
                episode_reward = 0.0
                steps = 0

                while steps < 10:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, done, truncated, info = env.step(int(action))
                    episode_reward += reward
                    steps += 1

                    if done or truncated:
                        break

                total_reward += episode_reward
                episodes_completed += 1

                logger.info(f"      ✓ Reward: {episode_reward:+.4f} ({steps} steps)")
                env.close()

            except Exception as e:
                logger.warning(f"      ⚠️ Test failed: {e}")

        if episodes_completed > 0:
            avg_reward = total_reward / episodes_completed
            logger.info(f"\n✅ Average validation reward: {avg_reward:+.4f} ({episodes_completed} episodes)")
            return True
        else:
            logger.warning("No validation episodes completed")
            return False

    except Exception as e:
        logger.error(f"\n❌ Validation failed: {e}")
        logger.error(traceback.format_exc())
        return False


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Main execution pipeline."""
    print_section("MULTI-PROBLEM REAL TRAINING - GNN MERGE STRATEGY", "=", 95)

    # ✅ NEW: Get environment variables for multi-problem training
    reward_variant = os.environ.get('REWARD_VARIANT', 'astar_search')
    difficulty = os.environ.get('DIFFICULTY', 'small').lower()
    max_problems = int(os.environ.get('MAX_PROBLEMS_PER_DIFFICULTY', '5'))
    curriculum_learning = os.environ.get('CURRICULUM_LEARNING', 'false').lower() == 'true'

    logger.info(f"Configuration:")
    logger.info(f"  Reward variant: {reward_variant}")
    logger.info(f"  Difficulty: {difficulty}")
    logger.info(f"  Max problems per difficulty: {max_problems}")
    logger.info(f"  Curriculum learning: {curriculum_learning}\n")

    # Phase 0: Load benchmarks from folder structure
    all_benchmarks = load_benchmarks_from_folders()
    if not all_benchmarks or sum(len(b) for b in all_benchmarks.values()) == 0:
        logger.error("No benchmarks loaded - aborting")
        return 1

    # Create benchmark sequence
    benchmark_sequence = get_benchmark_sequence(
        all_benchmarks,
        difficulty=difficulty,
        max_problems_per_difficulty=max_problems,
        curriculum_learning=curriculum_learning
    )

    if not benchmark_sequence:
        logger.error("No benchmarks in sequence - aborting")
        return 1

    # Phase 2: Run multi-problem training
    success = run_multi_problem_training(
        benchmark_sequence=benchmark_sequence,
        reward_variant=reward_variant,
        total_timesteps=50,  # ✅ NEW: Can be adjusted
        timesteps_per_problem=10,  # ✅ NEW: Timesteps per problem
    )

    if not success:
        logger.error("Training failed")
        return 1

    # Summary
    print_section("TRAINING COMPLETE", "=", 95)
    logger.info("✅ Multi-problem training pipeline completed successfully!\n")
    logger.info("Next steps:")
    logger.info("  1. View TensorBoard logs:")
    logger.info(f"     tensorboard --logdir={os.path.abspath('tb_logs/')}\n")
    logger.info("  2. Review training log:")
    logger.info(f"     {os.path.abspath('training_multi_problem.log')}\n")
    logger.info("  3. Trained models:")
    logger.info(f"     {os.path.abspath('misc/mvp_output/')}\n")

    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.cc code is in the following block:
#include "merge_and_shrink_algorithm.h"

#include "distances.h"
#include "factored_transition_system.h"
#include "fts_factory.h"
#include "label_reduction.h"
#include "labels.h"
#include "merge_and_shrink_representation.h"
#include "merge_strategy.h"
#include "merge_strategy_factory.h"
#include "shrink_strategy.h"
#include "transition_system.h"
#include "types.h"
#include "utils.h"

#include "../plugins/plugin.h"
#include "../task_utils/task_properties.h"
#include "../utils/component_errors.h"
#include "../utils/countdown_timer.h"
#include "../utils/markup.h"
#include "../utils/math.h"
#include "../utils/system.h"
#include "../utils/timer.h"

#include "merge_and_shrink_signals.h"

#include <cassert>
#include <iostream>
#include <limits>
#include <string>
#include <utility>
#include <vector>

#include <filesystem>

#include <nlohmann/json.hpp>
#include <fstream>

#include <cmath>     // for NaN/Inf handling if needed
#include <limits>    // for numeric_limits

#include <ctime>    // For std::time(nullptr)

#include <set> // added by editor



using json = nlohmann::json;

using namespace std;

using plugins::Bounds;
using utils::ExitCode;

namespace merge_and_shrink {
static void log_progress(const utils::Timer &timer, const string &msg, utils::LogProxy &log) {
    log << "M&S algorithm timer: " << timer << " (" << msg << ")" << endl;
}
MergeAndShrinkAlgorithm::MergeAndShrinkAlgorithm(
    const shared_ptr<MergeStrategyFactory> &merge_strategy,
    const shared_ptr<ShrinkStrategy> &shrink_strategy,
    const shared_ptr<LabelReduction> &label_reduction,
    bool prune_unreachable_states, bool prune_irrelevant_states,
    int max_states, int max_states_before_merge,
    int threshold_before_merge, double main_loop_max_time,
    utils::Verbosity verbosity)
    : merge_strategy_factory(merge_strategy),
      shrink_strategy(shrink_strategy),
      label_reduction(label_reduction),
      max_states(max_states),
      max_states_before_merge(max_states_before_merge),
      shrink_threshold_before_merge(threshold_before_merge),
      prune_unreachable_states(prune_unreachable_states),
      prune_irrelevant_states(prune_irrelevant_states),
      log(utils::get_log_for_verbosity(verbosity)),
      main_loop_max_time(main_loop_max_time),
      starting_peak_memory(0) {
    handle_shrink_limit_defaults();
    // Asserting fields (not parameters).
    assert(this->max_states_before_merge >= 1);
    assert(this->max_states >= this->max_states_before_merge);
}

// ============================================================================
// ✅ NEW: Export FD Index Mapping for Python Synchronization
// ============================================================================

void export_fd_index_mapping(
    const FactoredTransitionSystem& fts,
    const std::string& fd_output_dir,
    int iteration) {
    // Export a mapping from incorporated_variables (fingerprint) to current FD index.
    // This allows Python to always know where each transition system is, regardless
    // of internal reordering.

    json index_mapping;
    index_mapping["iteration"] = iteration;
    index_mapping["timestamp"] = std::time(nullptr);

    json systems = json::array();

    // Iterate over ALL systems in FTS (active or not)
    for (int fd_idx = 0; fd_idx < fts.get_size(); ++fd_idx) {
        if (!fts.is_active(fd_idx)) {
            continue;  // Skip inactive systems
        }

        const TransitionSystem& ts = fts.get_transition_system(fd_idx);

        json system_entry;
        system_entry["fd_index"] = fd_idx;
        system_entry["num_states"] = ts.get_size();
        system_entry["incorporated_variables"] = ts.get_incorporated_variables();
        system_entry["is_active"] = true;

        systems.push_back(system_entry);
    }

    index_mapping["systems"] = systems;

    // Write synchronously with no temp file (critical for reliability)
    std::string mapping_path = fd_output_dir + "/fd_index_mapping_" +
                               std::to_string(iteration) + ".json";

    std::ofstream mapping_file(mapping_path, std::ios::out | std::ios::trunc);
    if (!mapping_file.is_open()) {
        std::cerr << "[M&S] ERROR: Cannot write FD index mapping: " << mapping_path << std::endl;
        throw std::runtime_error("Cannot write FD index mapping");
    }

    mapping_file << index_mapping.dump(2);
    mapping_file.flush();
    mapping_file.close();

    std::cout << "[M&S] ✅ Exported FD index mapping to: " << mapping_path << std::endl;
}

void MergeAndShrinkAlgorithm::handle_shrink_limit_defaults() {
    // If none of the two state limits has been set: set default limit.
    if (max_states == -1 && max_states_before_merge == -1) {
        max_states = 50000;
    }

    // If one of the max_states options has not been set, set the other
    // so that it imposes no further limits.
    if (max_states_before_merge == -1) {
        max_states_before_merge = max_states;
    } else if (max_states == -1) {
        if (utils::is_product_within_limit(
                max_states_before_merge, max_states_before_merge, INF)) {
            max_states = max_states_before_merge * max_states_before_merge;
        } else {
            max_states = INF;
        }
    }

    if (max_states_before_merge > max_states) {
        max_states_before_merge = max_states;
        if (log.is_warning()) {
            log << "WARNING: "
                << "max_states_before_merge exceeds max_states, "
                << "correcting max_states_before_merge." << endl;
        }
    }

    utils::verify_argument(max_states >= 1,
                           "Transition system size must be at least 1.");

    utils::verify_argument(max_states_before_merge >= 1,
                           "Transition system size before merge must be at least 1.");

    if (shrink_threshold_before_merge == -1) {
        shrink_threshold_before_merge = max_states;
    }

    utils::verify_argument(shrink_threshold_before_merge >= 1,
                           "Threshold must be at least 1.");

    if (shrink_threshold_before_merge > max_states) {
        shrink_threshold_before_merge = max_states;
        if (log.is_warning()) {
            log << "WARNING: "
                << "threshold exceeds max_states, "
                << "correcting threshold." << endl;
        }
    }
}

void MergeAndShrinkAlgorithm::report_peak_memory_delta(bool final) const {
    if (final)
        log << "Final";
    else
        log << "Current";
    log << " peak memory increase of merge-and-shrink algorithm: "
        << utils::get_peak_memory_in_kb() - starting_peak_memory << " KB"
        << endl;
}

void MergeAndShrinkAlgorithm::dump_options() const {
    if (log.is_at_least_normal()) {
        if (merge_strategy_factory) { // deleted after merge strategy extraction
            merge_strategy_factory->dump_options();
            log << endl;
        }

        log << "Options related to size limits and shrinking: " << endl;
        log << "Transition system size limit: " << max_states << endl
            << "Transition system size limit right before merge: "
            << max_states_before_merge << endl;
        log << "Threshold to trigger shrinking right before merge: "
            << shrink_threshold_before_merge << endl;
        log << endl;

        shrink_strategy->dump_options(log);
        log << endl;

        log << "Pruning unreachable states: "
            << (prune_unreachable_states ? "yes" : "no") << endl;
        log << "Pruning irrelevant states: "
            << (prune_irrelevant_states ? "yes" : "no") << endl;
        log << endl;

        if (label_reduction) {
            label_reduction->dump_options(log);
        } else {
            log << "Label reduction disabled" << endl;
        }
        log << endl;

        log << "Main loop max time in seconds: " << main_loop_max_time << endl;
        log << endl;
    }
}

void MergeAndShrinkAlgorithm::warn_on_unusual_options() const {
    string dashes(79, '=');
    if (!label_reduction) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! You did not enable label reduction. " << endl
                << "This may drastically reduce the performance of merge-and-shrink!"
                << endl << dashes << endl;
        }
    } else if (label_reduction->reduce_before_merging() && label_reduction->reduce_before_shrinking()) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! You set label reduction to be applied twice in each merge-and-shrink" << endl
                << "iteration, both before shrinking and merging. This double computation effort" << endl
                << "does not pay off for most configurations!"
                << endl << dashes << endl;
        }
    } else {
        if (label_reduction->reduce_before_shrinking() &&
            (shrink_strategy->get_name() == "f-preserving"
             || shrink_strategy->get_name() == "random")) {
            if (log.is_warning()) {
                log << dashes << endl
                    << "WARNING! Bucket-based shrink strategies such as f-preserving random perform" << endl
                    << "best if used with label reduction before merging, not before shrinking!"
                    << endl << dashes << endl;
            }
        }
        if (label_reduction->reduce_before_merging() &&
            shrink_strategy->get_name() == "bisimulation") {
            if (log.is_warning()) {
                log << dashes << endl
                    << "WARNING! Shrinking based on bisimulation performs best if used with label" << endl
                    << "reduction before shrinking, not before merging!"
                    << endl << dashes << endl;
            }
        }
    }

    if (!prune_unreachable_states || !prune_irrelevant_states) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! Pruning is (partially) turned off!" << endl
                << "This may drastically reduce the performance of merge-and-shrink!"
                << endl << dashes << endl;
        }
    }
}

bool MergeAndShrinkAlgorithm::ran_out_of_time(
    const utils::CountdownTimer &timer) const {
    if (timer.is_expired()) {
        if (log.is_at_least_normal()) {
            log << "Ran out of time, stopping computation." << endl;
            log << endl;
        }
        return true;
    }
    return false;
}

// FILE: downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.cc
// REPLACE the main_loop function's merge section

void MergeAndShrinkAlgorithm::main_loop(
    FactoredTransitionSystem &fts,
    const TaskProxy &task_proxy) {

    utils::CountdownTimer timer(main_loop_max_time);
    if (log.is_at_least_normal()) {
        log << "Starting main loop ";
        if (main_loop_max_time == numeric_limits<double>::infinity()) {
            log << "without a time limit." << endl;
        } else {
            log << "with a time limit of " << main_loop_max_time << "s." << endl;
        }
    }

    int maximum_intermediate_size = 0;
    for (int i = 0; i < fts.get_size(); ++i) {
        int size = fts.get_transition_system(i).get_size();
        if (size > maximum_intermediate_size) {
            maximum_intermediate_size = size;
        }
    }

    if (label_reduction) {
        label_reduction->initialize(task_proxy);
    }

    unique_ptr<MergeStrategy> merge_strategy =
        merge_strategy_factory->compute_merge_strategy(task_proxy, fts);
    merge_strategy_factory = nullptr;

    auto log_main_loop_progress = [&timer, this](const string &msg) {
        log << "M&S algorithm main loop timer: "
            << timer.get_elapsed_time()
            << " (" << msg << ")" << endl;
    };

    std::string fd_output_dir = get_fd_output_directory();
    std::cout << "[M&S::MAIN] fd_output_dir: " << fd_output_dir << std::endl;

    try {
        std::filesystem::create_directories(fd_output_dir);
    } catch (const std::exception& e) {
        std::cerr << "[M&S::ERROR] Failed to create fd_output: " << e.what() << std::endl;
        throw;
    }

    int iteration = 0;

    while (fts.get_num_active_entries() > 1) {
        std::cout << "\n[M&S::MAIN] ============================================" << std::endl;
        std::cout << "[M&S::MAIN] Iteration " << iteration << " starting..." << std::endl;
        std::cout << "[M&S::MAIN] ============================================" << std::endl;

        // ✅ PHASE 1: GET NEXT MERGE PAIR FROM STRATEGY
        std::cout << "[M&S::MAIN] PHASE 1: Waiting for GNN decision..." << std::endl;

        pair<int, int> merge_indices;
        try {
            merge_indices = merge_strategy->get_next();
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Failed to get merge decision: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("get_next failed: ") + e.what(), fd_output_dir);
            throw;
        }

        std::cout << "[M&S::MAIN] PHASE 1 COMPLETE: Received decision: ("
                  << merge_indices.first << ", " << merge_indices.second << ")" << std::endl;

        if (ran_out_of_time(timer)) break;

        int merge_index1 = merge_indices.first;
        int merge_index2 = merge_indices.second;

        // ✅ VALIDATE INDICES IMMEDIATELY
        if (!fts.is_active(merge_index1)) {
            std::cerr << "[M&S::ERROR] merge_index1=" << merge_index1 << " is NOT ACTIVE!" << std::endl;
            export_error_signal(iteration, "merge_index1 is not active", fd_output_dir);
            throw std::runtime_error("Invalid merge index 1");
        }
        if (!fts.is_active(merge_index2)) {
            std::cerr << "[M&S::ERROR] merge_index2=" << merge_index2 << " is NOT ACTIVE!" << std::endl;
            export_error_signal(iteration, "merge_index2 is not active", fd_output_dir);
            throw std::runtime_error("Invalid merge index 2");
        }

        std::cout << "[M&S::MAIN] Merge indices validated: both are active" << std::endl;

        assert(merge_index1 != merge_index2);
        if (log.is_at_least_normal()) {
            log << "Next pair of indices: ("
                << merge_index1 << ", " << merge_index2 << ")" << endl;
            if (log.is_at_least_verbose()) {
                fts.statistics(merge_index1, log);
                fts.statistics(merge_index2, log);
            }
            log_main_loop_progress("after computation of next merge");
        }

        // ✅ PHASE 2: CAPTURE BEFORE DATA ***BEFORE*** ANY MODIFICATIONS
        std::cout << "[M&S::MAIN] PHASE 2: Capturing pre-merge data..." << std::endl;

        json before_data;
        try {
            before_data = export_merge_before_data(fts, merge_index1, merge_index2, iteration, false, false);
            std::cout << "[M&S::MAIN] PHASE 2 COMPLETE: Pre-merge data captured" << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Failed to capture before data: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("before_data failed: ") + e.what(), fd_output_dir);
            throw;
        }

        // ✅ PHASE 3: LABEL REDUCTION (BEFORE SHRINKING)
        bool reduced = false;
        if (label_reduction && label_reduction->reduce_before_shrinking()) {
            std::cout << "[M&S::MAIN] PHASE 3: Label reduction (before shrinking)..." << std::endl;
            try {
                reduced = label_reduction->reduce(merge_indices, fts, log);
                if (log.is_at_least_normal() && reduced) {
                    log_main_loop_progress("after label reduction");
                }
                std::cout << "[M&S::MAIN] PHASE 3 COMPLETE: reduced=" << reduced << std::endl;
            } catch (const std::exception& e) {
                std::cerr << "[M&S::ERROR] Label reduction failed: " << e.what() << std::endl;
                export_error_signal(iteration, std::string("label_reduction failed: ") + e.what(), fd_output_dir);
                throw;
            }
        }

        if (ran_out_of_time(timer)) break;

        // ✅ PHASE 4: SHRINKING
        std::cout << "[M&S::MAIN] PHASE 4: Shrinking..." << std::endl;
        bool shrunk = false;
        try {
            shrunk = shrink_before_merge_step(
                fts,
                merge_index1,
                merge_index2,
                max_states,
                max_states_before_merge,
                shrink_threshold_before_merge,
                *shrink_strategy,
                log);
            if (log.is_at_least_normal() && shrunk) {
                log_main_loop_progress("after shrinking");
            }
            std::cout << "[M&S::MAIN] PHASE 4 COMPLETE: shrunk=" << shrunk << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Shrinking failed: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("shrinking failed: ") + e.what(), fd_output_dir);
            throw;
        }

        // Update before_data with shrunk/reduced status
        before_data["shrunk"] = shrunk;
        before_data["reduced"] = reduced;

        if (ran_out_of_time(timer)) break;

        // ✅ PHASE 5: LABEL REDUCTION (BEFORE MERGING)
        if (label_reduction && label_reduction->reduce_before_merging()) {
            std::cout << "[M&S::MAIN] PHASE 5: Label reduction (before merging)..." << std::endl;
            try {
                reduced = label_reduction->reduce(merge_indices, fts, log);
                if (log.is_at_least_normal() && reduced) {
                    log_main_loop_progress("after label reduction");
                }
                before_data["reduced"] = reduced;  // Update
                std::cout << "[M&S::MAIN] PHASE 5 COMPLETE: reduced=" << reduced << std::endl;
            } catch (const std::exception& e) {
                std::cerr << "[M&S::ERROR] Label reduction (before merge) failed: " << e.what() << std::endl;
                export_error_signal(iteration, std::string("label_reduction_before_merge failed: ") + e.what(), fd_output_dir);
                throw;
            }
        }

        if (ran_out_of_time(timer)) break;

        // ✅ PHASE 6: WRITE BEFORE DATA NOW (before merge destroys original TS)
        std::cout << "[M&S::MAIN] PHASE 6: Writing merge_before file..." << std::endl;
        try {
            std::string before_path = fd_output_dir + "/merge_before_" + std::to_string(iteration) + ".json";
            write_json_file_atomic(before_data, before_path);
            std::cout << "[M&S::MAIN] PHASE 6 COMPLETE: " << before_path << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Failed to write before data: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("write_before failed: ") + e.what(), fd_output_dir);
            throw;
        }

        // ✅ PHASE 7: PERFORM ACTUAL MERGE
        std::cout << "[M&S::MAIN] PHASE 7: Performing merge..." << std::endl;
        int merged_index;
        try {
            merged_index = fts.merge(merge_index1, merge_index2, log);
            std::cout << "[M&S::MAIN] PHASE 7 COMPLETE: merged_index=" << merged_index << std::endl;
        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Merge operation failed: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("merge failed: ") + e.what(), fd_output_dir);
            throw;
        }

        // ✅ PHASE 8: EXPORT AFTER DATA AND TS
        std::cout << "[M&S::MAIN] PHASE 8: Exporting post-merge signals..." << std::endl;
        try {
            // Export merge_after
            json after_data = export_merge_after_data(fts, merged_index, iteration);
            std::string after_path = fd_output_dir + "/merge_after_" + std::to_string(iteration) + ".json";
            write_json_file_atomic(after_data, after_path);
            std::cout << "[M&S::MAIN]   Written: " << after_path << std::endl;

            // Export ts_N
            json ts_data = export_ts_data(fts, merged_index, iteration);
            std::string ts_path = fd_output_dir + "/ts_" + std::to_string(iteration) + ".json";
            write_json_file_atomic(ts_data, ts_path);
            std::cout << "[M&S::MAIN]   Written: " << ts_path << std::endl;

            // Export fd_index_mapping
            json mapping_data = export_fd_index_mapping_data(fts, iteration);
            std::string mapping_path = fd_output_dir + "/fd_index_mapping_" + std::to_string(iteration) + ".json";
            write_json_file_atomic(mapping_data, mapping_path);
            std::cout << "[M&S::MAIN]   Written: " << mapping_path << std::endl;

            std::cout << "[M&S::MAIN] PHASE 8 COMPLETE: All signals exported" << std::endl;

        } catch (const std::exception& e) {
            std::cerr << "[M&S::ERROR] Signal export failed: " << e.what() << std::endl;
            export_error_signal(iteration, std::string("signal_export failed: ") + e.what(), fd_output_dir);
            throw;
        }

        // ✅ PHASE 9: POST-MERGE OPERATIONS
        iteration++;

        int abs_size = fts.get_transition_system(merged_index).get_size();
        if (abs_size > maximum_intermediate_size) {
            maximum_intermediate_size = abs_size;
        }

        if (log.is_at_least_normal()) {
            if (log.is_at_least_verbose()) {
                fts.statistics(merged_index, log);
            }
            log_main_loop_progress("after merging");
        }

        if (ran_out_of_time(timer)) {
            break;
        }

        // Pruning
        if (prune_unreachable_states || prune_irrelevant_states) {
            bool pruned = prune_step(
                fts,
                merged_index,
                prune_unreachable_states,
                prune_irrelevant_states,
                log);
            if (log.is_at_least_normal() && pruned) {
                if (log.is_at_least_verbose()) {
                    fts.statistics(merged_index, log);
                }
                log_main_loop_progress("after pruning");
            }
        }

        if (!fts.is_factor_solvable(merged_index)) {
            if (log.is_at_least_normal()) {
                log << "Abstract problem is unsolvable, stopping computation." << endl << endl;
            }
            break;
        }

        if (ran_out_of_time(timer)) {
            break;
        }

        if (log.is_at_least_verbose()) {
            report_peak_memory_delta();
        }
        if (log.is_at_least_normal()) {
            log << endl;
        }
    }

    log << "End of merge-and-shrink algorithm, statistics:" << endl;
    log << "Main loop runtime: " << timer.get_elapsed_time() << endl;
    log << "Maximum intermediate abstraction size: "
        << maximum_intermediate_size << endl;
    shrink_strategy = nullptr;
    label_reduction = nullptr;
}


FactoredTransitionSystem MergeAndShrinkAlgorithm::build_factored_transition_system(
    const TaskProxy &task_proxy) {
    if (starting_peak_memory) {
        cerr << "Calling build_factored_transition_system twice is not "
             << "supported!" << endl;
        utils::exit_with(utils::ExitCode::SEARCH_CRITICAL_ERROR);
    }
    starting_peak_memory = utils::get_peak_memory_in_kb();

    utils::Timer timer;
    log << "Running merge-and-shrink algorithm..." << endl;
    task_properties::verify_no_axioms(task_proxy);
    dump_options();
    warn_on_unusual_options();
    log << endl;

    const bool compute_init_distances =
        shrink_strategy->requires_init_distances() ||
        merge_strategy_factory->requires_init_distances() ||
        prune_unreachable_states;
    const bool compute_goal_distances =
        shrink_strategy->requires_goal_distances() ||
        merge_strategy_factory->requires_goal_distances() ||
        prune_irrelevant_states;
    FactoredTransitionSystem fts =
        create_factored_transition_system(
            task_proxy,
            compute_init_distances,
            compute_goal_distances,
            log);
    if (log.is_at_least_normal()) {
        log_progress(timer, "after computation of atomic factors", log);
    }

    /*
      Prune all atomic factors according to the chosen options. Stop early if
      one factor is unsolvable.

      TODO: think about if we can prune already while creating the atomic FTS.
    */
    bool pruned = false;
    bool unsolvable = false;
    for (int index = 0; index < fts.get_size(); ++index) {
        assert(fts.is_active(index));
        if (prune_unreachable_states || prune_irrelevant_states) {
            bool pruned_factor = prune_step(
                fts,
                index,
                prune_unreachable_states,
                prune_irrelevant_states,
                log);
            pruned = pruned || pruned_factor;
        }
        if (!fts.is_factor_solvable(index)) {
            log << "Atomic FTS is unsolvable, stopping computation." << endl;
            unsolvable = true;
            break;
        }
    }
    if (log.is_at_least_normal()) {
        if (pruned) {
            log_progress(timer, "after pruning atomic factors", log);
        }
        log << endl;
    }

    // ####################################################################################################
    // === Export Atomic Transition Systems ===
    {
        std::string fd_output_dir = std::filesystem::absolute(
            std::filesystem::current_path() / "fd_output"
        ).string();
        std::filesystem::create_directories(fd_output_dir);

        std::string filename = fd_output_dir + "/merged_transition_systems.json";
        json all_ts;

        // Try to load existing merged systems if any
        std::ifstream infile(filename);
        if (infile) {
            infile >> all_ts;
            infile.close();
        }

        for (int i = 0; i < fts.get_size(); ++i) {
            if (!fts.is_active(i)) continue;

            const TransitionSystem& ts = fts.get_transition_system(i);

            json ts_json;
            ts_json["iteration"] = -1;  // Use -1 to mark atomic TS
            ts_json["num_states"] = ts.get_size();
            ts_json["init_state"] = ts.get_init_state();

            std::vector<int> goal_states;
            for (int j = 0; j < ts.get_size(); ++j)
                if (ts.is_goal_state(j))
                    goal_states.push_back(j);
            ts_json["goal_states"] = goal_states;

            ts_json["incorporated_variables"] = ts.get_incorporated_variables();

            std::vector<json> transitions;
            for (auto it = ts.begin(); it != ts.end(); ++it) {
                const auto& info = *it;
                const auto& label_group = info.get_label_group();
                const auto& trans_vec = info.get_transitions();

                for (int label : label_group) {
                    for (const auto& trans : trans_vec) {
                        transitions.push_back({
                            {"src", trans.src},
                            {"target", trans.target},
                            {"label", label}
                            });
                    }
                }
            }
            ts_json["transitions"] = transitions;

            all_ts.push_back(ts_json);
        }

        std::ofstream outfile(filename);
        outfile << all_ts.dump(4);  // Pretty print
        outfile.close();
    }

    // After the "Export Atomic Transition Systems" block, add this new block:

    // ####################################################################################################
    // === Export Causal Graph ===
    {
        std::string fd_output_dir_str = std::filesystem::absolute(
            std::filesystem::current_path() / "fd_output"
        ).string();
        std::filesystem::create_directories(fd_output_dir_str);

        std::string cg_filename = fd_output_dir_str + "/causal_graph.json";
        json cg_json;
        json edges = json::array();

        // Build causal graph edges from operator preconditions and effects
        // An edge (u, v) means variable u affects variable v through some operator
        std::set<std::pair<int, int>> unique_edges;

        for (const auto& op : task_proxy.get_operators()) {
            std::set<int> precond_vars;
            std::set<int> effect_vars;

            // Collect precondition variables
            for (const auto& precond : op.get_preconditions()) {
                precond_vars.insert(precond.get_variable().get_id());
            }

            // Collect effect variables
            for (const auto& effect : op.get_effects()) {
                effect_vars.insert(effect.get_fact().get_variable().get_id());

                // Also collect effect condition variables
                for (const auto& cond : effect.get_conditions()) {
                    precond_vars.insert(cond.get_variable().get_id());
                }
            }

            // Create edges: precondition var -> effect var
            for (int pv : precond_vars) {
                for (int ev : effect_vars) {
                    if (pv != ev) {
                        unique_edges.insert({pv, ev});
                    }
                }
            }

            // Effect vars can also affect each other (co-effects)
            for (int ev1 : effect_vars) {
                for (int ev2 : effect_vars) {
                    if (ev1 != ev2) {
                        unique_edges.insert({ev1, ev2});
                    }
                }
            }
        }

        // Convert to JSON array
        for (const auto& [from_var, to_var] : unique_edges) {
            edges.push_back({{"from", from_var}, {"to", to_var}});
        }

        cg_json["edges"] = edges;
        cg_json["num_variables"] = task_proxy.get_variables().size();
        cg_json["timestamp"] = std::time(nullptr);

        std::ofstream cg_file(cg_filename, std::ios::out | std::ios::trunc);
        if (cg_file.is_open()) {
            cg_file << cg_json.dump(2);
            cg_file.flush();
            cg_file.close();
            std::cout << "[M&S] ✅ Exported causal graph to: " << cg_filename
                      << " (" << edges.size() << " edges)" << std::endl;
        } else {
            std::cerr << "[M&S] ❌ Failed to create causal graph file: " << cg_filename << std::endl;
        }
    }

    // ####################################################################################################

    // ####################################################################################################
    // === Export Initial FD Index Mapping (for Python sync) ===
    {
        std::string fd_output_dir_str = get_fd_output_directory();
        std::filesystem::create_directories(fd_output_dir_str);
        export_fd_index_mapping(fts, fd_output_dir_str, -1);  // -1 = initial state
        std::cout << "[M&S] ✅ Exported initial FD index mapping" << std::endl;
    }
    // ####################################################################################################

    if (!unsolvable && main_loop_max_time > 0) {
        main_loop(fts, task_proxy);
    }
    const bool final = true;
    report_peak_memory_delta(final);
    log << "Merge-and-shrink algorithm runtime: " << timer << endl;
    log << endl;
    return fts;
}

void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature) {
    // Merge strategy option.
    feature.add_option<shared_ptr<MergeStrategyFactory>>(
        "merge_strategy",
        "See detailed documentation for merge strategies. "
        "We currently recommend SCC-DFP, which can be achieved using "
        "{{{merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector="
        "score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order"
        "]))}}}");

    // Shrink strategy option.
    feature.add_option<shared_ptr<ShrinkStrategy>>(
        "shrink_strategy",
        "See detailed documentation for shrink strategies. "
        "We currently recommend non-greedy shrink_bisimulation, which can be "
        "achieved using {{{shrink_strategy=shrink_bisimulation(greedy=false)}}}");

    // Label reduction option.
    feature.add_option<shared_ptr<LabelReduction>>(
        "label_reduction",
        "See detailed documentation for labels. There is currently only "
        "one 'option' to use label_reduction, which is {{{label_reduction=exact}}} "
        "Also note the interaction with shrink strategies.",
        plugins::ArgumentInfo::NO_DEFAULT);

    // Pruning options.
    feature.add_option<bool>(
        "prune_unreachable_states",
        "If true, prune abstract states unreachable from the initial state.",
        "true");
    feature.add_option<bool>(
        "prune_irrelevant_states",
        "If true, prune abstract states from which no goal state can be "
        "reached.",
        "true");

    add_transition_system_size_limit_options_to_feature(feature);

    feature.add_option<double>(
        "main_loop_max_time",
        "A limit in seconds on the runtime of the main loop of the algorithm. "
        "If the limit is exceeded, the algorithm terminates, potentially "
        "returning a factored transition system with several factors. Also "
        "note that the time limit is only checked between transformations "
        "of the main loop, but not during, so it can be exceeded if a "
        "transformation is runtime-intense.",
        "infinity",
        Bounds("0.0", "infinity"));
}

tuple<shared_ptr<MergeStrategyFactory>, shared_ptr<ShrinkStrategy>,
      shared_ptr<LabelReduction>, bool, bool, int, int, int, double>
get_merge_and_shrink_algorithm_arguments_from_options(
    const plugins::Options &opts) {
    return tuple_cat(
        make_tuple(
            opts.get<shared_ptr<MergeStrategyFactory>>("merge_strategy"),
            opts.get<shared_ptr<ShrinkStrategy>>("shrink_strategy"),
            opts.get<shared_ptr<LabelReduction>>(
                "label_reduction", nullptr),
            opts.get<bool>("prune_unreachable_states"),
            opts.get<bool>("prune_irrelevant_states")),
        get_transition_system_size_limit_arguments_from_options(opts),
        make_tuple(opts.get<double>("main_loop_max_time"))
        );
}

void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature) {
    feature.add_option<int>(
        "max_states",
        "maximum transition system size allowed at any time point.",
        "-1",
        Bounds("-1", "infinity"));
    feature.add_option<int>(
        "max_states_before_merge",
        "maximum transition system size allowed for two transition systems "
        "before being merged to form the synchronized product.",
        "-1",
        Bounds("-1", "infinity"));
    feature.add_option<int>(
        "threshold_before_merge",
        "If a transition system, before being merged, surpasses this soft "
        "transition system size limit, the shrink strategy is called to "
        "possibly shrink the transition system.",
        "-1",
        Bounds("-1", "infinity"));
}

tuple<int, int, int>
get_transition_system_size_limit_arguments_from_options(
    const plugins::Options &opts) {
    return make_tuple(
        opts.get<int>("max_states"),
        opts.get<int>("max_states_before_merge"),
        opts.get<int>("threshold_before_merge")
        );
}
}

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.h code is in the following block:
//#ifndef MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H
//#define MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H
//
//#include <memory>
//#include <vector>
//
//// Include these BEFORE opening namespace to avoid pollution
//#include "types.h"           // Provides INF, PRUNED_STATE
//#include "../utils/logging.h"
//
//// Forward declarations
//class TaskProxy;
//
//namespace merge_and_shrink {
//// Now types.h content is available and INF is visible
//
//namespace merge_and_shrink {
//
//// Forward declarations for external types
//namespace utils {
//class LogProxy;
//class CountdownTimer;
//enum class Verbosity;
//}  // namespace utils
//
//
//namespace plugins {
//class ConstructContext;
//class Feature;
//class Options;
//}
//
//namespace utils {
//class CountdownTimer;
//}
//
//namespace merge_and_shrink {
//class FactoredTransitionSystem;
//class LabelReduction;
//class MergeStrategyFactory;
//class ShrinkStrategy;
//
//class MergeAndShrinkAlgorithm {
//
//    // TODO: when the option parser supports it, the following should become
//    // unique pointers.
//    std::shared_ptr<MergeStrategyFactory> merge_strategy_factory;
//    std::shared_ptr<ShrinkStrategy> shrink_strategy;
//    std::shared_ptr<LabelReduction> label_reduction;
//
//    // Options for shrinking
//    // Hard limit: the maximum size of a transition system at any point.
//    int max_states;
//    // Hard limit: the maximum size of a transition system before being merged.
//    int max_states_before_merge;
//    /* A soft limit for triggering shrinking even if the hard limits
//       max_states and max_states_before_merge are not violated. */
//    int shrink_threshold_before_merge;
//
//    // Options for pruning
//    const bool prune_unreachable_states;
//    const bool prune_irrelevant_states;
//
//    mutable utils::LogProxy log;
//    const double main_loop_max_time;
//
//    long starting_peak_memory;
//
//    void report_peak_memory_delta(bool final = false) const;
//    void dump_options() const;
//    void warn_on_unusual_options() const;
//    bool ran_out_of_time(const utils::CountdownTimer &timer) const;
//    void statistics(int maximum_intermediate_size) const;
//    void main_loop(
//        FactoredTransitionSystem &fts,
//        const TaskProxy &task_proxy);
//    void handle_shrink_limit_defaults();
//public:
//    MergeAndShrinkAlgorithm(
//        const std::shared_ptr<MergeStrategyFactory> &merge_strategy,
//        const std::shared_ptr<ShrinkStrategy> &shrink_strategy,
//        const std::shared_ptr<LabelReduction> &label_reduction,
//        bool prune_unreachable_states, bool prune_irrelevant_states,
//        int max_states, int max_states_before_merge,
//        int threshold_before_merge, double main_loop_max_time,
//        utils::Verbosity verbosity);
//    FactoredTransitionSystem build_factored_transition_system(const TaskProxy &task_proxy);
//};
//
//extern void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature);
//std::tuple<std::shared_ptr<MergeStrategyFactory>,
//           std::shared_ptr<ShrinkStrategy>,
//           std::shared_ptr<LabelReduction>, bool, bool, int, int, int,
//           double>
//get_merge_and_shrink_algorithm_arguments_from_options(
//    const plugins::Options &opts);
//extern void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature);
//std::tuple<int, int, int>
//get_transition_system_size_limit_arguments_from_options(
//    const plugins::Options &opts);
//}
//
//#endif

#ifndef MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H
#define MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H

#include "../utils/logging.h"

#include "merge_and_shrink_signals.h"  // ✅ ADD THIS

#include <memory>

class TaskProxy;

namespace plugins {
class ConstructContext;
class Feature;
class Options;
}

namespace utils {
class CountdownTimer;
}

namespace merge_and_shrink {
class FactoredTransitionSystem;
class LabelReduction;
class MergeStrategyFactory;
class ShrinkStrategy;

void export_merge_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int ts1_id,
    int ts2_id,
    const std::string& fd_output_dir,
    int iteration,
    bool shrunk,
    bool reduced
);

// ✅ CRITICAL: Forward declare these BEFORE class
void export_fd_index_mapping(
    const FactoredTransitionSystem& fts,
    const std::string& fd_output_dir,
    int iteration);

std::string get_fd_output_directory();
void export_error_signal(
    int iteration,
    const std::string& error_message,
    const std::string& fd_output_dir
);

class MergeAndShrinkAlgorithm {
    // TODO: when the option parser supports it, the following should become
    // unique pointers.
    std::shared_ptr<MergeStrategyFactory> merge_strategy_factory;
    std::shared_ptr<ShrinkStrategy> shrink_strategy;
    std::shared_ptr<LabelReduction> label_reduction;

    // Options for shrinking
    // Hard limit: the maximum size of a transition system at any point.
    int max_states;
    // Hard limit: the maximum size of a transition system before being merged.
    int max_states_before_merge;
    /* A soft limit for triggering shrinking even if the hard limits
       max_states and max_states_before_merge are not violated. */
    int shrink_threshold_before_merge;

    // Options for pruning
    const bool prune_unreachable_states;
    const bool prune_irrelevant_states;

    mutable utils::LogProxy log;
    const double main_loop_max_time;

    long starting_peak_memory;

    void report_peak_memory_delta(bool final = false) const;
    void dump_options() const;
    void warn_on_unusual_options() const;
    bool ran_out_of_time(const utils::CountdownTimer &timer) const;
    void statistics(int maximum_intermediate_size) const;
    void main_loop(
        FactoredTransitionSystem &fts,
        const TaskProxy &task_proxy);
    void handle_shrink_limit_defaults();
public:
    MergeAndShrinkAlgorithm(
        const std::shared_ptr<MergeStrategyFactory> &merge_strategy,
        const std::shared_ptr<ShrinkStrategy> &shrink_strategy,
        const std::shared_ptr<LabelReduction> &label_reduction,
        bool prune_unreachable_states, bool prune_irrelevant_states,
        int max_states, int max_states_before_merge,
        int threshold_before_merge, double main_loop_max_time,
        utils::Verbosity verbosity);
    FactoredTransitionSystem build_factored_transition_system(const TaskProxy &task_proxy);
};

extern void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature);
std::tuple<std::shared_ptr<MergeStrategyFactory>,
           std::shared_ptr<ShrinkStrategy>,
           std::shared_ptr<LabelReduction>, bool, bool, int, int, int,
           double>
get_merge_and_shrink_algorithm_arguments_from_options(
    const plugins::Options &opts);
extern void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature);
std::tuple<int, int, int>
get_transition_system_size_limit_arguments_from_options(
    const plugins::Options &opts);
}

#endif


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_signals.cc code is in the following block:
#include "merge_and_shrink_signals.h"

#include "factored_transition_system.h"
#include "transition_system.h"
#include "distances.h"
#include "types.h"

#include <filesystem>
#include <fstream>
#include <ctime>
#include <iostream>
#include <iomanip>
#include <cmath>
#include <numeric>
#include <algorithm>
#include <stdexcept>
#include <sstream>

namespace fs = std::filesystem;
using json = nlohmann::json;

namespace merge_and_shrink {

// ============================================================================
// DIRECTORY MANAGEMENT
// ============================================================================

std::string get_fd_output_directory() {
    fs::path fd_output = fs::current_path() / "fd_output";
    try {
        fs::create_directories(fd_output);
    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Warning: Could not create fd_output: " << e.what() << std::endl;
    }
    return fd_output.string();
}

std::string get_gnn_output_directory() {
    fs::path gnn_output = fs::current_path() / "gnn_output";
    try {
        fs::create_directories(gnn_output);
    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Warning: Could not create gnn_output: " << e.what() << std::endl;
    }
    return gnn_output.string();
}

// ============================================================================
// ATOMIC FILE I/O
// ============================================================================

//void write_json_file_atomic(const json& data, const std::string& file_path) {
//    try {
//        fs::path final_path(file_path);
//        fs::path dir_path = final_path.parent_path();
//
//        // Ensure directory exists
//        fs::create_directories(dir_path);
//
//        // Create temporary file
//        fs::path temp_path = final_path.string() + ".tmp";
//
//        {
//            std::ofstream temp_file(temp_path, std::ios::out | std::ios::trunc);
//            if (!temp_file.is_open()) {
//                throw std::runtime_error("Cannot open temp file: " + temp_path.string());
//            }
//
//            // Write JSON with pretty printing
//            temp_file << data.dump(2);
//            temp_file.flush();
//
//            // Force to disk
//            if (std::ferror(temp_file.rdbuf()->_M_file ?
//                           std::fflush(temp_file.rdbuf()->_M_file) : 0)) {
//                throw std::runtime_error("Failed to flush temp file");
//            }
//
//            temp_file.close();
//        }
//
//        // Atomic rename
//        fs::rename(temp_path, final_path);
//
//        std::cout << "[SIGNALS] ✅ Wrote: " << file_path << std::endl;
//
//    } catch (const std::exception& e) {
//        std::cerr << "[SIGNALS] ❌ Error writing JSON: " << e.what() << std::endl;
//        throw;
//    }
//}

void write_json_file_atomic(const json& data, const std::string& file_path) {
    try {
        fs::path final_path(file_path);
        fs::path dir_path = final_path.parent_path();

        // Ensure directory exists
        fs::create_directories(dir_path);

        // Create temporary file
        fs::path temp_path = final_path.string() + ".tmp";

        {
            std::ofstream temp_file(temp_path, std::ios::out | std::ios::trunc);
            if (!temp_file.is_open()) {
                throw std::runtime_error("Cannot open temp file: " + temp_path.string());
            }

            // Write JSON with pretty printing
            temp_file << data.dump(2);

            // Standard C++ flush (portable)
            temp_file.flush();

            // Check for write errors
            if (temp_file.fail()) {
                throw std::runtime_error("Failed to write/flush to temp file");
            }

            temp_file.close();
        }

        // Atomic rename
        fs::rename(temp_path, final_path);

        std::cout << "[SIGNALS] Wrote: " << file_path << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Error writing JSON: " << e.what() << std::endl;
        throw;
    }
}

// ============================================================================
// F-VALUE STATISTICS COMPUTATION
// ============================================================================

json compute_f_statistics(const std::vector<int>& distances, int unreachable_marker) {
    // Filter to valid distances
    std::vector<double> valid_distances;

    for (int d : distances) {
        // Include if: not unreachable marker AND not negative AND reasonable value
        if (d != unreachable_marker &&
            d >= 0 &&
            d < 1000000000) {
            valid_distances.push_back(static_cast<double>(d));
        }
    }

    json stats;

    if (valid_distances.empty()) {
        stats["min"] = unreachable_marker;
        stats["max"] = unreachable_marker;
        stats["mean"] = 0.0;
        stats["std"] = 0.0;
        stats["valid_count"] = 0;
    } else {
        // Min
        double min_val = *std::min_element(valid_distances.begin(), valid_distances.end());
        stats["min"] = static_cast<int>(min_val);

        // Max
        double max_val = *std::max_element(valid_distances.begin(), valid_distances.end());
        stats["max"] = static_cast<int>(max_val);

        // Mean
        double sum = std::accumulate(valid_distances.begin(), valid_distances.end(), 0.0);
        double mean = sum / valid_distances.size();
        stats["mean"] = mean;

        // Standard deviation
        double variance = 0.0;
        for (double v : valid_distances) {
            variance += (v - mean) * (v - mean);
        }
        double std_dev = std::sqrt(variance / valid_distances.size());
        stats["std"] = std_dev;

        stats["valid_count"] = static_cast<int>(valid_distances.size());
    }

    return stats;
}

// ============================================================================
// PRODUCT STATE MAPPING
// ============================================================================

json build_product_mapping(int ts1_size, int ts2_size) {
    json mapping;

    int counter = 0;
    for (int s = 0; s < ts1_size * ts2_size; ++s) {
        int s1 = s / ts2_size;
        int s2 = s % ts2_size;

        mapping[std::to_string(s)] = {
            {"s1", s1},
            {"s2", s2}
        };

        counter++;

        // For very large products, sample instead of full export
        if (counter > 100000) {
            // Add a note and break
            mapping["_note"] = "Product mapping truncated (too large)";
            break;
        }
    }

    return mapping;
}

// ============================================================================
// A* SEARCH SIGNALS COMPUTATION
// ============================================================================

json compute_astar_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    const std::vector<int>& init_distances,
    const std::vector<int>& goal_distances) {

    const TransitionSystem& ts = fts.get_transition_system(merged_index);

    int reachable_count = 0;
    int unreachable_count = 0;
    long long sum_goal_dist = 0;
    int reachable_goal_count = 0;
    int best_goal_f = INF;

    // ✅ Compute reachability and search metrics
    for (size_t i = 0; i < init_distances.size(); ++i) {
        if (init_distances[i] != INF && goal_distances[i] != INF) {
            reachable_count++;
            sum_goal_dist += goal_distances[i];

            if (ts.is_goal_state(i)) {
                reachable_goal_count++;
                int f = init_distances[i] + goal_distances[i];
                if (f < best_goal_f) {
                    best_goal_f = f;
                }
            }
        } else {
            unreachable_count++;
        }
    }

    // Compute average search depth
    int search_depth = 0;
    if (reachable_goal_count > 0) {
        search_depth = static_cast<int>(std::round(
            static_cast<double>(sum_goal_dist) / reachable_goal_count
        ));
    }

    // Compute branching factor
    double branching_factor = 1.0;
    int num_transitions = 0;
    for (auto it = ts.begin(); it != ts.end(); ++it) {
        num_transitions += (*it).get_transitions().size();
    }

    if (reachable_count > 0 && num_transitions > 0) {
        branching_factor = static_cast<double>(num_transitions) /
                          static_cast<double>(reachable_count);

        // Safety checks
        if (std::isnan(branching_factor) || std::isinf(branching_factor)) {
            branching_factor = 1.0;
        }
        if (branching_factor < 1.0) {
            branching_factor = 1.0;
        }
    }

    bool solution_found = (best_goal_f != INF);

    json signals;
    signals["nodes_expanded"] = reachable_count;
    signals["unreachable_states"] = unreachable_count;
    signals["search_depth"] = search_depth;
    signals["branching_factor"] = branching_factor;
    signals["solution_cost"] = solution_found ? best_goal_f : 0;
    signals["solution_found"] = solution_found;

    return signals;
}

// ============================================================================
// MERGE BEFORE DATA EXPORT
// ============================================================================

// FILE: downward/src/search/merge_and_shrink/merge_and_shrink_signals.cc
// REPLACE export_merge_before_data function

json export_merge_before_data(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id,
    int iteration,
    bool shrunk,
    bool reduced) {

    json before_data;

    before_data["iteration"] = iteration;
    before_data["ts1_id"] = ts1_id;
    before_data["ts2_id"] = ts2_id;

    try {
        // ✅ VALIDATE: Check if indices are active FIRST
        if (!fts.is_active(ts1_id)) {
            std::cerr << "[SIGNALS] ERROR: ts1_id=" << ts1_id << " is NOT active!" << std::endl;
            before_data["error"] = "ts1_id is not active";
            before_data["ts1_active"] = false;
            return before_data;
        }
        if (!fts.is_active(ts2_id)) {
            std::cerr << "[SIGNALS] ERROR: ts2_id=" << ts2_id << " is NOT active!" << std::endl;
            before_data["error"] = "ts2_id is not active";
            before_data["ts2_active"] = false;
            return before_data;
        }

        before_data["ts1_active"] = true;
        before_data["ts2_active"] = true;

        const TransitionSystem& ts1 = fts.get_transition_system(ts1_id);
        const TransitionSystem& ts2 = fts.get_transition_system(ts2_id);

        const Distances& dist1 = fts.get_distances(ts1_id);
        const Distances& dist2 = fts.get_distances(ts2_id);

        // ✅ VALIDATE: Check if distances are computed
        if (!dist1.are_init_distances_computed() || !dist1.are_goal_distances_computed()) {
            std::cerr << "[SIGNALS] WARNING: ts1 distances not fully computed" << std::endl;
        }
        if (!dist2.are_init_distances_computed() || !dist2.are_goal_distances_computed()) {
            std::cerr << "[SIGNALS] WARNING: ts2 distances not fully computed" << std::endl;
        }

        const auto& init1 = dist1.get_init_distances();
        const auto& goal1 = dist1.get_goal_distances();
        const auto& init2 = dist2.get_init_distances();
        const auto& goal2 = dist2.get_goal_distances();

        // Compute F-values safely
        std::vector<int> f1(init1.size());
        std::vector<int> f2(init2.size());

        for (size_t i = 0; i < f1.size(); ++i) {
            if (i < init1.size() && i < goal1.size()) {
                if (init1[i] != INF && goal1[i] != INF) {
                    f1[i] = init1[i] + goal1[i];
                } else {
                    f1[i] = INF;
                }
            } else {
                f1[i] = INF;
            }
        }

        for (size_t j = 0; j < f2.size(); ++j) {
            if (j < init2.size() && j < goal2.size()) {
                if (init2[j] != INF && goal2[j] != INF) {
                    f2[j] = init2[j] + goal2[j];
                } else {
                    f2[j] = INF;
                }
            } else {
                f2[j] = INF;
            }
        }

        // Count transitions safely
        int ts1_transitions = 0;
        int ts2_transitions = 0;
        for (auto it = ts1.begin(); it != ts1.end(); ++it) {
            ts1_transitions += static_cast<int>((*it).get_transitions().size());
        }
        for (auto it = ts2.begin(); it != ts2.end(); ++it) {
            ts2_transitions += static_cast<int>((*it).get_transitions().size());
        }

        // Count goal states
        int ts1_goals = 0, ts2_goals = 0;
        for (int i = 0; i < ts1.get_size(); ++i) {
            if (ts1.is_goal_state(i)) ts1_goals++;
        }
        for (int j = 0; j < ts2.get_size(); ++j) {
            if (ts2.is_goal_state(j)) ts2_goals++;
        }

        // Populate JSON
        before_data["ts1_size"] = static_cast<int>(f1.size());
        before_data["ts2_size"] = static_cast<int>(f2.size());
        before_data["expected_product_size"] =
            static_cast<int>(f1.size()) * static_cast<int>(f2.size());

        before_data["ts1_transitions"] = ts1_transitions;
        before_data["ts2_transitions"] = ts2_transitions;
        before_data["ts1_density"] =
            static_cast<double>(ts1_transitions) / std::max(static_cast<int>(f1.size()), 1);
        before_data["ts2_density"] =
            static_cast<double>(ts2_transitions) / std::max(static_cast<int>(f2.size()), 1);

        before_data["ts1_goal_states"] = ts1_goals;
        before_data["ts2_goal_states"] = ts2_goals;

        before_data["ts1_f_values"] = f1;
        before_data["ts2_f_values"] = f2;

        before_data["ts1_f_stats"] = compute_f_statistics(f1);
        before_data["ts2_f_stats"] = compute_f_statistics(f2);

        before_data["ts1_variables"] = ts1.get_incorporated_variables();
        before_data["ts2_variables"] = ts2.get_incorporated_variables();

        // Only build product mapping for small products
        int product_size = static_cast<int>(f1.size()) * static_cast<int>(f2.size());
        if (product_size <= 100000) {
            before_data["product_mapping"] = build_product_mapping(
                static_cast<int>(f1.size()),
                static_cast<int>(f2.size())
            );
        } else {
            before_data["product_mapping_skipped"] = true;
            before_data["product_mapping_reason"] = "Product too large";
        }

        before_data["shrunk"] = shrunk;
        before_data["reduced"] = reduced;
        before_data["timestamp"] = static_cast<long>(std::time(nullptr));

        std::cout << "[SIGNALS] export_merge_before_data: ts1_size=" << f1.size()
                  << ", ts2_size=" << f2.size() << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] ERROR in export_merge_before_data: " << e.what() << std::endl;
        before_data["error"] = std::string(e.what());
    }

    return before_data;
}

// ============================================================================
// MERGE AFTER DATA EXPORT
// ============================================================================

json export_merge_after_data(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int iteration) {

    json after_data;
    after_data["iteration"] = iteration;

    try {
        const TransitionSystem& ts = fts.get_transition_system(merged_index);
        const Distances& distances = fts.get_distances(merged_index);

        const auto& init_dist = distances.get_init_distances();
        const auto& goal_dist = distances.get_goal_distances();

        // ✅ Compute F-values
        std::vector<int> f_after(init_dist.size());
        for (size_t s = 0; s < f_after.size(); ++s) {
            if (init_dist[s] != INF && goal_dist[s] != INF) {
                f_after[s] = init_dist[s] + goal_dist[s];
            } else {
                f_after[s] = INF;
            }
        }

        // ✅ Count reachable/unreachable
        int reachable_count = 0, unreachable_count = 0;
        for (size_t i = 0; i < f_after.size(); ++i) {
            if (init_dist[i] != INF && goal_dist[i] != INF) {
                reachable_count++;
            } else {
                unreachable_count++;
            }
        }

        // ✅ Count goal states
        int num_goals = 0;
        for (int i = 0; i < ts.get_size(); ++i) {
            if (ts.is_goal_state(i)) num_goals++;
        }

        // ✅ Count transitions
        int merged_transitions = 0;
        for (auto it = ts.begin(); it != ts.end(); ++it) {
            merged_transitions += (*it).get_transitions().size();
        }

        // ✅ Populate JSON
        after_data["merged_size"] = static_cast<int>(f_after.size());
        after_data["merged_goal_states"] = num_goals;
        after_data["merged_transitions"] = merged_transitions;
        after_data["merged_density"] =
            static_cast<double>(merged_transitions) /
            std::max(static_cast<int>(f_after.size()), 1);

        after_data["reachable_states"] = reachable_count;
        after_data["unreachable_states"] = unreachable_count;
        after_data["reachability_ratio"] =
            static_cast<double>(reachable_count) /
            std::max(static_cast<int>(f_after.size()), 1);

        after_data["f_values"] = f_after;
        after_data["f_stats"] = compute_f_statistics(f_after);

        after_data["shrinking_ratio"] =
            static_cast<double>(f_after.size()) /
            std::max(1, fts.get_transition_system(merged_index).get_size());

        // ✅ Compute A* signals
        json astar_signals = compute_astar_signals(
            fts, merged_index, init_dist, goal_dist
        );
        after_data["search_signals"] = astar_signals;

        after_data["timestamp"] = static_cast<long>(std::time(nullptr));

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Error in export_merge_after_data: " << e.what() << std::endl;
        after_data["error"] = std::string(e.what());
    }

    return after_data;
}

// ============================================================================
// TRANSITION SYSTEM DEFINITION EXPORT
// ============================================================================

json export_ts_data(
    const FactoredTransitionSystem& fts,
    int ts_index,
    int iteration) {

    json ts_json;
    ts_json["iteration"] = iteration;

    try {
        const TransitionSystem& ts = fts.get_transition_system(ts_index);

        ts_json["ts_index"] = ts_index;
        ts_json["num_states"] = ts.get_size();
        ts_json["init_state"] = ts.get_init_state();

        // ✅ Goal states
        std::vector<int> goal_states;
        for (int i = 0; i < ts.get_size(); ++i) {
            if (ts.is_goal_state(i)) {
                goal_states.push_back(i);
            }
        }
        ts_json["goal_states"] = goal_states;

        // ✅ Incorporated variables
        ts_json["incorporated_variables"] = ts.get_incorporated_variables();

        // ✅ Transitions (sample if too many)
        std::vector<json> transitions;
        int transition_count = 0;
        const int MAX_TRANSITIONS_TO_EXPORT = 10000;

        for (auto it = ts.begin(); it != ts.end(); ++it) {
            const auto& info = *it;
            const auto& label_group = info.get_label_group();
            const auto& trans_vec = info.get_transitions();

            for (int label : label_group) {
                for (const auto& trans : trans_vec) {
                    if (transition_count < MAX_TRANSITIONS_TO_EXPORT) {
                        transitions.push_back({
                            {"src", trans.src},
                            {"target", trans.target},
                            {"label", label}
                        });
                        transition_count++;
                    }
                }
            }
        }

        ts_json["transitions"] = transitions;

        if (transition_count >= MAX_TRANSITIONS_TO_EXPORT) {
            ts_json["_transitions_note"] = "Transitions truncated (too many)";
        }

        ts_json["timestamp"] = static_cast<long>(std::time(nullptr));

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Error in export_ts_data: " << e.what() << std::endl;
        ts_json["error"] = std::string(e.what());
    }

    return ts_json;
}

// ============================================================================
// FD INDEX MAPPING EXPORT - ✅ CRITICAL FOR SYNC
// ============================================================================

json export_fd_index_mapping_data(
    const FactoredTransitionSystem& fts,
    int iteration) {

    json index_mapping;
    index_mapping["iteration"] = iteration;
    index_mapping["timestamp"] = static_cast<long>(std::time(nullptr));

    json systems = json::array();

    try {
        // Iterate over all active systems
        for (int fd_idx = 0; fd_idx < fts.get_size(); ++fd_idx) {
            if (!fts.is_active(fd_idx)) {
                continue;
            }

            const TransitionSystem& ts = fts.get_transition_system(fd_idx);

            json system_entry;
            system_entry["fd_index"] = fd_idx;
            system_entry["num_states"] = ts.get_size();
            system_entry["incorporated_variables"] = ts.get_incorporated_variables();
            system_entry["is_active"] = true;

            systems.push_back(system_entry);
        }

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Error in export_fd_index_mapping_data: "
                  << e.what() << std::endl;
        index_mapping["error"] = std::string(e.what());
    }

    index_mapping["systems"] = systems;

    return index_mapping;
}

// ============================================================================
// ERROR SIGNAL EXPORT
// ============================================================================

void export_error_signal(
    int iteration,
    const std::string& error_message,
    const std::string& fd_output_dir) {

    try {
        json error_data;
        error_data["iteration"] = iteration;
        error_data["error"] = true;
        error_data["message"] = error_message;
        error_data["timestamp"] = static_cast<long>(std::time(nullptr));

        std::string error_path = fd_output_dir + "/gnn_error_" +
                                 std::to_string(iteration) + ".json";

        write_json_file_atomic(error_data, error_path);

        std::cerr << "[SIGNALS] ❌ Error signal exported: " << error_path << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[SIGNALS] Failed to export error signal: " << e.what() << std::endl;
    }
}

// ============================================================================
// MAIN EXPORT FUNCTION - Called after each merge
// ============================================================================

void export_merge_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int ts1_id,
    int ts2_id,
    const std::string& fd_output_dir,
    int iteration,
    bool shrunk,
    bool reduced) {

    std::cout << "\n[SIGNALS] ========================================" << std::endl;
    std::cout << "[SIGNALS] Exporting signals for iteration " << iteration << std::endl;
    std::cout << "[SIGNALS] ========================================\n" << std::endl;

    try {
        // ✅ 1. Export merge_before
        std::cout << "[SIGNALS] 1. Exporting merge_before..." << std::endl;
        json before_data = export_merge_before_data(
            fts, ts1_id, ts2_id, iteration, shrunk, reduced
        );
        std::string before_path = fd_output_dir + "/merge_before_" +
                                  std::to_string(iteration) + ".json";
        write_json_file_atomic(before_data, before_path);

        // ✅ 2. Export merge_after
        std::cout << "[SIGNALS] 2. Exporting merge_after..." << std::endl;
        json after_data = export_merge_after_data(fts, merged_index, iteration);
        std::string after_path = fd_output_dir + "/merge_after_" +
                                 std::to_string(iteration) + ".json";
        write_json_file_atomic(after_data, after_path);

        // ✅ 3. Export ts_N
        std::cout << "[SIGNALS] 3. Exporting ts definition..." << std::endl;
        json ts_data = export_ts_data(fts, merged_index, iteration);
        std::string ts_path = fd_output_dir + "/ts_" +
                              std::to_string(iteration) + ".json";
        write_json_file_atomic(ts_data, ts_path);

        // ✅ 4. Export fd_index_mapping (CRITICAL!)
        std::cout << "[SIGNALS] 4. Exporting FD index mapping..." << std::endl;
        json mapping_data = export_fd_index_mapping_data(fts, iteration);
        std::string mapping_path = fd_output_dir + "/fd_index_mapping_" +
                                   std::to_string(iteration) + ".json";
        write_json_file_atomic(mapping_data, mapping_path);

        std::cout << "\n[SIGNALS] ✅ All signals exported for iteration "
                  << iteration << "\n" << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "\n[SIGNALS] ❌ CRITICAL ERROR exporting signals: "
                  << e.what() << std::endl;
        export_error_signal(iteration,
                           std::string("Signal export failed: ") + e.what(),
                           fd_output_dir);
        throw;
    }
}

}  // namespace merge_and_shrink

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_signals.h code is in the following block:
#ifndef MERGE_AND_SHRINK_SIGNALS_H
#define MERGE_AND_SHRINK_SIGNALS_H

#include <string>
#include <vector>
#include <nlohmann/json.hpp>

using json = nlohmann::json;

namespace merge_and_shrink {

// Forward declarations
class FactoredTransitionSystem;
class TransitionSystem;
class Distances;

// ============================================================================
// SIGNAL EXPORT - Core Functions for Communication Protocol
// ============================================================================

/**
 * ✅ CRITICAL: Export all signals for a merge operation
 *
 * Called AFTER each merge decision is executed by C++.
 * Exports 4 JSON files:
 *   - merge_before_N.json (pre-merge state)
 *   - merge_after_N.json (post-merge state + A* signals)
 *   - ts_N.json (transition system definition)
 *   - fd_index_mapping_N.json (synchronization critical!)
 *
 * @param fts Factored transition system
 * @param merged_index Index of newly merged transition system
 * @param ts1_id First system ID
 * @param ts2_id Second system ID
 * @param fd_output_dir Directory for output files
 * @param iteration Iteration number
 */
void export_merge_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int ts1_id,
    int ts2_id,
    const std::string& fd_output_dir,
    int iteration,
    bool shrunk,
    bool reduced
);

// ============================================================================
// HELPER FUNCTIONS - Individual Signal Export
// ============================================================================

/**
 * Export pre-merge state information
 */
json export_merge_before_data(
    const FactoredTransitionSystem& fts,
    int ts1_id,
    int ts2_id,
    int iteration,
    bool shrunk,
    bool reduced
);

/**
 * Export post-merge state with A* signals
 */
json export_merge_after_data(
    const FactoredTransitionSystem& fts,
    int merged_index,
    int iteration
);

/**
 * Export transition system definition
 */
json export_ts_data(
    const FactoredTransitionSystem& fts,
    int ts_index,
    int iteration
);

/**
 * Export FD index mapping for Python synchronization
 * ✅ CRITICAL: Called after EVERY merge
 */
json export_fd_index_mapping_data(
    const FactoredTransitionSystem& fts,
    int iteration
);

// ============================================================================
// UTILITY FUNCTIONS - Computation of Signals
// ============================================================================

/**
 * Compute F-value statistics from distance arrays
 */
json compute_f_statistics(
    const std::vector<int>& distances,
    int unreachable_marker = 2147483647  // INF marker
);

/**
 * Compute A* search signals from merged transition system
 */
json compute_astar_signals(
    const FactoredTransitionSystem& fts,
    int merged_index,
    const std::vector<int>& init_distances,
    const std::vector<int>& goal_distances
);

/**
 * Build product state mapping for merge_before
 */
json build_product_mapping(int ts1_size, int ts2_size);

/**
 * Write JSON file atomically to disk
 */
void write_json_file_atomic(
    const json& data,
    const std::string& file_path
);

/**
 * Get absolute path to fd_output directory
 */
std::string get_fd_output_directory();

/**
 * Get absolute path to gnn_output directory (for reference)
 */
std::string get_gnn_output_directory();

// ============================================================================
// ERROR HANDLING - Export error signals
// ============================================================================

/**
 * Export error message to Python for diagnostics
 */
void export_error_signal(
    int iteration,
    const std::string& error_message,
    const std::string& fd_output_dir
);

}  // namespace merge_and_shrink

#endif

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_gnn.cc code is in the following block:
#include "merge_strategy_gnn.h"
#include "../utils/system.h"
#include "merge_and_shrink_signals.h"

#include <nlohmann/json.hpp>
#include <fstream>
#include <thread>
#include <chrono>
#include <iostream>
#include <filesystem>
#include <stdexcept>
#include <cstdlib>
#include <ctime>

using namespace std;
using json = nlohmann::json;
using namespace merge_and_shrink;
namespace fs = std::filesystem;

MergeStrategyGNN::MergeStrategyGNN(const FactoredTransitionSystem& fts)
    : MergeStrategy(fts),
      iteration(0) {
    cout << "\n[GNN::INIT] MergeStrategyGNN initialized" << endl;
}

std::filesystem::path get_gnn_input_directory() {
    std::filesystem::path cwd = std::filesystem::current_path();
    std::filesystem::path gnn_output = cwd / "gnn_output";

    try {
        std::filesystem::create_directories(gnn_output);
    } catch (const std::exception& e) {
        std::cerr << "[GNN::PATH] Warning: Could not create gnn_output: "
                  << e.what() << std::endl;
    }

    std::cout << "[GNN::PATH] Using gnn_input: " << gnn_output.string() << std::endl;
    return gnn_output;
}

std::pair<int, int> MergeStrategyGNN::get_next() {
    int idx = iteration;

    std::filesystem::path gnn_input_dir = get_gnn_input_directory();
    std::filesystem::path fd_output_dir = std::filesystem::current_path() / "fd_output";

    // ✅ CRITICAL: Ensure directories exist
    try {
        std::filesystem::create_directories(gnn_input_dir);
        std::filesystem::create_directories(fd_output_dir);
        std::cout << "[GNN::GET_NEXT] Directories verified:" << std::endl;
        std::cout << "[GNN::GET_NEXT]   gnn_input: " << gnn_input_dir.string() << std::endl;
        std::cout << "[GNN::GET_NEXT]   fd_output: " << fd_output_dir.string() << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "[GNN::ERROR] Failed to create directories: " << e.what() << std::endl;
        throw;
    }

    std::filesystem::path in_path = gnn_input_dir / ("merge_" + std::to_string(idx) + ".json");
    std::filesystem::path ack_path = fd_output_dir / ("gnn_ack_" + std::to_string(idx) + ".json");

    std::cout << "\n[GNN::ITERATION " << idx << "] ========================================" << std::endl;
    std::cout << "[GNN::ITERATION " << idx << "] Waiting for merge decision at:" << std::endl;
    std::cout << "[GNN::ITERATION " << idx << "]   " << in_path.string() << std::endl;

    // ✅ ENHANCED: Better timeout handling
    const int sleep_ms = 50;
    const int max_wait_ms = 600000;  // 10 minutes
    int elapsed_ms = 0;
    int last_logged = 0;

    while (!std::filesystem::exists(in_path)) {
        if (elapsed_ms > last_logged + 10000) {
            std::cout << "[GNN::ITERATION " << idx << "] ... waiting ("
                      << elapsed_ms / 1000 << "s) ..." << std::endl;
            last_logged = elapsed_ms;
        }

        if (elapsed_ms > max_wait_ms) {
            std::cerr << "\n[GNN::ERROR] ❌ TIMEOUT waiting for merge decision!" << std::endl;
            std::cerr << "[GNN::ERROR] Expected: " << in_path.string() << std::endl;

            // Diagnostic
            std::cout << "[GNN::DIAG] Contents of " << gnn_input_dir.string() << ":" << std::endl;
            try {
                for (const auto& entry : std::filesystem::directory_iterator(gnn_input_dir)) {
                    std::cout << "[GNN::DIAG]   " << entry.path().filename().string() << std::endl;
                }
            } catch (const std::exception& e) {
                std::cout << "[GNN::DIAG] Error reading directory: " << e.what() << std::endl;
            }

            export_error_signal(idx, "Timeout waiting for merge decision", fd_output_dir.string());
            throw std::runtime_error("GNN merge decision timeout");
        }

        std::this_thread::sleep_for(std::chrono::milliseconds(sleep_ms));
        elapsed_ms += sleep_ms;
    }

    std::cout << "[GNN::ITERATION " << idx << "] ✅ Decision file found!" << std::endl;

    // ✅ READ AND VALIDATE
    json merge_decision;
    try {
        std::ifstream fin(in_path);
        if (!fin.is_open()) {
            throw std::runtime_error("Cannot open: " + in_path.string());
        }
        fin >> merge_decision;
        fin.close();

        if (!merge_decision.contains("merge_pair") ||
            !merge_decision["merge_pair"].is_array() ||
            merge_decision["merge_pair"].size() != 2) {
            throw std::runtime_error("Invalid merge_pair in JSON");
        }

        std::cout << "[GNN::ITERATION " << idx << "] ✅ JSON parsed successfully" << std::endl;
    } catch (const std::exception& e) {
        std::cerr << "[GNN::ERROR] Failed to parse merge decision: " << e.what() << std::endl;
        export_error_signal(idx, std::string("Parse error: ") + e.what(), fd_output_dir.string());
        throw;
    }

    int u = merge_decision["merge_pair"][0].get<int>();
    int v = merge_decision["merge_pair"][1].get<int>();

    std::cout << "[GNN::ITERATION " << idx << "] Merge decision: (" << u << ", " << v << ")" << std::endl;

    // ✅ WRITE ACK ATOMICALLY
    try {
        std::filesystem::create_directories(fd_output_dir);

        json ack_data;
        ack_data["iteration"] = idx;
        ack_data["merge_pair"] = {u, v};
        ack_data["received"] = true;
        ack_data["timestamp"] = static_cast<long>(std::time(nullptr));

        std::filesystem::path temp_path = std::filesystem::path(ack_path.string() + ".tmp");
        {
            std::ofstream fout(temp_path);
            if (!fout.is_open()) {
                throw std::runtime_error("Cannot open temp ACK file");
            }
            fout << ack_data.dump(2);
            fout.flush();
            fout.close();

            if (fout.fail()) {
                throw std::runtime_error("Write/close failed");
            }
        }

        try {
            std::filesystem::rename(temp_path, ack_path);
        } catch (const std::filesystem::filesystem_error& e) {
            std::cerr << "[GNN::WARNING] Rename failed, using direct write: " << e.what() << std::endl;
            std::ofstream fout(ack_path);
            if (fout.is_open()) {
                fout << ack_data.dump(2);
                fout.close();
            } else {
                throw std::runtime_error("Cannot write ACK file");
            }
        }

        std::cout << "[GNN::ITERATION " << idx << "] ✅ ACK written to:" << std::endl;
        std::cout << "[GNN::ITERATION " << idx << "]   " << ack_path.string() << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[GNN::WARNING] ACK write failed (continuing): " << e.what() << std::endl;
    }

    iteration++;

    std::cout << "[GNN::ITERATION " << idx << "] ========================================" << std::endl;
    std::cout << "[GNN::ITERATION " << idx << "] ✅ Returning (" << u << ", " << v << ")\n" << std::endl;

    return {u, v};
}

void MergeStrategyGNN::set_next_merge_pair(
    const FactoredTransitionSystem&,
    const std::vector<std::shared_ptr<ShrinkStrategy>>&) {
    // Not used
}

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_gnn.h code is in the following block:
#pragma once

#include "merge_strategy.h"
#include "factored_transition_system.h"
#include "shrink_strategy.h"

#include <vector>
#include <memory>
#include <string>

namespace merge_and_shrink {

    class MergeStrategyGNN : public MergeStrategy {
    private:
        int iteration;
        std::string gnn_output_dir;
//        std::string fd_output_dir;

    public:
        explicit MergeStrategyGNN(const FactoredTransitionSystem& fts);

        std::pair<int, int> get_next() override;

        void set_next_merge_pair(
            const FactoredTransitionSystem&,
            const std::vector<std::shared_ptr<ShrinkStrategy>>&);
    };

}


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_factory_gnn.cc code is in the following block:
﻿//#include "merge_strategy_factory_gnn.h"
//#include "merge_strategy_gnn.h"
//
//#include "../plugins/plugin.h"            // for TypedFeature
//#include "../utils/logging.h"             // brings in utils::Verbosity
//
//using namespace std;

#include "merge_strategy_factory_gnn.h"
#include "merge_strategy_gnn.h"

#include "../plugins/plugin.h"
#include "../utils/logging.h"

using std::string;
using std::unique_ptr;
using std::make_unique;
using std::shared_ptr;

namespace merge_and_shrink {

    // 1) Constructor: forward verbosity to base
    MergeStrategyFactoryGNN::MergeStrategyFactoryGNN(utils::Verbosity verbosity)
        : MergeStrategyFactory(verbosity) {
    }

    // 2) Instantiation
    unique_ptr<MergeStrategy> MergeStrategyFactoryGNN::compute_merge_strategy(
        const TaskProxy& /*task_proxy*/,
        const FactoredTransitionSystem& fts) {
        return make_unique<MergeStrategyGNN>(fts);
    }

    string MergeStrategyFactoryGNN::name() const {
        return "merge_gnn";
    }

    void MergeStrategyFactoryGNN::dump_strategy_specific_options() const {
        // No extra options here
    }

    // 3) Plugin registration, exactly like the others
    class MergeStrategyFactoryGNNFeature
        : public plugins::TypedFeature<MergeStrategyFactory, MergeStrategyFactoryGNN> {
    public:
        MergeStrategyFactoryGNNFeature()
            : TypedFeature("merge_gnn") {
            document_title("GNN-based merge strategy");
            document_synopsis(
                "Reads the next merge pair from an external JSON file "
                "produced by a GNN.");
            add_merge_strategy_options_to_feature(*this);
        }

        // This unpacks (verbosity) from opts into the ctor
        shared_ptr<MergeStrategyFactoryGNN> create_component(
            const plugins::Options& opts) const override {
            return plugins::make_shared_from_arg_tuples<MergeStrategyFactoryGNN>(
                get_merge_strategy_arguments_from_options(opts)
            );
        }
    };

    // static registration
    static plugins::FeaturePlugin<MergeStrategyFactoryGNNFeature> _plugin;

}  // namespace merge_and_shrink


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_factory_gnn.h code is in the following block:
#ifndef MERGE_AND_SHRINK_MERGE_STRATEGY_FACTORY_GNN_H
#define MERGE_AND_SHRINK_MERGE_STRATEGY_FACTORY_GNN_H

#include "merge_strategy_factory.h"

namespace merge_and_shrink {

    class MergeStrategyFactoryGNN : public MergeStrategyFactory {
    public:
        // Match the other factories: take verbosity
        explicit MergeStrategyFactoryGNN(utils::Verbosity verbosity);

        // Create the strategy object
        std::unique_ptr<MergeStrategy> compute_merge_strategy(
            const TaskProxy& task_proxy,
            const FactoredTransitionSystem& fts) override;

        std::string name() const override;
        void dump_strategy_specific_options() const override;
        bool requires_init_distances()  const override { return false; }
        bool requires_goal_distances()  const override { return false; }
    };

}  // namespace merge_and_shrink

#endif


--------------------------------------------------------------------------------

