Please tell me how to implement this changes to my code, so that the changes given to me, you will show me the refactored code part that i just need to copy and paste into my existing code, and you will tell me where to paste it, and instead of what existing code to put it, in order to make those changes and improvements

"""""""""""""
Comprehensive Code Optimization Guide for Training Phase
Executive Summary
Before training on larger problems, implement these optimizations to reduce overhead. The most critical ones for long-running training are:

Caching (15-30% speedup): Avoid recomputing static/slow-changing values
File I/O (20-40% speedup): Batch JSON operations, reduce disk writes
Memory (30-50% reduction): Pre-allocate, avoid deep copies
Vectorization (10-25% speedup): Use NumPy/PyTorch batch operations
Profiling (baseline): Measure before/after to validate improvements


1. CRITICAL OPTIMIZATIONS (Do First - Highest Impact)
1.1 Cache Graph Properties in graph_tracker.py
Problem: Centrality, max_vars, max_iter recomputed every observation.
pythonCopy# FILE: graph_tracker.py - ADD TO __init__

class GraphTracker:
    def __init__(self, ts_json_path: str, cg_json_path: str, is_debug: bool = False):
        # ... existing code ...

        # ✅ ADD: Caching for expensive computations
        self._centrality_cache: Optional[Dict] = None
        self._centrality_cache_valid = False
        self._max_vars_cache: Optional[int] = None
        self._max_iter_cache: Optional[int] = None
        self._graph_hash_last = None  # Track if graph changed

    def _invalidate_caches(self):
        """Call after any graph modification."""
        self._centrality_cache_valid = False
        self._graph_hash_last = None

    def get_centrality(self) -> Dict:
        """Return cached centrality or compute once."""
        if not self._centrality_cache_valid:
            self._centrality_cache = nx.closeness_centrality(self.graph)
            self._centrality_cache_valid = True
        return self._centrality_cache

    def get_max_vars(self) -> int:
        """Return cached max_vars."""
        if self._max_vars_cache is None:
            self._max_vars_cache = max(
                (len(d.get("incorporated_variables", [])) for _, d in self.graph.nodes(data=True)),
                default=1
            ) or 1
        return self._max_vars_cache

    def get_max_iter(self) -> int:
        """Return cached max_iter."""
        if self._max_iter_cache is None:
            self._max_iter_cache = max(
                (d.get("iteration", 0) for _, d in self.graph.nodes(data=True)),
                default=0
            ) or 1
        return self._max_iter_cache

    def merge_nodes(self, node_ids: List[Union[int, str]]) -> None:
        # ... existing merge code ...
        self._invalidate_caches()  # ✅ ADD THIS
Usage in merge_env.py:
pythonCopy# In _get_observation()
self.centrality = self.graph_tracker.get_centrality()  # Now cached
self.max_vars = self.graph_tracker.get_max_vars()
self.max_iter = self.graph_tracker.get_max_iter()
Impact: 15-25% faster observation generation on large graphs.

1.2 Optimize _get_observation() with Pre-allocation
Problem: Creates new arrays every step, inefficient feature calculations.
pythonCopy# FILE: merge_env.py - REPLACE _get_observation METHOD

def _get_observation(self) -> Dict:
    """✅ OPTIMIZED: Pre-allocated arrays, vectorized operations."""
    max_nodes, max_edges = 100, 1000

    G = self.graph_tracker.graph

    # ✅ PRE-ALLOCATE arrays once
    if not hasattr(self, '_obs_cache'):
        self._obs_cache = {
            'x': np.zeros((max_nodes, self.feat_dim), dtype=np.float32),
            'edge_index': np.zeros((2, max_edges), dtype=np.int64),
            'edge_features': np.zeros((max_edges, 8), dtype=np.float32),
        }

    # ✅ REUSE arrays (faster than allocating new)
    x = self._obs_cache['x']
    x.fill(0)  # Clear instead of reallocate

    ei = self._obs_cache['edge_index']
    ei.fill(0)

    ef = self._obs_cache['edge_features']
    ef.fill(0)

    # ✅ VECTORIZED: Batch normalization
    degs = dict(G.degree()).values()
    max_deg = max(max(degs, default=0), 1)
    max_states_node = max((d.get("num_states", 0) for _, d in G.nodes(data=True)), default=1) or 1

    # ✅ NEW: Pre-compute normalizations
    f_values_all = []
    for _, d in G.nodes(data=True):
        f_vals = d.get("f_before", [])
        if f_vals:
            f_values_all.extend(f_vals)

    f_scale = max(f_values_all) if f_values_all else 1.0
    f_scale = max(f_scale, 1.0)

    # ✅ Batch node feature computation
    node_features_list = []
    idx = {}

    centrality = self.graph_tracker.get_centrality()

    for i, (nid, data) in enumerate(G.nodes(data=True)):
        if i >= max_nodes:
            break

        # ✅ Vectorized feature calculation
        ns_raw = float(data.get("num_states", 0))
        num_states = ns_raw / float(max_states_node)
        is_atomic = 1.0 if data.get("iteration", -1) == -1 else 0.0

        d_norm = G.degree(nid) / max_deg
        od_norm = G.out_degree(nid) / max_deg

        # ✅ Vectorized F-value stats
        f_vals = np.array(data.get("f_before", []), dtype=np.float32)

        if len(f_vals) > 0 and f_scale > 0:
            valid_f = f_vals[(f_vals != np.inf) & (f_vals >= 0) & (f_vals < 1e9)]
            if len(valid_f) > 0:
                avg_f_norm = float(np.mean(valid_f)) / f_scale
                max_f_norm = float(np.max(valid_f)) / f_scale
            else:
                avg_f_norm = 0.0
                max_f_norm = 0.0
        else:
            avg_f_norm = 0.0
            max_f_norm = 0.0

        num_vars_norm = len(data.get("incorporated_variables", [])) / float(self.max_vars)
        iter_idx_norm = data.get("iteration", 0) / float(self.max_iter)
        central = float(centrality.get(nid, 0.0))

        f_min, f_mean, f_max, f_std = self.graph_tracker.f_stats(nid)
        if f_scale > 0:
            f_min = max(0.0, min(f_min / f_scale, 1.0))
            f_mean = max(0.0, min(f_mean / f_scale, 1.0))
            f_max = max(0.0, min(f_max / f_scale, 1.0))
            f_std = max(0.0, min(f_std / f_scale, 1.0))
        else:
            f_min = f_mean = f_max = f_std = 0.0

        reachable_ratio = 1.0
        if len(f_vals) > 0:
            unreachable = np.sum((f_vals == np.inf) | (f_vals >= 1e9))
            reachable_ratio = float(1.0 - unreachable / len(f_vals))

        num_neighbors = len(list(G.neighbors(nid)))
        neighbor_risk = float(num_neighbors) / max(len(G.nodes), 1)

        # ✅ SINGLE ARRAY ASSIGNMENT (faster than building list and converting)
        x[i, :] = [
            num_states, is_atomic, d_norm, od_norm,
            avg_f_norm, max_f_norm, 0.0, reachable_ratio,
            num_vars_norm, iter_idx_norm, central,
            f_min, f_mean, f_max, f_std,
            neighbor_risk, ns_raw / 10000.0,
            0.0, 0.0
        ]

        idx[nid] = i

    num_nodes_feat = len(idx)

    # ✅ VECTORIZED: Edge processing
    edges = [
        (idx[u], idx[v])
        for u, v in G.edges()
        if u in idx and v in idx
    ]
    ne = len(edges)

    for j, (u, v) in enumerate(edges[:max_edges]):
        ei[0, j] = u
        ei[1, j] = v

    # ✅ OPTIMIZED: Batch edge feature extraction
    if ne > 0:
        edge_features = self._extract_edge_features_vectorized(G, edges, idx)
        ef[:ne, :] = edge_features[:ne]

    return {
        "x": x,
        "edge_index": ei,
        "edge_features": ef,
        "num_nodes": np.int32(num_nodes_feat),
        "num_edges": np.int32(ne),
    }

def _extract_edge_features_vectorized(self, G, edges, idx):
    """✅ VECTORIZED: Compute edge features for all edges at once."""
    n_edges = len(edges)
    if n_edges == 0:
        return np.zeros((0, 8), dtype=np.float32)

    # ✅ Pre-compute node properties once
    node_sizes = {}
    node_f_valid = {}

    max_size_in_graph = max(
        (d.get("num_states", 1) for _, d in G.nodes(data=True)), default=1
    )

    for nid, data in G.nodes(data=True):
        node_sizes[nid] = data.get("num_states", 1)
        f_vals = data.get("f_before", [])
        f_valid = [f for f in f_vals if f != float('inf') and f < 1e9]
        node_f_valid[nid] = len(f_valid) / max(len(f_vals), 1) if f_vals else 0.5

    # ✅ VECTORIZED: Compute all features in one pass
    features = np.zeros((n_edges, 8), dtype=np.float32)

    centrality = self.graph_tracker.get_centrality()

    for edge_idx, (i, j) in enumerate(edges):
        # Find original node IDs
        u = [nid for nid, idx_val in idx.items() if idx_val == i][0]
        v = [nid for nid, idx_val in idx.items() if idx_val == j][0]

        u_size = node_sizes.get(u, 1)
        v_size = node_sizes.get(v, 1)

        # Feature 1: Size difference
        max_size = max(max_size_in_graph, 1)
        size_diff = abs(u_size - v_size) / max_size

        # Feature 2: Product size ratio
        product_size = (u_size * v_size) / max(max_size * max_size, 1)
        merge_size_ratio = np.clip(product_size, 0.0, 1.0)

        # Feature 3-8: Others (simplified for performance)
        u_vars = len(G.nodes[u].get("incorporated_variables", []))
        v_vars = len(G.nodes[v].get("incorporated_variables", []))
        shared_ratio = u_vars + v_vars  # Simplified

        u_reach = node_f_valid.get(u, 0.5)
        v_reach = node_f_valid.get(v, 0.5)
        reach_sim = 1.0 - abs(u_reach - v_reach)

        u_iter = G.nodes[u].get("iteration", 0)
        v_iter = G.nodes[v].get("iteration", 0)
        max_iter = self.max_iter or 1
        iter_diff = abs(u_iter - v_iter) / max_iter

        u_cent = centrality.get(u, 0.0)
        v_cent = centrality.get(v, 0.0)
        cent_sim = 1.0 - abs(u_cent - v_cent)

        u_deg = G.degree(u) / max(G.number_of_nodes(), 1)
        v_deg = G.degree(v) / max(G.number_of_nodes(), 1)
        merge_risk = np.clip((u_deg + v_deg) * 0.5, 0.0, 1.0)

        features[edge_idx] = [
            size_diff,
            merge_size_ratio,
            shared_ratio / 10.0,  # Normalize
            reach_sim,
            iter_diff,
            cent_sim,
            0.5,  # Simplified f_consistency
            merge_risk
        ]

    return features
Impact: 20-35% faster observation generation.

1.3 Batch JSON I/O in merge_and_shrink_algorithm.cc
Problem: Each merge writes 3 JSON files individually (sync I/O).
Solution: Buffer writes and flush periodically.
cppCopy// FILE: downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.cc
// ADD AFTER main_loop()

class JSONOutputBuffer {
private:
    std::map<std::string, json> buffer;
    const int FLUSH_EVERY = 10;  // Flush every N iterations
    int iteration_count = 0;

public:
    void add_before(int iter, const json& data) {
        buffer["before_" + std::to_string(iter)] = data;
        maybe_flush();
    }

    void add_after(int iter, const json& data) {
        buffer["after_" + std::to_string(iter)] = data;
        maybe_flush();
    }

    void add_ts(int iter, const json& data) {
        buffer["ts_" + std::to_string(iter)] = data;
        maybe_flush();
    }

    void maybe_flush() {
        iteration_count++;
        if (iteration_count % FLUSH_EVERY == 0) {
            flush_all();
        }
    }

    void flush_all() {
        if (buffer.empty()) return;

        for (auto& [key, data] : buffer) {
            std::string path = fd_output_dir + "/" + key + ".json";
            // ✅ BATCHED: Write all buffered at once
            std::ofstream f(path);
            f << data.dump(2);
            f.close();
        }
        buffer.clear();
    }
};

// USE IN main_loop():
JSONOutputBuffer json_buffer;

// Instead of:
// (write immediately after each merge)

// DO:
// json_buffer.add_before(iteration, before_data);
// json_buffer.add_after(iteration, after_data);
// json_buffer.add_ts(iteration, ts_data);
// (at end: json_buffer.flush_all())
Impact: 30-50% faster merge operations (reduces I/O blocking).

2. SECONDARY OPTIMIZATIONS (Good ROI)
2.1 Cache Reward Computations in reward_function_variants.py
pythonCopy# FILE: reward_function_variants.py

class AStarSearchReward(RewardFunctionBase):
    def __init__(self, **kwargs):
        super().__init__("EnhancedAStarSearchReward")
        # ... existing init ...
        self._last_merge_info_hash = None
        self._cached_f_stability = None

    def compute(self, merge_info: MergeInfo, **kwargs) -> float:
        """✅ Cache expensive F-stability computation."""
        # Compute hash of important fields
        current_hash = hash((
            tuple(merge_info.f_before[:10]),  # Sample
            tuple(merge_info.f_after[:10]),
            merge_info.states_before,
            merge_info.states_after,
        ))

        # ✅ Return cached if same merge pattern
        if current_hash == self._last_merge_info_hash and self._cached_f_stability is not None:
            f_stability = self._cached_f_stability
        else:
            f_stability = self._compute_robust_f_stability(merge_info)
            self._last_merge_info_hash = current_hash
            self._cached_f_stability = f_stability

        # ... rest of compute() using cached f_stability ...
Impact: 10-15% faster reward computation for similar merges.

2.2 Reduce Graph Operations in gnn_policy.py
Problem: Batch processing creates redundant tensors.
pythonCopy# FILE: gnn_policy.py - OPTIMIZE _mask_invalid_edges

def _mask_invalid_edges(self, logits: Tensor, num_edges: int) -> Tensor:
    """✅ OPTIMIZED: In-place masking to avoid copies."""
    E = logits.size(0)
    num_edges_clamped = max(1, min(int(num_edges), E))

    # ✅ In-place masking (avoids tensor copy)
    masked = logits.clone()
    if num_edges_clamped < E:
        masked[num_edges_clamped:] = -1e9  # In-place assignment

    return masked
Impact: 5-10% faster policy inference.

3. MEMORY OPTIMIZATIONS
3.1 Use Memory-Mapped Files for Large Data
pythonCopy# FILE: reward_info_extractor.py - OPTIMIZE _load_and_validate_json

def _load_and_validate_json(self, path: str, phase: str, timeout: float = 30.0) -> Optional[Dict]:
    """✅ Use memory mapping for large JSON files."""
    import mmap

    start_time = time.time()

    while time.time() - start_time < timeout:
        if not os.path.exists(path):
            time.sleep(0.1)
            continue

        try:
            # ✅ Memory-mapped file access (faster for large files)
            with open(path, 'r+b') as f:
                with mmap.mmap(f.fileno(), 0) as mmapped_file:
                    # Read into buffer
                    data_bytes = mmapped_file[:]
                    content = data_bytes.decode('utf-8')

                    if content.strip():
                        data = json.loads(content)
                        return data

        except (json.JSONDecodeError, IOError):
            time.sleep(0.1)
            continue

    return None
Impact: 20-40% faster for large JSON files (>10MB).

3.2 Pre-allocate Transition System Data
pythonCopy# FILE: graph_tracker.py - ADD TO _add_or_update_node

def _add_or_update_node(self, ts: Dict[str, Any]) -> None:
    """✅ OPTIMIZED: Avoid storing full transition lists."""
    ts_data = ts.copy()

    # ✅ DON'T store full transitions - just count them
    transitions = ts_data.pop("transitions", [])
    ts_data["num_transitions"] = len(transitions)  # ✅ Only store count

    # ✅ Sparse storage for large f_before lists
    f_before = ts_data.get("f_before", [])
    if len(f_before) > 10000:
        # ✅ Subsample if too large
        ts_data["f_before_sampled"] = np.array(f_before)[::10].tolist()
        ts_data.pop("f_before", None)

    # ... rest of method ...
Impact: 30-60% memory reduction for large transition systems.
""""""""""""""""



Those are the files of my project:


The file default_policy.py code is this:
import numpy as np
from merge_env import MergeEnv


class DefaultMergePolicy:
    """
    A greedy baseline policy that chooses the merge action that results in
    the smallest number of total abstract states.
    """

    def __init__(self, env: MergeEnv):
        """Initializes the policy with a reference to the environment."""
        self.env = env

    def _get_total_states(self, tracker) -> int:
        """Calculates the sum of states across all nodes."""
        return sum(d.get("num_states", 0) for _, d in tracker.graph.nodes(data=True))

    def predict(self, observation: dict, deterministic: bool = True) -> tuple[np.ndarray, None]:
        """Predicts the best action by choosing smallest state explosion."""
        graph_tracker = self.env.graph_tracker
        if not graph_tracker or not graph_tracker.graph.edges:
            return np.array([0], dtype=np.int64), None

        # Get the list of currently valid edges
        edges = list(graph_tracker.graph.edges)
        if not edges:
            return np.array([0], dtype=np.int64), None

        best_action_index = 0
        min_resulting_states = float('inf')

        for i, (u, v) in enumerate(edges):
            try:
                # Simulate WITHOUT deep copy (too slow)
                # Instead, just use heuristic: prefer merging smaller systems
                u_states = graph_tracker.graph.nodes[u].get("num_states", 0)
                v_states = graph_tracker.graph.nodes[v].get("num_states", 0)
                resulting_states = u_states * v_states

                if resulting_states < min_resulting_states:
                    min_resulting_states = resulting_states
                    best_action_index = i
            except Exception as e:
                continue

        return np.array([best_action_index], dtype=np.int64), None

    def run_episode(self) -> tuple[float, int, int]:
        """
        Runs a full episode using this greedy policy.

        Returns:
            A tuple containing:
            - The total reward accumulated during the episode.
            - The final plan cost from the planner log.
            - The final number of expansions from the planner log.
        """
        obs, info = self.env.reset()
        done = False
        total_reward = 0.0
        final_plan_cost = 0
        final_expansions = 0
        steps = 0
        max_steps = 1000

        while not done and steps < max_steps:
            try:
                action, _ = self.predict(obs)
                scalar_action = int(action[0])

                obs, reward, terminated, truncated, info = self.env.step(scalar_action)
                done = terminated or truncated
                total_reward += reward
                steps += 1

                # ✅ FIX: Extract from info dict correctly
                if isinstance(info, dict):
                    if "plan_cost" in info:
                        final_plan_cost = int(info["plan_cost"])
                    if "num_expansions" in info:
                        final_expansions = int(info["num_expansions"])

            except Exception as e:
                import traceback
                traceback.print_exc()
                break

        return total_reward, final_plan_cost, final_expansions

--------------------------------------------------------------------------------

The file random_policy.py code is this:
import numpy as np
import time
from merge_env import MergeEnv


class RandomMergePolicy:
    """A simple baseline policy that always selects a valid merge action uniformly at random."""

    def __init__(self, env: MergeEnv):
        """Initializes the policy with a reference to the environment."""
        self.env = env

    def predict(self, observation: dict, deterministic: bool = True) -> tuple[np.ndarray, None]:
        """Predicts a random action based on the current observation."""
        num_edges_arr = np.asarray(observation.get("num_edges", 0)).reshape(-1)
        num_edges = int(num_edges_arr[0]) if num_edges_arr.size > 0 else 0

        action = 0 if num_edges <= 0 else np.random.randint(num_edges)
        return np.array([action], dtype=np.int64), None

    def run_episode(self) -> tuple[float, int, int]:
        """
        Runs a full episode in the environment using this random policy.

        Returns:
            A tuple containing:
            - The total reward accumulated during the episode.
            - The final plan cost from the planner log.
            - The final number of expansions from the planner log.
        """
        obs, info = self.env.reset()
        done = False
        total_reward = 0.0
        final_plan_cost = 0
        final_expansions = 0
        steps = 0
        max_steps = 1000

        while not done and steps < max_steps:
            try:
                action, _ = self.predict(obs)
                scalar_action = int(action[0])

                obs, reward, terminated, truncated, info = self.env.step(scalar_action)
                done = terminated or truncated
                total_reward += reward
                steps += 1

                # ✅ FIX: Extract from info dict correctly
                if isinstance(info, dict):
                    if "plan_cost" in info:
                        final_plan_cost = int(info["plan_cost"])
                    if "num_expansions" in info:
                        final_expansions = int(info["num_expansions"])

            except Exception as e:
                import traceback
                traceback.print_exc()
                break

        return total_reward, final_plan_cost, final_expansions

--------------------------------------------------------------------------------

The file gnn_model.py code is this:
# FILE: gnn_model.py (COMPLETE REWRITE WITH ATTENTION & EDGE FEATURES)
import torch
from torch import Tensor, nn
from torch_geometric.nn import GCNConv, GATConv
import torch.nn as nn
from typing import Tuple, Optional
import numpy as np

import logging
logger = logging.getLogger(__name__)


# ============================================================================
# ATTENTION BACKBONE: Improved GCN with Multi-Head Attention
# ============================================================================

class GCNWithAttention(nn.Module):
    """✅ NEW: GCN backbone with multi-head attention for focusing on key relationships."""

    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int = 3, n_heads: int = 4):
        super().__init__()

        # GCN layers
        layers = []
        dims = [input_dim] + [hidden_dim] * (n_layers - 1) + [hidden_dim]
        for i in range(n_layers):
            layers.append(GCNConv(dims[i], dims[i + 1]))
        self.convs = nn.ModuleList(layers)


        # ✅ NEW: Graph Attention Network layer for learning which relationships matter
        self.attention = GATConv(
            in_channels=hidden_dim,
            out_channels=hidden_dim,
            heads=n_heads,
            concat=True,
            dropout=0.1,
            add_self_loops=False
        )

        # Post-attention projection (if concat=True, attention outputs n_heads*out_channels)
        self.attention_proj = nn.Linear(hidden_dim * n_heads, hidden_dim)

        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:
        """Forward pass with attention mechanism."""
        device = x.device
        dtype = x.dtype
        edge_index = edge_index.to(device, dtype=torch.long)

        # Standard GCN pass
        for conv in self.convs:
            x = self.activation(conv(x, edge_index))
            x = self.dropout(x)

        # ✅ NEW: Apply attention on top of GCN embeddings
        # This learns which node pairs are important for merge decisions
        if edge_index.numel() > 0:
            try:
                attn_out = self.attention(x, edge_index)
                # Project back to hidden_dim
                attn_out = self.activation(self.attention_proj(attn_out))
                # Residual connection: blend GCN output with attention output
                x = x + attn_out * 0.3  # Small weight to preserve GCN learning
            except Exception as e:
                # Fallback if attention fails
                logger.warning(f"Attention layer failed: {e}, skipping")

        return x


# ============================================================================
# EDGE FEATURE ENCODER: Encode merge candidate properties
# ============================================================================

class EdgeFeatureEncoder(nn.Module):
    """✅ NEW: Encodes rich features about merge candidates."""

    def __init__(self, num_edge_features: int = 8, output_dim: int = 16):
        super().__init__()

        # Map edge features through neural net
        self.encoder = nn.Sequential(
            nn.Linear(num_edge_features, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, output_dim),
            nn.ReLU()
        )

        self.output_dim = output_dim

    def forward(self, edge_features: Tensor) -> Tensor:
        """
        Encode edge features into embeddings.

        Args:
            edge_features: [E, num_edge_features] raw edge features

        Returns:
            [E, output_dim] encoded edge features
        """
        if edge_features.numel() == 0:
            return torch.zeros(0, self.output_dim, device=edge_features.device)

        return self.encoder(edge_features)


# ============================================================================
# ATTENTION-WEIGHTED EDGE SCORER
# ============================================================================

class AttentionWeightedEdgeScorer(nn.Module):
    """✅ NEW: Score edges using attention + edge features + node embeddings."""

    def __init__(self, hidden_dim: int, edge_feature_dim: int = 16):
        super().__init__()

        # Combine node embeddings with edge features
        # Input: [src_embedding, tgt_embedding, edge_features]
        total_dim = 2 * hidden_dim + edge_feature_dim

        self.mlp = nn.Sequential(
            nn.Linear(total_dim, 2 * total_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2 * total_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 1),
        )

        # ✅ NEW: Attention weights for edge features
        # Learn which edge features are most important
        self.edge_attention = nn.Sequential(
            nn.Linear(edge_feature_dim, 16),
            nn.Tanh(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(
            self,
            node_embs: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tensor:
        """
        Score edges using attention-weighted combination of features.

        Args:
            node_embs: [N, H] node embeddings
            edge_index: [2, E] edge list in COO format
            edge_features: [E, D] edge features (optional)

        Returns:
            [E] edge scores
        """
        # ✅ SAFETY: Handle empty edge lists
        if edge_index.numel() == 0 or edge_index.shape[1] == 0:
            return torch.zeros(0, device=node_embs.device, dtype=torch.float32)

        src_idx, tgt_idx = edge_index
        num_nodes = node_embs.shape[0]

        # ✅ SAFETY: Validate indices
        max_idx = max(src_idx.max().item(), tgt_idx.max().item()) if len(src_idx) > 0 else -1
        if max_idx >= num_nodes:
            print(f"WARNING: Edge index contains invalid node ID {max_idx} >= {num_nodes}")
            return torch.zeros(len(src_idx), device=node_embs.device, dtype=torch.float32)

        src_emb = node_embs[src_idx]  # [E, H]
        tgt_emb = node_embs[tgt_idx]  # [E, H]

        # ✅ NEW: Handle edge features with attention
        if edge_features is not None and edge_features.numel() > 0:
            # Attention weights for edge features
            edge_attn_weights = self.edge_attention(edge_features)  # [E, 1]

            # Scale edge features by attention weights
            edge_feats_weighted = edge_features * edge_attn_weights  # [E, D]

            # Concatenate node embeddings with weighted edge features
            edge_feat = torch.cat([src_emb, tgt_emb, edge_feats_weighted], dim=1)  # [E, 2H+D]
        else:
            # Fallback: just use node embeddings
            edge_feat = torch.cat([src_emb, tgt_emb], dim=1)  # [E, 2H]

        score = self.mlp(edge_feat).squeeze(-1)  # [E]

        # ✅ SAFETY: Clamp to avoid explosion
        score = torch.clamp(score, min=-1e6, max=1e6)

        # ✅ SAFETY: Replace NaN/Inf with safe defaults
        score = torch.nan_to_num(score, nan=0.0, posinf=1e6, neginf=-1e6)

        return score


# ============================================================================
# UNIFIED GNN MODEL WITH ATTENTION & EDGE FEATURES
# ============================================================================

class GNNModel(nn.Module):
    """✅ COMPLETE: Full GNN with attention, edge features, and robust validation."""

    def __init__(
            self,
            input_dim: int,
            hidden_dim: int,
            n_layers: int = 3,
            n_heads: int = 4,
            edge_feature_dim: int = 8
    ):
        super().__init__()

        # ✅ NEW: GCN with attention backbone
        self.backbone = GCNWithAttention(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            n_layers=n_layers,
            n_heads=n_heads
        )

        # ✅ NEW: Edge feature encoder
        self.edge_encoder = EdgeFeatureEncoder(
            num_edge_features=edge_feature_dim,
            output_dim=16
        )

        # ✅ NEW: Attention-weighted edge scorer
        self.scorer = AttentionWeightedEdgeScorer(
            hidden_dim=hidden_dim,
            edge_feature_dim=16
        )

    def forward(
            self,
            x: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        """Forward pass with validation."""
        # ✅ INPUT VALIDATION
        if x.dim() != 2:
            raise ValueError(f"Node features must be 2D, got {x.dim()}D")
        if edge_index.dim() != 2 or edge_index.shape[0] != 2:
            raise ValueError(f"Edge index must be [2, E], got {edge_index.shape}")

        device = x.device
        edge_index = edge_index.to(device, dtype=torch.long)

        # ✅ EDGE INDEX VALIDATION - CRITICAL!
        num_nodes = x.shape[0]
        if edge_index.numel() > 0:
            max_idx = edge_index.max().item()
            if max_idx >= num_nodes:
                logger.warning(f"Edge index {max_idx} >= num_nodes {num_nodes}. Clamping...")
                edge_index = torch.clamp(edge_index, 0, num_nodes - 1)

            # ✅ NEW: Check for negative indices
            min_idx = edge_index.min().item()
            if min_idx < 0:
                logger.warning(f"Negative edge index {min_idx} found. Clamping...")
                edge_index = torch.clamp(edge_index, 0, num_nodes - 1)

        # Rest of forward pass...
        node_embs = self.backbone(x, edge_index)

        if torch.isnan(node_embs).any():
            logger.warning("NaN in node embeddings, replacing with 0")
            node_embs = torch.nan_to_num(node_embs, nan=0.0)

        encoded_edge_features = None
        if edge_features is not None and edge_features.numel() > 0:
            try:
                encoded_edge_features = self.edge_encoder(edge_features.float())
            except Exception as e:
                logger.warning(f"Could not encode edge features: {e}")

        edge_logits = self.scorer(node_embs, edge_index, encoded_edge_features)

        return edge_logits, node_embs

--------------------------------------------------------------------------------

The file gnn_policy.py code is this:
# FILE: gnn_policy.py (CRITICAL FIXES)
import traceback

import numpy as np
import torch
from torch import nn, Tensor
from torch.distributions import Categorical
from stable_baselines3.common.policies import ActorCriticPolicy
from typing import Tuple, Dict, Any, Optional
from gnn_model import GNNModel

import logging
logger = logging.getLogger(__name__)


class GNNExtractor(nn.Module):
    """✅ UPDATED: Wraps GNNModel with edge feature support."""

    def __init__(self, input_dim: int, hidden_dim: int, edge_feature_dim: int = 8):
        super().__init__()
        self.gnn = GNNModel(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            n_layers=3,
            n_heads=4,  # ✅ NEW: Attention heads
            edge_feature_dim=edge_feature_dim  # ✅ NEW: Edge feature support
        )
        self.edge_feature_dim = edge_feature_dim

    def forward(
        self,
        x: Tensor,
        edge_index: Tensor,
        edge_features: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        """✅ UPDATED: Pass edge features to GNN."""
        edge_logits, node_embs = self.gnn(x, edge_index, edge_features)
        return edge_logits, node_embs


class GNNPolicy(ActorCriticPolicy):
    """✅ FIXED: Robust policy with action validation."""

    def __init__(
            self,
            observation_space,
            action_space,
            lr_schedule,
            net_arch=None,
            activation_fn=nn.ReLU,
            hidden_dim: int = 128,
            **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            net_arch=[],
            activation_fn=activation_fn,
            **kwargs
        )

        self.node_feat_dim = observation_space["x"].shape[-1]
        self.hidden_dim = hidden_dim

        self.extractor = GNNExtractor(input_dim=self.node_feat_dim, hidden_dim=self.hidden_dim)
        self.value_net = nn.Linear(self.hidden_dim, 1)
        self.action_net = nn.Identity()

        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1))

    def _mask_invalid_edges(self, logits: Tensor, num_edges: int) -> Tensor:
        """
        ✅ FIXED: Mask invalid edges without breaking everything.

        GUARANTEE: At least one edge remains unmasked.
        """
        E = logits.size(0)

        # ✅ SAFETY: Clamp num_edges to valid range [1, E]
        num_edges_clamped = max(1, min(int(num_edges), E))

        mask = torch.arange(E, device=logits.device) < num_edges_clamped

        # ✅ GUARANTEE: At least one edge is available
        if not mask.any():
            mask[0] = True

        masked = logits.clone()
        masked[~mask] = -1e9

        return masked

    def _sample_or_argmax(self, logits: Tensor, deterministic: bool) -> Tuple[Tensor, Tensor]:
        """
        ✅ FIXED: Sample action safely with fallback.

        Returns (action, log_prob)
        """
        try:
            logits = torch.clamp(logits, min=-100, max=100)
            probs = torch.softmax(logits, dim=0)

            # ✅ SAFETY: Check if distribution is valid
            if torch.isnan(probs).any() or torch.isinf(probs).any():
                print("WARNING: Invalid probabilities, using uniform")
                probs = torch.ones_like(logits) / len(logits)

            dist = Categorical(probs=probs)

            if deterministic:
                action = probs.argmax(dim=0)
            else:
                action = dist.sample()

            logp = dist.log_prob(action)

            # ✅ SAFETY: Check log_prob
            if torch.isnan(logp) or torch.isinf(logp):
                logp = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)

            return action, logp

        except Exception as e:
            print(f"WARNING: Sampling failed: {e}, using argmax fallback")
            action = logits.argmax(dim=0)
            logp = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
            return action, logp

    @torch.no_grad()
    def predict(
            self,
            observation: Dict[str, Any],
            state=None,
            mask=None,
            deterministic=False
    ):
        """✅ FIXED: Properly handle observation batching from Stable-Baselines3."""
        self.eval()
        device = self.device

        # ================================================================
        # PHASE 1: EXTRACT OBSERVATIONS WITH PROPER BATCHING DETECTION
        # ================================================================

        x = torch.as_tensor(observation["x"], dtype=torch.float32, device=device)
        ei = torch.as_tensor(observation["edge_index"], dtype=torch.long, device=device)

        # ✅ FIX: Extract edge_features with proper batching handling
        edge_features = None
        if "edge_features" in observation:
            ef_raw = observation["edge_features"]
            edge_features = torch.as_tensor(ef_raw, dtype=torch.float32, device=device)

        # ================================================================
        # PHASE 2: DETECT BATCH DIMENSION
        # ================================================================

        # Determine if observations are already batched
        if x.ndim == 2:
            # Unbatched: [N, feat_dim]
            is_batched = False
            B = 1
            x = x.unsqueeze(0)  # [1, N, feat_dim]
            ei = ei.unsqueeze(0)  # [1, 2, E]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)  # [1, E, 8]

        elif x.ndim == 3:
            # Already batched: [batch, N, feat_dim]
            is_batched = True
            B = x.shape[0]

            # ✅ CRITICAL FIX: Ensure edge_features is also batched
            if edge_features is not None:
                if edge_features.ndim == 2:
                    # edge_features is [E, 8], need to broadcast to [batch, E, 8]
                    edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)
                # else: already [batch, E, 8], keep as is
        else:
            raise ValueError(f"Invalid observation dimension: {x.ndim}")

        # ================================================================
        # PHASE 3: EXTRACT DIMENSIONS
        # ================================================================

        # Handle num_nodes/num_edges which might be scalars or arrays
        ne = observation.get("num_edges", None)
        num_nodes_obs = observation.get("num_nodes", None)

        # Parse num_edges
        if ne is None:
            ne = [ei.shape[-1]] * B
        elif isinstance(ne, np.ndarray):
            if ne.ndim == 0:
                ne = [int(ne)] * B
            else:
                ne = ne.reshape(-1).tolist()
        elif isinstance(ne, (int, float)):
            ne = [int(ne)] * B
        elif isinstance(ne, torch.Tensor):
            ne = ne.cpu().numpy().reshape(-1).tolist()
        else:
            ne = [int(n) for n in ne] if hasattr(ne, '__iter__') else [int(ne)] * B

        # Ensure ne list has correct length
        if len(ne) == 1 and B > 1:
            ne = ne * B
        elif len(ne) != B:
            logger.warning(f"num_edges length {len(ne)} != batch size {B}, padding")
            ne = (ne + [ei.shape[-1]] * B)[:B]

        # ================================================================
        # PHASE 4: INPUT VALIDATION
        # ================================================================

        try:
            if x.dim() < 2:
                raise ValueError(f"x must be at least 2D, got {x.dim()}D: {x.shape}")
            if ei.dim() < 2:
                raise ValueError(f"edge_index must be at least 2D, got {ei.dim()}D: {ei.shape}")

            num_nodes = x.shape[-2]

            if ei.numel() > 0:
                max_idx = ei.max().item()
                if max_idx >= num_nodes:
                    logger.warning(f"Edge index {max_idx} >= num_nodes {num_nodes}. Clamping.")
                    ei = torch.clamp(ei, max=num_nodes - 1)

        except Exception as e:
            logger.error(f"Input validation failed: {e}")
            return np.zeros(B, dtype=int), None

        # ================================================================
        # PHASE 5: PROCESS EACH SAMPLE
        # ================================================================

        actions = []

        for i in range(B):
            try:
                # Extract sample from batch
                x_i = x[i]  # [N, feat_dim]
                ei_i = ei[i]  # [2, E]

                # ✅ FIX: Properly extract edge features for this sample
                if edge_features is not None:
                    edge_feats_i = edge_features[i]  # [E, 8]
                else:
                    edge_feats_i = None

                # ✅ VALIDATE SHAPES before passing to extractor
                if x_i.ndim != 2:
                    logger.error(f"Sample {i}: x_i has wrong shape {x_i.shape}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue
                if ei_i.numel() > 0 and ei_i.ndim != 2:
                    logger.error(f"Sample {i}: ei_i has wrong shape {ei_i.ndim}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue
                if edge_feats_i is not None and edge_feats_i.ndim != 2:
                    logger.error(f"Sample {i}: edge_feats_i has wrong shape {edge_feats_i.shape}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue

                # Forward through GNN
                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                # Validate logits
                if torch.isnan(logits_i).any() or torch.isinf(logits_i).any():
                    logger.warning(f"Sample {i}: Invalid logits, using uniform")
                    logits_i = torch.ones_like(logits_i)

                num_edges = int(ne[i]) if i < len(ne) else logits_i.shape[0]

                if num_edges <= 0 or logits_i.shape[0] == 0:
                    a_i = torch.tensor(0, dtype=torch.long, device=device)
                else:
                    masked = self._mask_invalid_edges(logits_i, num_edges)
                    a_i, _ = self._sample_or_argmax(masked, deterministic)

                # Validate action
                if a_i < 0 or (logits_i.shape[0] > 0 and a_i >= logits_i.shape[0]):
                    logger.warning(f"Sample {i}: Invalid action {a_i}, clamping to 0")
                    a_i = torch.tensor(0, dtype=torch.long, device=device)

                actions.append(a_i)

            except Exception as e:
                logger.error(f"Sample {i} processing failed: {e}")
                logger.error(traceback.format_exc())
                actions.append(torch.tensor(0, dtype=torch.long, device=device))

        # ================================================================
        # PHASE 6: RETURN RESULT
        # ================================================================

        actions_tensor = torch.stack(actions) if actions else torch.zeros(B, dtype=torch.long, device=device)
        return actions_tensor.cpu().numpy(), None

    def forward(self, obs: Dict[str, Any], deterministic: bool = False
                ) -> Tuple[Tensor, Tensor, Tensor]:
        """✅ FIXED: Properly handle batched observations."""
        device = self.device

        # ================================================================
        # EXTRACT AND VALIDATE OBSERVATIONS
        # ================================================================

        x = torch.as_tensor(obs["x"], dtype=torch.float32, device=device)
        ei = torch.as_tensor(obs["edge_index"], dtype=torch.long, device=device)

        # Extract edge_features with proper batching
        edge_features = None
        if "edge_features" in obs:
            edge_features = torch.as_tensor(obs["edge_features"], dtype=torch.float32, device=device)

        # ================================================================
        # DETECT AND HANDLE BATCHING
        # ================================================================

        if x.ndim == 2:
            # Unbatched input
            x = x.unsqueeze(0)  # [1, N, feat]
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)  # [1, E, feat]
        elif x.ndim == 3:
            # Already batched - ensure edge_features is also batched
            B = x.shape[0]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)

        # ================================================================
        # EXTRACT ACTUAL DIMENSIONS
        # ================================================================

        num_nodes_obs = obs.get("num_nodes", None)
        if num_nodes_obs is not None:
            if isinstance(num_nodes_obs, torch.Tensor):
                actual_num_nodes = int(num_nodes_obs.item()) if num_nodes_obs.dim() == 0 else int(
                    num_nodes_obs[0].item())
            else:
                actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0])
        else:
            actual_num_nodes = x.shape[-2]

        ne_obs = obs.get("num_edges", None)
        if ne_obs is not None:
            if isinstance(ne_obs, torch.Tensor):
                actual_num_edges = int(ne_obs.item()) if ne_obs.dim() == 0 else int(ne_obs[0].item())
            else:
                actual_num_edges = int(np.asarray(ne_obs).flat[0])
        else:
            actual_num_edges = ei.shape[-1]

        # ================================================================
        # TRIM TO ACTUAL SIZES
        # ================================================================

        x_trimmed = x[..., :actual_num_nodes, :]

        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        # Trim edge_features accordingly
        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # ================================================================
        # PROCESS BATCH
        # ================================================================

        B = x_trimmed.shape[0]
        actions, values, logps = [], [], []

        for i in range(B):
            try:
                # Extract sample
                x_i = x_trimmed[i]  # [N, feat]
                ei_i = ei_trimmed[i]  # [2, E]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None  # [E, 8]

                # Forward through GNN
                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                # Compute value
                if node_embs_i.dim() == 1:
                    node_embs_i = node_embs_i.unsqueeze(0)

                if node_embs_i.shape[0] > 0:
                    v_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True)).squeeze(-1)
                else:
                    v_i = torch.zeros((), dtype=torch.float32, device=device)

                # Sample action
                ne_i = actual_num_edges

                if ne_i <= 0 or logits_i.shape[0] == 0:
                    a_i = torch.zeros((), dtype=torch.long, device=device)
                    lp_i = torch.zeros((), dtype=torch.float32, device=device)
                else:
                    masked = self._mask_invalid_edges(logits_i, ne_i)
                    a_i, lp_i = self._sample_or_argmax(masked, deterministic)

                actions.append(a_i)
                values.append(v_i)
                logps.append(lp_i)

            except Exception as e:
                logger.error(f"Batch {i} forward failed: {e}")
                logger.error(traceback.format_exc())
                actions.append(torch.zeros((), dtype=torch.long, device=device))
                values.append(torch.zeros((), device=device))
                logps.append(torch.zeros((), device=device))

        actions = torch.stack(actions)
        values = torch.stack(values).unsqueeze(-1)
        logps = torch.stack(logps)

        return actions, values, logps

    def evaluate_actions(self, obs: Dict[str, Tensor], actions: Tensor
                         ) -> Tuple[Tensor, Tensor, Tensor]:
        """✅ FIXED: Evaluate actions with proper batching."""
        device = self.device

        x = obs["x"].to(device, dtype=torch.float32)
        ei = obs["edge_index"].to(device, dtype=torch.long)

        # Extract edge_features with batching awareness
        edge_features = None
        if "edge_features" in obs:
            edge_features = obs["edge_features"].to(device, dtype=torch.float32)

        # ✅ Handle batching
        if x.ndim == 2:
            x = x.unsqueeze(0)
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)
        elif x.ndim == 3:
            B = x.shape[0]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)

        # Extract dimensions
        num_nodes_obs = obs.get("num_nodes", None)
        actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0]) if num_nodes_obs is not None else x.shape[-2]

        ne = obs.get("num_edges", None)
        actual_num_edges = int(np.asarray(ne).flat[0]) if ne is not None else ei.shape[-1]

        # Trim
        x_trimmed = x[..., :actual_num_nodes, :]
        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # Process batch
        B = x_trimmed.shape[0]
        values, logps, ents = [], [], []

        for i in range(B):
            try:
                x_i = x_trimmed[i]
                ei_i = ei_trimmed[i]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None

                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                if node_embs_i.shape[0] > 0:
                    v_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True)).squeeze(-1)
                else:
                    v_i = torch.zeros((), device=device)

                ne_i = actual_num_edges

                if ne_i <= 0 or logits_i.shape[0] == 0:
                    logp_i = torch.zeros((), device=device, requires_grad=True)
                    ent_i = torch.zeros((), device=device, requires_grad=True)
                else:
                    masked = self._mask_invalid_edges(logits_i, ne_i)
                    dist = Categorical(logits=masked)
                    action_clamped = torch.clamp(actions[i], 0, logits_i.shape[0] - 1)
                    logp_i = dist.log_prob(action_clamped)
                    ent_i = dist.entropy()

                values.append(v_i)
                logps.append(logp_i)
                ents.append(ent_i)

            except Exception as e:
                logger.error(f"Evaluate batch {i} failed: {e}")
                values.append(torch.zeros((), device=device, requires_grad=True))
                logps.append(torch.zeros((), device=device, requires_grad=True))
                ents.append(torch.zeros((), device=device, requires_grad=True))

        return torch.stack(values), torch.stack(logps), torch.stack(ents)

    def predict_values(self, obs: Dict[str, Tensor]) -> Tensor:
        """✅ FIXED: Predict values with proper batching."""
        device = self.device

        x = obs["x"].to(device, dtype=torch.float32)
        ei = obs["edge_index"].to(device, dtype=torch.long)

        # Extract edge_features with batching awareness
        edge_features = None
        if "edge_features" in obs:
            edge_features = obs["edge_features"].to(device, dtype=torch.float32)

        # Handle batching
        if x.ndim == 2:
            x = x.unsqueeze(0)
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)
        elif x.ndim == 3:
            B = x.shape[0]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)

        # Extract dimensions
        num_nodes_obs = obs.get("num_nodes", None)
        actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0]) if num_nodes_obs is not None else x.shape[-2]

        ne = obs.get("num_edges", None)
        actual_num_edges = int(np.asarray(ne).flat[0]) if ne is not None else ei.shape[-1]

        # Trim
        x_trimmed = x[..., :actual_num_nodes, :]
        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # Process batch
        B = x_trimmed.shape[0]
        vals = []

        for i in range(B):
            try:
                x_i = x_trimmed[i]
                ei_i = ei_trimmed[i]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None

                _, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                if node_embs_i.shape[0] > 0:
                    val_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True))
                else:
                    val_i = torch.zeros((1, 1), device=device)

                vals.append(val_i)

            except Exception as e:
                logger.error(f"Predict values batch {i} failed: {e}")
                vals.append(torch.zeros((1, 1), device=device))

        return torch.cat(vals, dim=0)

--------------------------------------------------------------------------------

The file graph_tracker.py code is this:
# -*- coding: utf-8 -*-
"""
This module provides the GraphTracker class, a data structure for managing the
state of the merge-and-shrink heuristic construction process.

It represents the set of transition systems (TS) as nodes in a directed graph,
where edges represent causal dependencies from the Fast Downward planner. The class
is responsible for loading the initial state from planner output, performing
merge operations on nodes, and updating the graph based on new information from
the planner. It serves as the core state management component for the MergeEnv.
"""

# ------------------------------------------------------------------------------
#  Imports
# ------------------------------------------------------------------------------
import json
import logging
import time
from json import JSONDecoder
from typing import List, Union, Dict, Tuple, Any, FrozenSet, Optional

import networkx as nx
import numpy as np

# matplotlib is an optional dependency for visualization
try:
    import matplotlib.pyplot as plt
except ImportError:
    plt = None

# ------------------------------------------------------------------------------
#  Configuration and Constants
# ------------------------------------------------------------------------------
# --- Setup basic logging ---
# Consistent logging configuration with merge_env.py
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)

# --- Constants for file I/O ---
FILE_RETRY_COUNT = 60
FILE_RETRY_DELAY_S = 0.2


# ------------------------------------------------------------------------------
#  Helper Functions
# ------------------------------------------------------------------------------

def _load_json_robustly(path: str, retries: int = FILE_RETRY_COUNT, delay: float = FILE_RETRY_DELAY_S) -> Any:
    """
    Parses the first complete JSON object from a file path with retries.

    This function is designed to handle cases where a file might be read while
    another process is writing to it. It ensures the file is not empty and that
    the content appears to be a complete JSON object (ends with '}' or ']')
    before attempting to parse it.

    Method of Action:
    1. Loop for a specified number of `retries`.
    2. Read the file content, ignoring UTF-8 errors.
    3. If the file is empty or doesn't end with a closing brace/bracket,
       it's considered incomplete. Wait and retry.
    4. Use `JSONDecoder.raw_decode` to parse only the *first* valid JSON
       object, which avoids errors from trailing, partially-written data.
    5. If any error occurs (`OSError`, `JSONDecodeError`), wait and retry.
    6. If all retries fail, raise a `RuntimeError` with the last known error.
    """
    decoder = JSONDecoder()
    last_error = None
    for _ in range(retries):
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read().lstrip()

            if not content:
                # File is empty, wait for content to be written.
                raise json.JSONDecodeError("File is empty", content, 0)

            # Heuristic check for completeness to avoid parsing mid-write.
            tail = content.rstrip()
            if not tail or tail[-1] not in ("]", "}"):
                raise json.JSONDecodeError("JSON appears incomplete (no closing bracket/brace)", content, len(content))

            # Decode the first object, ignoring any trailing garbage.
            obj, _ = decoder.raw_decode(content)
            return obj

        except (OSError, json.JSONDecodeError) as e:
            last_error = e
            time.sleep(delay)

    raise RuntimeError(f"Failed to load valid JSON from '{path}' after {retries} retries. Last error: {last_error}")


def product_state_index(s1: int, s2: int, n2: int) -> int:
    """
    Maps a pair of local states to a single index in their Cartesian product.

    This is a standard row-major order mapping. Given two state spaces of sizes
    `n1` and `n2`, a state `s1` from the first space and `s2` from the second
    are mapped to a unique index in the combined space of size `n1 * n2`.

    Args:
        s1 (int): The index of the state in the first transition system.
        s2 (int): The index of the state in the second transition system.
        n2 (int): The total number of states in the second transition system.

    Returns:
        int: The unique index in the product state space.
    """
    return s1 * n2 + s2


def merge_transition_systems(ts1: Dict[str, Any], ts2: Dict[str, Any]) -> Dict[str, Any]:
    """
    Creates a new transition system (TS) via Cartesian product of two TSs.

    This is the core "merge" operation in the merge-and-shrink algorithm. The
    resulting TS has a state space that is the product of the two input state
    spaces.

    Method of Action:
    1.  The number of states in the new TS is `n1 * n2`.
    2.  The new initial state is the product of the individual initial states.
    3.  The new goal states are the product of the individual goal state sets.
    4.  The list of "incorporated variables" is the concatenation of the inputs.
    5.  The "iteration" number is incremented to mark this as a composite,
        non-atomic system.

    Args:
        ts1 (Dict): The first transition system dictionary.
        ts2 (Dict): The second transition system dictionary.

    Returns:
        Dict: The new, merged transition system dictionary.
    """
    n1, n2 = ts1["num_states"], ts2["num_states"]

    merged_ts = {
        "num_states": n1 * n2,
        "init_state": product_state_index(ts1["init_state"], ts2["init_state"], n2),
        "goal_states": [
            product_state_index(g1, g2, n2)
            for g1 in ts1["goal_states"]
            for g2 in ts2["goal_states"]
        ],
        "incorporated_variables": ts1["incorporated_variables"] + ts2["incorporated_variables"],
        # Incrementing the iteration level signifies a merge operation.
        # Atomic systems have iteration = -1.
        "iteration": max(ts1.get("iteration", -1), ts2.get("iteration", -1)) + 1,
    }
    return merged_ts


# ------------------------------------------------------------------------------
#  GraphTracker Class
# ------------------------------------------------------------------------------

class GraphTracker:
    """
    Manages the graph of transition systems for the merge-and-shrink process.

    This class holds a `networkx.DiGraph` where each node represents a transition
    system (TS). Initially, nodes correspond to atomic TSs for individual problem
    variables. The class provides methods to merge nodes (creating a new composite
    TS node) and update node properties based on new data from the planner.

    Attributes:
        graph (nx.DiGraph): The graph of transition systems.
        varset_to_node (Dict): A mapping from a frozenset of variable IDs to the
                                corresponding node ID in the graph. This allows for
                                efficient lookups.
        next_node_id (int): A counter for allocating unique IDs to new merged nodes.
    """

    def __init__(self, ts_json_path: str, cg_json_path: str, is_debug: bool = False):
        """
        Initializes the GraphTracker by loading the initial graph structure.

        Args:
            ts_json_path (str): Path to the JSON file containing the list of
                                initial transition systems.
            cg_json_path (str): Path to the JSON file defining the causal graph
                                edges between variables.
            is_debug (bool): If True, runs in debug mode which may alter behavior,
                             e.g., by creating a dummy graph if files are missing.
        """
        self.graph = nx.DiGraph()
        self.varset_to_node: Dict[FrozenSet, Union[int, str]] = {}
        self.next_node_id: int = 0
        self.is_debug = is_debug

        logging.info("Initializing GraphTracker...")
        try:
            self._load_atomic_systems(ts_json_path)
            self._load_causal_edges(cg_json_path)
        except Exception as e:
            logging.error(f"Failed during initial graph loading: {e}")
            if not self.is_debug:
                # In non-debug mode, this is a fatal error.
                raise
            else:
                # In debug mode, we can proceed with an empty graph.
                logging.warning("Proceeding with an empty graph in debug mode.")

    def update_graph(self, ts_json_path: str) -> None:
        """
        Updates the graph with new transition system data from a JSON file.

        This method reads a TS list from the given path and updates the properties
        of existing nodes. This is typically used after a merge operation, where
        the planner provides an updated TS file for the newly created node.

        Args:
            ts_json_path (str): The path to the JSON file with TS data.
        """
        logging.info(f"Updating graph from '{ts_json_path}'...")
        try:
            data = _load_json_robustly(ts_json_path)
            ts_list = data if isinstance(data, list) else [data]

            for ts in ts_list:
                if not isinstance(ts, dict):
                    continue
                self._add_or_update_node(ts)

        except Exception as e:
            logging.warning(f"Could not parse or process TS JSON from '{ts_json_path}': {e}")

    def merge_nodes(self, node_ids: List[Union[int, str]]) -> None:
        """
        Merges two nodes in the graph to create a new, composite node.

        This is the primary state-changing operation driven by the RL agent.

        Method of Action:
        1.  Retrieves the TS data for the two nodes to be merged (`A` and `B`).
        2.  Computes the new merged TS using `merge_transition_systems`.
        3.  Assigns a new, unique ID (`C`) to the merged TS.
        4.  Adds the new node `C` to the graph.
        5.  Rewires all incoming/outgoing edges from `A` and `B` to point to `C`.
        6.  Removes the original nodes `A` and `B` from the graph.

        Text Diagram of Edge Rewiring:
        BEFORE MERGE:
        [P1] -> [A] -> [S1]
        [P2] -> [B] -> [S2]

        AFTER MERGING A and B into C:
        [P1] -> [C] -> [S1]
        [P2] -> [C] -> [S2]
        """
        """
        ✅ FIXED: Merges two nodes with validation.
        """
        if len(node_ids) != 2:
            raise ValueError(f"merge_nodes requires exactly two node IDs, got {len(node_ids)}")

        a, b = node_ids

        # ✅ NEW: Comprehensive validation
        if a not in self.graph:
            raise KeyError(f"Node {a} not in graph. Available: {list(self.graph.nodes())}")
        if b not in self.graph:
            raise KeyError(f"Node {b} not in graph. Available: {list(self.graph.nodes())}")

        # ✅ NEW: Prevent self-merge
        if a == b:
            raise ValueError(f"Cannot merge node with itself: {a}")

        # ✅ NEW: Verify nodes are connected (optional, but good validation)
        if not (self.graph.has_edge(a, b) or self.graph.has_edge(b, a)):
            print(f"[WARNING] Merging disconnected nodes {a}, {b}")

        logging.info(f"Merging nodes {a} and {b}...")
        ts1 = self.graph.nodes[a]
        ts2 = self.graph.nodes[b]

        # 1. Compute the merged transition system.
        merged_ts = merge_transition_systems(ts1, ts2)
        new_id = self.next_node_id
        self.next_node_id += 1

        # 2. Add the new merged node to the graph.
        self.graph.add_node(new_id, **merged_ts)
        var_key = frozenset(merged_ts["incorporated_variables"])
        self.varset_to_node[var_key] = new_id

        # 3. Rewire edges from the original nodes to the new node.
        self._rewire_edges(a, new_id)
        self._rewire_edges(b, new_id)

        # 4. Remove the original nodes.
        self.graph.remove_nodes_from([a, b])
        logging.info(f"Successfully merged nodes into new node {new_id} with {merged_ts['num_states']} states.")

    def f_stats(self, node_id: Union[int, str]) -> Tuple[float, float, float, float]:
        """
        Calculates statistics for the 'f_before' values of a given node.

        The 'f_before' list contains heuristic values for each abstract state
        within the node's transition system. These stats are used as features
        for the GNN policy.

        Args:
            node_id: The ID of the node to analyze.

        Returns:
            A tuple of (min, mean, max, std_dev) of the f-values. Returns
            (0.0, 0.0, 0.0, 0.0) if no f-values are present.
        """
        if node_id not in self.graph.nodes:
            return 0.0, 0.0, 0.0, 0.0

        f_values = self.graph.nodes[node_id].get("f_before", [])

        if not f_values:
            return 0.0, 0.0, 0.0, 0.0

        arr = np.array(f_values, dtype=np.float32)
        return float(arr.min()), float(arr.mean()), float(arr.max()), float(arr.std())

    def _load_atomic_systems(self, ts_json_path: str) -> None:
        """
        Loads the initial set of atomic transition systems from a JSON file.

        These form the initial nodes of the graph. Atomic systems are identified
        by having `iteration == -1`.
        """
        logging.info(f"Loading atomic systems from '{ts_json_path}'...")
        data = _load_json_robustly(ts_json_path)
        ts_list = data if isinstance(data, list) else [data]

        num_loaded = 0
        for ts in ts_list:
            if isinstance(ts, dict) and ts.get("iteration", -1) == -1:
                self._add_or_update_node(ts)
                num_loaded += 1

        # Set the next node ID to be higher than any existing integer ID to avoid collisions.
        int_ids = [n for n in self.graph.nodes if isinstance(n, int)]
        self.next_node_id = max(int_ids, default=-1) + 1
        logging.info(f"Loaded {num_loaded} atomic systems. Next node ID set to {self.next_node_id}.")

    # REPLACE THE OLD METHOD WITH THIS NEW ONE
    def _load_causal_edges(self, cg_json_path: str) -> None:
        """Loads the causal graph edges from a JSON file into the graph."""
        logging.info(f"Loading causal edges from '{cg_json_path}'...")
        try:
            with open(cg_json_path, "r") as f:
                data = json.load(f)

            edges = data.get("edges", [])
            if not edges:
                logging.warning("Causal graph file contains no 'edges' key or the list is empty.")
                return

            # Debug: log what nodes exist before trying to add edges
            logging.info(f"Current graph nodes before adding edges: {list(self.graph.nodes())}")

            num_added = 0
            for edge in edges:
                src = edge.get("from")
                tgt = edge.get("to")

                # This check is crucial and now more explicit
                if src is not None and tgt is not None:
                    if self.graph.has_node(src) and self.graph.has_node(tgt):
                        self.graph.add_edge(src, tgt)
                        num_added += 1
                        logging.info(f"Added edge ({src}, {tgt})")
                    else:
                        logging.warning(
                            f"Skipping edge ({src}, {tgt}) because one or both nodes do not exist in the graph. "
                            f"Current nodes: {list(self.graph.nodes())}"
                        )
                else:
                    logging.warning(f"Edge has None values: from={src}, to={tgt}")

            logging.info(f"Loaded {num_added} causal edges. Final edge count: {len(list(self.graph.edges()))}")

        except FileNotFoundError:
            logging.warning(f"Causal graph file '{cg_json_path}' not found. No edges loaded.")
            if not self.is_debug:
                raise
        except Exception as e:
            logging.error(f"An unexpected error occurred while loading causal edges: {e}", exc_info=True)
            if not self.is_debug:
                raise

    def _add_or_update_node(self, ts: Dict[str, Any]) -> None:
        """
        Adds a new node or updates an existing node's data based on a TS dict.

        The identity of a node is determined by its set of "incorporated_variables".
        If a node representing a given set of variables already exists, its
        attributes are updated. Otherwise, a new node is created.

        This method also removes the large 'transitions' list from the node data
        to save memory.

        ✅ NEW: Validates that TS has meaningful data before adding.
        """
        # Validate input
        if not ts or not isinstance(ts, dict):
            logging.warning("Skipping invalid TS: not a dict or empty")
            return

        ts_data = ts.copy()
        ts_data.pop("transitions", None)

        # ✅ NEW: Comprehensive validation
        inc_vars = ts_data.get("incorporated_variables", [])

        if not inc_vars:
            logging.warning("❌ Skipping TS with NO INCORPORATED VARIABLES")
            logging.warning(f"   TS keys: {list(ts.keys())}")
            logging.warning(f"   TS content: {ts}")
            return

        # ✅ NEW: Validate other critical fields
        num_states = ts_data.get("num_states", 0)
        if num_states <= 0:
            logging.warning(f"⚠️  Skipping TS with invalid num_states: {num_states}")
            return

        # ✅ NEW: Check if TS looks complete
        required_fields = ["num_states", "init_state", "goal_states"]
        missing = [k for k in required_fields if k not in ts_data]
        if missing:
            logging.warning(f"⚠️  TS missing fields: {missing}")
            logging.warning(f"   Available fields: {list(ts_data.keys())}")
            # Don't skip - continue with defaults
            for field in missing:
                if field == "init_state":
                    ts_data[field] = 0
                elif field == "goal_states":
                    ts_data[field] = [0]

        # Now proceed with normal logic
        var_key = frozenset(inc_vars)
        existing_node_id = self.varset_to_node.get(var_key)

        if existing_node_id is not None and existing_node_id in self.graph:
            logging.info(f"✓ Updating existing node {existing_node_id} with {num_states} states")
            self.graph.nodes[existing_node_id].update(ts_data)
        else:
            # New node
            is_atomic = ts_data.get("iteration", -1) == -1
            if is_atomic:
                node_id = inc_vars[0]
                logging.info(f"✓ Adding atomic node {node_id} for variable {inc_vars[0]}")
            else:
                node_id = self.next_node_id
                self.next_node_id += 1
                logging.info(f"✓ Adding merged node {node_id} for variables {inc_vars}")

            self.graph.add_node(node_id, **ts_data)
            self.varset_to_node[var_key] = node_id

            # Update counter
            if isinstance(node_id, int):
                self.next_node_id = max(self.next_node_id, node_id + 1)

            logging.info(f"   → Node {node_id} has {num_states} states")


    def _rewire_edges(self, old_id: Union[int, str], new_id: Union[int, str]) -> None:
        """
        Moves all incoming and outgoing edges from an old node to a new node.
        """
        # Rewire incoming edges: for every predecessor `p` of `old_id`, add edge `(p, new_id)`.
        if old_id in self.graph:
            for predecessor in list(self.graph.predecessors(old_id)):
                if predecessor != new_id:  # Avoid self-loops with the other merged node
                    self.graph.add_edge(predecessor, new_id)
            # Rewire outgoing edges: for every successor `s` of `old_id`, add edge `(new_id, s)`.
            for successor in list(self.graph.successors(old_id)):
                if successor != new_id:
                    self.graph.add_edge(new_id, successor)

    def display(self) -> None:
        """
        Renders and displays the current state of the graph using matplotlib.

        Note: This is intended for interactive debugging and requires the
        `matplotlib` library to be installed.
        """
        if plt is None:
            logging.warning("matplotlib is not installed. Cannot display graph.")
            return

        plt.figure(figsize=(12, 9))
        pos = nx.spring_layout(self.graph, seed=42)

        labels = {
            n: f"ID: {n}\n|S|={d.get('num_states', '?')}\nIter: {d.get('iteration', '?')}"
            for n, d in self.graph.nodes(data=True)
        }

        nx.draw_networkx(
            self.graph,
            pos,
            labels=labels,
            node_size=1500,
            node_color="lightblue",
            font_size=8,
            arrows=True,
            arrowstyle="-|>",
            arrowsize=15,
        )
        plt.title("Transition System Causal Graph", fontsize=16)
        plt.axis("off")
        plt.tight_layout()
        plt.show()


if __name__ == "__main__":
    # This block serves as a "smoke test" to verify basic functionality.
    # It requires dummy JSON files to be present in the same directory.
    logging.info("--- Running GraphTracker Smoke Test ---")

    # Create dummy files for testing purposes
    DUMMY_CG_FILE = "causal_graph_test.json"
    DUMMY_TS_FILE = "ts_test.json"

    cg_data = {"edges": [{"from": 0, "to": 1}, {"from": 1, "to": 2}]}
    ts_data = [
        {"num_states": 2, "init_state": 0, "goal_states": [1], "incorporated_variables": [0], "iteration": -1},
        {"num_states": 3, "init_state": 1, "goal_states": [2], "incorporated_variables": [1], "iteration": -1},
        {"num_states": 4, "init_state": 2, "goal_states": [0], "incorporated_variables": [2], "iteration": -1},
    ]

    with open(DUMMY_CG_FILE, "w") as f:
        json.dump(cg_data, f)
    with open(DUMMY_TS_FILE, "w") as f:
        json.dump(ts_data, f)

    try:
        # 1. Test initialization
        tracker = GraphTracker(ts_json_path=DUMMY_TS_FILE, cg_json_path=DUMMY_CG_FILE, is_debug=True)
        print("\nInitial Graph Nodes:", list(tracker.graph.nodes()))
        print("Initial Graph Edges:", list(tracker.graph.edges()))
        # tracker.display() # Uncomment for visual inspection

        # 2. Test merging
        if len(tracker.graph.nodes) >= 2:
            nodes_to_merge = [0, 1]
            tracker.merge_nodes(nodes_to_merge)
            print(f"\nGraph Nodes after merging {nodes_to_merge}:", list(tracker.graph.nodes()))
            print("Graph Edges after merging:", list(tracker.graph.edges()))
            # tracker.display()

            # 3. Test f_stats on the new node
            new_node_id = list(tracker.graph.nodes)[-1]
            # Add some dummy f-values to test f_stats
            tracker.graph.nodes[new_node_id]['f_before'] = [10, 20, 30, 40]
            stats = tracker.f_stats(new_node_id)
            print(
                f"\nF-stats for new node {new_node_id}: min={stats[0]}, mean={stats[1]}, max={stats[2]}, std={stats[3]}")

        else:
            print("\nNot enough nodes to test merge.")

        logging.info("--- Smoke Test Completed Successfully ---")

    except Exception as e:
        logging.error(f"--- Smoke Test FAILED: {e} ---")

    finally:
        # Clean up dummy files
        import os

        if os.path.exists(DUMMY_CG_FILE): os.remove(DUMMY_CG_FILE)
        if os.path.exists(DUMMY_TS_FILE): os.remove(DUMMY_TS_FILE)

--------------------------------------------------------------------------------

The file merge_env.py code is this:
# FILE: merge_env.py

import re
import tempfile
import glob
import os
import json
import time
import subprocess
import traceback
from json import JSONDecoder
from typing import Tuple, Dict, Optional
import datetime
import gymnasium as gym
import shutil
import numpy as np
from gymnasium import spaces
from torch.utils.tensorboard import SummaryWriter
import networkx as nx

from graph_tracker import GraphTracker
from reward_info_extractor import RewardInfoExtractor, MergeInfo, validate_extracted_info
from reward_function_variants import create_reward_function

import logging

logger = logging.getLogger(__name__)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

TB_LOGDIR = "tb_debug"
writer = SummaryWriter(log_dir=TB_LOGDIR)


def _safe_load_list(path: str, retries: int = 60, delay: float = 0.25) -> list:
    """Robustly load the *first* JSON value from `path` with retries."""
    dec = JSONDecoder()
    last_err = None
    for _ in range(retries):
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                s = f.read().lstrip()
            if not s:
                time.sleep(delay)
                continue
            obj, _ = dec.raw_decode(s)
            return obj if isinstance(obj, list) else []
        except (OSError, json.JSONDecodeError) as e:
            last_err = e
            time.sleep(delay)
    return []


def write_json_atomic(obj, final_path: str):
    """Write `obj` to JSON at `final_path` atomically."""
    dir_ = os.path.dirname(final_path) or "."
    fd, tmp_path = tempfile.mkstemp(dir=dir_, suffix=".tmp")
    with os.fdopen(fd, "w") as f:
        json.dump(obj, f)
        f.flush()
        os.fsync(f.fileno())
    os.replace(tmp_path, final_path)


def _wait_json_complete(path: str, timeout: float = 180.0, poll: float = 0.25) -> bool:
    """Wait until `path` exists and looks complete."""
    deadline = time.time() + timeout
    last_size = -1
    while time.time() < deadline:
        try:
            if not os.path.exists(path):
                time.sleep(poll)
                continue
            size = os.path.getsize(path)
            if size <= 0:
                time.sleep(poll)
                continue

            with open(path, "rb") as f:
                if size > 8192:
                    f.seek(-8192, os.SEEK_END)
                tail = f.read()

            tail = tail.rstrip(b"\r\n\t ")
            if not tail:
                time.sleep(poll)
                continue

            last_byte = tail[-1]
            looks_closed = last_byte in (ord("]"), ord("}"))

            if looks_closed and size == last_size:
                return True

            last_size = size
            time.sleep(poll)
        except OSError:
            time.sleep(poll)
    return False


class MergeEnv(gym.Env):
    """
    Gym environment wrapping Fast Downward's merge-and-shrink heuristic.
    ✅ FIXED: Now properly stores domain and problem file paths.
    """

    metadata = {"render.modes": []}

    def __init__(
            self,
            domain_file: str,
            problem_file: str,
            max_merges: int = 20,
            debug: bool = False,
            reward_variant: str = 'rich',
            max_states: int = 4000,
            threshold_before_merge: int = 1,
            **reward_kwargs
    ) -> None:
        """
        Initialize the merge-and-shrink learning environment.

        Args:
            domain_file: Absolute or relative path to domain.pddl
            problem_file: Absolute or relative path to problem.pddl
            max_merges: Maximum number of merges allowed per episode
            debug: If True, use in-memory debug mode (no real FD)
            reward_variant: Which reward function to use ('rich', 'astar_search', etc.)
            max_states: Max abstract states for M&S algorithm
            threshold_before_merge: Threshold for triggering shrinking
            **reward_kwargs: Additional kwargs for reward function (w_f_stability, etc.)
        """
        super().__init__()

        # ✅ CRITICAL: Store file paths as ABSOLUTE paths
        self.domain_file = os.path.abspath(domain_file)
        self.problem_file = os.path.abspath(problem_file)

        # ✅ CRITICAL FIX #1: Initialize fd_base_dir (was missing!)
        self.fd_base_dir = os.path.abspath("downward")

        # Verify files exist (fail early)
        if not os.path.exists(self.domain_file):
            raise FileNotFoundError(f"Domain file not found: {self.domain_file}")
        if not os.path.exists(self.problem_file):
            raise FileNotFoundError(f"Problem file not found: {self.problem_file}")

        logger.info(f"Environment initialized with:")
        logger.info(f"  Domain:  {self.domain_file}")
        logger.info(f"  Problem: {self.problem_file}")
        logger.info(f"  FD base: {self.fd_base_dir}")

        # ✅ Store M&S hyperparameters
        self.max_states = max_states
        self.threshold_before_merge = threshold_before_merge
        logger.info(f"  max_states: {self.max_states}")
        logger.info(f"  threshold_before_merge: {self.threshold_before_merge}")

        # ✅ Setup reward function with ALL parameters
        self.max_merges = max(1, max_merges)
        try:
            self.reward_function = create_reward_function(reward_variant, **reward_kwargs)
            logger.info(f"✓ Initialized reward function: {reward_variant}")
        except Exception as e:
            logger.error(f"Failed to initialize reward function '{reward_variant}': {e}")
            raise

        # ✅ Setup signal extraction
        self.reward_info_extractor = RewardInfoExtractor(fd_output_dir=os.path.join(self.fd_base_dir, "fd_output"))
        logger.info("✓ Initialized reward info extractor")

        # ✅ Initialize state variables
        self.current_merge_step = 0
        self.process = None  # FD subprocess
        self.fd_log_file = None
        self.graph_tracker: GraphTracker = None

        # ✅ Setup logging directory
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("logs", exist_ok=True)
        self.log_path = f"logs/run_{timestamp}.jsonl"

        # ✅ Initialize observation tracking
        self.prev_total_states = 0
        self.max_vars = 1
        self.max_iter = 1
        self.centrality = {}

        # ✅ Define observation and action spaces (for gym.Env)
        self.feat_dim = 19  # Number of node features per node ✅ NEW: Expanded feature set
        # self.observation_space = spaces.Dict({
        #     "x": spaces.Box(0.0, 1.0, shape=(100, self.feat_dim), dtype=np.float32),
        #     "edge_index": spaces.Box(0, 100, shape=(2, 1000), dtype=np.int64),
        #     "num_nodes": spaces.Box(0, 100, shape=(), dtype=np.int32),
        #     "num_edges": spaces.Box(0, 1000, shape=(), dtype=np.int32),
        # })
        # ✅ UPDATED: Add edge features to observation space
        self.observation_space = spaces.Dict({
            "x": spaces.Box(0.0, 1.0, shape=(100, self.feat_dim), dtype=np.float32),
            "edge_index": spaces.Box(0, 100, shape=(2, 1000), dtype=np.int64),
            "edge_features": spaces.Box(  # ✅ NEW
                -1.0, 1.0,
                shape=(1000, 8),
                dtype=np.float32
            ),
            "num_nodes": spaces.Box(0, 100, shape=(), dtype=np.int32),
            "num_edges": spaces.Box(0, 1000, shape=(), dtype=np.int32),
        })
        self.action_space = spaces.Discrete(1000)  # Max 1000 possible merges

        # ✅ Store debug mode
        self.debug = debug

    def reset(self, *, seed=None, options=None) -> Tuple[Dict, Dict]:
        """Reset the env."""
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                self.process.wait(timeout=5.0)
        except Exception:
            pass
        if self.fd_log_file:
            try:
                self.fd_log_file.close()
            except Exception:
                pass
            self.fd_log_file = None

        super().reset(seed=seed)

        # for folder in ("gnn_output", "fd_output"):
        #     d = os.path.join("downward", folder)
        #     if not os.path.isdir(d):
        #         continue
        #     for fname in os.listdir(d):
        #         if folder == "fd_output" and fname == "log.txt":
        #             continue
        #         try:
        #             os.remove(os.path.join(d, fname))
        #         except Exception:
        #             pass

        # ✅ CRITICAL: Ensure output directories exist BEFORE cleaning
        os.makedirs("downward/gnn_output", exist_ok=True)
        os.makedirs("downward/fd_output", exist_ok=True)

        # ✅ Clean old files
        for folder in ("gnn_output", "fd_output"):
            d = os.path.join("downward", folder)
            if not os.path.isdir(d):
                continue
            for fname in os.listdir(d):
                if folder == "fd_output" and fname == "log.txt":
                    continue
                try:
                    fpath = os.path.join(d, fname)
                    if os.path.isfile(fpath):
                        os.remove(fpath)
                        logger.debug(f"Cleaned: {fpath}")
                except Exception as e:
                    logger.warning(f"Could not clean {fpath}: {e}")

        toy_dir = os.environ.get("TOY_TS", None)
        ts_file, cg_file = "merged_transition_systems.json", "causal_graph.json"
        if toy_dir and self.debug:
            ts_path = os.path.join(toy_dir, ts_file)
            cg_path = os.path.join(toy_dir, cg_file)
        else:
            ts_path = cg_path = None
            for attempt in range(1, 4):
                try:
                    ts_path, cg_path = self._handshake_with_fd()
                    break
                except Exception as e:
                    logger.warning(f"⚠️ Handshake attempt {attempt} failed: {e}")
                    time.sleep(1.0)
            if ts_path is None or cg_path is None:
                if self.debug:
                    logger.warning("⚠️ Handshake failed; falling back to toy data")
                    toy_dir = os.environ.get("TOY_TS", "toy")
                    ts_path = os.path.join(toy_dir, ts_file)
                    cg_path = os.path.join(toy_dir, cg_file)
                else:
                    raise RuntimeError("Handshake with FD failed and debug mode is off")

        self.current_merge_step = 0
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_path = f"logs/run_{ts}.jsonl"

        self.graph_tracker = GraphTracker(ts_json_path=ts_path, cg_json_path=cg_path, is_debug=self.debug)
        G = self.graph_tracker.graph
        self.max_vars = max(
            (len(d.get("incorporated_variables", [])) for _, d in G.nodes(data=True)),
            default=1
        ) or 1
        self.max_iter = max(
            (d.get("iteration", 0) for _, d in G.nodes(data=True)),
            default=0
        ) or 1
        self.centrality = nx.closeness_centrality(G)

        obs = self._get_observation()
        self.prev_total_states = self._count_total_states()
        self.state = obs
        return obs, {}

    def _handshake_with_fd(self) -> Tuple[str, str]:
        """Launch FD and wait for initial JSONs."""
        dw_dir = os.path.abspath("downward")
        if not os.path.isdir(dw_dir):
            raise RuntimeError(f"Downward folder not found: {dw_dir}")

        def _tail(path: str, n: int = 120) -> str:
            if not os.path.exists(path):
                return "(no log file yet)"
            try:
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    lines = f.readlines()
                return "".join(lines[-n:])
            except Exception as e:
                return f"(failed to read log tail: {e})"

        def _robust_copy(src: str, dst: str, tries: int = 80, delay: float = 0.1):
            for _ in range(tries):
                try:
                    with open(src, "rb") as fin, open(dst, "wb") as fout:
                        fout.write(fin.read())
                    return
                except OSError:
                    time.sleep(delay)
            raise RuntimeError(f"Could not copy {src} -> {dst}")

        def _wait_stable(path: str, grace: float = 0.15, timeout: float = 30.0) -> bool:
            end = time.time() + timeout
            last = -1
            while time.time() < end:
                if os.path.exists(path):
                    try:
                        sz = os.path.getsize(path)
                    except OSError:
                        sz = -1
                    if sz > 0 and sz == last:  # File exists and size hasn't changed
                        return True
                    last = sz
                time.sleep(grace)
            return False

        ## STEP 1: Clean up old files
        logger.info("[Handshake] Cleaning up old files...")
        for fname in ("causal_graph.json", "merged_transition_systems.json", "output.sas"):
            p = os.path.join(dw_dir, fname)
            if os.path.exists(p):
                try:
                    os.remove(p)
                except Exception as e:
                    logger.warning(f"⚠️ Could not delete {p}: {e}")

        for subdir_name in ["gnn_output", "fd_output"]:
            subdir_path = os.path.join(dw_dir, subdir_name)
            os.makedirs(subdir_path, exist_ok=True)
            for f in os.listdir(subdir_path):
                try:
                    os.remove(os.path.join(subdir_path, f))
                except Exception:
                    pass

        ## STEP 2: Safely terminate any previous FD process
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                self.process.wait(timeout=3.0)
        except (subprocess.TimeoutExpired, AttributeError):
            if hasattr(self, 'process') and self.process:
                self.process.kill()
        except Exception:
            pass

        ## STEP 3: Build the FD command with GNN merge strategy
        # ✅ NEW: Build FD command from scratch using stored file paths
        fd_translate_bin = os.path.join(dw_dir, "builds/release/bin/translate/translate.py")
        fd_downward_exe = os.path.join(dw_dir, "builds/release/bin/downward.exe")

        # ✅ Use stored paths (absolute)
        domain_path = self.domain_file
        problem_path = self.problem_file

        logger.info(f"[Handshake] Building FD command:")
        logger.info(f"  Domain:  {domain_path}")
        logger.info(f"  Problem: {problem_path}")

        # ✅ Build complete FD command with merge_gnn strategy
        cmd = (
            f'python "{fd_translate_bin}" "{domain_path}" "{problem_path}" '
            f'--sas-file output.sas && '
            f'"{fd_downward_exe}" '
            r'--search "astar(merge_and_shrink('
            r'merge_strategy=merge_gnn(),'  # ✅ Uses GNN for merge decisions
            r'shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),'
            r'label_reduction=exact(before_shrinking=true,before_merging=false),'
            f'max_states={self.max_states},'  # ✅ Use stored parameter
            f'threshold_before_merge={self.threshold_before_merge}'  # ✅ Use stored parameter
            r'))" < output.sas'
        )

        logger.info(f"[Handshake] FD Command: {cmd[:150]}...")

        ## STEP 4: Launch FD process
        fd_dir = os.path.join(dw_dir, "fd_output")
        log_path = os.path.join(fd_dir, "log.txt")
        self.fd_log_file = open(log_path, "w", buffering=1, encoding="utf-8")

        self.process = subprocess.Popen(
            cmd,
            shell=True,
            cwd=dw_dir,
            stdout=self.fd_log_file,
            stderr=self.fd_log_file,
        )

        ## STEP 5: Wait for translator
        logger.info("[Handshake] Waiting for translator to produce 'output.sas'...")
        sas_path = os.path.join(dw_dir, "output.sas")
        if not _wait_stable(sas_path, timeout=60.0):
            tail = _tail(log_path)
            raise RuntimeError(f"Translator failed to produce output.sas.\nLog tail:\n{tail}")
        logger.info("[Handshake] 'output.sas' is ready.")

        ## STEP 6: Wait for initial JSONs
        logger.info("[Handshake] Waiting for planner to produce initial JSONs...")
        cg_root = os.path.join(dw_dir, "causal_graph.json")
        ts_root = os.path.join(dw_dir, "merged_transition_systems.json")
        start, timeout_s = time.time(), 240.0

        while not (os.path.exists(cg_root) and os.path.exists(ts_root)):
            if self.process and (self.process.poll() is not None):
                tail = _tail(log_path)
                raise RuntimeError(
                    f"FD exited early (code {self.process.returncode}) while waiting for JSONs.\nLog tail:\n{tail}")
            if time.time() - start > timeout_s:
                tail = _tail(log_path)
                raise RuntimeError(f"Timeout waiting for initial JSONs after {timeout_s:.0f}s.\nLog tail:\n{tail}")
            time.sleep(0.1)

        ## STEP 7: Copy and verify
        logger.info("[Handshake] Initial JSONs found. Verifying and copying...")
        if not (_wait_stable(cg_root) and _wait_stable(ts_root)):
            tail = _tail(log_path)
            raise RuntimeError(f"Root JSONs never stabilized.\nLog tail:\n{tail}")

        fd_cg = os.path.join(fd_dir, "causal_graph.json")
        fd_ts = os.path.join(fd_dir, "merged_transition_systems.json")

        dec = JSONDecoder()
        for _ in range(80):
            _robust_copy(cg_root, fd_cg)
            _robust_copy(ts_root, fd_ts)
            try:
                with open(fd_ts, "r", encoding="utf-8", errors="ignore") as f:
                    s = f.read().lstrip()
                if not s:
                    time.sleep(0.1)
                    continue
                obj, _ = dec.raw_decode(s)
                break
            except json.JSONDecodeError:
                time.sleep(0.1)
        else:
            tail = _tail(log_path)
            raise RuntimeError("Copied TS JSON is not parseable.\n" + tail)

        logger.info("[Handshake] Handshake complete.")
        return fd_ts, fd_cg

    def _log_step(self, src: int, tgt: int, info: dict, reward: float, done: bool):
        """Append merge-step metrics to log file."""
        total_states = self._count_total_states()

        entry = {
            "step": int(self.current_merge_step),
            "merge_choice": [int(src), int(tgt)],
            "plan_cost": int(info.get("plan_cost", 0)),
            "num_expansions": int(info.get("num_expansions", 0)),
            "num_significant_f_changes": int(info.get("num_significant_f_changes", 0)),
            "delta_states": int(info.get("delta_states", 0)),
            "total_states": int(total_states),
            "reward": float(reward),
            "done": bool(done),
            "timestamp": time.time(),
        }

        with open(self.log_path, "a") as f:
            json.dump(entry, f)
            f.write("\n")

    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        """✅ FIXED: Action handling with complete validation."""
        try:
            # Check if FD process crashed
            if self.process is not None:
                ret = self.process.poll()
                if ret is not None and ret < 10:
                    print(f"[ERROR] FD process exited with code {ret}")
                    return self.state, 0.0, True, False, {"error": "process_crashed"}

            # Get current edges
            edges = list(self.graph_tracker.graph.edges)
            if not edges:
                print("[WARNING] No edges available for merging")
                return self.state, 0.0, True, False, {"error": "no_edges"}

            # ✅ FIX: Action validation and clamping
            action = int(action)
            num_valid_edges = len(edges)

            # Clamp to valid range: [0, num_valid_edges-1]
            action_idx = max(0, min(action % max(num_valid_edges, 1), num_valid_edges - 1))

            print(f"[DEBUG] Action {action} → index {action_idx} (edges: {num_valid_edges})")

            src, tgt = edges[action_idx]

            # ✅ VALIDATION: Check nodes exist and are different
            if src == tgt:
                print(f"[ERROR] Self-merge attempted: {src} == {tgt}")
                return self.state, 0.0, True, False, {"error": "self_merge"}

            if src not in self.graph_tracker.graph or tgt not in self.graph_tracker.graph:
                print(f"[ERROR] Merge nodes invalid: ({src}, {tgt})")
                return self.state, 0.0, True, False, {"error": "invalid_nodes"}

            if self.debug:
                # Need to extract (src, tgt) for debug mode
                edges = list(self.graph_tracker.graph.edges())
                action_idx = max(0, min(int(action) % max(len(edges), 1), len(edges) - 1))
                src, tgt = edges[action_idx]
                return self._step_debug(src, tgt)
            else:
                return self._step_real(action)  # ✅ Pass action, not (src, tgt)

        except Exception as e:
            print(f"[ERROR] Exception in step(): {e}")
            import traceback
            traceback.print_exc()
            return self.state, 0.0, True, False, {"error": str(e)}

    def _step_debug(self, src: int, tgt: int):
        """
        ✅ ENHANCED: In-memory merge (debug mode) with detailed logging.

        Shows every step of the merge process with:
        - State before and after
        - Computation details
        - Graph metrics
        - Reward calculation
        """
        logger.info("\n" + "=" * 90)
        logger.info(f"STEP {self.current_merge_step}: DEBUG MODE MERGE")
        logger.info("=" * 90)

        try:
            # ====================================================================
            # PHASE 1: ENTRY LOGGING & VALIDATION
            # ====================================================================

            logger.info("\n[PHASE 1] ENTRY VALIDATION")
            logger.info(f"  Merge indices: ({src}, {tgt})")
            logger.info(f"  Current merge step: {self.current_merge_step}")
            logger.info(f"  Max merges allowed: {self.max_merges}")

            # ====================================================================
            # PHASE 2: PRE-MERGE STATE SNAPSHOT
            # ====================================================================

            logger.info("\n[PHASE 2] PRE-MERGE STATE SNAPSHOT")

            # Count total states before merge
            old_total = self._count_total_states()
            logger.info(f"  Total states before merge: {old_total}")

            # Log node details
            G = self.graph_tracker.graph
            logger.info(f"  Num nodes before: {len(G.nodes)}")
            logger.info(f"  Num edges before: {len(G.edges)}")

            # Log specific nodes being merged
            if src in G.nodes and tgt in G.nodes:
                src_data = G.nodes[src]
                tgt_data = G.nodes[tgt]
                logger.info(f"\n  Node {src} (source):")
                logger.info(f"    - num_states: {src_data.get('num_states', 'N/A')}")
                logger.info(f"    - iteration: {src_data.get('iteration', 'N/A')}")
                logger.info(f"    - incorporated_variables: {src_data.get('incorporated_variables', 'N/A')}")

                logger.info(f"\n  Node {tgt} (target):")
                logger.info(f"    - num_states: {tgt_data.get('num_states', 'N/A')}")
                logger.info(f"    - iteration: {tgt_data.get('iteration', 'N/A')}")
                logger.info(f"    - incorporated_variables: {tgt_data.get('incorporated_variables', 'N/A')}")

            # ====================================================================
            # PHASE 3: PERFORM MERGE
            # ====================================================================

            logger.info("\n[PHASE 3] PERFORMING MERGE")
            logger.info(f"  Calling graph_tracker.merge_nodes([{src}, {tgt}])...")

            self.graph_tracker.merge_nodes([src, tgt])
            self.current_merge_step += 1

            logger.info(f"  ✓ Merge completed, current_merge_step = {self.current_merge_step}")

            # ====================================================================
            # PHASE 4: POST-MERGE STATE SNAPSHOT
            # ====================================================================

            logger.info("\n[PHASE 4] POST-MERGE STATE SNAPSHOT")

            new_total = self._count_total_states()
            delta_states = new_total - old_total

            logger.info(f"  Total states after merge: {new_total}")
            logger.info(f"  Delta states (new - old): {delta_states}")
            logger.info(f"  State explosion ratio: {delta_states / max(old_total, 1):.4f}")

            # Log updated graph
            G = self.graph_tracker.graph
            logger.info(f"  Num nodes after: {len(G.nodes)}")
            logger.info(f"  Num edges after: {len(G.edges)}")

            # ====================================================================
            # PHASE 5: UPDATE GRAPH METRICS
            # ====================================================================

            logger.info("\n[PHASE 5] UPDATE GRAPH METRICS")

            self.max_vars = max(
                (len(d.get("incorporated_variables", [])) for _, d in G.nodes(data=True)),
                default=1
            ) or 1
            logger.info(f"  max_vars: {self.max_vars}")

            self.max_iter = max(
                (d.get("iteration", 0) for _, d in G.nodes(data=True)),
                default=0
            ) or 1
            logger.info(f"  max_iter: {self.max_iter}")

            self.centrality = nx.closeness_centrality(G)
            logger.info(f"  Centrality computed for {len(self.centrality)} nodes")

            # ====================================================================
            # PHASE 6: COMPUTE REWARD
            # ====================================================================

            logger.info("\n[PHASE 6] REWARD COMPUTATION")

            reward = -max(abs(delta_states), 0.1)
            logger.info(f"  Reward formula: -max(|delta_states|, 0.1)")
            logger.info(f"  Computed reward: {reward:.4f}")

            # ====================================================================
            # PHASE 7: TENSORBOARD LOGGING
            # ====================================================================

            logger.info("\n[PHASE 7] TENSORBOARD LOGGING")

            try:
                writer.add_scalar("env/num_nodes", len(self.graph_tracker.graph.nodes),
                                  self.current_merge_step)
                logger.info(f"  ✓ Logged num_nodes: {len(self.graph_tracker.graph.nodes)}")

                writer.add_scalar("env/num_edges", len(self.graph_tracker.graph.edges),
                                  self.current_merge_step)
                logger.info(f"  ✓ Logged num_edges: {len(self.graph_tracker.graph.edges)}")

                writer.add_scalar("env/total_states", new_total, self.current_merge_step)
                logger.info(f"  ✓ Logged total_states: {new_total}")

                writer.add_scalar("env/delta_states", delta_states, self.current_merge_step)
                logger.info(f"  ✓ Logged delta_states: {delta_states}")

                writer.add_scalar("env/reward", reward, self.current_merge_step)
                logger.info(f"  ✓ Logged reward: {reward:.4f}")
            except Exception as e:
                logger.warning(f"  ⚠️ TensorBoard logging error: {e}")

            # ====================================================================
            # PHASE 8: BUILD OBSERVATION
            # ====================================================================

            logger.info("\n[PHASE 8] BUILD OBSERVATION")

            obs = self._get_observation()
            logger.info(f"  Observation built successfully")
            logger.info(f"  - x shape: {obs['x'].shape}")
            logger.info(f"  - edge_index shape: {obs['edge_index'].shape}")
            logger.info(f"  - num_nodes: {obs['num_nodes']}")
            logger.info(f"  - num_edges: {obs['num_edges']}")

            # ====================================================================
            # PHASE 9: TERMINATION CHECK
            # ====================================================================

            logger.info("\n[PHASE 9] TERMINATION CHECK")

            num_remaining = len(G.nodes)
            done = (num_remaining <= 1) or (self.current_merge_step >= self.max_merges)

            logger.info(f"  Num remaining nodes: {num_remaining}")
            logger.info(f"  Current step: {self.current_merge_step} / {self.max_merges}")
            logger.info(f"  Done: {done}")

            if done:
                if num_remaining <= 1:
                    logger.info(f"  Reason: Only {num_remaining} node(s) left")
                else:
                    logger.info(f"  Reason: Max merges reached")

            # ====================================================================
            # PHASE 10: BUILD INFO DICT
            # ====================================================================

            logger.info("\n[PHASE 10] BUILD INFO DICT")

            info = {
                "num_nodes": len(self.graph_tracker.graph.nodes),
                "num_edges": len(self.graph_tracker.graph.edges),
                "total_states": new_total,
                "delta_states": delta_states,
                "plan_cost": 0,
                "num_expansions": 0,
                "error": None,
            }
            logger.info(f"  Info dict built with {len(info)} keys")

            # ====================================================================
            # PHASE 11: LOG STEP & UPDATE STATE
            # ====================================================================

            logger.info("\n[PHASE 11] LOGGING & STATE UPDATE")

            self._log_step(src, tgt, info, reward, done)
            logger.info(f"  ✓ Step logged to file")

            self.state = obs
            logger.info(f"  ✓ State updated")

            # ====================================================================
            # PHASE 12: RETURN
            # ====================================================================

            logger.info("\n[PHASE 12] RETURN")
            logger.info(f"  Returning: obs, reward={reward:.4f}, done={done}, info")
            logger.info("=" * 90 + "\n")

            return obs, reward, done, False, info

        except Exception as e:
            logger.error(f"\n❌ ERROR in _step_debug: {e}")
            logger.error(f"Traceback:\n{traceback.format_exc()}")
            logger.info("=" * 90 + "\n")
            return self.state, 0.0, True, False, {"error": str(e)}

    def _step_real(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        """
        ✅ ENHANCED: Execute one merge step in REAL mode with BAD MERGE DETECTION.

        This function now:
        1. Validates action and extracts merge pair
        2. Writes merge decision to FD
        3. Waits for FD acknowledgment
        4. Waits for updated TS files
        5. Updates graph tracker
        6. ✅ DETECTS BAD MERGE SITUATIONS
        7. Extracts signals and computes reward
        8. Returns result with comprehensive logging
        """

        logger.info("\n" + "=" * 90)
        logger.info(f"STEP {self.current_merge_step}: REAL MODE MERGE WITH BAD MERGE DETECTION")
        logger.info("=" * 90)

        try:
            # ====================================================================
            # PHASE 1: VALIDATE INPUT & EXTRACT MERGE PAIR
            # ====================================================================

            logger.info("\n[PHASE 1] INPUT VALIDATION & ACTION EXTRACTION")
            logger.info(f"  Input action: {action} (type: {type(action).__name__})")

            if not hasattr(self, 'graph_tracker') or self.graph_tracker is None:
                raise RuntimeError("graph_tracker not initialized")

            # Get current edges
            edges = list(self.graph_tracker.graph.edges())
            logger.info(f"  Available edges in graph: {len(edges)}")

            if not edges:
                logger.warning("  ⚠️ No edges available for merging")
                return self.state, 0.0, True, False, {"error": "no_edges"}

            # ✅ FIX: Action validation and extraction
            action = int(action)
            num_valid_edges = len(edges)
            action_idx = max(0, min(action % max(num_valid_edges, 1), num_valid_edges - 1))

            logger.info(f"  Action index calculation:")
            logger.info(
                f"    - action % max(num_valid_edges, 1) = {action} % {max(num_valid_edges, 1)} = {action % max(num_valid_edges, 1)}")
            logger.info(f"    - clamped to [0, {num_valid_edges - 1}]: {action_idx}")

            node_a, node_b = edges[action_idx]
            logger.info(f"  ✓ Extracted merge pair from edge {action_idx}: ({node_a}, {node_b})")

            # ✅ VALIDATION: Check nodes exist and are different
            if node_a == node_b:
                logger.error(f"  ❌ ERROR: Self-merge attempted: {node_a} == {node_b}")
                return self.state, 0.0, True, False, {"error": "self_merge"}

            if node_a not in self.graph_tracker.graph or node_b not in self.graph_tracker.graph:
                logger.error(f"  ❌ ERROR: Merge nodes invalid: ({node_a}, {node_b})")
                return self.state, 0.0, True, False, {"error": "invalid_nodes"}

            src = node_a
            tgt = node_b

            logger.info(f"  ✓ Validation passed")

            # ====================================================================
            # PHASE 2: PRE-MERGE DIAGNOSTICS & DECISION LOGGING
            # ====================================================================

            logger.info(f"\n[PHASE 2] PRE-MERGE STATE ANALYSIS")

            pre_merge_total = sum(d.get('num_states', 0) for _, d in self.graph_tracker.graph.nodes(data=True))
            logger.info(f"    - Total states before merge: {pre_merge_total}")
            logger.info(f"    - Num nodes: {len(self.graph_tracker.graph.nodes)}")
            logger.info(f"    - Num edges: {len(self.graph_tracker.graph.edges)}")

            # Get node-specific data
            src_data = self.graph_tracker.graph.nodes[src]
            tgt_data = self.graph_tracker.graph.nodes[tgt]
            src_size = src_data.get('num_states', 1)
            tgt_size = tgt_data.get('num_states', 1)
            expected_product = src_size * tgt_size

            logger.info(f"\n    Nodes being merged:")
            logger.info(f"      - Node {src}: {src_size} states")
            logger.info(f"      - Node {tgt}: {tgt_size} states")
            logger.info(f"      - Expected product: {expected_product} states")

            # ====================================================================
            # PHASE 3: WRITE MERGE DECISION
            # ====================================================================

            logger.info(f"\n[PHASE 3] WRITE MERGE DECISION")

            merge_decision = {
                "iteration": self.current_merge_step,
                "merge_pair": [int(src), int(tgt)],
                "timestamp": time.time()
            }

            gnn_output_dir = os.path.abspath(os.path.join(self.fd_base_dir, "gnn_output"))
            os.makedirs(gnn_output_dir, exist_ok=True)

            merge_decision_path = os.path.join(gnn_output_dir, f"merge_{self.current_merge_step}.json")

            # ✅ Write atomically
            import tempfile
            try:
                fd, temp_path = tempfile.mkstemp(
                    dir=gnn_output_dir,
                    suffix=".json",
                    prefix=f"merge_{self.current_merge_step}_"
                )
                with os.fdopen(fd, 'w') as f:
                    json.dump(merge_decision, f, indent=2)
                    f.flush()
                    os.fsync(f.fileno())

                os.replace(temp_path, merge_decision_path)
                logger.info(f"  ✓ Wrote merge decision: {merge_decision_path}")

            except Exception as e:
                logger.error(f"  ❌ Failed to write merge decision: {e}")
                raise

            # ====================================================================
            # PHASE 4: WAIT FOR FD ACKNOWLEDGMENT
            # ====================================================================

            logger.info(f"\n[PHASE 4] WAIT FOR FD ACKNOWLEDGMENT")

            ack_path = os.path.join(
                self.fd_base_dir,
                "fd_output",
                f"gnn_ack_{self.current_merge_step}.json"
            )

            logger.info(f"  Expected ACK file: {ack_path}")

            start_time = time.time()
            ACK_TIMEOUT = 30.0

            while time.time() - start_time < ACK_TIMEOUT:
                elapsed = time.time() - start_time

                if os.path.exists(ack_path):
                    try:
                        with open(ack_path) as f:
                            ack_data = json.load(f)
                        elapsed = time.time() - start_time
                        logger.info(f"  ✓ ACK received after {elapsed:.2f}s")
                        break
                    except json.JSONDecodeError:
                        time.sleep(0.1)
                        continue

                if int(elapsed) > 0 and int(elapsed) % 5 == 0:
                    logger.debug(f"  Still waiting... ({elapsed:.0f}s)")

                time.sleep(0.1)
            else:
                elapsed = time.time() - start_time
                raise TimeoutError(
                    f"FD did not acknowledge merge within {ACK_TIMEOUT}s (waited {elapsed:.1f}s)")

            # ====================================================================
            # PHASE 5: WAIT FOR UPDATED TS FILE
            # ====================================================================

            logger.info(f"\n[PHASE 5] WAIT FOR UPDATED TS FILE")

            ts_path = os.path.join(
                self.fd_base_dir,
                "fd_output",
                f"ts_{self.current_merge_step}.json"
            )

            logger.info(f"  Expected TS file: {ts_path}")

            start_time = time.time()
            TS_TIMEOUT = 60.0

            while time.time() - start_time < TS_TIMEOUT:
                elapsed = time.time() - start_time

                if os.path.exists(ts_path):
                    try:
                        with open(ts_path) as f:
                            ts_data = json.load(f)
                        elapsed = time.time() - start_time
                        logger.info(f"  ✓ TS file received after {elapsed:.2f}s")
                        break
                    except (json.JSONDecodeError, IOError):
                        time.sleep(0.2)
                        continue

                if int(elapsed) > 0 and int(elapsed) % 10 == 0:
                    logger.debug(f"  Still waiting... ({elapsed:.0f}s)")

                time.sleep(0.2)
            else:
                elapsed = time.time() - start_time
                raise TimeoutError(
                    f"TS file not produced within {TS_TIMEOUT}s (waited {elapsed:.1f}s)")

            # ====================================================================
            # PHASE 6: UPDATE GRAPH TRACKER
            # ====================================================================

            logger.info(f"\n[PHASE 6] UPDATE GRAPH TRACKER")
            logger.info(f"  Calling graph_tracker.merge_nodes([{src}, {tgt}])...")

            try:
                self.graph_tracker.merge_nodes([src, tgt])
                logger.info(f"  ✓ Merged in graph tracker")
            except Exception as e:
                logger.error(f"  ❌ Graph merge failed: {e}")
                raise

            # ====================================================================
            # PHASE 7: EXTRACT SIGNALS & COMPUTE REWARD
            # ====================================================================

            logger.info(f"\n[PHASE 7] EXTRACT SIGNALS & COMPUTE REWARD")
            logger.info(f"  Calling reward_info_extractor.extract_merge_info(iteration={self.current_merge_step})...")

            try:
                merge_info = self.reward_info_extractor.extract_merge_info(
                    iteration=self.current_merge_step,
                    timeout=10.0
                )

                if merge_info is None:
                    logger.warning(f"  ⚠️ Failed to extract merge info, using defaults")
                    reward = 0.0
                else:
                    logger.info(f"  ✓ Merge info extracted successfully")

                    # ✅ ENHANCED: Log extracted signals with bad merge context
                    logger.info(f"\n  Extracted signals:")
                    logger.info(f"    - iteration: {merge_info.iteration}")
                    logger.info(f"    - states_before: {merge_info.states_before}")
                    logger.info(f"    - states_after: {merge_info.states_after}")
                    logger.info(f"    - delta_states: {merge_info.delta_states}")
                    logger.info(f"    - f_value_stability: {merge_info.f_value_stability:.4f}")
                    logger.info(f"    - num_significant_f_changes: {merge_info.num_significant_f_changes}")
                    logger.info(f"    - state_explosion_penalty: {merge_info.state_explosion_penalty:.4f}")
                    logger.info(f"    - nodes_expanded: {merge_info.nodes_expanded}")
                    logger.info(f"    - search_depth: {merge_info.search_depth}")
                    logger.info(f"    - solution_cost: {merge_info.solution_cost}")
                    logger.info(f"    - branching_factor: {merge_info.branching_factor:.4f}")
                    logger.info(f"    - solution_found: {merge_info.solution_found}")

                    # ✅ NEW: Pre-reward bad merge diagnostics
                    logger.info(f"\n  [BAD MERGE DIAGNOSTIC CHECKS]:")

                    # Check 1: State explosion
                    if merge_info.states_after > expected_product * 1.5:
                        logger.warning(
                            f"    ⚠️  State explosion detected: {merge_info.states_after} > {expected_product * 1.5:.0f}")

                    # Check 2: Goal reachability
                    goal_reachable = any(f != float('inf') and f < 1_000_000_000 for f in merge_info.f_after)
                    if not goal_reachable:
                        logger.error(f"    ❌ CRITICAL: Goal unreachable after merge!")

                    # Check 3: F-stability
                    if merge_info.f_value_stability < 0.3:
                        logger.warning(f"    ⚠️  Poor F-stability: {merge_info.f_value_stability:.4f}")

                    # Check 4: Unreachable states
                    unreachable_count = sum(1 for f in merge_info.f_after if f == float('inf') or f >= 1_000_000_000)
                    unreachable_ratio = unreachable_count / max(merge_info.states_after, 1)
                    if unreachable_ratio > 0.7:
                        logger.warning(f"    ⚠️  High unreachability: {unreachable_ratio * 100:.1f}%")

                    # Check 5: Branching factor
                    if merge_info.branching_factor > 8.0:
                        logger.warning(f"    ⚠️  High branching factor: {merge_info.branching_factor:.4f}")

                    logger.info(f"\n  Computing reward:")
                    logger.info(f"    - Reward function: {self.reward_function.name}")
                    logger.info(f"    - Calling compute() with merge_info and signals...")

                    reward = self.reward_function.compute(
                        merge_info=merge_info,
                        search_expansions=merge_info.nodes_expanded,
                        plan_cost=merge_info.solution_cost,
                        is_terminal=False
                    )

                    logger.info(f"    ✓ Reward computed: {reward:.4f}")

                    # Log component breakdown
                    components = self.reward_function.get_components_dict()
                    logger.info(f"\n  Reward component breakdown:")
                    for key, val in components.items():
                        logger.info(f"    - {key:<30} {val:.4f}")

            except Exception as e:
                logger.warning(f"  ⚠️ Reward computation failed: {e}")
                logger.warning(f"  Using reward = 0.0")
                reward = 0.0

            # ====================================================================
            # PHASE 8: CHECK EPISODE TERMINATION
            # ====================================================================

            logger.info(f"\n[PHASE 8] TERMINATION CHECK")

            num_remaining = self.graph_tracker.graph.number_of_nodes()
            logger.info(f"  Num remaining nodes: {num_remaining}")
            logger.info(f"  Current merge step: {self.current_merge_step} / {self.max_merges}")

            done = (num_remaining <= 1) or (self.current_merge_step >= self.max_merges - 1)
            logger.info(f"  Done: {done}")

            if done:
                if num_remaining <= 1:
                    logger.info(f"  Reason: Only {num_remaining} node(s) remaining")
                if self.current_merge_step >= self.max_merges - 1:
                    logger.info(f"  Reason: Max merges reached")

            # ====================================================================
            # PHASE 9: BUILD OBSERVATION
            # ====================================================================

            logger.info(f"\n[PHASE 9] BUILD OBSERVATION")

            try:
                obs = self._get_observation()
                logger.info(f"  ✓ Observation built successfully")
                logger.info(f"    - x shape: {obs['x'].shape}")
                logger.info(f"    - edge_index shape: {obs['edge_index'].shape}")
                logger.info(f"    - num_nodes: {obs['num_nodes']}")
                logger.info(f"    - num_edges: {obs['num_edges']}")
            except Exception as e:
                logger.error(f"  ❌ Failed to build observation: {e}")
                obs = self._get_observation()

            # ====================================================================
            # PHASE 10: INCREMENT STEP COUNTER
            # ====================================================================

            logger.info(f"\n[PHASE 10] STEP COUNTER INCREMENT")

            logger.info(f"  Before: current_merge_step = {self.current_merge_step}")
            self.current_merge_step += 1
            logger.info(f"  After: current_merge_step = {self.current_merge_step}")

            # ====================================================================
            # PHASE 11: BUILD INFO DICT & LOG
            # ====================================================================

            logger.info(f"\n[PHASE 11] BUILD INFO DICT & LOG STEP")

            info = {
                "merge_pair": [int(src), int(tgt)],
                "num_nodes": num_remaining,
                "step": self.current_merge_step - 1,
            }

            if merge_info is not None:
                from validate_merge_signals import validate_merge_signals
                is_valid, issues = validate_merge_signals(merge_info)
                if not is_valid:
                    logger.error(f"[SIGNAL VALIDATION] ✗ Merge signals invalid: {issues}")
                    merge_info = None

                info.update(merge_info.to_dict())
                logger.info(f"  ✓ Added merge_info to info dict")

            self._log_step(src, tgt, info, reward, done)
            logger.info(f"  ✓ Step logged")

            # ====================================================================
            # PHASE 12: RETURN
            # ====================================================================

            logger.info(f"\n[PHASE 12] RETURN RESULT")
            logger.info(f"  Returning:")
            logger.info(f"    - obs: shape {obs['x'].shape}")
            logger.info(f"    - reward: {reward:.4f}")
            logger.info(f"    - done: {done}")
            logger.info(f"    - truncated: False")
            logger.info(f"    - info: {len(info)} keys")
            logger.info("=" * 90 + "\n")

            return obs, float(reward), done, False, info

        except Exception as e:
            logger.error(f"\n❌ STEP FAILED: {e}")
            logger.error(f"Traceback:\n{traceback.format_exc()}")
            logger.info("=" * 90 + "\n")
            return self.state, 0.0, True, False, {"error": str(e)}

    def _wait_for_json_file_safe(self, path: str, step: int, timeout_seconds: float = 60.0) -> Optional[Dict]:
        """
        ✅ ROBUST: Wait for JSON file with proper validation and diagnostics.

        NOW WITH PHASE TRACKING to identify exactly where handshake breaks down.

        Returns: Parsed JSON dict, or None if timeout/error occurs
        """
        start_time = time.time()
        last_size = -1
        last_modified = -1
        consecutive_parse_errors = 0
        max_consecutive_errors = 3

        # ✅ Track which phase of handshake we're in
        file_basename = os.path.basename(path)

        if "gnn_ack" in file_basename:
            phase_name = "ACK"
            phase_desc = "FD acknowledging merge decision"
        elif "ts_" in file_basename:
            phase_name = "TS"
            phase_desc = "Updated transition system"
        elif "merge_before" in file_basename:
            phase_name = "BEFORE"
            phase_desc = "Pre-merge metrics"
        elif "merge_after" in file_basename:
            phase_name = "AFTER"
            phase_desc = "Post-merge metrics"
        else:
            phase_name = "DATA"
            phase_desc = "Data file"

        logger.info(f"[Step {step}] [PHASE: {phase_name}] Starting wait...")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Waiting for: {path}")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Description: {phase_desc}")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Timeout: {timeout_seconds}s")

        while time.time() - start_time < timeout_seconds:
            elapsed = time.time() - start_time

            if not os.path.exists(path):
                if int(elapsed) > 0 and int(elapsed) % 15 == 0:  # Every 15 seconds
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] Still waiting... ({elapsed:.0f}s)")
                time.sleep(0.5)
                continue

            try:
                # ✅ Check file size and modification time
                current_size = os.path.getsize(path)
                current_mtime = os.path.getmtime(path)

                if current_size == 0:
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] File exists but EMPTY (0 bytes)")
                    consecutive_parse_errors += 1
                    if consecutive_parse_errors >= max_consecutive_errors:
                        logger.error(
                            f"[Step {step}] [PHASE: {phase_name}] File empty for {consecutive_parse_errors} checks - giving up")
                        return None
                    time.sleep(1.0)
                    continue

                # ✅ Wait for file to stabilize (size and mtime haven't changed)
                if current_size == last_size and current_mtime == last_modified:
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] File stable at {current_size} bytes, parsing...")

                    try:
                        with open(path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        if not content.strip():
                            logger.warning(f"[Step {step}] [PHASE: {phase_name}] File content is empty/whitespace")
                            consecutive_parse_errors += 1
                            time.sleep(1.0)
                            continue

                        # ✅ Validate JSON structure
                        content_stripped = content.strip()
                        if not (content_stripped.startswith('{') or content_stripped.startswith('[')):
                            logger.error(
                                f"[Step {step}] [PHASE: {phase_name}] Invalid JSON: doesn't start with {{ or [")
                            logger.error(f"[Step {step}] [PHASE: {phase_name}] First 100 chars: {content[:100]}")
                            consecutive_parse_errors += 1
                            time.sleep(1.0)
                            continue

                        # ✅ Try to parse
                        data = json.loads(content)

                        logger.info(
                            f"[Step {step}] [PHASE: {phase_name}] ✅ Successfully parsed JSON ({current_size} bytes)")
                        logger.info(f"[Step {step}] [PHASE: {phase_name}] Elapsed time: {elapsed:.1f}s")
                        consecutive_parse_errors = 0  # Reset on success
                        return data

                    except json.JSONDecodeError as e:
                        logger.warning(
                            f"[Step {step}] [PHASE: {phase_name}] JSON parse error (line {e.lineno}, col {e.colno}): {e.msg}")
                        logger.debug(f"[Step {step}] [PHASE: {phase_name}] Content preview: {content[:200]}")
                        consecutive_parse_errors += 1

                        if consecutive_parse_errors >= max_consecutive_errors:
                            logger.error(
                                f"[Step {step}] [PHASE: {phase_name}] JSON parsing failed {max_consecutive_errors} times - giving up")
                            return None

                        time.sleep(2.0)
                        continue

                else:
                    # File size or mtime changed - still writing
                    logger.debug(
                        f"[Step {step}] [PHASE: {phase_name}] File size changing: {last_size} → {current_size} bytes (still writing)")
                    last_size = current_size
                    last_modified = current_mtime
                    consecutive_parse_errors = 0
                    time.sleep(0.5)
                    continue

            except (OSError, IOError) as e:
                logger.debug(f"[Step {step}] [PHASE: {phase_name}] File I/O error: {e}")
                consecutive_parse_errors += 1
                time.sleep(1.0)
                continue

            except Exception as e:
                logger.error(f"[Step {step}] [PHASE: {phase_name}] Unexpected error: {e}", exc_info=True)
                return None

            # Check if FD died
            if self.process and self.process.poll() is not None:
                rc = self.process.returncode
                logger.error(f"[Step {step}] [PHASE: {phase_name}] ❌ FD process DIED with code {rc}")
                logger.error(
                    f"[Step {step}] [PHASE: {phase_name}] FD crashed before producing {os.path.basename(path)}")
                return None

        # TIMEOUT
        logger.error(
            f"[Step {step}] [PHASE: {phase_name}] ❌ TIMEOUT after {timeout_seconds}s waiting for {os.path.basename(path)}")
        logger.error(f"[Step {step}] [PHASE: {phase_name}] This indicates:")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   1. FD process crashed or hung")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   2. JSON file not being written by FD")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   3. File I/O or permission problem")

        # Diagnostic: list what files DO exist
        fd_output_dir = "downward/fd_output"
        if os.path.exists(fd_output_dir):
            logger.error(f"[Step {step}] [PHASE: {phase_name}] Files in fd_output/:")
            try:
                for fname in sorted(os.listdir(fd_output_dir)):
                    fpath = os.path.join(fd_output_dir, fname)
                    if os.path.isfile(fpath):
                        size = os.path.getsize(fpath)
                        logger.error(f"[Step {step}] [PHASE: {phase_name}]   - {fname} ({size} bytes)")
            except:
                pass

        return None

    def _diagnose_fd_output(self, step: int) -> None:
        """✅ NEW: Detailed diagnostics when FD output fails."""
        fd_output_dir = "downward/fd_output"

        logger.info(f"\n[DIAGNOSIS] Checking FD output state for step {step}...")

        # Check directory
        if not os.path.exists(fd_output_dir):
            logger.error(f"[DIAGNOSIS] fd_output directory DOES NOT EXIST")
            return

        logger.info(f"[DIAGNOSIS] Files in {fd_output_dir}:")
        try:
            for fname in sorted(os.listdir(fd_output_dir)):
                fpath = os.path.join(fd_output_dir, fname)
                if os.path.isfile(fpath):
                    size = os.path.getsize(fpath)
                    mtime = os.path.getmtime(fpath)
                    age_sec = time.time() - mtime
                    logger.info(f"  - {fname:<30} {size:>10} bytes (age: {age_sec:.1f}s)")
        except Exception as e:
            logger.error(f"[DIAGNOSIS] Error listing files: {e}")

        # Check for expected files
        expected_ts = os.path.join(fd_output_dir, f"ts_{step}.json")
        expected_before = os.path.join(fd_output_dir, f"merge_before_{step}.json")
        expected_after = os.path.join(fd_output_dir, f"merge_after_{step}.json")

        logger.info(f"[DIAGNOSIS] Expected files for step {step}:")
        for expected_path in [expected_ts, expected_before, expected_after]:
            exists = os.path.exists(expected_path)
            status = "✓ EXISTS" if exists else "✗ MISSING"
            logger.info(f"  {status}: {os.path.basename(expected_path)}")

            if exists:
                try:
                    size = os.path.getsize(expected_path)
                    with open(expected_path, 'r') as f:
                        content = f.read()
                    logger.info(f"    Size: {size} bytes")
                    logger.info(f"    Preview: {content[:100]}...")
                except Exception as e:
                    logger.warning(f"    Error reading: {e}")

        # Check FD process
        if self.process:
            retcode = self.process.poll()
            if retcode is not None:
                logger.error(f"[DIAGNOSIS] FD process has EXITED with code {retcode}")
            else:
                logger.info(f"[DIAGNOSIS] FD process is RUNNING (PID: {self.process.pid})")

    def _diagnose_signals(self, iteration: int):
        """✅ NEW: Verify signal correctness."""
        before_path = os.path.join("downward", "fd_output", f"merge_before_{iteration}.json")
        after_path = os.path.join("downward", "fd_output", f"merge_after_{iteration}.json")

        if not os.path.exists(before_path) or not os.path.exists(after_path):
            logger.error(f"Signal files missing for iteration {iteration}")
            logger.error(f"  Before path: {before_path} (exists: {os.path.exists(before_path)})")
            logger.error(f"  After path: {after_path} (exists: {os.path.exists(after_path)})")
            return False

        try:
            with open(before_path) as f:
                before = json.load(f)
            with open(after_path) as f:
                after = json.load(f)
        except Exception as e:
            logger.error(f"Failed to load signal files: {e}")
            return False

        # ✅ VERIFICATION 1: State count consistency
        ts1_size = before.get("ts1_size", 0)
        ts2_size = before.get("ts2_size", 0)
        expected_merged_size = ts1_size * ts2_size
        actual_merged_size = after.get("num_states", 0)

        logger.info(f"[VERIFY] Iteration {iteration}:")
        logger.info(f"  TS1 size: {ts1_size}, TS2 size: {ts2_size}")
        logger.info(f"  Expected merged size: {expected_merged_size}")
        logger.info(f"  Actual merged size: {actual_merged_size}")

        if actual_merged_size > expected_merged_size * 1.1:
            logger.warning(f"  ⚠️ Merged size larger than expected (not properly shrunk?)")

        # ✅ VERIFICATION 2: F-value list lengths
        f1_len = len(before.get("ts1_f_values", []))
        f2_len = len(before.get("ts2_f_values", []))
        f_after_len = len(after.get("f_values", []))

        logger.info(f"  F-values: |f1|={f1_len}, |f2|={f2_len}, |f_after|={f_after_len}")

        if f1_len != ts1_size or f2_len != ts2_size:
            logger.error(f"  ❌ F-value list size mismatch!")
            return False

        # ✅ VERIFICATION 3: A* signals present and valid
        signals = after.get("search_signals", {})
        logger.info(f"  A* signals: {signals}")

        if not signals:
            logger.warning(f"  ⚠️ No A* signals exported")
        else:
            for key in ["nodes_expanded", "branching_factor", "solution_found"]:
                if key not in signals:
                    logger.error(f"  ❌ Missing signal: {key}")
                    return False

        return True
    def _parse_fd_log(self) -> Tuple[int, int]:
        """Read FD log and extract plan cost and expansions."""
        if self.fd_log_file:
            try:
                self.fd_log_file.flush()
            except Exception:
                pass

        log_path = os.path.join("downward", "fd_output", "log.txt")
        plan_cost, expansions = 0, 0
        if os.path.exists(log_path):
            try:
                text = open(log_path, "r", encoding="utf-8", errors="ignore").read()
            except Exception:
                text = ""
            if text:
                m1 = list(re.finditer(r"Plan length:\s*(\d+)", text))
                if m1:
                    plan_cost = int(m1[-1].group(1))
                m2 = list(re.finditer(r"[Ee]xpanded\s+(\d+)\s+state(?:s)?(?:\(\w*\))?", text))
                if m2:
                    expansions = int(m2[-1].group(1))
        return plan_cost, expansions

    # ============================================================================
    # ✅ NEW: Extract rich edge features for GNN learning
    # ============================================================================

    def _extract_edge_features(self) -> np.ndarray:
        """
        ✅ NEW: Extract rich features about merge candidates.

        For each edge (u, v), compute:
        1. Relative size difference
        2. Expected merge size ratio
        3. Shared variable count
        4. Reachability metrics
        5. Iteration difference
        6. Centrality similarity
        7. F-value consistency
        8. Merge risk indicator

        Returns:
            [E, 8] edge features
        """
        G = self.graph_tracker.graph
        edges = list(G.edges())

        if not edges:
            return np.zeros((0, 8), dtype=np.float32)

        edge_features = []

        for u, v in edges:
            u_data = G.nodes[u]
            v_data = G.nodes[v]

            # Feature 1: Relative size difference (normalized)
            u_size = u_data.get("num_states", 1)
            v_size = v_data.get("num_states", 1)
            max_size_in_graph = max(
                d.get("num_states", 1) for _, d in G.nodes(data=True)
            )
            size_diff = float(abs(u_size - v_size)) / max(max_size_in_graph, 1)

            # Feature 2: Expected merged size ratio (% of reachable states)
            # Heuristic: product of sizes as fraction of typical max
            product_size = (u_size * v_size) / max(max_size_in_graph * max_size_in_graph, 1)
            merge_size_ratio = np.clip(product_size, 0.0, 1.0)

            # Feature 3: Shared variables (normalized)
            u_vars = set(u_data.get("incorporated_variables", []))
            v_vars = set(v_data.get("incorporated_variables", []))
            shared_vars = len(u_vars & v_vars)
            total_vars = len(u_vars | v_vars)
            shared_ratio = shared_vars / max(total_vars, 1)

            # Feature 4: Reachability consistency
            u_f = u_data.get("f_before", [])
            v_f = v_data.get("f_before", [])
            u_reachable = sum(1 for f in u_f if f != float('inf') and f < 1_000_000_000) / max(len(u_f), 1)
            v_reachable = sum(1 for f in v_f if f != float('inf') and f < 1_000_000_000) / max(len(v_f), 1)
            reachability_similarity = 1.0 - abs(u_reachable - v_reachable)

            # Feature 5: Iteration difference (normalized)
            u_iter = u_data.get("iteration", 0)
            v_iter = v_data.get("iteration", 0)
            max_iter = max((d.get("iteration", 0) for _, d in G.nodes(data=True)), default=1)
            iter_diff = float(abs(u_iter - v_iter)) / max(max_iter, 1)

            # Feature 6: Centrality similarity
            u_centrality = self.centrality.get(u, 0.0)
            v_centrality = self.centrality.get(v, 0.0)
            centrality_similarity = 1.0 - abs(u_centrality - v_centrality)

            # Feature 7: F-value consistency (std of combined distributions)
            f_combined = u_f + v_f
            if f_combined:
                # Filter to valid values
                f_valid = [f for f in f_combined if f != float('inf') and f < 1_000_000_000]
                if f_valid:
                    f_std = float(np.std(f_valid)) / (1.0 + float(np.mean(f_valid)))
                    f_consistency = np.clip(1.0 - f_std, 0.0, 1.0)
                else:
                    f_consistency = 0.0
            else:
                f_consistency = 0.5

            # Feature 8: Merge risk indicator
            # Combines: degree, transition density, reachability
            u_degree = G.degree(u) / max(G.number_of_nodes(), 1)
            v_degree = G.degree(v) / max(G.number_of_nodes(), 1)
            u_trans = u_data.get("num_transitions", 0) / max(u_size, 1)
            v_trans = v_data.get("num_transitions", 0) / max(v_size, 1)

            merge_risk = np.clip(
                (u_degree + v_degree) * 0.5 + (u_trans + v_trans) * 0.25,
                0.0, 1.0
            )

            edge_features.append([
                size_diff,
                merge_size_ratio,
                shared_ratio,
                reachability_similarity,
                iter_diff,
                centrality_similarity,
                f_consistency,
                merge_risk
            ])

        return np.array(edge_features, dtype=np.float32)

    def _get_observation(self) -> Dict:
        max_nodes, max_edges = 100, 1000
        node_feats, idx = [], {}

        G = self.graph_tracker.graph

        degs = dict(G.degree()).values()
        max_deg = max(max(degs, default=0), 1)

        max_states_node = max((d.get("num_states", 0) for _, d in G.nodes(data=True)), default=1) or 1

        f_scale = 1.0
        for _, d in G.nodes(data=True):
            vals = d.get("f_before", [])
            if vals:
                try:
                    f_scale = max(f_scale, float(max(vals)))
                except Exception:
                    pass

        self.max_vars = max(
            (len(d.get("incorporated_variables", [])) for _, d in G.nodes(data=True)),
            default=1
        ) or 1
        self.max_iter = max(
            (d.get("iteration", 0) for _, d in G.nodes(data=True)),
            default=0
        ) or 1

        if not hasattr(self, "centrality") or not self.centrality:
            self.centrality = nx.closeness_centrality(G)

        for i, (nid, data) in enumerate(G.nodes(data=True)):
            ns_raw = float(data.get("num_states", 0))
            num_states = ns_raw / float(max_states_node)
            is_atomic = 1.0 if data.get("iteration", -1) == -1 else 0.0

            d = G.degree(nid) / max_deg
            od = G.out_degree(nid) / max_deg

            f_vals = data.get("f_before", [])
            if f_vals and f_scale > 0:
                avg_f = float(np.mean(f_vals)) / f_scale
                max_f = float(max(f_vals)) / f_scale
            else:
                avg_f = 0.0
                max_f = 0.0

            num_vars = len(data.get("incorporated_variables", [])) / float(self.max_vars)
            iter_idx = data.get("iteration", 0) / float(self.max_iter)
            central = float(self.centrality.get(nid, 0.0))

            f_min, f_mean, f_max, f_std = self.graph_tracker.f_stats(nid)
            if f_scale > 0:
                f_min = (f_min / f_scale) if f_min is not None else 0.0
                f_mean = (f_mean / f_scale) if f_mean is not None else 0.0
                f_max = (f_max / f_scale) if f_max is not None else 0.0
                f_std = (f_std / f_scale) if f_std is not None else 0.0
            else:
                f_min = f_mean = f_max = f_std = 0.0

            # v = [
            #     num_states, is_atomic, d, od, avg_f, max_f,
            #     num_vars, iter_idx, central,
            #     f_min, f_mean, f_max, f_std,
            # ]
            # v = [float(np.clip(x, 0.0, 1.0)) for x in v]
            # v += [0.0] * (self.feat_dim - len(v))

            # ✅ ENHANCED: 19-dimensional feature set for richer GNN learning signal

            # 1. Basic structural features (4 dims)
            num_states_norm = ns_raw / float(max_states_node)
            is_atomic_f = 1.0 if data.get("iteration", -1) == -1 else 0.0
            degree_norm = G.degree(nid) / max_deg
            out_degree_norm = G.out_degree(nid) / max_deg

            # 2. Heuristic quality features (4 dims)
            f_vals = data.get("f_before", [])
            if f_vals and f_scale > 0:
                avg_f_norm = float(np.mean(f_vals)) / f_scale
                max_f_norm = float(max(f_vals)) / f_scale
                # ✅ NEW: Heuristic concentration (uniformity indicator)
                f_vals_arr = np.array(f_vals, dtype=np.float32)
                f_median = np.median(f_vals_arr)
                heuristic_concentration = float(np.std(f_vals_arr)) / (1.0 + f_median)
                heuristic_concentration = float(np.clip(heuristic_concentration, 0.0, 1.0))
            else:
                avg_f_norm = 0.0
                max_f_norm = 0.0
                heuristic_concentration = 0.0

            # 3. Abstraction complexity features (3 dims)
            num_vars_norm = len(data.get("incorporated_variables", [])) / float(self.max_vars)
            iter_idx_norm = data.get("iteration", 0) / float(self.max_iter)
            centrality_norm = float(self.centrality.get(nid, 0.0))

            # 4. F-statistics as direct features (4 dims)
            f_min_norm, f_mean_norm, f_max_norm, f_std_norm = self.graph_tracker.f_stats(nid)
            if f_scale > 0:
                f_min_norm = max(0.0, min(f_min_norm / f_scale, 1.0))
                f_mean_norm = max(0.0, min(f_mean_norm / f_scale, 1.0))
                f_max_norm = max(0.0, min(f_max_norm / f_scale, 1.0))
                f_std_norm = max(0.0, min(f_std_norm / f_scale, 1.0))
            else:
                f_min_norm = f_mean_norm = f_max_norm = f_std_norm = 0.0

            # 5. Merge risk indicators (2 dims)
            num_neighbors = len(list(G.neighbors(nid)))
            neighbor_risk = float(num_neighbors) / max(len(G.nodes), 1)

            # ✅ NEW: Reachability indicator (proxy for heuristic quality)
            reachable_ratio = 1.0
            if f_vals:
                unreachable_count = sum(1 for f in f_vals if f == float('inf') or f >= 1_000_000_000)
                reachable_ratio = 1.0 - (unreachable_count / float(len(f_vals)))
                reachable_ratio = float(np.clip(reachable_ratio, 0.0, 1.0))

            # Combine all features: 19 dimensions
            v = [
                # Structural (4)
                num_states_norm,
                is_atomic_f,
                degree_norm,
                out_degree_norm,

                # Heuristic quality (4)
                avg_f_norm,
                max_f_norm,
                heuristic_concentration,
                reachable_ratio,

                # Abstraction (3)
                num_vars_norm,
                iter_idx_norm,
                centrality_norm,

                # F-statistics (4)
                f_min_norm,
                f_mean_norm,
                f_max_norm,
                f_std_norm,

                # Merge risk (2)
                neighbor_risk,
                float(np.clip(ns_raw / 10000.0, 0.0, 1.0)),  # Size indicator
            ]

            # Ensure all are valid floats in [0, 1]
            v = [float(np.clip(x, 0.0, 1.0)) for x in v]

            # Pad to fixed dimension
            v += [0.0] * max(0, self.feat_dim - len(v))

            node_feats.append(v)
            idx[nid] = i

        x = np.zeros((max_nodes, self.feat_dim), dtype=np.float32)
        for i, v in enumerate(node_feats[:max_nodes]):
            x[i] = v

        edges = [
            (idx[u], idx[v])
            for u, v in G.edges()
            if u in idx and v in idx
        ]
        ne = len(edges)

        ei = np.zeros((2, max_edges), dtype=np.int64)
        for j, (u, v) in enumerate(edges[:max_edges]):
            ei[0, j], ei[1, j] = u, v

        # ✅ NEW: Compute edge features
        try:
            edge_features = self._extract_edge_features()
        except Exception as e:
            logger.warning(f"Could not extract edge features: {e}")
            edge_features = np.zeros((0, 8), dtype=np.float32)

        # Pad edge features to [max_edges, 8]
        ef = np.zeros((max_edges, 8), dtype=np.float32)
        for j in range(min(edge_features.shape[0], max_edges)):
            ef[j] = edge_features[j]

        return {
            "x": x,
            "edge_index": ei,
            "edge_features": ef,  # ✅ NEW: Include edge features
            "num_nodes": np.int32(len(node_feats)),
            "num_edges": np.int32(ne),
        }

        # return {
        #     "x": x,
        #     "edge_index": ei,
        #     "num_nodes": np.int32(len(node_feats)),
        #     "num_edges": np.int32(ne),
        # }

    def _count_total_states(self) -> int:
        return sum(d["num_states"] for _, d in self.graph_tracker.graph.nodes(data=True))

    def close(self):
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                try:
                    self.process.wait(timeout=3.0)
                except subprocess.TimeoutExpired:
                    self.process.kill()
        except Exception:
            pass
        finally:
            self.process = None

        try:
            if self.fd_log_file:
                self.fd_log_file.flush()
                self.fd_log_file.close()
        except Exception:
            pass
        finally:
            self.fd_log_file = None

--------------------------------------------------------------------------------

The file common_utils.py code is this:
# -*- coding: utf-8 -*-
"""
COMPREHENSIVE CENTRAL UTILITIES FILE
Single source of truth for ALL shared code across the project.
"""

import os
import glob
import random
import logging
import json
import tempfile
from typing import List, Dict, Any, Optional, Tuple, Callable
from pathlib import Path

import numpy as np
from tqdm import tqdm
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import BaseCallback

import gymnasium as gym
from gymnasium import spaces

logger = logging.getLogger(__name__)

# ============================================================================
# 1. FAST DOWNWARD COMMAND TEMPLATE (CENTRALIZED)
# ============================================================================

# DOWNWARD_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "downward"))
#
# # Windows version
# FD_COMMAND_TEMPLATE = (
#     f'python "{DOWNWARD_DIR}\\builds\\release\\bin\\translate\\translate.py" '
#     r'"{domain}" "{problem}" --sas-file output.sas && '
#     f'"{DOWNWARD_DIR}\\builds\\release\\bin\\downward.exe" '
#     r'--search "astar(merge_and_shrink('
#     r'merge_strategy=merge_gnn(),'
#     r'shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),'
#     r'label_reduction=exact(before_shrinking=true,before_merging=false),'
#     r'max_states=4000,threshold_before_merge=1'
#     r'))"'
# )

# FILE: common_utils.py (UPDATE THIS)

import os
import sys

# Get absolute path to downward folder
DOWNWARD_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), "downward"))

# ✅ FIXED: Run translate and downward from the downward/ directory
# The key is to:
# 1. Use absolute paths for domain/problem (so they're found from downward/ cwd)
# 2. Run translate FIRST, verify output.sas exists
# 3. Then run downward to read that file
FD_COMMAND_TEMPLATE = (
    f'python "{DOWNWARD_DIR}\\builds\\release\\bin\\translate\\translate.py" '
    r'"{domain}" "{problem}" --sas-file output.sas && '
    f'"{DOWNWARD_DIR}\\builds\\release\\bin\\downward.exe" '
    r'--search "astar(merge_and_shrink('
    r'merge_strategy=merge_gnn(),'
    r'shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),'
    r'label_reduction=exact(before_shrinking=true,before_merging=false),'
    r'max_states=4000,threshold_before_merge=1'
    r'))" < output.sas'
)


# ============================================================================
# 2. SIMPLE SINGLE-PROBLEM ENVIRONMENT (NO MULTIENV)
# ============================================================================

# ============================================================================
# 2. SIMPLE SINGLE-PROBLEM ENVIRONMENT (FIXED)
# ============================================================================

class SimpleTrainingEnv(gym.Env):
    def __init__(self, domain_file: str, problem_file: str,
                 reward_variant: str = 'rich', debug: bool = False,
                 **reward_kwargs):
        super().__init__()

        from merge_env import MergeEnv

        # ✅ FIXED: Store environment as self.merge_env
        self.merge_env = MergeEnv(
            domain_file=domain_file,
            problem_file=problem_file,
            max_merges=50,
            debug=debug,
            reward_variant=reward_variant,
            **reward_kwargs
        )

        # ✅ CRITICAL: These MUST be set before training
        self.observation_space = self.merge_env.observation_space
        self.action_space = self.merge_env.action_space

        # ✅ ADD: Store reference to metadata
        self.metadata = {"render_modes": []}

    def reset(self, **kwargs):
        return self.merge_env.reset(**kwargs)
    def step(self, action):
        return self.merge_env.step(action)

    def close(self):
        try:
            self.merge_env.close()
        except:
            pass

    def render(self, mode='human'):
        pass


# ============================================================================
# 3. CALLBACKS (UNIFIED)
# ============================================================================

class SimpleProgressCallback(BaseCallback):
    """✅ FIXED: Simple progress tracking without issues."""

    def __init__(self, total_steps: int):
        super().__init__()
        self.total_steps = total_steps
        self.pbar = None

    def _on_training_start(self):
        """Initialize progress bar."""
        self.pbar = tqdm(total=self.total_steps, desc="Training", unit="steps")

    def _on_step(self) -> bool:
        """Update progress bar."""
        if self.pbar:
            self.pbar.update(1)
        return True

    def _on_training_end(self):
        """Close progress bar."""
        if self.pbar:
            self.pbar.close()


# ============================================================================
# 4. CHECKPOINT UTILITIES
# ============================================================================

def find_latest_checkpoint(checkpoint_dir: str) -> Optional[str]:
    """Finds the latest checkpoint by step count."""
    if not os.path.isdir(checkpoint_dir):
        return None

    checkpoints = glob.glob(os.path.join(checkpoint_dir, "*_steps.zip"))
    if not checkpoints:
        return None

    try:
        latest = max(checkpoints, key=lambda f: int(f.split('_')[-2]))
        logger.info(f"Found checkpoint: {latest}")
        return latest
    except (ValueError, IndexError):
        logger.warning(f"Could not parse checkpoint names in {checkpoint_dir}")
        return None


# ============================================================================
# 5. TRAINING WORKFLOW (COMPLETE & FIXED)
# ============================================================================

def train_model(
        model_save_path: str,
        benchmarks: List[Tuple[str, str]],
        hyperparams: Dict[str, Any],
        total_timesteps: int = 500,
        tb_log_dir: str = "tb_logs/",
        tb_log_name: str = "MVP_Training",
        debug_mode: bool = True,
        max_states: int = 4000,
        threshold_before_merge: int = 1,
        reward_variant: str = 'astar_search',
) -> Optional[PPO]:
    """
    Train a GNN policy using RL with REAL Fast Downward feedback.

    Args:
        model_save_path: Path to save the trained model
        benchmarks: List of (domain_file, problem_file) tuples
        hyperparams: Dictionary of PPO and reward function hyperparameters
        total_timesteps: Total training timesteps
        tb_log_dir: TensorBoard log directory
        tb_log_name: TensorBoard run name
        debug_mode: If True, use debug mode (no real FD)
        max_states: M&S max_states parameter
        threshold_before_merge: M&S threshold parameter
        reward_variant: Which reward function to use

    Returns:
        Trained PPO model, or None if training failed
    """

    # ✅ STEP 1: Validate and extract reward parameters
    valid_variants = [
        'simple_stability',
        'information_preservation',
        'hybrid',
        'conservative',
        'progressive',
        'rich',
        'astar_search'
    ]

    if reward_variant not in valid_variants:
        logger.error(f"Invalid reward variant: {reward_variant}")
        logger.error(f"Valid options: {', '.join(valid_variants)}")
        return None

    # ✅ STEP 2: Extract reward-specific kwargs from hyperparams
    reward_kwargs = {}

    # Define all possible keys for each variant
    reward_param_map = {
        'rich': ['w_f_stability', 'w_state_efficiency', 'w_transition_quality', 'w_reachability'],
        'astar_search': ['w_search_efficiency', 'w_solution_quality', 'w_f_stability', 'w_state_control'],
        'hybrid': ['w_f_stability', 'w_state_control', 'w_transition', 'w_search'],
        'simple_stability': ['alpha', 'beta', 'lambda_shrink', 'f_threshold'],
        'information_preservation': ['alpha', 'beta', 'lambda_density'],
        'conservative': ['stability_threshold'],
        'progressive': [],  # No special params, uses defaults
    }

    # Extract parameters for this variant
    if reward_variant in reward_param_map:
        for key in reward_param_map[reward_variant]:
            if key in hyperparams:
                reward_kwargs[key] = hyperparams[key]

    logger.info(f"\n{'=' * 80}")
    logger.info(f"REWARD VARIANT: {reward_variant}")
    logger.info(f"{'=' * 80}")
    if reward_kwargs:
        logger.info("Reward function parameters:")
        for k, v in reward_kwargs.items():
            logger.info(f"  {k:<30} = {v}")
    else:
        logger.info("(Using default parameters for reward function)")
    logger.info(f"{'=' * 80}\n")

    # ✅ STEP 3: Create environment with reward variant
    from merge_env import MergeEnv

    if not benchmarks or len(benchmarks) == 0:
        logger.error("No benchmarks provided!")
        return None

    domain_file, problem_file = benchmarks[0]

    logger.info(f"Creating environment with reward_variant={reward_variant}...")
    logger.info(f"  Domain:  {domain_file}")
    logger.info(f"  Problem: {problem_file}")

    env = MergeEnv(
        domain_file=domain_file,
        problem_file=problem_file,
        max_merges=50,
        debug=debug_mode,
        reward_variant=reward_variant,
        max_states=max_states,
        threshold_before_merge=threshold_before_merge,
        **reward_kwargs
    )

    env = Monitor(env)
    logger.info("✓ Environment created and wrapped with Monitor")

    # ✅ STEP 4: Create and train model
    from gnn_policy import GNNPolicy

    logger.info("Creating PPO model with GNN policy...")

    model = PPO(
        policy=GNNPolicy,
        env=env,
        learning_rate=hyperparams.get('learning_rate', 0.0003),
        n_steps=hyperparams.get('n_steps', 64),
        batch_size=hyperparams.get('batch_size', 32),
        ent_coef=hyperparams.get('ent_coef', 0.01),
        verbose=1,
        tensorboard_log=tb_log_dir,
        policy_kwargs={"hidden_dim": 64},
    )

    logger.info("✓ PPO model created")
    logger.info(f"\nStarting training for {total_timesteps} timesteps...")
    logger.info(f"Reward variant: {reward_variant}\n")

    try:
        model.learn(
            total_timesteps=total_timesteps,
            tb_log_name=tb_log_name,
            reset_num_timesteps=True,
        )
        logger.info(f"✓ Training complete")
    except KeyboardInterrupt:
        logger.warning("⚠️ Training interrupted by user")
    except Exception as e:
        logger.error(f"❌ Training failed with error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        env.close()
        return None

    # Save model
    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)
    model.save(model_save_path)
    logger.info(f"✓ Model saved: {model_save_path}")

    env.close()
    return model


# ============================================================================
# 6. BENCHMARK LOADING
# ============================================================================

def load_benchmarks_from_pattern(
        domain_file: str,
        problem_pattern: str,
        set_name: str = "Unknown"
) -> List[Tuple[str, str]]:
    """Loads benchmark problems matching a glob pattern."""
    if not os.path.exists(domain_file):
        logger.warning(f"Domain file not found: {domain_file}")
        return []

    problems = sorted(glob.glob(problem_pattern))
    if not problems:
        logger.warning(f"No problems found matching: {problem_pattern}")
        return []

    benchmarks = [(domain_file, p) for p in problems]
    logger.info(f"{set_name}: Loaded {len(benchmarks)} problems")
    return benchmarks


# ============================================================================
# 7. JSON UTILITIES
# ============================================================================

def write_json_atomic(obj: Any, path: str) -> None:
    """Atomically writes JSON to a file."""
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    fd, tmp_path = tempfile.mkstemp(
        dir=os.path.dirname(path) or ".",
        suffix=".tmp"
    )
    try:
        with os.fdopen(fd, "w") as f:
            json.dump(obj, f)
            f.flush()
            os.fsync(f.fileno())
        os.replace(tmp_path, path)
    except:
        try:
            os.remove(tmp_path)
        except:
            pass
        raise

--------------------------------------------------------------------------------

The file reward_function_variants.py code is this:
# -*- coding: utf-8 -*-
"""
REWARD FUNCTION VARIANTS WITH BAD MERGE DETECTION
==================================================

This file contains multiple reward function implementations for merge strategy learning.
Each variant emphasizes different aspects of merge quality, with enhanced bad merge detection.
"""

import numpy as np
import logging
from typing import Dict, Tuple, Optional, List
from reward_info_extractor import MergeInfo

logger = logging.getLogger(__name__)


class RewardFunctionBase:
    """Base class for all reward functions with bad merge detection."""

    def __init__(self, name: str):
        self.name = name
        self.component_values = {}
        self.bad_merge_reasons: List[str] = []

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """
        Computes reward for a merge.

        Args:
            merge_info: Extracted merge information
            search_expansions: Number of states expanded so far
            plan_cost: Current plan cost (if terminal)
            is_terminal: Whether this is the final merge

        Returns:
            Scalar reward value (may be heavily penalized if bad merge detected)
        """
        raise NotImplementedError

    def get_components_dict(self) -> Dict[str, float]:
        """Returns the constituent components of the reward for logging."""
        return self.component_values.copy()

    def _log_bad_merge_detected(self, reason: str) -> None:
        """Log a bad merge detection event."""
        self.bad_merge_reasons.append(reason)
        logger.warning(f"  ⚠️  BAD MERGE DETECTED: {reason}")


class SimpleStabilityReward(RewardFunctionBase):
    """VARIANT 1: Simple reward - penalizes F-value changes and state explosion."""

    def __init__(self, alpha: float = 1.0, beta: float = 0.1, lambda_shrink: float = 0.02,
                 f_threshold: float = 5.0):
        super().__init__("SimpleStabilityReward")
        self.alpha = alpha
        self.beta = beta
        self.lambda_shrink = lambda_shrink
        self.f_threshold = f_threshold

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Compute reward with bad merge detection."""

        logger.info(f"\n[REWARD] Computing {self.name}")

        # Component 1: F-value stability
        f_stability_term = self.alpha * merge_info.f_value_stability
        logger.info(f"  [1] F-stability: {f_stability_term:.4f}")

        # Component 2: State explosion
        state_explosion = merge_info.state_explosion_penalty
        state_term = -self.lambda_shrink * state_explosion
        logger.info(f"  [2] State explosion penalty: {state_term:.4f}")

        # Component 3: Search effort
        if search_expansions > 0:
            exp_norm = min(search_expansions / 200_000.0, 1.0)
            exp_term = -self.beta * exp_norm
        else:
            exp_term = 0.0
        logger.info(f"  [3] Search effort: {exp_term:.4f}")

        # Combine
        reward = f_stability_term + state_term + exp_term
        logger.info(f"  Base reward: {reward:.4f}")

        # ✅ BAD MERGE DETECTION
        bad_merge_penalty = self._detect_bad_merges(merge_info)
        reward += bad_merge_penalty

        if bad_merge_penalty < 0:
            logger.warning(f"  ⚠️  Applied bad merge penalty: {bad_merge_penalty:.4f}")
            logger.warning(f"  Final reward: {reward:.4f}")

        self.component_values = {
            'f_stability': f_stability_term,
            'state_explosion': state_term,
            'search_effort': exp_term,
            'bad_merge_penalty': bad_merge_penalty,
            'total': reward
        }

        return reward

    def _detect_bad_merges(self, merge_info: MergeInfo) -> float:
        """✅ NEW: Detect bad merges and apply penalties."""
        penalty = 0.0

        # CHECK 1: State explosion (TS1_size × TS2_size explosion not controlled)
        expected_merged_size = merge_info.ts1_size * merge_info.ts2_size
        if merge_info.states_after > expected_merged_size * 1.2:
            # Shrinking didn't work as expected
            explosion_ratio = (merge_info.states_after - expected_merged_size) / max(expected_merged_size, 1)
            explosion_penalty = -0.5 * min(explosion_ratio, 2.0)
            penalty += explosion_penalty
            self._log_bad_merge_detected(f"State explosion not controlled: {merge_info.states_after} vs {expected_merged_size}")

        # CHECK 2: F-stability degradation (low preservation)
        if merge_info.f_value_stability < 0.3:
            penalty -= 0.8
            self._log_bad_merge_detected(f"Critical F-value degradation: {merge_info.f_value_stability:.4f}")

        # CHECK 3: Many unreachable states
        reachable_count = sum(1 for f in merge_info.f_after if f != float('inf') and f < 1_000_000_000)
        unreachable_ratio = 1.0 - (reachable_count / max(merge_info.states_after, 1))
        if unreachable_ratio > 0.7:
            penalty -= 1.0
            self._log_bad_merge_detected(f"High unreachability: {unreachable_ratio*100:.1f}% unreachable")

        # CHECK 4: Significant F-value changes (unstable)
        if merge_info.num_significant_f_changes > merge_info.states_after * 0.5:
            penalty -= 0.3
            self._log_bad_merge_detected(f"Significant F-changes: {merge_info.num_significant_f_changes} changes")

        return penalty


class InformationPreservationReward(RewardFunctionBase):
    """VARIANT 2: Information preservation - minimizes heuristic quality loss."""

    def __init__(self, alpha: float = 2.0, beta: float = 0.05, lambda_density: float = 0.1):
        super().__init__("InformationPreservationReward")
        self.alpha = alpha
        self.beta = beta
        self.lambda_density = lambda_density

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Reward information preservation with bad merge detection."""

        info_preserve = self.alpha * merge_info.f_value_stability
        state_penalty = -self.beta * (merge_info.delta_states / max(merge_info.states_before, 10))
        transition_penalty = -self.lambda_density * abs(merge_info.transition_density_change)

        reward = info_preserve + state_penalty + transition_penalty

        # ✅ BAD MERGE DETECTION
        bad_merge_penalty = self._detect_bad_merges(merge_info)
        reward += bad_merge_penalty

        self.component_values = {
            'info_preservation': info_preserve,
            'state_penalty': state_penalty,
            'transition_penalty': transition_penalty,
            'bad_merge_penalty': bad_merge_penalty,
            'total': reward
        }

        return reward

    def _detect_bad_merges(self, merge_info: MergeInfo) -> float:
        """Detect bad merges in information preservation context."""
        penalty = 0.0

        # Very low F-stability is catastrophic for information preservation
        if merge_info.f_value_stability < 0.2:
            penalty = -1.5
            self._log_bad_merge_detected(f"Critical info loss: f_stability={merge_info.f_value_stability:.4f}")

        # Explosive state growth
        if merge_info.delta_states > merge_info.states_before * 2.0:
            penalty -= 0.7
            self._log_bad_merge_detected(f"State count tripled: {merge_info.states_before} → {merge_info.states_after}")

        # Transition density explosion
        if merge_info.transition_density_change > 0.5:
            penalty -= 0.4
            self._log_bad_merge_detected(f"Transition density explosion: +{merge_info.transition_density_change:.2f}")

        return penalty


class HybridMergeQualityReward(RewardFunctionBase):
    """VARIANT 3: Hybrid - balances multiple quality metrics."""

    def __init__(self, w_f_stability: float = 0.4, w_state_control: float = 0.3,
                 w_transition: float = 0.1, w_search: float = 0.2):
        super().__init__("HybridMergeQualityReward")

        total_w = w_f_stability + w_state_control + w_transition + w_search
        self.w_f_stability = w_f_stability / total_w
        self.w_state_control = w_state_control / total_w
        self.w_transition = w_transition / total_w
        self.w_search = w_search / total_w

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Weighted combination with bad merge detection."""

        f_score = merge_info.f_value_stability

        if merge_info.states_before > 0:
            explosion_ratio = merge_info.delta_states / merge_info.states_before
            state_score = max(0, 1.0 - abs(explosion_ratio))
        else:
            state_score = 0.5

        trans_change = merge_info.transition_density_change
        transition_score = 0.5 - min(0.5, trans_change / 2.0) if trans_change >= 0 else 0.5
        transition_score = float(np.clip(transition_score, 0.0, 1.0))

        total_states_after = merge_info.states_after
        if total_states_after > 0:
            valid_f_count = sum(1 for f in merge_info.f_after if f != float('inf'))
            reachability_score = valid_f_count / total_states_after
        else:
            reachability_score = 0.5

        composite = (
            self.w_f_stability * f_score +
            self.w_state_control * state_score +
            self.w_transition * transition_score +
            self.w_search * (1.0 - min(search_expansions / 100_000.0, 1.0))
        )

        reward = 2.0 * composite - 1.0

        # ✅ BAD MERGE DETECTION
        bad_merge_penalty = self._detect_bad_merges(merge_info)
        reward += bad_merge_penalty

        self.component_values = {
            'f_stability': f_score,
            'state_control': state_score,
            'transition': transition_score,
            'composite': composite,
            'bad_merge_penalty': bad_merge_penalty,
            'total': float(reward)
        }

        return float(reward)

    def _detect_bad_merges(self, merge_info: MergeInfo) -> float:
        """Detect bad merges in hybrid context."""
        penalty = 0.0

        # Multiple bad indicators simultaneously
        issues_count = 0

        if merge_info.f_value_stability < 0.35:
            issues_count += 1
        if merge_info.delta_states > merge_info.states_before * 1.5:
            issues_count += 1
        if merge_info.transition_density_change > 0.3:
            issues_count += 1

        # Cumulative penalty for multiple issues
        if issues_count >= 2:
            penalty = -0.5 * issues_count
            self._log_bad_merge_detected(f"Multiple quality issues detected: {issues_count} indicators")

        return penalty


class ConservativeReward(RewardFunctionBase):
    """VARIANT 4: Conservative - heavily penalizes risky merges."""

    def __init__(self, stability_threshold: float = 0.7):
        super().__init__("ConservativeReward")
        self.stability_threshold = stability_threshold

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Conservative approach with strict bad merge detection."""

        if merge_info.f_value_stability < self.stability_threshold:
            base_reward = -2.0
            self._log_bad_merge_detected(f"Below stability threshold: {merge_info.f_value_stability:.4f}")
        elif merge_info.delta_states > 0:
            explosion_ratio = merge_info.delta_states / max(merge_info.states_before, 1)
            base_reward = -0.5 * explosion_ratio
            if explosion_ratio > 0.5:
                self._log_bad_merge_detected(f"State explosion: {explosion_ratio*100:.1f}%")
        else:
            base_reward = merge_info.f_value_stability * 0.5

        reward = base_reward

        self.component_values = {
            'stability_check': base_reward,
            'total': reward
        }

        return reward


class ProgressiveReward(RewardFunctionBase):
    """VARIANT 5: Progressive - adapts based on episode progress."""

    def __init__(self):
        super().__init__("ProgressiveReward")
        self.merge_count = 0
        self._episode_initialized = False

    def reset_episode(self):
        """Call at episode start."""
        self.merge_count = 0
        self._episode_initialized = True

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Adapts reward based on merge count."""
        self.merge_count += 1
        progress = min(self.merge_count / 50.0, 1.0)

        if progress < 0.3:
            conservatism = 1.0
        elif progress < 0.7:
            conservatism = 0.5
        else:
            conservatism = 0.1

        stability_reward = merge_info.f_value_stability * 1.0
        state_penalty = -(1.0 - conservatism) * (
            merge_info.delta_states / max(merge_info.states_before, 1)
        )
        reward = stability_reward + state_penalty

        self.component_values = {
            'progress': progress,
            'conservatism': conservatism,
            'stability_reward': stability_reward,
            'state_penalty': state_penalty,
            'total': reward
        }

        return reward


class RichMergeQualityReward(RewardFunctionBase):
    """VARIANT 6: Rich - combines multiple quality signals."""

    def __init__(self,
                 w_f_stability: float = 0.35,
                 w_state_efficiency: float = 0.30,
                 w_transition_quality: float = 0.20,
                 w_reachability: float = 0.15):
        super().__init__("RichMergeQualityReward")

        total_w = w_f_stability + w_state_efficiency + w_transition_quality + w_reachability
        self.w_f_stability = w_f_stability / total_w
        self.w_state_efficiency = w_state_efficiency / total_w
        self.w_transition_quality = w_transition_quality / total_w
        self.w_reachability = w_reachability / total_w

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """Rich reward with comprehensive bad merge detection."""

        f_stability_score = merge_info.f_value_stability

        if merge_info.states_before > 0:
            explosion_ratio = merge_info.delta_states / merge_info.states_before
            state_efficiency = max(-1.0, 1.0 - explosion_ratio)
        else:
            state_efficiency = 0.5

        trans_change = merge_info.transition_density_change
        if trans_change < 0:
            transition_quality = 0.5
        else:
            transition_quality = 0.5 - min(0.5, trans_change / 2.0)
        transition_quality = float(np.clip(transition_quality, 0.0, 1.0))

        total_states_after = merge_info.states_after
        if total_states_after > 0:
            valid_f_count = sum(1 for f in merge_info.f_after if f != float('inf'))
            reachability_score = valid_f_count / total_states_after
        else:
            reachability_score = 0.5

        composite = (
            self.w_f_stability * f_stability_score +
            self.w_state_efficiency * state_efficiency +
            self.w_transition_quality * transition_quality +
            self.w_reachability * reachability_score
        )

        reward = 2.0 * composite - 1.0

        # ✅ BAD MERGE DETECTION - Enhanced for rich variant
        bad_merge_penalty = self._detect_bad_merges(merge_info, composite)
        reward += bad_merge_penalty

        if is_terminal and merge_info.f_value_stability > 0.8:
            reward += 0.5

        self.component_values = {
            'f_stability': f_stability_score,
            'state_efficiency': state_efficiency,
            'transition_quality': transition_quality,
            'reachability': reachability_score,
            'composite': composite,
            'bad_merge_penalty': bad_merge_penalty,
            'total': float(reward)
        }

        return float(reward)

    def _detect_bad_merges(self, merge_info: MergeInfo, composite: float) -> float:
        """✅ COMPREHENSIVE: Detect bad merges across all dimensions."""
        penalty = 0.0

        # CHECK 1: F-stability catastrophic failure
        if merge_info.f_value_stability < 0.25:
            penalty -= 1.2
            self._log_bad_merge_detected(
                f"F-stability catastrophic: {merge_info.f_value_stability:.4f} (threshold: 0.25)")

        # CHECK 2: State explosion uncontrolled
        expected_product = merge_info.ts1_size * merge_info.ts2_size
        if merge_info.states_after > expected_product and merge_info.states_after > merge_info.states_before * 3:
            penalty -= 1.0
            self._log_bad_merge_detected(
                f"Uncontrolled state explosion: {merge_info.states_before} → {merge_info.states_after}")

        # CHECK 3: Most states became unreachable
        unreachable_count = sum(1 for f in merge_info.f_after if f == float('inf') or f >= 1_000_000_000)
        unreachable_ratio = unreachable_count / max(merge_info.states_after, 1)
        if unreachable_ratio > 0.8:
            penalty -= 1.5
            self._log_bad_merge_detected(
                f"Critical unreachability: {unreachable_ratio*100:.1f}% of states unreachable")

        # CHECK 4: Goal unreachability
        goal_reachable = any(
            f != float('inf') and f < 1_000_000_000
            for f in merge_info.f_after
        )
        if not goal_reachable:
            penalty -= 2.0
            self._log_bad_merge_detected("CRITICAL: Goal unreachable after merge!")

        # CHECK 5: Transition density explosion
        if merge_info.transition_density_change > 0.5:
            penalty -= 0.6
            self._log_bad_merge_detected(
                f"Transition density explosion: +{merge_info.transition_density_change:.2f}")

        # CHECK 6: Significant F-value instability
        if merge_info.num_significant_f_changes > merge_info.states_after * 0.6:
            penalty -= 0.8
            self._log_bad_merge_detected(
                f"High F-value instability: {merge_info.num_significant_f_changes} changes")

        # CHECK 7: Composite score tells us overall quality
        if composite < 0.2:
            penalty -= 0.4
            self._log_bad_merge_detected(f"Poor composite score: {composite:.4f}")

        return penalty


class AStarSearchReward(RewardFunctionBase):
    """
    ✅ ENHANCED: A*-informed reward with robust signal for long-term learning.

    Key improvements:
    - Robust F-value stability measurement
    - Softer bad merge penalties that preserve learning signal
    - Better A* signal normalization
    - Reward shaping for smoother learning curves
    - Success bonuses for terminal states
    """

    def __init__(self,
                 w_search_efficiency: float = 0.25,
                 w_solution_quality: float = 0.20,
                 w_f_stability: float = 0.40,
                 w_state_control: float = 0.15):
        super().__init__("EnhancedAStarSearchReward")

        # Normalize weights
        total_w = w_search_efficiency + w_solution_quality + w_f_stability + w_state_control
        self.w_search_efficiency = w_search_efficiency / total_w
        self.w_solution_quality = w_solution_quality / total_w
        self.w_f_stability = w_f_stability / total_w
        self.w_state_control = w_state_control / total_w

        # ✅ NEW: Calibration parameters for robust normalization
        self.bf_comfort_zone = 3.0  # BF values 1-3 are good
        self.depth_comfort_zone = 50  # Typical depth for many problems
        self.stability_threshold = 0.3  # Threshold for acceptable stability

    def compute(self, merge_info: MergeInfo, search_expansions: int = 0,
                plan_cost: int = 0, is_terminal: bool = False) -> float:
        """✅ ENHANCED: Robust A*-informed reward with better learning signal."""

        logger.info(f"\n[REWARD] Computing {self.name}")

        # ====================================================================
        # COMPONENT 1: SEARCH EFFICIENCY (Branching Factor)
        # ====================================================================

        # ✅ IMPROVED: More nuanced branching factor scoring
        bf = merge_info.branching_factor

        if bf < 1.0 or np.isnan(bf) or np.isinf(bf):
            bf_score = 0.5
            bf_reason = "invalid BF, using neutral"
        elif bf <= 1.1:
            bf_score = 1.0  # Optimal: nearly linear branching
            bf_reason = "optimal"
        elif bf <= self.bf_comfort_zone:
            # Smooth interpolation in comfort zone
            bf_score = 1.0 - (bf - 1.0) / (self.bf_comfort_zone - 1.0) * 0.3
            bf_reason = f"good (in comfort zone)"
        elif bf <= 6.0:
            # Tolerable range with increasing penalty
            bf_score = 0.7 - (bf - self.bf_comfort_zone) / (6.0 - self.bf_comfort_zone) * 0.4
            bf_reason = f"tolerable"
        else:
            # Beyond 6: still provide some credit for trying
            bf_score = max(0.1, 0.3 - (bf - 6.0) / 10.0)
            bf_reason = "high BF, minimal credit"

        bf_score = float(np.clip(bf_score, 0.0, 1.0))
        logger.info(f"  [1] BRANCHING FACTOR: {bf:.3f} → score {bf_score:.3f} ({bf_reason})")

        # ====================================================================
        # COMPONENT 2: SOLUTION QUALITY (Search Depth)
        # ====================================================================

        # ✅ IMPROVED: Depth scoring with solution bonus
        depth_score = 0.5  # Default
        solution_bonus = 0.0

        if merge_info.solution_found:
            depth = merge_info.search_depth

            if depth < 1:
                depth_score = 0.5
            elif depth <= self.depth_comfort_zone:
                # Good depths: log scale to avoid over-penalizing
                depth_score = 1.0 - np.log(depth + 1) / np.log(self.depth_comfort_zone + 1) * 0.5
                depth_score = float(np.clip(depth_score, 0.5, 1.0))
            else:
                # Beyond comfort zone: still positive credit
                depth_score = max(0.2, 0.5 - (np.log(depth + 1) - np.log(self.depth_comfort_zone + 1)) / 2.0)
                depth_score = float(np.clip(depth_score, 0.0, 0.5))

            # ✅ NEW: Solution found is genuinely good
            solution_bonus = 0.3
            logger.info(f"  [2] SOLUTION QUALITY: depth={depth} → score {depth_score:.3f} + bonus {solution_bonus:.3f}")
        else:
            depth_score = 0.2  # Penalty for no solution
            logger.info(f"  [2] SOLUTION QUALITY: NO SOLUTION → score {depth_score:.3f}")

        # ====================================================================
        # COMPONENT 3: F-VALUE STABILITY (CRITICAL FOR LEARNING)
        # ====================================================================

        # ✅ ENHANCED: Better F-stability measurement
        f_stability = self._compute_robust_f_stability(merge_info)
        logger.info(f"  [3] F-VALUE STABILITY: {f_stability:.3f}")

        # ✅ NEW: Stability-based learning signal
        if f_stability > 0.7:
            stability_bonus = 0.2
            stability_reason = "excellent stability"
        elif f_stability > 0.5:
            stability_bonus = 0.1
            stability_reason = "good stability"
        elif f_stability > 0.3:
            stability_bonus = 0.0
            stability_reason = "acceptable stability"
        else:
            stability_bonus = -0.1
            stability_reason = "poor stability (but not penalized heavily)"

        logger.info(f"      → {stability_reason}, bonus: {stability_bonus:.3f}")

        # ====================================================================
        # COMPONENT 4: STATE CONTROL
        # ====================================================================

        # ✅ IMPROVED: More sophisticated state explosion handling
        if merge_info.states_before > 0:
            explosion_ratio = merge_info.delta_states / merge_info.states_before

            if explosion_ratio < -0.5:
                # State reduction is excellent
                state_score = 1.0
                state_reason = "excellent reduction"
            elif explosion_ratio < 0:
                # Some reduction
                state_score = 0.8
                state_reason = "reduction"
            elif explosion_ratio < 0.3:
                # Moderate increase is acceptable
                state_score = 0.8 - explosion_ratio * 0.3
                state_reason = "minor increase (acceptable)"
            elif explosion_ratio < 1.0:
                # Noticeable increase
                state_score = 0.65 - (explosion_ratio - 0.3) * 0.2
                state_reason = "significant increase"
            else:
                # Major explosion: still provide small credit
                state_score = max(0.1, 0.4 - np.log(explosion_ratio + 1) * 0.2)
                state_reason = "major explosion (penalized)"
        else:
            state_score = 0.5
            state_reason = "no baseline"

        state_score = float(np.clip(state_score, 0.0, 1.0))
        logger.info(
            f"  [4] STATE CONTROL: ratio={merge_info.delta_states / max(merge_info.states_before, 1):.3f} → score {state_score:.3f} ({state_reason})")

        # ====================================================================
        # WEIGHTED COMBINATION
        # ====================================================================

        composite = (
                self.w_search_efficiency * bf_score +
                self.w_solution_quality * depth_score +
                self.w_f_stability * f_stability +
                self.w_state_control * state_score
        )

        logger.info(f"\n  [COMPOSITE]: {composite:.3f}")
        logger.info(f"    = {self.w_search_efficiency:.3f}*{bf_score:.3f} (bf)")
        logger.info(f"    + {self.w_solution_quality:.3f}*{depth_score:.3f} (depth)")
        logger.info(f"    + {self.w_f_stability:.3f}*{f_stability:.3f} (stability)")
        logger.info(f"    + {self.w_state_control:.3f}*{state_score:.3f} (state)")

        # Scale to [-1, 1]
        reward = 2.0 * composite - 1.0

        # ====================================================================
        # BONUSES & PENALTIES (MILD - Preserve Learning Signal)
        # ====================================================================

        # ✅ Solution bonus
        reward += solution_bonus
        logger.info(f"\n  [BONUSES]:")
        logger.info(f"    + Solution bonus: {solution_bonus:.3f}")

        # ✅ Stability bonus
        reward += stability_bonus
        logger.info(f"    + Stability bonus: {stability_bonus:.3f}")

        # ✅ BAD MERGE DETECTION (SOFT PENALTIES)
        bad_merge_penalty = self._detect_bad_merges_soft(merge_info)
        reward += bad_merge_penalty

        if bad_merge_penalty != 0.0:
            logger.info(f"    + Bad merge penalty: {bad_merge_penalty:.3f}")

        # ✅ TERMINAL BONUS
        if is_terminal and merge_info.solution_found:
            terminal_bonus = 0.3
            reward += terminal_bonus
            logger.info(f"    + Terminal bonus: {terminal_bonus:.3f}")

        # ====================================================================
        # FINAL CLIPPING & LOGGING
        # ====================================================================

        reward = float(np.clip(reward, -1.0, 1.0))

        logger.info(f"\n  [FINAL REWARD]: {reward:.4f}")
        logger.info(f"  [RANGE]: [-1.0, +1.0] (clipped)\n")

        # Store components
        self.component_values = {
            'bf_score': float(bf_score),
            'depth_score': float(depth_score),
            'solution_bonus': float(solution_bonus),
            'f_stability': float(f_stability),
            'stability_bonus': float(stability_bonus),
            'state_score': float(state_score),
            'bad_merge_penalty': float(bad_merge_penalty),
            'composite': float(composite),
            'total': float(reward)
        }

        return reward

    def _compute_robust_f_stability(self, merge_info: MergeInfo) -> float:
        """
        ✅ ENHANCED: Compute F-stability with robust handling of edge cases.

        This is the MOST IMPORTANT metric for learning quality.
        """

        # ✅ NEW: Better handling of valid vs invalid values
        f_before_valid = [
            f for f in (merge_info.f_before if merge_info.f_before else [])
            if f != float('inf') and f >= 0 and f < 1_000_000_000
        ]

        f_after_valid = [
            f for f in (merge_info.f_after if merge_info.f_after else [])
            if f != float('inf') and f >= 0 and f < 1_000_000_000
        ]

        # Edge case 1: No valid data
        if not f_before_valid or not f_after_valid:
            logger.debug(
                f"      [F-STABILITY] Insufficient valid data: {len(f_before_valid)} before, {len(f_after_valid)} after")
            return 0.5  # Neutral score

        # Edge case 2: Too few samples
        if len(f_after_valid) < 2:
            return 0.5

        # ✅ IMPROVED: Use median-based stability (robust to outliers)
        before_median = float(np.median(f_before_valid))
        after_median = float(np.median(f_after_valid))

        before_std = float(np.std(f_before_valid)) if len(f_before_valid) > 1 else 0.0
        after_std = float(np.std(f_after_valid)) if len(f_after_valid) > 1 else 0.0

        # Normalized change in median
        if before_median > 0:
            median_change = abs(after_median - before_median) / before_median
        else:
            median_change = 0.0

        # Normalized change in variance
        if before_std > 0:
            std_change = abs(after_std - before_std) / before_std
        else:
            std_change = 0.0 if after_std == 0 else 1.0

        # ✅ IMPROVED: Weighted combination (median is more important than std)
        change_metric = 0.7 * np.clip(median_change, 0, 1) + 0.3 * np.clip(std_change, 0, 1)

        # Convert to stability score: lower change = higher stability
        f_stability = max(0.0, 1.0 - change_metric)

        logger.debug(
            f"      [F-STABILITY] median: {before_median:.1f}→{after_median:.1f}, std: {before_std:.1f}→{after_std:.1f}")
        logger.debug(f"      [F-STABILITY] change_metric: {change_metric:.3f}, stability: {f_stability:.3f}")

        return float(np.clip(f_stability, 0.0, 1.0))

    def _detect_bad_merges_soft(self, merge_info: MergeInfo) -> float:
        """
        ✅ ENHANCED: Detect bad merges with SOFT penalties that preserve learning signal.

        Key: Don't over-penalize—let the main reward components handle it.
        Only penalize CRITICAL failures.
        """
        penalty = 0.0
        self.bad_merge_reasons = []

        # CRITICAL CHECK 1: Goal becomes unreachable
        goal_reachable = any(
            f != float('inf') and f < 1_000_000_000
            for f in (merge_info.f_after if merge_info.f_after else [])
        )

        if not goal_reachable:
            penalty -= 0.5  # ✅ MILD penalty for critical failure
            self._log_bad_merge_detected("Goal unreachable (CRITICAL)")
            return penalty  # Return early—this is catastrophic

        # CHECK 2: Very poor F-stability (only if REALLY bad)
        if merge_info.f_value_stability < 0.15:
            penalty -= 0.15  # ✅ VERY MILD
            self._log_bad_merge_detected(f"F-stability extremely poor: {merge_info.f_value_stability:.3f}")

        # CHECK 3: Unreachable states > 90% (indicates broken abstraction)
        unreachable_count = sum(
            1 for f in (merge_info.f_after if merge_info.f_after else [])
            if f == float('inf') or f >= 1_000_000_000
        )

        total_states = len(merge_info.f_after) if merge_info.f_after else 1
        unreachability_ratio = unreachable_count / max(total_states, 1)

        if unreachability_ratio > 0.9:
            penalty -= 0.1  # ✅ MILD: most states unreachable
            self._log_bad_merge_detected(f"High unreachability: {unreachability_ratio * 100:.1f}%")

        # ✅ REMOVED: Other penalties (let composite score handle them)

        return penalty

def create_reward_function(variant: str, **kwargs) -> RewardFunctionBase:
    """
    Creates a reward function instance by name.

    Supported variants:
    - 'simple_stability': Basic stability-focused reward
    - 'information_preservation': Preserve heuristic information
    - 'hybrid': Balance multiple metrics
    - 'conservative': Risk-averse approach
    - 'progressive': Adaptive based on progress
    - 'rich': Comprehensive multi-signal reward
    - 'astar_search': A*-informed with bad merge detection (RECOMMENDED)
    """
    variants = {
        'simple_stability': SimpleStabilityReward,
        'information_preservation': InformationPreservationReward,
        'hybrid': HybridMergeQualityReward,
        'conservative': ConservativeReward,
        'progressive': ProgressiveReward,
        'rich': RichMergeQualityReward,
        'astar_search': AStarSearchReward,
    }

    if variant not in variants:
        raise ValueError(f"Unknown variant: {variant}. Supported: {list(variants.keys())}")

    return variants[variant](**kwargs)

--------------------------------------------------------------------------------

The file reward_info_extractor.py code is this:
# FILE: reward_info_extractor.py (COMPLETE REPLACEMENT - CRITICAL FIXES)
import os
import json
import numpy as np
import logging
import time
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
import traceback

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s'
)
logger = logging.getLogger(__name__)

# ============================================================================
# CONSTANTS (MOVED TO TOP)
# ============================================================================

# Fast Downward's INF constant (typical value)
# This is the maximum possible distance in FD
LARGE_VALUE_THRESHOLD = 1_000_000_000  # 1 billion (clearly unreachable)
FD_INF_CANDIDATES = [
    2 ** 31 - 1,  # Typical on 32-bit systems
    2 ** 31,  # Alternative
    2 ** 32 - 1,  # Alternative
    999999999,  # Some versions use this
    1_000_000_000,  # Common threshold
    float('inf'),  # JSON Infinity (if properly encoded)
]


def is_unreachable_value(value: Any) -> bool:
    """
    ✅ FIXED: Properly detect unreachable states.

    A value is unreachable if:
    - It's NaN
    - It's infinity (float or large int)
    - It's negative (error code)
    """
    try:
        # Handle float('inf')
        if isinstance(value, float):
            if np.isnan(value) or np.isinf(value):
                return True

        # Handle integers that represent INF
        if isinstance(value, int):
            if value < 0:  # Negative = error/unreachable
                return True
            if value in FD_INF_CANDIDATES:  # Known INF encodings
                return True
            if value > LARGE_VALUE_THRESHOLD:  # Heuristic: suspiciously large
                return True

        return False
    except (TypeError, ValueError):
        return True


def is_valid_fvalue(value: Any) -> bool:
    """
    ✅ NEW: Validate that an F-value is actually valid.

    Valid F-values are:
    - Non-negative numbers
    - Less than a reasonable threshold (reachability assumption)
    - Not NaN or Inf
    """
    if is_unreachable_value(value):
        return False

    try:
        num_value = float(value)
        # Sanity check: F-values should be reasonable distances
        # (0 to ~1000 for typical planning problems)
        if num_value < 0 or num_value > LARGE_VALUE_THRESHOLD:
            return False
        return True
    except (TypeError, ValueError):
        return False


def safe_compute_average(values: List[Any], context: str = "") -> Tuple[float, int]:
    """
    ✅ NEW: Safely compute average with detailed validation.

    Returns:
        (average, valid_count)
    """
    if not values:
        logger.warning(f"[{context}] Empty value list for averaging")
        return 0.0, 0

    # Filter to only valid values
    valid_values = [v for v in values if is_valid_fvalue(v)]

    removed_count = len(values) - len(valid_values)
    if removed_count > 0:
        logger.info(f"[{context}] Removed {removed_count}/{len(values)} invalid F-values")

    if not valid_values:
        logger.error(f"[{context}] ALL values were invalid! Using 0.5 as default")
        return 0.5, 0

    # Compute with numpy (handles float conversion)
    avg = float(np.mean(np.array(valid_values, dtype=np.float32)))

    # Final sanity check
    if np.isnan(avg) or np.isinf(avg):
        logger.error(f"[{context}] Computed average is NaN/Inf! Values: {valid_values[:10]}")
        return 0.5, len(valid_values)

    return avg, len(valid_values)


@dataclass
class MergeInfo:
    """✅ COMPLETE: Container with ALL necessary fields."""
    iteration: int
    states_before: int
    states_after: int
    delta_states: int
    f_before: List[int]
    f_after: List[int]
    f_value_stability: float
    num_significant_f_changes: int
    avg_f_change: float
    max_f_change: float
    ts1_id: int
    ts2_id: int
    ts1_size: int
    ts2_size: int
    ts1_transitions: int
    ts2_transitions: int
    merged_transitions: int
    state_explosion_penalty: float
    f_preservation_score: float
    transition_density_change: float
    nodes_expanded: int = 0
    search_depth: int = 0
    solution_cost: int = 0
    branching_factor: float = 1.0
    solution_found: bool = False

    def to_dict(self) -> Dict[str, Any]:
        return {
            'iteration': self.iteration,
            'states_before': self.states_before,
            'states_after': self.states_after,
            'delta_states': self.delta_states,
            'f_value_stability': float(self.f_value_stability),
            'num_significant_f_changes': self.num_significant_f_changes,
            'avg_f_change': float(self.avg_f_change),
            'max_f_change': float(self.max_f_change),
            'state_explosion_penalty': float(self.state_explosion_penalty),
            'f_preservation_score': float(self.f_preservation_score),
            'transition_density_change': float(self.transition_density_change),
            'nodes_expanded': self.nodes_expanded,
            'search_depth': self.search_depth,
            'solution_cost': self.solution_cost,
            'branching_factor': float(self.branching_factor),
            'solution_found': self.solution_found,
        }

    def validate(self) -> Tuple[bool, List[str]]:
        """✅ ENHANCED: Comprehensive validation."""
        issues = []

        # Check numeric ranges
        if not (0 <= self.f_value_stability <= 1):
            issues.append(f"f_value_stability out of [0,1]: {self.f_value_stability}")

        if self.states_before <= 0 or self.states_after <= 0:
            issues.append(f"Invalid state counts: {self.states_before} → {self.states_after}")

        if self.branching_factor < 1.0:
            self.branching_factor = 1.0
            issues.append("Corrected branching_factor to 1.0")

        # Check for NaN/Inf
        for field in ['f_value_stability', 'avg_f_change', 'max_f_change', 'state_explosion_penalty',
                      'transition_density_change']:
            val = getattr(self, field)
            if isinstance(val, float):
                if np.isnan(val):
                    setattr(self, field, 0.0)
                    issues.append(f"Replaced NaN in {field}")
                elif np.isinf(val):
                    setattr(self, field, 1.0 if 'stability' in field or 'preservation' in field else 0.0)
                    issues.append(f"Replaced Inf in {field}")

        return len(issues) == 0, issues


class RewardInfoExtractor:
    """✅ COMPLETELY REFACTORED: Robust F-value extraction."""

    def __init__(self, fd_output_dir: str = "downward/fd_output"):
        self.fd_output_dir = fd_output_dir
        self.f_change_threshold = 5.0
        os.makedirs(fd_output_dir, exist_ok=True)
        logger.info(f"✓ RewardInfoExtractor initialized (output_dir: {fd_output_dir})")

    def extract_merge_info(self, iteration: int, timeout: float = 30.0) -> Optional[MergeInfo]:
        """✅ ENHANCED: Complete extraction with F-value validation."""

        logger.info(f"\n{'=' * 80}")
        logger.info(f"[EXTRACT] Iteration {iteration}: Extracting merge info")
        logger.info(f"{'=' * 80}")

        try:
            before_path = os.path.join(self.fd_output_dir, f"merge_before_{iteration}.json")
            after_path = os.path.join(self.fd_output_dir, f"merge_after_{iteration}.json")

            # ====================================================================
            # LOAD FILES
            # ====================================================================
            logger.info(f"\n[LOAD] Loading files...")
            logger.info(f"  - before: {before_path}")
            logger.info(f"  - after:  {after_path}")

            before_data = self._load_and_validate_json(before_path, 'before', timeout)
            after_data = self._load_and_validate_json(after_path, 'after', timeout)

            if before_data is None or after_data is None:
                logger.error(f"[LOAD] ✗ Failed to load before or after data")
                return None

            logger.info(f"[LOAD] ✓ Both files loaded successfully")

            # ====================================================================
            # EXTRACT F-VALUES (WITH COMPREHENSIVE VALIDATION)
            # ====================================================================
            logger.info(f"\n[F-VALUES] Extracting F-values...")

            ts1_f = before_data.get("ts1_f_values", [])
            ts2_f = before_data.get("ts2_f_values", [])
            f_after_raw = after_data.get("f_values", [])

            logger.info(f"  - Raw ts1_f length: {len(ts1_f)}")
            logger.info(f"  - Raw ts2_f length: {len(ts2_f)}")
            logger.info(f"  - Raw f_after length: {len(f_after_raw)}")

            # ✅ AGGRESSIVE FILTERING: Remove all unreachable values
            logger.info(f"\n[VALIDATE] Filtering invalid F-values...")

            ts1_valid = [f for f in ts1_f if is_valid_fvalue(f)]
            logger.info(f"  - ts1_valid: {len(ts1_valid)} / {len(ts1_f)} "
                        f"(removed {len(ts1_f) - len(ts1_valid)})")

            ts2_valid = [f for f in ts2_f if is_valid_fvalue(f)]
            logger.info(f"  - ts2_valid: {len(ts2_valid)} / {len(ts2_f)} "
                        f"(removed {len(ts2_f) - len(ts2_f) - len(ts2_valid)})")

            f_after_valid = [f for f in f_after_raw if is_valid_fvalue(f)]
            logger.info(f"  - after_valid: {len(f_after_valid)} / {len(f_after_raw)} "
                        f"(removed {len(f_after_raw) - len(f_after_valid)})")

            # ====================================================================
            # TS IDs
            # ====================================================================
            logger.info(f"\n[IDS] Extracting TS IDs...")
            ts1_id = before_data.get("ts1_id", -1)
            ts2_id = before_data.get("ts2_id", -1)

            logger.info(f"  - ts1_id: {ts1_id}")
            logger.info(f"  - ts2_id: {ts2_id}")

            if ts1_id == ts2_id:
                logger.error(f"[IDS] ✗ TS1 and TS2 have same ID")
                return None

            logger.info(f"[IDS] ✓ IDs are different")

            # ====================================================================
            # F-STABILITY (WITH PROPER VALIDATION)
            # ====================================================================
            logger.info(f"\n[STABILITY] Computing F-stability...")

            product_mapping = before_data.get("product_mapping", {})
            ts1_size = before_data.get("ts1_size", 1)
            ts2_size = before_data.get("ts2_size", 1)

            logger.info(f"  - ts1_size: {ts1_size}, ts2_size: {ts2_size}")
            logger.info(f"  - Product space size: {ts1_size * ts2_size}")

            # ✅ PRODUCT MAPPING METHOD (if available)
            if product_mapping:
                logger.info(f"  - Using product state mapping")

                f_before_product = []
                for s in range(ts1_size * ts2_size):
                    s_key = str(s)
                    if s_key in product_mapping:
                        s1 = product_mapping[s_key].get("s1", -1)
                        s2 = product_mapping[s_key].get("s2", -1)

                        if (s1 >= 0 and s1 < len(ts1_f) and
                                s2 >= 0 and s2 < len(ts2_f)):
                            f1_val = ts1_f[s1]
                            f2_val = ts2_f[s2]

                            # ✅ ONLY add if BOTH are valid
                            if is_valid_fvalue(f1_val) and is_valid_fvalue(f2_val):
                                combined_f = max(float(f1_val), float(f2_val))
                                f_before_product.append(combined_f)

                logger.info(f"  - f_before_product length: {len(f_before_product)}")

                # ✅ SAFE AVERAGING
                if f_before_product and f_after_valid:
                    avg_f_before, cnt_before = safe_compute_average(f_before_product, "f_before_product")
                    avg_f_after, cnt_after = safe_compute_average(f_after_valid, "f_after")

                    logger.info(f"  - avg_f_before: {avg_f_before:.4f} ({cnt_before} valid states)")
                    logger.info(f"  - avg_f_after:  {avg_f_after:.4f} ({cnt_after} valid states)")

                    delta = avg_f_after - avg_f_before
                    logger.info(f"  - delta: {delta:.4f}")

                    if avg_f_before > 0:
                        f_stability = 1.0 - abs(delta) / avg_f_before
                        f_stability = float(np.clip(f_stability, 0.0, 1.0))
                        logger.info(f"  - f_stability: 1.0 - |{delta:.4f}| / {avg_f_before:.4f} = {f_stability:.4f}")
                    else:
                        f_stability = 0.5
                        logger.warning(f"  - avg_f_before is 0, using default 0.5")
                else:
                    avg_f_before = 1.0
                    avg_f_after = 1.0
                    f_stability = 0.5
                    logger.warning(f"  - Insufficient data for product mapping method")

            else:
                # ✅ FALLBACK: Weighted average
                logger.info(f"  - Using weighted average method")

                if ts1_valid and ts2_valid:
                    w1 = len(ts1_valid) / (len(ts1_valid) + len(ts2_valid))
                    w2 = len(ts2_valid) / (len(ts1_valid) + len(ts2_valid))
                    avg_f_before, _ = safe_compute_average(ts1_valid + ts2_valid, "weighted_before")

                    logger.info(f"  - w1: {w1:.4f}, w2: {w2:.4f}")
                    logger.info(f"  - weighted avg_f_before: {avg_f_before:.4f}")
                else:
                    avg_f_before = 1.0
                    logger.warning(f"  - One of ts1_valid or ts2_valid is empty")

                avg_f_after, _ = safe_compute_average(f_after_valid, "after")
                logger.info(f"  - avg_f_after: {avg_f_after:.4f}")

                if avg_f_before > 0:
                    f_stability = 1.0 - abs(avg_f_after - avg_f_before) / avg_f_before
                    f_stability = float(np.clip(f_stability, 0.0, 1.0))
                    logger.info(f"  - f_stability: {f_stability:.4f}")
                else:
                    f_stability = 0.5
                    logger.warning(f"  - avg_f_before is 0, using default 0.5")

            # ====================================================================
            # SIGNIFICANT CHANGES
            # ====================================================================
            logger.info(f"\n[CHANGES] Counting significant F-changes (threshold: {self.f_change_threshold})...")

            num_sig_changes = 0
            if ts1_valid and f_after_valid and len(ts1_valid) > 0 and len(f_after_valid) > 0:
                combined_before = ts1_valid + ts2_valid
                min_len = min(len(combined_before), len(f_after_valid))

                try:
                    abs_changes = np.abs(
                        np.array(combined_before[:min_len], dtype=np.float32) -
                        np.array(f_after_valid[:min_len], dtype=np.float32)
                    )
                    num_sig_changes = int(np.sum(abs_changes > self.f_change_threshold))
                    logger.info(f"  - Compared {min_len} values")
                    logger.info(f"  - Significant changes (> {self.f_change_threshold}): {num_sig_changes}")
                except Exception as e:
                    logger.warning(f"  - Error computing changes: {e}")
                    num_sig_changes = 0

            # ====================================================================
            # SIZES & TRANSITIONS
            # ====================================================================
            logger.info(f"\n[SIZES] Extracting sizes and transitions...")

            states_before = max(1, ts1_size * ts2_size)
            states_after = max(1, len(f_after_raw))
            delta_states = states_after - states_before

            logger.info(f"  - states_before: {states_before}")
            logger.info(f"  - states_after: {states_after}")
            logger.info(f"  - delta_states: {delta_states}")

            state_explosion = self._compute_state_explosion_penalty(delta_states, states_before)
            logger.info(f"  - state_explosion_penalty: {state_explosion:.4f}")

            ts1_trans = before_data.get("ts1_num_transitions", 0)
            ts2_trans = before_data.get("ts2_num_transitions", 0)
            merged_trans = after_data.get("num_transitions", 0)

            logger.info(f"  - ts1_transitions: {ts1_trans}")
            logger.info(f"  - ts2_transitions: {ts2_trans}")
            logger.info(f"  - merged_transitions: {merged_trans}")

            trans_density = self._compute_transition_density_change(
                ts1_trans, ts2_trans, merged_trans,
                ts1_size, ts2_size, states_after
            )
            logger.info(f"  - transition_density_change: {trans_density:.4f}")

            # ====================================================================
            # A* SIGNALS
            # ====================================================================
            logger.info(f"\n[A*] Extracting A* search signals...")

            search_signals = after_data.get("search_signals", {})
            logger.info(f"  - search_signals present: {bool(search_signals)}")

            nodes_expanded = int(search_signals.get("nodes_expanded", 0))
            search_depth = int(search_signals.get("search_depth", 0))
            solution_cost = int(search_signals.get("solution_cost", 0))
            branching_factor = float(search_signals.get("branching_factor", 1.0))
            solution_found = bool(search_signals.get("solution_found", False))

            logger.info(f"  - nodes_expanded: {nodes_expanded}")
            logger.info(f"  - search_depth: {search_depth}")
            logger.info(f"  - solution_cost: {solution_cost}")
            logger.info(f"  - branching_factor: {branching_factor:.4f}")
            logger.info(f"  - solution_found: {solution_found}")

            # Safety checks
            if nodes_expanded < 0:
                nodes_expanded = 0
            if search_depth < 0:
                search_depth = 0
            if solution_cost < 0:
                solution_cost = 0
            if branching_factor < 1.0 or np.isnan(branching_factor) or np.isinf(branching_factor):
                branching_factor = 1.0

            # ====================================================================
            # CREATE MERGE INFO
            # ====================================================================
            logger.info(f"\n[CREATE] Creating MergeInfo object...")

            # ✅ COMPUTE max_f_change safely
            if ts1_valid and ts2_valid:
                combined_before = np.array(ts1_valid + ts2_valid, dtype=np.float32)
                max_f_change = float(np.max(np.abs(combined_before - np.mean(combined_before))))
            else:
                max_f_change = 0.0

            # ✅ COMPUTE avg_f_change safely
            if len(ts1_valid) > 0 and len(ts2_valid) > 0:
                avg_f_before_tmp, _ = safe_compute_average(ts1_valid + ts2_valid, "for_delta")
                avg_f_after_tmp, _ = safe_compute_average(f_after_valid, "for_delta")
                avg_f_change = float(avg_f_before_tmp - avg_f_after_tmp)
            else:
                avg_f_change = 0.0

            merge_info = MergeInfo(
                iteration=iteration,
                states_before=states_before,
                states_after=states_after,
                delta_states=delta_states,
                f_before=ts1_f + ts2_f,
                f_after=f_after_raw,
                f_value_stability=f_stability,
                num_significant_f_changes=num_sig_changes,
                avg_f_change=avg_f_change,
                max_f_change=max_f_change,
                ts1_id=ts1_id,
                ts2_id=ts2_id,
                ts1_size=ts1_size,
                ts2_size=ts2_size,
                ts1_transitions=ts1_trans,
                ts2_transitions=ts2_trans,
                merged_transitions=merged_trans,
                state_explosion_penalty=state_explosion,
                f_preservation_score=f_stability,
                transition_density_change=trans_density,
                nodes_expanded=nodes_expanded,
                search_depth=search_depth,
                solution_cost=solution_cost,
                branching_factor=branching_factor,
                solution_found=solution_found,
            )

            logger.info(f"[CREATE] ✓ MergeInfo created")

            # Validate
            is_valid, issues = merge_info.validate()
            if issues:
                for issue in issues:
                    logger.info(f"  - {issue}")

            logger.info(f"\n[SUMMARY] Iteration {iteration}:")
            logger.info(f"  - TS{ts1_id}({ts1_size}) + TS{ts2_id}({ts2_size}) → {states_after} states")
            logger.info(f"  - f_stability={f_stability:.3f}")
            logger.info(f"  - A* nodes_expanded={nodes_expanded}")
            logger.info(f"  - solution_found={solution_found}")

            return merge_info

        except Exception as e:
            logger.error(f"\n[ERROR] Failed to extract merge info for iteration {iteration}: {e}")
            logger.error(f"Traceback: {traceback.format_exc()}")
            return None

    def _load_and_validate_json(self, path: str, phase: str, timeout: float = 30.0) -> Optional[Dict]:
        """✅ Load JSON with timeout and basic validation."""
        start_time = time.time()

        while time.time() - start_time < timeout:
            if not os.path.exists(path):
                time.sleep(0.1)
                continue

            try:
                with open(path, 'r') as f:
                    data = json.load(f)

                if not isinstance(data, dict):
                    logger.warning(f"JSON at {path} is not a dict")
                    return None

                return data

            except json.JSONDecodeError as e:
                logger.debug(f"JSON parse error at {path}: {e}, retrying...")
                time.sleep(0.1)

        logger.error(f"Timeout waiting for valid JSON at {path}")
        return None

    def _compute_state_explosion_penalty(self, delta_states: int, states_before: int) -> float:
        """Compute state explosion penalty safely."""
        if states_before <= 0:
            return 0.0
        pct_increase = delta_states / float(max(states_before, 1))
        penalty = min(1.0, max(0.0, pct_increase / 0.5))
        return float(penalty)

    def _compute_transition_density_change(self, ts1_trans: int, ts2_trans: int,
                                           merged_trans: int, ts1_size: int,
                                           ts2_size: int, merged_size: int) -> float:
        """Change in transition density."""
        if ts1_size <= 0 or ts2_size <= 0 or merged_size <= 0:
            return 0.0
        try:
            density_before_ts1 = ts1_trans / float(ts1_size)
            density_before_ts2 = ts2_trans / float(ts2_size)
            density_after = merged_trans / float(merged_size)
            expected_density = (ts1_trans + ts2_trans) / float(max(merged_size, 1))
            density_change = density_after - expected_density
            return float(density_change)
        except Exception as e:
            logger.warning(f"Error computing transition density: {e}")
            return 0.0


def validate_extracted_info(merge_info: MergeInfo) -> Tuple[bool, List[str]]:
    """✅ COMPLETE: Full validation of extracted info."""
    if merge_info is None:
        return False, ["merge_info is None"]

    return merge_info.validate()

--------------------------------------------------------------------------------

The file train_real_working.py code is this:
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MULTI-PROBLEM REAL TRAINING - GNN Merge Strategy with Dataset Support
======================================================================

Trains a GNN policy using actual Fast Downward with multiple problems across
difficulty levels (small, medium, hard).

Features:
  ✓ Load problems from benchmarks/small|medium|hard/ structure
  ✓ Train on single or multiple difficulty levels
  ✓ Curriculum learning (easy→medium→hard)
  ✓ Mixed training (random difficulty per episode)
  ✓ Full compatibility with existing framework
  ✓ Comprehensive logging and diagnostics

Run with:
    python train_real_working.py

Environment Variables:
    REWARD_VARIANT: Which reward function to use
      Options: simple_stability, information_preservation, hybrid, conservative,
               progressive, rich, astar_search (default: astar_search)

    DIFFICULTY: Which difficulty to train on
      Options: small, medium, hard, mixed (default: mixed)

    MAX_PROBLEMS_PER_DIFFICULTY: Max problems per difficulty (default: 5)

    CURRICULUM_LEARNING: Enable curriculum learning (default: false)
      When true: train on small→medium→hard sequentially
      When false: random sampling from selected difficulties

Example:
    REWARD_VARIANT=astar_search DIFFICULTY=mixed python train_real_working.py
    CURRICULUM_LEARNING=true DIFFICULTY=mixed python train_real_working.py
"""

import sys
import os
import logging
import glob
import json
import traceback
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import random

# Setup paths
sys.path.insert(0, os.getcwd())
os.makedirs("downward/gnn_output", exist_ok=True)
os.makedirs("downward/fd_output", exist_ok=True)
os.makedirs("logs", exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("training_multi_problem.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


def print_section(title: str, symbol: str = "=", width: int = 90):
    """Print a formatted section header."""
    logger.info("\n" + symbol * width)
    logger.info(f"// {title.upper()}")
    logger.info(symbol * width + "\n")


def print_subsection(title: str):
    """Print a formatted subsection header."""
    logger.info("\n" + "-" * 80)
    logger.info(f">>> {title}")
    logger.info("-" * 80)


# ============================================================================
# PHASE 0: BENCHMARK LOADING
# ============================================================================

def load_benchmarks_from_folders() -> Dict[str, List[Tuple[str, str]]]:
    """
    Load benchmarks from benchmarks/small|medium|hard/ directory structure.

    Expected structure:
        benchmarks/
        ├── small/
        │   ├── domain.pddl
        │   ├── problem_small_00.pddl
        │   └── ...
        ├── medium/
        │   ├── domain.pddl
        │   ├── problem_medium_00.pddl
        │   └── ...
        └── hard/
            ├── domain.pddl
            ├── problem_hard_00.pddl
            └── ...

    Returns:
        Dict mapping difficulty → list of (domain_file, problem_file) tuples
    """
    print_section("PHASE 0: LOAD BENCHMARKS FROM FOLDER STRUCTURE")

    benchmarks_dir = os.path.abspath("benchmarks")

    if not os.path.isdir(benchmarks_dir):
        logger.error(f"Benchmarks directory not found: {benchmarks_dir}")
        logger.error("Expected structure:")
        logger.error("  benchmarks/")
        logger.error("  ├── small/")
        logger.error("  ├── medium/")
        logger.error("  └── hard/")
        return {}

    logger.info(f"Benchmarks directory: {benchmarks_dir}\n")

    difficulties = ["parking"]
                    # "small", "medium", "large"]
    all_benchmarks = {}

    for difficulty in difficulties:
        difficulty_dir = os.path.join(benchmarks_dir, difficulty)

        logger.info(f"Loading {difficulty.upper()} difficulty problems...")

        if not os.path.isdir(difficulty_dir):
            logger.warning(f"  ⚠️ Directory not found: {difficulty_dir}")
            all_benchmarks[difficulty] = []
            continue

        # Find domain file
        domain_file = os.path.join(difficulty_dir, "domain.pddl")
        if not os.path.exists(domain_file):
            logger.warning(f"  ⚠️ Domain file not found: {domain_file}")
            all_benchmarks[difficulty] = []
            continue

        logger.info(f"  ✓ Domain: {domain_file}")

        # Find problem files (look for problem_*.pddl patterns)
        problem_patterns = [
            f"problem_{difficulty}_*.pddl",
            f"problem_*.pddl"
        ]

        problems = []
        for pattern in problem_patterns:
            problems = sorted(glob.glob(os.path.join(difficulty_dir, pattern)))
            if problems:
                break

        if not problems:
            logger.warning(f"  ⚠️ No problem files found in {difficulty_dir}")
            all_benchmarks[difficulty] = []
            continue

        logger.info(f"  ✓ Found {len(problems)} problem(s)")
        for i, prob in enumerate(problems[:3], 1):  # Show first 3
            logger.info(f"    {i}. {os.path.basename(prob)}")
        if len(problems) > 3:
            logger.info(f"    ... and {len(problems) - 3} more")

        # Create benchmark list (absolute paths)
        benchmarks_list = [
            (os.path.abspath(domain_file), os.path.abspath(prob))
            for prob in problems
        ]

        all_benchmarks[difficulty] = benchmarks_list
        logger.info(f"  ✓ Loaded {len(benchmarks_list)} benchmark(s) for {difficulty}\n")

    # Summary
    total = sum(len(b) for b in all_benchmarks.values())
    if total == 0:
        logger.error("No benchmarks loaded!")
        return {}

    logger.info(f"✅ Loaded {total} total benchmarks across all difficulties:")
    for difficulty in difficulties:
        count = len(all_benchmarks[difficulty])
        logger.info(f"   {difficulty:<10} {count:>3} problems")

    return all_benchmarks


def get_benchmark_sequence(
        all_benchmarks: Dict[str, List[Tuple[str, str]]],
        difficulty: str = "mixed",
        max_problems_per_difficulty: int = 5,
        curriculum_learning: bool = False
) -> List[Tuple[str, str]]:
    """
    Create a sequence of benchmark problems for training.

    Args:
        all_benchmarks: All loaded benchmarks by difficulty
        difficulty: "small", "medium", "hard", or "mixed"
        max_problems_per_difficulty: Max problems to use per difficulty
        curriculum_learning: If True, train small→medium→hard sequentially

    Returns:
        List of (domain_file, problem_file) tuples for training
    """
    print_subsection("Create Benchmark Sequence")

    sequence = []

    if difficulty == "mixed":
        difficulties = ["small", "medium", "hard"]
    else:
        difficulties = [difficulty]

    logger.info(f"Difficulty mode: {difficulty}")
    logger.info(f"Max problems per difficulty: {max_problems_per_difficulty}")
    logger.info(f"Curriculum learning: {curriculum_learning}\n")

    if curriculum_learning and difficulty == "mixed":
        # Curriculum: small → medium → hard
        logger.info("Using CURRICULUM LEARNING: small → medium → hard\n")

        for diff in difficulties:
            if diff not in all_benchmarks or not all_benchmarks[diff]:
                continue

            problems = all_benchmarks[diff][:max_problems_per_difficulty]
            sequence.extend(problems)
            logger.info(f"  Added {len(problems)} {diff} problems (total: {len(sequence)})")

    else:
        # Random mixing or single difficulty
        for diff in difficulties:
            if diff not in all_benchmarks or not all_benchmarks[diff]:
                continue

            problems = all_benchmarks[diff][:max_problems_per_difficulty]
            sequence.extend(problems)
            logger.info(f"  Added {len(problems)} {diff} problems")

        # Shuffle if mixed
        if difficulty == "mixed":
            random.shuffle(sequence)
            logger.info(f"\nShuffled sequence randomly")

    logger.info(f"\n✅ Total problems in sequence: {len(sequence)}")

    return sequence


def create_problem_iterator(
        benchmark_sequence: List[Tuple[str, str]],
        epochs: int = 1
):
    """
    Create an iterator that cycles through problems for multiple epochs.

    Args:
        benchmark_sequence: List of (domain, problem) tuples
        epochs: Number of times to cycle through the sequence

    Yields:
        (domain_file, problem_file, problem_name, epoch, step)
    """
    for epoch in range(epochs):
        for step, (domain_file, problem_file) in enumerate(benchmark_sequence):
            problem_name = os.path.basename(problem_file)
            yield domain_file, problem_file, problem_name, epoch + 1, step + 1


# ============================================================================
# PHASE 1: ENVIRONMENT INITIALIZATION
# ============================================================================

# FILE: train_real_working.py
# REPLACE THIS FUNCTION

def init_training_environment(
        domain_file: str,
        problem_file: str,
        reward_variant: str = "astar_search",
        **reward_kwargs  # ✅ ADD THIS
) -> Optional[Tuple]:
    """Initialize a training environment for a single problem."""
    try:
        from merge_env import MergeEnv

        domain_path = os.path.abspath(domain_file)
        problem_path = os.path.abspath(problem_file)

        logger.info(f"Creating environment...")
        logger.info(f"  Domain:  {domain_path}")
        logger.info(f"  Problem: {problem_path}")

        # ✅ FIX: Pass the reward_kwargs dictionary instead of hard-coded values
        env = MergeEnv(
            domain_file=domain_path,
            problem_file=problem_path,
            max_merges=50,
            debug=False,  # REAL FD
            reward_variant=reward_variant,
            **reward_kwargs # ✅ USE THE PASSED ARGUMENTS
        )

        logger.info("✓ Environment created\n")
        return env, domain_path, problem_path

    except Exception as e:
        logger.error(f"✗ Environment creation failed: {e}")
        logger.error(traceback.format_exc())
        return None


# ============================================================================
# PHASE 2: MULTI-PROBLEM TRAINING
# ============================================================================

# FILE: train_real_working.py
# REPLACE THIS FUNCTION

def run_multi_problem_training(
        benchmark_sequence: List[Tuple[str, str]],
        reward_variant: str = "astar_search",
        total_timesteps: int = 10000,
        timesteps_per_problem: int = 500,
        **reward_kwargs  # ✅ This was already here and is correct
) -> bool:
    """
    Run training on multiple problems with real FD.

    Args:
        benchmark_sequence: List of (domain, problem) tuples to train on
        reward_variant: Reward function variant
        total_timesteps: Total training timesteps (if 0, use problems-based limit)
        timesteps_per_problem: Timesteps to train on each problem
        **reward_kwargs: Reward function parameters

    Returns:
        True if training succeeded, False otherwise
    """
    print_section("PHASE 2: MULTI-PROBLEM TRAINING")

    try:
        from merge_env import MergeEnv
        from gnn_policy import GNNPolicy
        from stable_baselines3 import PPO
        from stable_baselines3.common.monitor import Monitor

        # ✅ NEW: Load or create model once for all problems
        model_path = "mvp_output/gnn_model.zip"
        model = None

        if os.path.exists(model_path):
            logger.info(f"Loading existing model: {model_path}")
            try:
                model = PPO.load(model_path)
                logger.info("✓ Model loaded, will continue training\n")
            except Exception as e:
                logger.warning(f"Could not load model: {e}")
                logger.warning("Will create new model\n")

        # Variables for tracking
        total_steps = 0
        problems_trained = 0

        # Iterate through problems
        for domain_file, problem_file, problem_name, epoch, problem_step in \
                create_problem_iterator(benchmark_sequence, epochs=1):

            print_subsection(f"Problem {problem_step}/{len(benchmark_sequence)}: {problem_name}")

            logger.info(f"Epoch {epoch}, Step {problem_step}")
            logger.info(f"Training timesteps for this problem: {timesteps_per_problem}\n")

            # ✅ FIX: Pass reward_kwargs to the init function
            result = init_training_environment(
                domain_file,
                problem_file,
                reward_variant,
                **reward_kwargs
            )

            if result is None:
                logger.error(f"Failed to initialize environment for {problem_name}")
                logger.error("Skipping this problem\n")
                continue

            env, domain_path, problem_path = result
            env = Monitor(env)

            # Create model if not loaded
            if model is None:
                logger.info("Creating new PPO model with GNN policy...")
                model = PPO(
                    policy=GNNPolicy,
                    env=env,
                    learning_rate=0.0003,
                    n_steps=64,
                    batch_size=32,
                    ent_coef=0.01,
                    verbose=0,
                    tensorboard_log="tb_logs/",
                    policy_kwargs={"hidden_dim": 64},
                )
                logger.info("✓ New model created\n")
            else:
                # Update model's environment
                model.set_env(env)

            # Train on this problem
            logger.info(f"Training for {timesteps_per_problem} timesteps...")
            try:
                model.learn(
                    total_timesteps=timesteps_per_problem,
                    tb_log_name=f"multi_problem_training_p{problem_step}",
                    reset_num_timesteps=False,
                )
                logger.info("✓ Training completed for this problem\n")

                total_steps += timesteps_per_problem
                problems_trained += 1

            except KeyboardInterrupt:
                logger.warning("\n⚠️ Training interrupted by user")
                break
            except Exception as e:
                logger.error(f"✗ Training failed for {problem_name}: {e}")
                logger.error(traceback.format_exc())
                continue
            finally:
                env.close()

            # Check if we've reached total timesteps limit
            if total_timesteps > 0 and total_steps >= total_timesteps:
                logger.info(f"\n✓ Reached total timesteps limit: {total_steps}/{total_timesteps}")
                break

        # Save final model
        if model is not None:
            model_filename = f"gnn_model_multi_problem_{reward_variant}.zip"
            model_path_out = f"mvp_output/{model_filename}"
            model.save(model_path_out)
            logger.info(f"\n✓ Final model saved to: {model_path_out}")
            logger.info(f"  Problems trained on: {problems_trained}")
            logger.info(f"  Total timesteps: {total_steps}")

        return True

    except Exception as e:
        logger.error(f"\n❌ Multi-problem training failed: {e}")
        logger.error(traceback.format_exc())
        return False


# ============================================================================
# PHASE 3: VALIDATION (OPTIONAL)
# ============================================================================

def validate_on_benchmark_sample(model, benchmarks_dict: Dict, n_problems: int = 3) -> bool:
    """
    Quick validation: test trained model on a sample of problems.

    Args:
        model: Trained PPO model
        benchmarks_dict: All loaded benchmarks
        n_problems: Number of problems to test on

    Returns:
        True if validation succeeded
    """
    print_section("PHASE 3: VALIDATION ON SAMPLE")

    try:
        from merge_env import MergeEnv

        if model is None:
            logger.warning("No model to validate")
            return False

        # Sample problems from each difficulty
        test_problems = []
        for difficulty in ["small", "medium", "hard"]:
            if difficulty in benchmarks_dict and benchmarks_dict[difficulty]:
                # Take first problem from each difficulty
                test_problems.append(benchmarks_dict[difficulty][0])

        logger.info(f"Testing on {len(test_problems)} sampled problems:\n")

        total_reward = 0.0
        episodes_completed = 0

        for i, (domain_file, problem_file) in enumerate(test_problems, 1):
            problem_name = os.path.basename(problem_file)
            logger.info(f"  [{i}] {problem_name}")

            try:
                env = MergeEnv(
                    domain_file=os.path.abspath(domain_file),
                    problem_file=os.path.abspath(problem_file),
                    max_merges=10,
                    debug=False,
                    reward_variant="astar_search",
                )

                obs, _ = env.reset()
                episode_reward = 0.0
                steps = 0

                while steps < 10:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, done, truncated, info = env.step(int(action))
                    episode_reward += reward
                    steps += 1

                    if done or truncated:
                        break

                total_reward += episode_reward
                episodes_completed += 1

                logger.info(f"      ✓ Reward: {episode_reward:+.4f} ({steps} steps)")
                env.close()

            except Exception as e:
                logger.warning(f"      ⚠️ Test failed: {e}")

        if episodes_completed > 0:
            avg_reward = total_reward / episodes_completed
            logger.info(f"\n✅ Average validation reward: {avg_reward:+.4f} ({episodes_completed} episodes)")
            return True
        else:
            logger.warning("No validation episodes completed")
            return False

    except Exception as e:
        logger.error(f"\n❌ Validation failed: {e}")
        logger.error(traceback.format_exc())
        return False


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Main execution pipeline."""
    print_section("MULTI-PROBLEM REAL TRAINING - GNN MERGE STRATEGY", "=", 95)

    # ✅ NEW: Get environment variables for multi-problem training
    reward_variant = os.environ.get('REWARD_VARIANT', 'astar_search')
    difficulty = os.environ.get('DIFFICULTY', 'parking').lower()
    max_problems = int(os.environ.get('MAX_PROBLEMS_PER_DIFFICULTY', '5'))
    curriculum_learning = os.environ.get('CURRICULUM_LEARNING', 'false').lower() == 'true'

    logger.info(f"Configuration:")
    logger.info(f"  Reward variant: {reward_variant}")
    logger.info(f"  Difficulty: {difficulty}")
    logger.info(f"  Max problems per difficulty: {max_problems}")
    logger.info(f"  Curriculum learning: {curriculum_learning}\n")

    # Phase 0: Load benchmarks from folder structure
    all_benchmarks = load_benchmarks_from_folders()
    if not all_benchmarks or sum(len(b) for b in all_benchmarks.values()) == 0:
        logger.error("No benchmarks loaded - aborting")
        return 1

    # Create benchmark sequence
    benchmark_sequence = get_benchmark_sequence(
        all_benchmarks,
        difficulty=difficulty,
        max_problems_per_difficulty=max_problems,
        curriculum_learning=curriculum_learning
    )

    if not benchmark_sequence:
        logger.error("No benchmarks in sequence - aborting")
        return 1

    # Phase 2: Run multi-problem training
    success = run_multi_problem_training(
        benchmark_sequence=benchmark_sequence,
        reward_variant=reward_variant,
        total_timesteps=5,  # ✅ NEW: Can be adjusted
        timesteps_per_problem=1,  # ✅ NEW: Timesteps per problem
    )

    if not success:
        logger.error("Training failed")
        return 1

    # Summary
    print_section("TRAINING COMPLETE", "=", 95)
    logger.info("✅ Multi-problem training pipeline completed successfully!\n")
    logger.info("Next steps:")
    logger.info("  1. View TensorBoard logs:")
    logger.info(f"     tensorboard --logdir={os.path.abspath('tb_logs/')}\n")
    logger.info("  2. Review training log:")
    logger.info(f"     {os.path.abspath('training_multi_problem.log')}\n")
    logger.info("  3. Trained models:")
    logger.info(f"     {os.path.abspath('mvp_output/')}\n")

    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)


--------------------------------------------------------------------------------

The file baseline_benchmarking.py code is this:
# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-
# """
# BASELINE BENCHMARKING HARNESS FOR GNN MERGE STRATEGY LEARNING
# ==============================================================
#
# This script benchmarks STANDARD Fast Downward algorithms on your problem set
# and produces a CSV report for comparison with your GNN policy.
#
# Usage:
#     python baseline_benchmarking.py
#
# Output:
#     baseline_performance_summary.csv
#
# This CSV can later be compared with your GNN policy performance metrics.
# """
#
# import sys
# import os
# import logging
# import glob
# import json
# import subprocess
# import time
# import re
# import tempfile
# import shutil
# from typing import List, Dict, Tuple, Optional, Any
# from pathlib import Path
# from datetime import datetime
# import csv
#
# # Setup
# sys.path.insert(0, os.getcwd())
#
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(levelname)-8s - [%(filename)s] - %(message)s',
#     handlers=[
#         logging.StreamHandler(sys.stdout),
#         logging.FileHandler("baseline_benchmarking.log", encoding='utf-8'),
#     ],
#     force=True
# )
# logger = logging.getLogger(__name__)
#
#
# # ============================================================================
# # CONFIGURATION: CRITICAL SETTINGS
# # ============================================================================
#
# class BenchmarkConfig:
#     """Central configuration for all benchmark settings."""
#
#     # Time limits
#     TIME_LIMIT_PER_RUN_S = 300  # 5 minutes per problem per planner
#
#     # Fast Downward locations (ABSOLUTE PATHS)
#     FD_TRANSLATE_BIN = os.path.abspath("downward/builds/release/bin/translate/translate.py")
#     FD_DOWNWARD_BIN = os.path.abspath("downward/builds/release/bin/downward.exe")
#
#     # Output CSV file
#     OUTPUT_CSV = "baseline_performance_summary.csv"
#
#     # Working directories
#     FD_TEMP_DIR = "baseline_temp"  # Temporary SAS files
#
#     # Define benchmark sets (problems grouped by domain)
#     # For your MVP: just one domain with one problem
#     BENCHMARK_SETS = {
#         "Small": {
#             "domain": "domain.pddl",
#             "problem_pattern": "problem_small_*.pddl",
#             "description": "Small 8-puzzle problems"
#         }
#     }
#
#     # Define baseline planners to test
#     # Each planner has: name + search config string
#     BASELINES = [
#         {
#             "name": "FD ASTAR LM-Cut",
#             "search_config": "astar(lmcut())"
#         },
#         {
#             "name": "FD ASTAR DFP",
#             "search_config": (
#                 "astar(merge_and_shrink("
#                 "merge_strategy=merge_dfp(),"
#                 "shrink_strategy=shrink_bisimulation(greedy=false),"
#                 "label_reduction=exact(before_shrinking=true,before_merging=false),"
#                 "max_states=50000,threshold_before_merge=1))"
#             )
#         },
#         {
#             "name": "FD ASTAR SCC-DFP",
#             "search_config": (
#                 "astar(merge_and_shrink("
#                 "merge_strategy=merge_sccs(order_of_sccs=topological,"
#                 "merge_selector=score_based_filtering("
#                 "scoring_functions=[goal_relevance,dfp,total_order])),"
#                 "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
#                 "label_reduction=exact(before_shrinking=true,before_merging=false),"
#                 "max_states=50000,threshold_before_merge=1))"
#             )
#         },
#         {
#             "name": "FD ASTAR MIASM",
#             "search_config": (
#                 "astar(merge_and_shrink("
#                 "merge_strategy=merge_miasm(scoring_function=miasm_utils("
#                 "shrink_strategy=shrink_bisimulation(greedy=true,at_limit=return))),"
#                 "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
#                 "label_reduction=exact(before_shrinking=true,before_merging=false),"
#                 "max_states=50000,threshold_before_merge=1))"
#             )
#         },
#     ]
#
#
# # ============================================================================
# # STEP 1: VERIFY SETUP
# # ============================================================================
#
# def verify_fd_setup() -> bool:
#     """Verify that Fast Downward is properly installed."""
#     logger.info("\n" + "=" * 80)
#     logger.info("STEP 1: VERIFY FAST DOWNWARD SETUP")
#     logger.info("=" * 80)
#
#     checks = [
#         ("FD Translate Script", BenchmarkConfig.FD_TRANSLATE_BIN),
#         ("FD Downward Binary", BenchmarkConfig.FD_DOWNWARD_BIN),
#     ]
#
#     all_ok = True
#     for name, path in checks:
#         if os.path.exists(path):
#             logger.info(f"  ✓ {name:<30} {path}")
#         else:
#             logger.error(f"  ✗ {name:<30} NOT FOUND: {path}")
#             all_ok = False
#
#     if not all_ok:
#         logger.error("\n❌ FD setup verification FAILED")
#         logger.error("   Please ensure Fast Downward is built in downward/builds/release/")
#         return False
#
#     logger.info("\n✅ FD setup verified")
#     return True
#
#
# # ============================================================================
# # STEP 2: LOAD BENCHMARKS
# # ============================================================================
#
# def load_all_benchmarks() -> Dict[str, List[Tuple[str, str]]]:
#     """
#     Load all benchmark problems from disk.
#
#     Returns:
#         Dictionary mapping set_name -> list of (domain_file, problem_file) tuples
#     """
#     logger.info("\n" + "=" * 80)
#     logger.info("STEP 2: LOAD BENCHMARK PROBLEMS")
#     logger.info("=" * 80)
#
#     all_benchmarks = {}
#
#     for set_name, set_config in BenchmarkConfig.BENCHMARK_SETS.items():
#         logger.info(f"\nLoading benchmark set: {set_name}")
#
#         domain_file = set_config["domain"]
#         problem_pattern = set_config["problem_pattern"]
#
#         # Check domain exists
#         if not os.path.exists(domain_file):
#             logger.error(f"  ✗ Domain file not found: {domain_file}")
#             return {}
#         logger.info(f"  ✓ Domain: {domain_file}")
#
#         # Find problems
#         problems = sorted(glob.glob(problem_pattern))
#         if not problems:
#             logger.error(f"  ✗ No problems found matching: {problem_pattern}")
#             return {}
#
#         logger.info(f"  ✓ Found {len(problems)} problem(s)")
#         for i, prob in enumerate(problems, 1):
#             logger.info(f"    {i}. {prob}")
#
#         # Create benchmark list (absolute paths)
#         benchmarks = [
#             (os.path.abspath(domain_file), os.path.abspath(prob))
#             for prob in problems
#         ]
#
#         all_benchmarks[set_name] = benchmarks
#
#     if not all_benchmarks:
#         logger.error("\n❌ No benchmarks loaded")
#         return {}
#
#     logger.info(f"\n✅ Loaded {len(all_benchmarks)} benchmark set(s)")
#     return all_benchmarks
#
#
# # ============================================================================
# # STEP 3: RUN SINGLE PROBLEM
# # ============================================================================
#
# def run_single_fd_problem(
#         domain_file: str,
#         problem_file: str,
#         search_config: str,
#         time_limit: int,
#         temp_dir: str
# ) -> Dict[str, Any]:
#     """
#     Run Fast Downward on a SINGLE problem with a SINGLE search configuration.
#
#     This is the CORE WORKER function. It performs 3 steps:
#     1. TRANSLATE: Convert PDDL to SAS
#     2. SEARCH: Run planner with search config
#     3. PARSE: Extract metrics from output
#
#     Args:
#         domain_file: Path to domain.pddl
#         problem_file: Path to problem_N.pddl
#         search_config: Search strategy string (e.g., "astar(lmcut())")
#         time_limit: Timeout in seconds
#         temp_dir: Directory for temporary SAS files
#
#     Returns:
#         Dictionary with metrics or error info
#     """
#     problem_name = os.path.basename(problem_file)
#
#     try:
#         # ====== STEP 3a: TRANSLATE ======
#         logger.debug(f"    [TRANSLATE] Starting for {problem_name}...")
#
#         sas_file = os.path.join(temp_dir, "output.sas")
#
#         translate_cmd = (
#             f'python "{BenchmarkConfig.FD_TRANSLATE_BIN}" '
#             f'"{domain_file}" "{problem_file}" '
#             f'--sas-file "{sas_file}"'
#         )
#
#         logger.debug(f"    [TRANSLATE] Command: {translate_cmd[:100]}...")
#
#         result = subprocess.run(
#             translate_cmd,
#             shell=True,
#             cwd=temp_dir,
#             capture_output=True,
#             text=True,
#             timeout=time_limit
#         )
#
#         if result.returncode != 0:
#             logger.debug(f"    [TRANSLATE] Failed: {result.stderr[:200]}")
#             return {
#                 "solved": False,
#                 "reason": "translate_error",
#                 "error": result.stderr[:500]
#             }
#
#         if not os.path.exists(sas_file):
#             logger.debug(f"    [TRANSLATE] output.sas not created")
#             return {
#                 "solved": False,
#                 "reason": "translate_no_output"
#             }
#
#         logger.debug(f"    [TRANSLATE] Success ({os.path.getsize(sas_file)} bytes)")
#
#         # ====== STEP 3b: SEARCH ======
#         logger.debug(f"    [SEARCH] Starting with config: {search_config[:50]}...")
#
#         search_cmd = (
#             f'"{BenchmarkConfig.FD_DOWNWARD_BIN}" '
#             f'--search "{search_config}" '
#             f'< "{sas_file}"'
#         )
#
#         search_start = time.time()
#
#         result = subprocess.run(
#             search_cmd,
#             shell=True,
#             cwd=temp_dir,
#             capture_output=True,
#             text=True,
#             timeout=time_limit
#         )
#
#         search_time = time.time() - search_start
#
#         output_text = result.stdout + result.stderr
#
#         logger.debug(f"    [SEARCH] Completed in {search_time:.2f}s")
#
#         # ====== STEP 3c: PARSE ======
#         logger.debug(f"    [PARSE] Extracting metrics...")
#
#         # Check for solution
#         if "Solution found" not in output_text and "Plan length:" not in output_text:
#             logger.debug(f"    [PARSE] No solution found")
#             return {
#                 "solved": False,
#                 "reason": "no_solution",
#                 "time": search_time
#             }
#
#         logger.debug(f"    [PARSE] Solution detected!")
#
#         # Extract metrics
#         metrics = _parse_fd_output(output_text)
#
#         if metrics is None:
#             logger.debug(f"    [PARSE] Could not extract metrics")
#             return {
#                 "solved": True,  # Solution found but parse failed
#                 "reason": "parse_error",
#                 "time": search_time
#             }
#
#         metrics["solved"] = True
#         metrics["time"] = search_time
#
#         logger.debug(f"    [PARSE] Extracted: cost={metrics.get('cost', '?')}, "
#                      f"expansions={metrics.get('expansions', '?')}")
#
#         return metrics
#
#     except subprocess.TimeoutExpired:
#         logger.debug(f"    [TIMEOUT] Exceeded {time_limit}s")
#         return {
#             "solved": False,
#             "reason": "timeout",
#             "time": time_limit
#         }
#
#     except Exception as e:
#         logger.debug(f"    [ERROR] {e}")
#         return {
#             "solved": False,
#             "reason": "exception",
#             "error": str(e)[:200]
#         }
#
#
# def _parse_fd_output(output_text: str) -> Optional[Dict[str, Any]]:
#     """
#     Parse Fast Downward text output to extract metrics.
#
#     Looks for patterns like:
#     - "Plan length: 42"
#     - "Expanded 1234 states"
#     - "Search time: 1.23s"
#
#     Args:
#         output_text: Combined stdout + stderr from FD
#
#     Returns:
#         Dictionary with extracted metrics, or None if parsing fails
#     """
#     metrics = {}
#
#     # Extract plan cost (length)
#     match_cost = re.search(r"Plan length:\s*(\d+)", output_text)
#     if match_cost:
#         metrics["cost"] = int(match_cost.group(1))
#
#     # Extract expansions (all occurrences, take last)
#     matches_exp = list(re.finditer(r"Expanded\s+(\d+)\s+states?", output_text))
#     if matches_exp:
#         metrics["expansions"] = int(matches_exp[-1].group(1))
#
#     # Extract search time
#     matches_time = list(re.finditer(r"Search time:\s+([\d.]+)s", output_text))
#     if matches_time:
#         metrics["search_time"] = float(matches_time[-1].group(1))
#
#     # Require at least cost and expansions
#     if "cost" not in metrics or "expansions" not in metrics:
#         return None
#
#     return metrics
#
#
# # ============================================================================
# # STEP 4: RUN BASELINE PLANNER (all problems)
# # ============================================================================
#
# def run_baseline_on_benchmark_set(
#         domain_file: str,
#         problem_files: List[str],
#         baseline_name: str,
#         search_config: str
# ) -> Dict[str, Any]:
#     """
#     Run ONE baseline planner on ONE benchmark set (all its problems).
#
#     This aggregates results across multiple problems.
#
#     Args:
#         domain_file: Absolute path to domain.pddl
#         problem_files: List of absolute paths to problem files
#         baseline_name: Human-readable name for report
#         search_config: FD search strategy string
#
#     Returns:
#         Dictionary with aggregate metrics for this baseline
#     """
#     logger.info(f"\n  Running baseline: {baseline_name}")
#     logger.info(f"  Problems: {len(problem_files)}")
#
#     # Create temp directory
#     temp_dir = os.path.join(BenchmarkConfig.FD_TEMP_DIR, baseline_name.replace(" ", "_"))
#     os.makedirs(temp_dir, exist_ok=True)
#
#     try:
#         solved_count = 0
#         times_on_solved = []
#         expansions_on_solved = []
#         costs_on_solved = []
#         errors = []
#
#         # Run each problem
#         # Run each problem
#         for i, problem_file in enumerate(problem_files, 1):
#             problem_name = os.path.basename(problem_file)
#
#             # (No log message here... run the problem first)
#
#             result = run_single_fd_problem(
#                 domain_file=domain_file,
#                 problem_file=problem_file,
#                 search_config=search_config,
#                 time_limit=BenchmarkConfig.TIME_LIMIT_PER_RUN_S,
#                 temp_dir=temp_dir
#             )
#
#             if result.get("solved"):
#                 solved_count += 1
#                 times_on_solved.append(result.get("time", 0))
#                 expansions_on_solved.append(result.get("expansions", 0))
#                 costs_on_solved.append(result.get("cost", 0))
#
#                 # Log the FULL line at once
#                 logger.info(
#                     f"    [{i}/{len(problem_files)}] {problem_name:<30} ✓ SOLVED (t={result.get('time', 0):.2f}s)")
#             else:
#                 reason = result.get("reason", "unknown")
#
#                 # Log the FULL line at once
#                 logger.warning(f"    [{i}/{len(problem_files)}] {problem_name:<30} ✗ {reason.upper()}")
#                 errors.append(reason)
#
#         # ====== AGGREGATE RESULTS ======
#         total_problems = len(problem_files)
#         solve_rate = (solved_count / total_problems * 100) if total_problems > 0 else 0
#
#         avg_time = sum(times_on_solved) / len(times_on_solved) if times_on_solved else 0
#         avg_expansions = sum(expansions_on_solved) / len(expansions_on_solved) if expansions_on_solved else 0
#         avg_cost = sum(costs_on_solved) / len(costs_on_solved) if costs_on_solved else 0
#
#         summary = {
#             "name": baseline_name,
#             "set_size": total_problems,
#             "solved": solved_count,
#             "solve_rate_%": solve_rate,
#             "avg_time_on_solved_s": avg_time,
#             "avg_expansions_on_solved": int(avg_expansions),
#             "avg_cost_on_solved": int(avg_cost),
#             "errors": ", ".join(set(errors)) if errors else "None"
#         }
#
#         logger.info(f"  → Solved: {solved_count}/{total_problems} ({solve_rate:.1f}%)")
#         logger.info(f"  → Avg time (solved): {avg_time:.2f}s")
#         logger.info(f"  → Avg expansions: {int(avg_expansions)}")
#
#         return summary
#
#     finally:
#         # Cleanup temp directory
#         try:
#             shutil.rmtree(temp_dir)
#         except:
#             pass
#
#
# # ============================================================================
# # STEP 5: GENERATE REPORT
# # ============================================================================
#
# def generate_report(all_results: List[Dict[str, Any]]) -> None:
#     """
#     Generate CSV report from all baseline results.
#
#     Saves to: baseline_performance_summary.csv
#
#     Args:
#         all_results: List of result dictionaries from all baselines
#     """
#     logger.info("\n" + "=" * 80)
#     logger.info("STEP 5: GENERATE REPORT")
#     logger.info("=" * 80)
#
#     if not all_results:
#         logger.error("No results to report")
#         return
#
#     output_csv = BenchmarkConfig.OUTPUT_CSV
#
#     # Define column order
#     fieldnames = [
#         "name",
#         "set_size",
#         "solved",
#         "solve_rate_%",
#         "avg_time_on_solved_s",
#         "avg_expansions_on_solved",
#         "avg_cost_on_solved",
#         "errors"
#     ]
#
#     logger.info(f"\nWriting report to: {output_csv}")
#
#     with open(output_csv, "w", newline="") as f:
#         writer = csv.DictWriter(f, fieldnames=fieldnames)
#         writer.writeheader()
#         writer.writerows(all_results)
#
#     logger.info(f"✓ Report written ({len(all_results)} baselines)")
#
#     # Print summary table
#     logger.info("\n" + "=" * 80)
#     logger.info("BASELINE PERFORMANCE SUMMARY")
#     logger.info("=" * 80)
#
#     logger.info(f"\n{'Planner':<35} {'Solved':<15} {'Avg Time (s)':<15} {'Avg Expansions':<15}")
#     logger.info("-" * 80)
#
#     for result in all_results:
#         name = result["name"][:33]
#         solved = f"{result['solved']}/{result['set_size']} ({result['solve_rate_%']:.0f}%)"
#         time_str = f"{result['avg_time_on_solved_s']:.2f}"
#         exp_str = f"{result['avg_expansions_on_solved']:,}"
#         logger.info(f"{name:<35} {solved:<15} {time_str:<15} {exp_str:<15}")
#
#     logger.info("-" * 80)
#     logger.info(f"\nFull report saved: {os.path.abspath(output_csv)}")
#
#
# # ============================================================================
# # MAIN EXECUTION
# # ============================================================================
#
# def main():
#     """Main execution orchestration."""
#     logger.info("\n" + "=" * 80)
#     logger.info("BASELINE BENCHMARKING HARNESS FOR GNN MERGE STRATEGY LEARNING")
#     logger.info("=" * 80)
#
#     # Step 1: Verify FD setup
#     if not verify_fd_setup():
#         return 1
#
#     # Step 2: Load benchmarks
#     all_benchmarks = load_all_benchmarks()
#     if not all_benchmarks:
#         return 1
#
#     # Step 3: Run all baselines on all benchmark sets
#     logger.info("\n" + "=" * 80)
#     logger.info("STEP 3: RUN BASELINE PLANNERS")
#     logger.info("=" * 80)
#
#     all_results = []
#
#     for set_name, benchmarks in all_benchmarks.items():
#         domain_file = benchmarks[0][0]  # Same domain for all
#         problem_files = [b[1] for b in benchmarks]  # All problems
#
#         logger.info(f"\n{set_name} Benchmark Set")
#         logger.info("-" * 80)
#
#         for baseline_config in BenchmarkConfig.BASELINES:
#             result = run_baseline_on_benchmark_set(
#                 domain_file=domain_file,
#                 problem_files=problem_files,
#                 baseline_name=baseline_config["name"],
#                 search_config=baseline_config["search_config"]
#             )
#             all_results.append(result)
#
#     # Step 4: Generate report
#     generate_report(all_results)
#
#     # Summary
#     logger.info("\n" + "=" * 80)
#     logger.info("✅ BENCHMARKING COMPLETE")
#     logger.info("=" * 80)
#     logger.info(f"\nNext steps:")
#     logger.info(f"  1. Review CSV: {BenchmarkConfig.OUTPUT_CSV}")
#     logger.info(f"  2. Use these metrics as BASELINE for GNN policy comparison")
#     logger.info(f"  3. Train GNN with: python train_mvp_debug.py")
#     logger.info(f"  4. Evaluate GNN and compare against these baselines")
#
#     return 0
#
#
# if __name__ == "__main__":
#     exit_code = main()
#     sys.exit(exit_code)

# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BASELINE BENCHMARKING HARNESS FOR GNN MERGE STRATEGY LEARNING
==============================================================

This script benchmarks STANDARD Fast Downward algorithms on your problem set
and produces a CSV report for comparison with your GNN policy.

Usage:
    python baseline_benchmarking.py

Output:
    baseline_performance_summary.csv
"""

import sys
import os
import logging
import glob
import json
import subprocess
import time
import re
import tempfile
import shutil
from typing import List, Dict, Tuple, Optional, Any
from pathlib import Path
from datetime import datetime
import csv

# Setup
sys.path.insert(0, os.getcwd())

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - [%(filename)s] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("baseline_benchmarking.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


# ============================================================================
# CONFIGURATION: CRITICAL SETTINGS
# ============================================================================

class BenchmarkConfig:
    """Central configuration for all benchmark settings."""

    # Time limits
    TIME_LIMIT_PER_RUN_S = 300  # 5 minutes per problem per planner

    # ✅ FIXED: Use absolute path to downward directory
    DOWNWARD_DIR = os.path.abspath("downward")
    FD_TRANSLATE_BIN = os.path.join(DOWNWARD_DIR, "builds/release/bin/translate/translate.py")
    FD_DOWNWARD_BIN = os.path.join(DOWNWARD_DIR, "builds/release/bin/downward.exe")

    # Output CSV file
    OUTPUT_CSV = "evaluation_results/baseline_performance_summary.csv"

    # Working directories
    FD_TEMP_DIR = "baseline_temp"  # Temporary SAS files

    # Define benchmark sets (problems grouped by domain)
    BENCHMARK_SETS = {
        "Small": {
            "domain": "domain.pddl",
            "problem_pattern": "problem_small_*.pddl",
            "description": "Small 8-puzzle problems"
        }
    }

    # Define baseline planners to test
    BASELINES = [
        {
            "name": "FD ASTAR LM-Cut",
            "search_config": "astar(lmcut())"
        },
        {
            "name": "FD ASTAR DFP (Stateless)",
            "search_config": (
                "astar(merge_and_shrink("
                "merge_strategy=merge_stateless("
                "merge_selector=score_based_filtering("
                "scoring_functions=[goal_relevance(),dfp(),total_order()])),"  # ✅ CORRECT: () added
                "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
                "label_reduction=exact(before_shrinking=true,before_merging=false),"
                "max_states=50000,threshold_before_merge=1))"
            )
        },
        {
            "name": "FD ASTAR SCC-DFP",
            "search_config": (
                "astar(merge_and_shrink("
                "merge_strategy=merge_sccs("  # ✅ Use merge_sccs for SCC-DFP
                "order_of_sccs=topological,"
                "merge_selector=score_based_filtering("
                "scoring_functions=[goal_relevance(),dfp(),total_order()])),"  # ✅ () added
                "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
                "label_reduction=exact(before_shrinking=true,before_merging=false),"
                "max_states=50000,threshold_before_merge=1))"
            )
        },
        {
            "name": "FD ASTAR Bisimulation",
            "search_config": (
                "astar(merge_and_shrink("
                "merge_strategy=merge_stateless("
                "merge_selector=score_based_filtering("
                "scoring_functions=[total_order()])),"  # ✅ () added
                "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
                "label_reduction=exact(before_shrinking=true,before_merging=false),"
                "max_states=50000,threshold_before_merge=1))"
            )
        },
        {
            "name": "FD ASTAR Blind",
            "search_config": "astar(blind())"
        },
        {
            "name": "FD ASTAR Add Heuristic",
            "search_config": "astar(add())"
        },
        {
            "name": "FD ASTAR Max Heuristic",
            "search_config": "astar(max())"
        },
    ]


# ============================================================================
# STEP 1: VERIFY SETUP
# ============================================================================

def verify_fd_setup() -> bool:
    """Verify that Fast Downward is properly installed."""
    logger.info("\n" + "=" * 80)
    logger.info("STEP 1: VERIFY FAST DOWNWARD SETUP")
    logger.info("=" * 80)

    checks = [
        ("FD Translate Script", BenchmarkConfig.FD_TRANSLATE_BIN),
        ("FD Downward Binary", BenchmarkConfig.FD_DOWNWARD_BIN),
    ]

    all_ok = True
    for name, path in checks:
        if os.path.exists(path):
            logger.info(f"  ✓ {name:<30} {path}")
        else:
            logger.error(f"  ✗ {name:<30} NOT FOUND: {path}")
            all_ok = False

    if not all_ok:
        logger.error("\n❌ FD setup verification FAILED")
        logger.error("   Please ensure Fast Downward is built in downward/builds/release/")
        return False

    logger.info("\n✅ FD setup verified")
    logger.info("\nRunning simplified baseline planners:")
    logger.info("  - LM-Cut (landmark-based heuristic)")
    logger.info("  - Blind (uninformed A*)")
    logger.info("  - Add (additive heuristic)")
    logger.info("  - Max (max heuristic)")
    logger.info("\nNote: Complex M&S strategies require specific compilation flags.")
    return True


# ============================================================================
# STEP 2: LOAD BENCHMARKS
# ============================================================================

def load_all_benchmarks() -> Dict[str, List[Tuple[str, str]]]:
    """
    Load all benchmark problems from disk.

    Returns:
        Dictionary mapping set_name -> list of (domain_file, problem_file) tuples
    """
    logger.info("\n" + "=" * 80)
    logger.info("STEP 2: LOAD BENCHMARK PROBLEMS")
    logger.info("=" * 80)

    all_benchmarks = {}

    for set_name, set_config in BenchmarkConfig.BENCHMARK_SETS.items():
        logger.info(f"\nLoading benchmark set: {set_name}")

        domain_file = set_config["domain"]
        problem_pattern = set_config["problem_pattern"]

        # Check domain exists
        if not os.path.exists(domain_file):
            logger.error(f"  ✗ Domain file not found: {domain_file}")
            return {}
        logger.info(f"  ✓ Domain: {domain_file}")

        # Find problems
        problems = sorted(glob.glob(problem_pattern))
        if not problems:
            logger.error(f"  ✗ No problems found matching: {problem_pattern}")
            return {}

        logger.info(f"  ✓ Found {len(problems)} problem(s)")
        for i, prob in enumerate(problems, 1):
            logger.info(f"    {i}. {prob}")

        # Create benchmark list (absolute paths)
        benchmarks = [
            (os.path.abspath(domain_file), os.path.abspath(prob))
            for prob in problems
        ]

        all_benchmarks[set_name] = benchmarks

    if not all_benchmarks:
        logger.error("\n❌ No benchmarks loaded")
        return {}

    logger.info(f"\n✅ Loaded {len(all_benchmarks)} benchmark set(s)")
    return all_benchmarks


# ============================================================================
# STEP 3: RUN SINGLE PROBLEM
# ============================================================================

def run_single_fd_problem(
        domain_file: str,
        problem_file: str,
        search_config: str,
        time_limit: int,
        temp_dir: str
) -> Dict[str, Any]:
    """
    Run Fast Downward on a SINGLE problem with a SINGLE search configuration.

    ✅ FIXED: Complete diagnostic logging and proper working directory handling.
    """
    problem_name = os.path.basename(problem_file)

    try:
        # ====== STEP 3a: TRANSLATE ======
        logger.info(f"    [TRANSLATE] Starting for {problem_name}...")
        logger.info(f"    [TRANSLATE] Domain:  {os.path.abspath(domain_file)}")
        logger.info(f"    [TRANSLATE] Problem: {os.path.abspath(problem_file)}")

        os.makedirs(temp_dir, exist_ok=True)
        sas_file = os.path.join(temp_dir, "output.sas")

        # ✅ FIX: Use absolute paths for all file arguments
        abs_domain = os.path.abspath(domain_file)
        abs_problem = os.path.abspath(problem_file)
        abs_sas = os.path.abspath(sas_file)
        abs_translate_bin = os.path.abspath(BenchmarkConfig.FD_TRANSLATE_BIN)

        translate_cmd = (
            f'python "{abs_translate_bin}" '
            f'"{abs_domain}" "{abs_problem}" '
            f'--sas-file "{abs_sas}"'
        )

        logger.info(f"    [TRANSLATE] Command: {translate_cmd[:150]}...")

        # ✅ FIX: Run from PROJECT ROOT, not downward/
        # This ensures translate.py finds all its dependencies
        result = subprocess.run(
            translate_cmd,
            shell=True,
            cwd=os.path.abspath("."),  # ✅ CRITICAL: Run from project root
            capture_output=True,
            text=True,
            timeout=time_limit
        )

        if result.returncode != 0:
            # ✅ FIX: Log at INFO level so errors are always visible
            logger.info(f"    [TRANSLATE] ❌ FAILED with return code {result.returncode}")
            logger.info(f"    [TRANSLATE] STDOUT:\n{result.stdout}")
            logger.info(f"    [TRANSLATE] STDERR:\n{result.stderr}")
            return {
                "solved": False,
                "reason": "translate_error",
                "error": (result.stderr if result.stderr else result.stdout)[:500]
            }

        if not os.path.exists(abs_sas):
            logger.error(f"    [TRANSLATE] ❌ output.sas not created")
            logger.error(f"    [TRANSLATE] Expected at: {abs_sas}")
            logger.error(f"    [TRANSLATE] Temp dir exists: {os.path.exists(temp_dir)}")
            logger.error(
                f"    [TRANSLATE] Files in temp dir: {os.listdir(temp_dir) if os.path.exists(temp_dir) else 'N/A'}")
            return {
                "solved": False,
                "reason": "translate_no_output"
            }

        # ✅ FIX: Validate SAS file has content
        sas_size = os.path.getsize(abs_sas)
        if sas_size == 0:
            logger.error(f"    [TRANSLATE] ❌ output.sas is EMPTY (0 bytes)")
            return {
                "solved": False,
                "reason": "translate_empty_output"
            }

        logger.info(f"    [TRANSLATE] ✅ Success ({sas_size} bytes)")

        # ====== STEP 3b: SEARCH ======
        logger.info(f"    [SEARCH] Starting with config: {search_config[:50]}...")

        abs_downward_bin = os.path.abspath(BenchmarkConfig.FD_DOWNWARD_BIN)

        search_cmd = (
            f'"{abs_downward_bin}" '
            f'--search "{search_config}" '
            f'< "{abs_sas}"'
        )

        search_start = time.time()

        # ✅ RUN from downward/ directory (FD's preferred working directory)
        result = subprocess.run(
            search_cmd,
            shell=True,
            cwd=os.path.dirname(abs_downward_bin),  # downward/builds/release/bin/
            capture_output=True,
            text=True,
            timeout=time_limit
        )

        search_time = time.time() - search_start

        output_text = result.stdout + result.stderr

        if result.returncode != 0:
            logger.info(f"    [SEARCH] ⚠️  Non-zero return code: {result.returncode}")
            logger.info(f"    [SEARCH] Output:\n{output_text[:500]}")

        logger.info(f"    [SEARCH] ✅ Completed in {search_time:.2f}s")

        # ====== STEP 3c: PARSE ======
        logger.info(f"    [PARSE] Extracting metrics...")

        # Check for solution
        if "Solution found" not in output_text and "Plan length:" not in output_text:
            logger.info(f"    [PARSE] ❌ No solution found in output")
            return {
                "solved": False,
                "reason": "no_solution",
                "time": search_time
            }

        logger.info(f"    [PARSE] ✅ Solution detected!")

        # Extract metrics
        metrics = _parse_fd_output(output_text)

        if metrics is None:
            logger.warning(f"    [PARSE] ⚠️  Could not extract metrics from solution")
            return {
                "solved": True,
                "reason": "parse_error",
                "time": search_time
            }

        metrics["solved"] = True
        metrics["time"] = search_time

        logger.info(f"    [PARSE] ✅ Extracted: cost={metrics.get('cost', '?')}, "
                    f"expansions={metrics.get('expansions', '?')}")

        return metrics

    except subprocess.TimeoutExpired:
        logger.warning(f"    [TIMEOUT] ❌ Exceeded {time_limit}s")
        return {
            "solved": False,
            "reason": "timeout",
            "time": time_limit
        }

    except Exception as e:
        logger.error(f"    [ERROR] ❌ {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "solved": False,
            "reason": "exception",
            "error": str(e)[:200]
        }


def _parse_fd_output(output_text: str) -> Optional[Dict[str, Any]]:
    """
    Parse Fast Downward text output to extract metrics.

    Looks for patterns like:
    - "Plan length: 42"
    - "Expanded 1234 states"
    - "Search time: 1.23s"

    Args:
        output_text: Combined stdout + stderr from FD

    Returns:
        Dictionary with extracted metrics, or None if parsing fails
    """
    metrics = {}

    # Extract plan cost (length)
    match_cost = re.search(r"Plan length:\s*(\d+)", output_text)
    if match_cost:
        metrics["cost"] = int(match_cost.group(1))

    # Extract expansions (all occurrences, take last)
    matches_exp = list(re.finditer(r"Expanded\s+(\d+)\s+states?", output_text))
    if matches_exp:
        metrics["expansions"] = int(matches_exp[-1].group(1))

    # Extract search time
    matches_time = list(re.finditer(r"Search time:\s+([\d.]+)s", output_text))
    if matches_time:
        metrics["search_time"] = float(matches_time[-1].group(1))

    # Require at least cost and expansions
    if "cost" not in metrics or "expansions" not in metrics:
        return None

    return metrics


# ============================================================================
# STEP 4: RUN BASELINE PLANNER (all problems)
# ============================================================================

def run_baseline_on_benchmark_set(
        domain_file: str,
        problem_files: List[str],
        baseline_name: str,
        search_config: str
) -> Dict[str, Any]:
    """
    Run ONE baseline planner on ONE benchmark set (all its problems).

    This aggregates results across multiple problems.

    Args:
        domain_file: Absolute path to domain.pddl
        problem_files: List of absolute paths to problem files
        baseline_name: Human-readable name for report
        search_config: FD search strategy string

    Returns:
        Dictionary with aggregate metrics for this baseline
    """
    logger.info(f"\n  Running baseline: {baseline_name}")
    logger.info(f"  Problems: {len(problem_files)}")

    # Create temp directory
    temp_dir = os.path.join(BenchmarkConfig.FD_TEMP_DIR, baseline_name.replace(" ", "_"))
    os.makedirs(temp_dir, exist_ok=True)

    try:
        solved_count = 0
        times_on_solved = []
        expansions_on_solved = []
        costs_on_solved = []
        errors = []

        # Run each problem
        for i, problem_file in enumerate(problem_files, 1):
            problem_name = os.path.basename(problem_file)

            result = run_single_fd_problem(
                domain_file=domain_file,
                problem_file=problem_file,
                search_config=search_config,
                time_limit=BenchmarkConfig.TIME_LIMIT_PER_RUN_S,
                temp_dir=temp_dir
            )

            if result.get("solved"):
                solved_count += 1
                times_on_solved.append(result.get("time", 0))
                expansions_on_solved.append(result.get("expansions", 0))
                costs_on_solved.append(result.get("cost", 0))

                logger.info(
                    f"    [{i}/{len(problem_files)}] {problem_name:<30} ✓ SOLVED (t={result.get('time', 0):.2f}s)")
            else:
                reason = result.get("reason", "unknown")

                logger.warning(f"    [{i}/{len(problem_files)}] {problem_name:<30} ✗ {reason.upper()}")

                # ✅ FIXED: Log error details for debugging
                if "error" in result:
                    logger.debug(f"        Error details: {result['error']}")

                errors.append(reason)

        # ====== AGGREGATE RESULTS ======
        total_problems = len(problem_files)
        solve_rate = (solved_count / total_problems * 100) if total_problems > 0 else 0

        avg_time = sum(times_on_solved) / len(times_on_solved) if times_on_solved else 0
        avg_expansions = sum(expansions_on_solved) / len(expansions_on_solved) if expansions_on_solved else 0
        avg_cost = sum(costs_on_solved) / len(costs_on_solved) if costs_on_solved else 0

        summary = {
            "name": baseline_name,
            "set_size": total_problems,
            "solved": solved_count,
            "solve_rate_%": solve_rate,
            "avg_time_on_solved_s": avg_time,
            "avg_expansions_on_solved": int(avg_expansions),
            "avg_cost_on_solved": int(avg_cost),
            "errors": ", ".join(set(errors)) if errors else "None"
        }

        logger.info(f"  → Solved: {solved_count}/{total_problems} ({solve_rate:.1f}%)")
        logger.info(f"  → Avg time (solved): {avg_time:.2f}s")
        logger.info(f"  → Avg expansions: {int(avg_expansions)}")

        return summary

    finally:
        # Cleanup temp directory
        try:
            shutil.rmtree(temp_dir)
        except:
            pass


# ============================================================================
# STEP 5: GENERATE REPORT
# ============================================================================

def generate_report(all_results: List[Dict[str, Any]]) -> None:
    """
    Generate CSV report from all baseline results.

    Saves to: baseline_performance_summary.csv

    Args:
        all_results: List of result dictionaries from all baselines
    """
    logger.info("\n" + "=" * 80)
    logger.info("STEP 5: GENERATE REPORT")
    logger.info("=" * 80)

    if not all_results:
        logger.error("No results to report")
        return

    output_csv = BenchmarkConfig.OUTPUT_CSV

    # Define column order
    fieldnames = [
        "name",
        "set_size",
        "solved",
        "solve_rate_%",
        "avg_time_on_solved_s",
        "avg_expansions_on_solved",
        "avg_cost_on_solved",
        "errors"
    ]

    logger.info(f"\nWriting report to: {output_csv}")

    with open(output_csv, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(all_results)

    logger.info(f"✓ Report written ({len(all_results)} baselines)")

    # Print summary table
    logger.info("\n" + "=" * 80)
    logger.info("BASELINE PERFORMANCE SUMMARY")
    logger.info("=" * 80)

    logger.info(f"\n{'Planner':<35} {'Solved':<15} {'Avg Time (s)':<15} {'Avg Expansions':<15}")
    logger.info("-" * 80)

    for result in all_results:
        name = result["name"][:33]
        solved = f"{result['solved']}/{result['set_size']} ({result['solve_rate_%']:.0f}%)"
        time_str = f"{result['avg_time_on_solved_s']:.2f}"
        exp_str = f"{result['avg_expansions_on_solved']:,}"
        logger.info(f"{name:<35} {solved:<15} {time_str:<15} {exp_str:<15}")

    logger.info("-" * 80)
    logger.info(f"\nFull report saved: {os.path.abspath(output_csv)}")


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    """Main execution orchestration."""
    logger.info("\n" + "=" * 80)
    logger.info("BASELINE BENCHMARKING HARNESS FOR GNN MERGE STRATEGY LEARNING")
    logger.info("=" * 80)

    # Step 1: Verify FD setup
    if not verify_fd_setup():
        return 1

    # Step 2: Load benchmarks
    all_benchmarks = load_all_benchmarks()
    if not all_benchmarks:
        return 1

    # Step 3: Run all baselines on all benchmark sets
    logger.info("\n" + "=" * 80)
    logger.info("STEP 3: RUN BASELINE PLANNERS")
    logger.info("=" * 80)

    all_results = []

    for set_name, benchmarks in all_benchmarks.items():
        domain_file = benchmarks[0][0]  # Same domain for all
        problem_files = [b[1] for b in benchmarks]  # All problems

        logger.info(f"\n{set_name} Benchmark Set")
        logger.info("-" * 80)

        for baseline_config in BenchmarkConfig.BASELINES:
            result = run_baseline_on_benchmark_set(
                domain_file=domain_file,
                problem_files=problem_files,
                baseline_name=baseline_config["name"],
                search_config=baseline_config["search_config"]
            )
            all_results.append(result)

    # Step 4: Generate report
    generate_report(all_results)

    # Summary
    logger.info("\n" + "=" * 80)
    logger.info("✅ BENCHMARKING COMPLETE")
    logger.info("=" * 80)
    logger.info(f"\nNext steps:")
    logger.info(f"  1. Review CSV: {BenchmarkConfig.OUTPUT_CSV}")
    logger.info(f"  2. Use these metrics as BASELINE for GNN policy comparison")
    logger.info(f"  3. Train GNN with: python train_mvp_debug.py")
    logger.info(f"  4. Evaluate GNN and compare against these baselines")

    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

--------------------------------------------------------------------------------

The file pddl_generator/domain_templates.py code is this:
# -*- coding: utf-8 -*-
"""
Fixed domain definitions for problem generation.
These domains are well-studied and scalable.
"""

# ============================================================================
# DOMAIN 1: BLOCKS WORLD
# ============================================================================
BLOCKS_WORLD_DOMAIN = """(define (domain blocks-world)
  (:requirements :strips :typing)
  (:types block)
  (:predicates
    (on ?x ?y - block)
    (ontable ?x - block)
    (clear ?x - block)
    (holding ?x - block)
    (arm-empty)
  )

  (:action pick-up
    :parameters (?x - block)
    :precondition (and (clear ?x) (ontable ?x) (arm-empty))
    :effect (and (not (ontable ?x)) (not (clear ?x)) (not (arm-empty)) (holding ?x))
  )

  (:action put-down
    :parameters (?x - block)
    :precondition (holding ?x)
    :effect (and (ontable ?x) (clear ?x) (arm-empty) (not (holding ?x)))
  )

  (:action stack
    :parameters (?x ?y - block)
    :precondition (and (holding ?x) (clear ?y))
    :effect (and (on ?x ?y) (clear ?x) (not (holding ?x)) (arm-empty) (not (clear ?y)))
  )

  (:action unstack
    :parameters (?x ?y - block)
    :precondition (and (on ?x ?y) (clear ?x) (arm-empty))
    :effect (and (holding ?x) (clear ?y) (not (on ?x ?y)) (not (arm-empty)) (not (clear ?x)))
  )
)
"""

# ============================================================================
# DOMAIN 2: LOGISTICS
# ============================================================================
LOGISTICS_DOMAIN = """(define (domain logistics)
  (:requirements :strips :typing)
  (:types
    truck location object city
  )
  (:predicates
    (in ?obj - object ?truck - truck)
    (at ?truck - truck ?loc - location)
    (at-obj ?obj - object ?loc - location)
    (connected ?from ?to - location)
    (in-city ?loc - location ?city - city)
    (obj-at-city ?obj - object ?city - city)
    (truck-at-city ?truck - truck ?city - city)
  )

  (:action load
    :parameters (?obj - object ?truck - truck ?loc - location)
    :precondition (and (at-obj ?obj ?loc) (at ?truck ?loc))
    :effect (and (in ?obj ?truck) (not (at-obj ?obj ?loc)))
  )

  (:action unload
    :parameters (?obj - object ?truck - truck ?loc - location)
    :precondition (and (in ?obj ?truck) (at ?truck ?loc))
    :effect (and (not (in ?obj ?truck)) (at-obj ?obj ?loc))
  )

  (:action drive
    :parameters (?truck - truck ?from ?to - location)
    :precondition (and (at ?truck ?from) (connected ?from ?to))
    :effect (and (at ?truck ?to) (not (at ?truck ?from)))
  )
)
"""

# ============================================================================
# DOMAIN 3: GRIPPER
# ============================================================================
GRIPPER_DOMAIN = """(define (domain gripper)
  (:requirements :strips :typing)
  (:types room object gripper)
  (:predicates
    (at-robot ?room - room)
    (at ?obj - object ?room - room)
    (free ?gripper - gripper)
    (carry ?obj - object ?gripper - gripper)
    (connect ?from ?to - room)
  )

  (:action move
    :parameters (?from ?to - room)
    :precondition (and (at-robot ?from) (connect ?from ?to))
    :effect (and (at-robot ?to) (not (at-robot ?from)))
  )

  (:action pick
    :parameters (?obj - object ?room - room ?gripper - gripper)
    :precondition (and (at-robot ?room) (at ?obj ?room) (free ?gripper))
    :effect (and (carry ?obj ?gripper) (not (at ?obj ?room)) (not (free ?gripper)))
  )

  (:action drop
    :parameters (?obj - object ?room - room ?gripper - gripper)
    :precondition (and (at-robot ?room) (carry ?obj ?gripper))
    :effect (and (at ?obj ?room) (free ?gripper) (not (carry ?obj ?gripper)))
  )
)
"""

DOMAINS = {
    "blocks_world": BLOCKS_WORLD_DOMAIN,
    "logistics": LOGISTICS_DOMAIN,
    "gripper": GRIPPER_DOMAIN,
}

--------------------------------------------------------------------------------

The file pddl_generator/problem_generator.py code is this:
# -*- coding: utf-8 -*-
"""
Core PDDL problem generator.
Creates valid, solvable problems of varying sizes.
"""

import os
import random
from typing import List, Dict, Tuple
from pathlib import Path

from domain_templates import DOMAINS
from size_config import SIZE_CONFIGS


class PDDLProblemGenerator:
    """Generates valid PDDL problems for training."""

    def __init__(self, output_dir: str = "benchmarks", seed: int = 42):
        self.output_dir = output_dir
        self.seed = seed
        random.seed(seed)
        Path(output_dir).mkdir(parents=True, exist_ok=True)

    # ========================================================================
    # BLOCKS WORLD PROBLEM GENERATION
    # ========================================================================

    def generate_blocks_world_problem(
            self,
            num_blocks: int,
            num_stacks: int,
            problem_id: int
    ) -> str:
        """
        Generate a Blocks World problem.

        Creates a solvable problem: random initial state → all blocks on table →
        then builds stacks as goal.

        Args:
            num_blocks: Total number of blocks
            num_stacks: Number of stacks to create in goal
            problem_id: Unique problem identifier

        Returns:
            PDDL problem string
        """
        blocks = [f"b{i}" for i in range(num_blocks)]

        # Initial state: all blocks on table
        init_facts = [f"(ontable {b})" for b in blocks]
        init_facts.append("(arm-empty)")
        for b in blocks:
            init_facts.append(f"(clear {b})")

        # Goal: create stacks
        # Stack 0: b0, b1, b2, ... (blocks_per_stack elements)
        # Stack 1: b_k, b_k+1, ...
        # etc.
        blocks_per_stack = num_blocks // num_stacks
        goal_facts = []

        block_idx = 0
        for stack_id in range(num_stacks):
            stack_blocks = blocks[block_idx:block_idx + blocks_per_stack]
            if stack_blocks:
                # Stack blocks: b0 on b1 on b2 on table
                for i in range(len(stack_blocks) - 1):
                    goal_facts.append(f"(on {stack_blocks[i]} {stack_blocks[i + 1]})")
                goal_facts.append(f"(ontable {stack_blocks[-1]})")
            block_idx += blocks_per_stack

        # Add remaining blocks on table
        for b in blocks[block_idx:]:
            goal_facts.append(f"(ontable {b})")

        problem = f"""(define (problem blocks-world-{problem_id})
  (:domain blocks-world)
  (:objects {' '.join(blocks)} - block)
  (:init
    {' '.join(init_facts)}
  )
  (:goal (and
    {' '.join(goal_facts)}
  ))
)
"""
        return problem

    # ========================================================================
    # LOGISTICS PROBLEM GENERATION
    # ========================================================================

    def generate_logistics_problem(
            self,
            num_cities: int,
            locations_per_city: int,
            trucks_per_city: int,
            num_objects: int,
            problem_id: int
    ) -> str:
        """
        Generate a Logistics problem.

        Creates a solvable problem: objects scattered across cities →
        deliver all to a central location.
        """
        cities = [f"city{i}" for i in range(num_cities)]
        locations = []
        trucks = []
        objects = [f"obj{i}" for i in range(num_objects)]

        # Create locations and trucks
        loc_idx = 0
        for city in cities:
            for loc_in_city in range(locations_per_city):
                loc_name = f"loc-{city}-{loc_in_city}"
                locations.append((loc_name, city))
                loc_idx += 1

            for truck_in_city in range(trucks_per_city):
                truck_name = f"truck-{city}-{truck_in_city}"
                trucks.append((truck_name, city))

        # Initial state: objects at random locations, trucks at first location of their city
        init_facts = []
        for obj in objects:
            loc_choice = random.choice(locations)
            init_facts.append(f"(at-obj {obj} {loc_choice[0]})")
            init_facts.append(f"(obj-at-city {obj} {loc_choice[1]})")

        for truck, city in trucks:
            init_facts.append(f"(at {truck} loc-{city}-0)")
            init_facts.append(f"(truck-at-city {truck} {city})")

        # Connected locations within each city (line graph)
        for city in cities:
            city_locs = [loc for loc, c in locations if c == city]
            for i in range(len(city_locs) - 1):
                init_facts.append(f"(connected {city_locs[i]} {city_locs[i + 1]})")
                init_facts.append(f"(connected {city_locs[i + 1]} {city_locs[i]})")

        # Connect cities: connect first location of each city
        for i in range(len(cities) - 1):
            loc1 = f"loc-{cities[i]}-0"
            loc2 = f"loc-{cities[i + 1]}-0"
            init_facts.append(f"(connected {loc1} {loc2})")
            init_facts.append(f"(connected {loc2} {loc1})")

        # Goal: all objects at first location of first city
        goal_loc = f"loc-{cities[0]}-0"
        goal_facts = [f"(at-obj {obj} {goal_loc})" for obj in objects]

        # Object type definitions
        obj_str = " ".join(objects)
        loc_str = " ".join([loc for loc, _ in locations])
        truck_str = " ".join([truck for truck, _ in trucks])
        city_str = " ".join(cities)

        problem = f"""(define (problem logistics-{problem_id})
  (:domain logistics)
  (:objects
    {truck_str} - truck
    {loc_str} - location
    {obj_str} - object
    {city_str} - city
  )
  (:init
    {' '.join(init_facts)}
  )
  (:goal (and
    {' '.join(goal_facts)}
  ))
)
"""
        return problem

    # ========================================================================
    # GRIPPER PROBLEM GENERATION
    # ========================================================================

    def generate_gripper_problem(
            self,
            num_rooms: int,
            num_objects: int,
            num_grippers: int,
            problem_id: int
    ) -> str:
        """
        Generate a Gripper problem.

        Classic gripper problem: move objects from room A to room B.
        """
        rooms = [f"room{i}" for i in range(num_rooms)]
        objects = [f"obj{i}" for i in range(num_objects)]
        grippers = [f"gripper{i}" for i in range(num_grippers)]

        # Initial state: all objects in first room, robot in first room
        init_facts = [f"(at-robot {rooms[0]})"]
        for obj in objects:
            init_facts.append(f"(at {obj} {rooms[0]})")
        for gripper in grippers:
            init_facts.append(f"(free {gripper})")

        # Connect rooms in a line graph
        for i in range(len(rooms) - 1):
            init_facts.append(f"(connect {rooms[i]} {rooms[i + 1]})")
            init_facts.append(f"(connect {rooms[i + 1]} {rooms[i]})")

        # Goal: all objects in last room
        goal_loc = rooms[-1]
        goal_facts = [f"(at {obj} {goal_loc})" for obj in objects]

        room_str = " ".join(rooms)
        obj_str = " ".join(objects)
        gripper_str = " ".join(grippers)

        problem = f"""(define (problem gripper-{problem_id})
  (:domain gripper)
  (:objects
    {room_str} - room
    {obj_str} - object
    {gripper_str} - gripper
  )
  (:init
    {' '.join(init_facts)}
  )
  (:goal (and
    {' '.join(goal_facts)}
  ))
)
"""
        return problem

    # ========================================================================
    # PUBLIC API
    # ========================================================================

    def generate_problem_set(
            self,
            domain: str,
            size: str,  # "small", "medium", "large"
            num_problems: int = 5
    ) -> List[str]:
        """
        Generate a set of problems for a domain at a specific size.

        Args:
            domain: "blocks_world", "logistics", or "gripper"
            size: "small", "medium", or "large"
            num_problems: Number of problems to generate

        Returns:
            List of PDDL problem strings
        """
        if domain not in SIZE_CONFIGS:
            raise ValueError(f"Unknown domain: {domain}")
        if size not in SIZE_CONFIGS[domain]:
            raise ValueError(f"Unknown size: {size}")

        config = SIZE_CONFIGS[domain][size]
        problems = []

        for problem_id in range(num_problems):
            if domain == "blocks_world":
                problem = self.generate_blocks_world_problem(
                    num_blocks=config["num_blocks"],
                    num_stacks=config["num_stacks"],
                    problem_id=problem_id
                )
            elif domain == "logistics":
                problem = self.generate_logistics_problem(
                    num_cities=config["num_cities"],
                    locations_per_city=config["locations_per_city"],
                    trucks_per_city=config["trucks_per_city"],
                    num_objects=config["objects"],
                    problem_id=problem_id
                )
            elif domain == "gripper":
                problem = self.generate_gripper_problem(
                    num_rooms=config["num_rooms"],
                    num_objects=config["num_objects"],
                    num_grippers=config["num_grippers"],
                    problem_id=problem_id
                )

            problems.append(problem)

        return problems

    def save_domain_and_problems(
            self,
            domain: str,
            size: str,
            num_problems: int = 5
    ) -> Tuple[str, List[str]]:
        """
        Generate and save domain and problems to disk.

        Returns:
            (domain_path, problem_paths)
        """
        # Create directory
        domain_dir = Path(self.output_dir) / domain / size
        domain_dir.mkdir(parents=True, exist_ok=True)

        # Save domain
        domain_path = domain_dir / "domain_new.pddl"
        with open(domain_path, "w") as f:
            f.write(DOMAINS[domain])
        print(f"✓ Domain saved: {domain_path}")

        # Generate and save problems
        problems = self.generate_problem_set(domain, size, num_problems)
        problem_paths = []

        for i, problem_str in enumerate(problems):
            problem_path = domain_dir / f"problem_{size}_{i:02d}.pddl"
            with open(problem_path, "w") as f:
                f.write(problem_str)
            problem_paths.append(str(problem_path))

        print(f"✓ Generated {len(problems)} problems at {domain_dir}")
        return str(domain_path), problem_paths


# ============================================================================
# CONVENIENCE FUNCTION
# ============================================================================

def generate_all_benchmarks(output_dir: str = "benchmarks"):
    """Generate all domain/size combinations."""
    gen = PDDLProblemGenerator(output_dir)

    domains = ["blocks_world", "logistics", "gripper"]
    sizes = ["small", "medium", "large"]

    for domain in domains:
        for size in sizes:
            print(f"\nGenerating {domain} - {size}...")
            gen.save_domain_and_problems(domain, size, num_problems=15)

    print("\n✅ All benchmarks generated!")


if __name__ == "__main__":
    generate_all_benchmarks()

--------------------------------------------------------------------------------

The file pddl_generator/problem_validator.py code is this:
# -*- coding: utf-8 -*-
"""
Validates that generated PDDL problems are well-formed and solvable.
"""

import subprocess
import os
from pathlib import Path
from typing import Tuple, Optional


class PDDLValidator:
    """Validates PDDL problems using Fast Downward or VAL."""

    def __init__(self, fd_path: str = "./downward/builds/release/bin/downward.exe"):
        self.fd_path = fd_path

    def validate_syntax(self, domain_file: str, problem_file: str) -> Tuple[bool, str]:
        """
        Check PDDL syntax using Fast Downward's translate.

        Returns:
            (is_valid, message)
        """
        try:
            result = subprocess.run(
                [
                    "python",
                    "./downward/builds/release/bin/translate/translate.py",
                    domain_file,
                    problem_file,
                    "--sas-file", "/tmp/test.sas"
                ],
                capture_output=True,
                text=True,
                timeout=10
            )

            if result.returncode == 0:
                return True, "Syntax valid"
            else:
                return False, result.stderr

        except Exception as e:
            return False, str(e)

    def validate_solvability(
            self,
            domain_file: str,
            problem_file: str,
            timeout: int = 60
    ) -> Tuple[bool, str]:
        """
        Check if problem is solvable using a simple search.

        Returns:
            (is_solvable, message)
        """
        try:
            result = subprocess.run(
                [
                    self.fd_path,
                    "--search", "astar(lmcut())",
                    domain_file,
                    problem_file
                ],
                capture_output=True,
                text=True,
                timeout=timeout
            )

            output = result.stdout + result.stderr

            if "Solution found" in output or "Plan length" in output:
                return True, "Problem is solvable"
            else:
                return False, "No solution found"

        except subprocess.TimeoutExpired:
            return False, "Timeout (problem too hard or unsolvable)"
        except Exception as e:
            return False, str(e)

    def validate_problem(
            self,
            domain_file: str,
            problem_file: str,
            check_solvability: bool = True
    ) -> Tuple[bool, str]:
        """Full validation: syntax + optionally solvability."""

        # Check syntax
        is_valid, msg = self.validate_syntax(domain_file, problem_file)
        if not is_valid:
            return False, f"Syntax error: {msg}"

        # Check solvability
        if check_solvability:
            is_solvable, msg = self.validate_solvability(domain_file, problem_file)
            if not is_solvable:
                return False, f"Solvability check failed: {msg}"

        return True, "Problem is valid and solvable"


if __name__ == "__main__":
    # Test validation
    validator = PDDLValidator()

    domain = "benchmarks/blocks_world/small/domain_new.pddl"
    problem = "benchmarks/blocks_world/small/problem_small_00.pddl"

    if os.path.exists(domain) and os.path.exists(problem):
        is_valid, msg = validator.validate_problem(domain, problem)
        print(f"{'✓' if is_valid else '✗'} {msg}")
    else:
        print("Test files not found. Run problem_generator.py first.")

--------------------------------------------------------------------------------

The file pddl_generator/size_config.py code is this:
# -*- coding: utf-8 -*-
"""
Problem size configurations for each domain.
Defines what "small", "medium", "large" mean for each domain.
"""

SIZE_CONFIGS = {
    "blocks_world": {
        "small": {
            "num_blocks": 4,
            "num_stacks": 2,
            "description": "4 blocks, 2 stacks"
        },
        "medium": {
            "num_blocks": 8,
            "num_stacks": 3,
            "description": "8 blocks, 3 stacks"
        },
        "large": {
            "num_blocks": 12,
            "num_stacks": 4,
            "description": "12 blocks, 4 stacks"
        },
    },
    "logistics": {
        "small": {
            "num_cities": 2,
            "locations_per_city": 2,
            "trucks_per_city": 1,
            "objects": 4,
            "description": "2 cities, 4 objects"
        },
        "medium": {
            "num_cities": 3,
            "locations_per_city": 2,
            "trucks_per_city": 1,
            "objects": 8,
            "description": "3 cities, 8 objects"
        },
        "large": {
            "num_cities": 4,
            "locations_per_city": 3,
            "trucks_per_city": 2,
            "objects": 12,
            "description": "4 cities, 12 objects"
        },
    },
    "gripper": {
        "small": {
            "num_rooms": 3,
            "num_objects": 4,
            "num_grippers": 2,
            "description": "3 rooms, 4 objects"
        },
        "medium": {
            "num_rooms": 4,
            "num_objects": 8,
            "num_grippers": 2,
            "description": "4 rooms, 8 objects"
        },
        "large": {
            "num_rooms": 5,
            "num_objects": 12,
            "num_grippers": 2,
            "description": "5 rooms, 12 objects"
        },
    },
}

--------------------------------------------------------------------------------

The file analysis_and_visualization.py code is this:
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ANALYSIS AND VISUALIZATION MODULE - COMPLETE REFACTORING
========================================================
Generates ALL necessary plots for comprehensive evaluation reporting.

Plots Generated:
  ✓ Solve rate comparison (bar chart)
  ✓ Time comparison (box plots + scatter)
  ✓ Expansions comparison (log scale)
  ✓ Plan cost comparison
  ✓ Efficiency frontier (2D scatter)
  ✓ Per-problem heatmap
  ✓ Time distribution (violin plots)
  ✓ Learning curves (if available)
  ✓ Statistical significance tests
  ✓ Cumulative distribution
  ✓ Summary dashboard (HTML)
"""

import sys
import os
import logging
import argparse
import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import numpy as np
from datetime import datetime

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

try:
    import matplotlib

    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    from matplotlib import rcParams
    import seaborn as sns

    HAS_MATPLOTLIB = True
except ImportError:
    HAS_MATPLOTLIB = False
    logger.warning("matplotlib/seaborn not installed")

try:
    import pandas as pd

    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False
    logger.warning("pandas not installed")

# ============================================================================
# CONFIGURATION
# ============================================================================

PLOT_CONFIG = {
    "style": "seaborn-v0_8-whitegrid",
    "figsize": (14, 8),
    "dpi": 150,
    "font_size": 11,
}

COLORS = {
    "GNN": "#2E86AB",
    "FD": "#A23B72",
    "FD_LM-Cut": "#F18F01",
    "FD_Blind": "#C73E1D",
    "FD_Add": "#06A77D",
    "FD_Max": "#D62828",
}


# ============================================================================
# DATA LOADING
# ============================================================================

def load_results_csv(csv_path: str) -> Optional['pd.DataFrame']:
    """Load evaluation results from CSV."""
    if not HAS_PANDAS:
        logger.warning("pandas not available")
        return None

    if not os.path.exists(csv_path):
        logger.error(f"CSV not found: {csv_path}")
        return None

    df = pd.read_csv(csv_path)
    logger.info(f"Loaded {len(df)} results from CSV")
    return df


# ============================================================================
# PLOT 1: SOLVE RATE COMPARISON
# ============================================================================

def plot_solve_rate_comparison(df: 'pd.DataFrame', output_path: str):
    """Bar chart comparing solve rates."""
    if not HAS_MATPLOTLIB or df is None:
        return

    plt.style.use(PLOT_CONFIG["style"])
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

    # Compute solve rates
    solve_rates = []
    planner_names = []

    for planner in sorted(df['planner_name'].unique()):
        df_planner = df[df['planner_name'] == planner]
        rate = (df_planner['solved'].sum() / len(df_planner)) * 100
        solve_rates.append(rate)
        planner_names.append(planner)

    # Plot
    colors_list = [COLORS.get(name, "#555") for name in planner_names]
    bars = ax.bar(range(len(solve_rates)), solve_rates, color=colors_list, alpha=0.8, edgecolor='black')

    ax.set_ylabel("Solve Rate (%)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.set_xlabel("Planner", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.set_xticks(range(len(planner_names)))
    ax.set_xticklabels(planner_names, rotation=45, ha='right')
    ax.set_ylim([0, 105])
    ax.axhline(y=100, color='green', linestyle='--', alpha=0.3, label='Perfect')
    ax.axhline(y=80, color='orange', linestyle='--', alpha=0.3, label='Target')

    # Add value labels
    for i, (bar, rate) in enumerate(zip(bars, solve_rates)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2, height + 2,
                f'{rate:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')

    ax.set_title("Solve Rate Comparison", fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)
    ax.legend()

    plt.tight_layout()
    plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
    logger.info(f"✓ {output_path}")
    plt.close()


# ============================================================================
# PLOT 2: TIME COMPARISON (BOX + SCATTER)
# ============================================================================

def plot_time_comparison(df: 'pd.DataFrame', output_path: str):
    """Box plot and scatter showing time distribution."""
    if not HAS_MATPLOTLIB or df is None:
        return

    plt.style.use(PLOT_CONFIG["style"])
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), dpi=PLOT_CONFIG["dpi"])

    df_solved = df[df['solved']]

    # Box plot
    planners = sorted(df_solved['planner_name'].unique())
    data_for_box = [df_solved[df_solved['planner_name'] == p]['wall_clock_time'].values for p in planners]

    bp = ax1.boxplot(data_for_box, labels=planners, patch_artist=True)
    for patch, planner in zip(bp['boxes'], planners):
        patch.set_facecolor(COLORS.get(planner, "#555"))
        patch.set_alpha(0.7)

    ax1.set_ylabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax1.set_title("Time Distribution (Box Plot)", fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
    ax1.set_xticklabels(planners, rotation=45, ha='right')
    ax1.grid(axis='y', alpha=0.3)

    # Scatter plot
    for planner in planners:
        df_p = df_solved[df_solved['planner_name'] == planner]
        x = np.random.normal(list(planners).index(planner), 0.04, size=len(df_p))
        ax2.scatter(x, df_p['wall_clock_time'], alpha=0.6, s=100,
                    color=COLORS.get(planner, "#555"), label=planner, edgecolor='black')

    ax2.set_ylabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax2.set_xlabel("Planner", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax2.set_xticks(range(len(planners)))
    ax2.set_xticklabels(planners, rotation=45, ha='right')
    ax2.set_title("Time Distribution (Scatter)", fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
    logger.info(f"✓ {output_path}")
    plt.close()


# ============================================================================
# PLOT 3: EXPANSIONS COMPARISON (LOG SCALE)
# ============================================================================

def plot_expansions_comparison(df: 'pd.DataFrame', output_path: str):
    """Bar chart of expansions on log scale."""
    if not HAS_MATPLOTLIB or df is None:
        return

    plt.style.use(PLOT_CONFIG["style"])
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

    df_solved = df[df['solved']]

    expansions = []
    planner_names = []

    for planner in sorted(df_solved['planner_name'].unique()):
        df_p = df_solved[df_solved['planner_name'] == planner]
        avg_exp = df_p['nodes_expanded'].mean()
        expansions.append(avg_exp)
        planner_names.append(planner)

    colors_list = [COLORS.get(name, "#555") for name in planner_names]
    bars = ax.bar(range(len(expansions)), expansions, color=colors_list, alpha=0.8, edgecolor='black')

    ax.set_ylabel("Average Nodes Expanded (log scale)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.set_xlabel("Planner", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.set_yscale('log')
    ax.set_xticks(range(len(planner_names)))
    ax.set_xticklabels(planner_names, rotation=45, ha='right')

    # Add value labels
    for bar, exp in zip(bars, expansions):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width() / 2, height * 2,
                f'{int(exp):,}', ha='center', va='bottom', fontsize=9)

    ax.set_title("Average Nodes Expanded Comparison (Log Scale)",
                 fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
    logger.info(f"✓ {output_path}")
    plt.close()


# ============================================================================
# PLOT 4: EFFICIENCY FRONTIER
# ============================================================================

def plot_efficiency_frontier(df: 'pd.DataFrame', output_path: str):
    """Scatter plot: time vs expansions."""
    if not HAS_MATPLOTLIB or df is None:
        return

    plt.style.use(PLOT_CONFIG["style"])
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

    df_solved = df[df['solved']]

    for planner in sorted(df_solved['planner_name'].unique()):
        df_p = df_solved[df_solved['planner_name'] == planner]
        ax.scatter(df_p['wall_clock_time'], df_p['nodes_expanded'],
                   label=planner, s=150, alpha=0.6, color=COLORS.get(planner, "#555"),
                   edgecolor='black', linewidth=1.5)

    ax.set_xlabel("Wall Clock Time (seconds)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.set_ylabel("Nodes Expanded (log scale)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.set_yscale('log')
    ax.set_xscale('log')
    ax.legend(fontsize=PLOT_CONFIG["font_size"])
    ax.set_title("Efficiency Frontier (Time vs Expansions)",
                 fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
    ax.grid(True, alpha=0.3, which='both')

    plt.tight_layout()
    plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
    logger.info(f"✓ {output_path}")
    plt.close()


# ============================================================================
# PLOT 5: PER-PROBLEM HEATMAP
# ============================================================================

def plot_per_problem_heatmap(df: 'pd.DataFrame', output_path: str, max_problems: int = 30):
    """Heatmap: problems vs planners, color = solve status."""
    if not HAS_MATPLOTLIB or df is None:
        return

    plt.style.use(PLOT_CONFIG["style"])

    problems = sorted(df['problem_name'].unique())[:max_problems]
    planners = sorted(df['planner_name'].unique())

    # Create matrix
    matrix = np.zeros((len(planners), len(problems)))

    for i, planner in enumerate(planners):
        for j, problem in enumerate(problems):
            df_cell = df[(df['planner_name'] == planner) & (df['problem_name'] == problem)]
            if len(df_cell) > 0:
                solved = int(df_cell.iloc[0]['solved'])
                matrix[i, j] = solved
            else:
                matrix[i, j] = -1  # Missing

    fig, ax = plt.subplots(figsize=(16, 6), dpi=PLOT_CONFIG["dpi"])

    im = ax.imshow(matrix, cmap='RdYlGn', aspect='auto', vmin=-1, vmax=1)

    ax.set_xticks(range(len(problems)))
    ax.set_yticks(range(len(planners)))
    ax.set_xticklabels([p.replace('problem_', '').replace('.pddl', '')[:15] for p in problems],
                       rotation=90, fontsize=8)
    ax.set_yticklabels(planners, fontsize=PLOT_CONFIG["font_size"])

    ax.set_title(f"Per-Problem Solve Status (first {max_problems} problems)",
                 fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')

    # Add colorbar
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label("Solved", fontsize=PLOT_CONFIG["font_size"])

    plt.tight_layout()
    plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
    logger.info(f"✓ {output_path}")
    plt.close()


# ============================================================================
# PLOT 6: TIME VIOLIN PLOTS
# ============================================================================

def plot_time_violin(df: 'pd.DataFrame', output_path: str):
    """Violin plot of time distribution."""
    if not HAS_MATPLOTLIB or df is None or not HAS_PANDAS:
        return

    plt.style.use(PLOT_CONFIG["style"])
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

    df_solved = df[df['solved']].copy()

    planners = sorted(df_solved['planner_name'].unique())

    parts = ax.violinplot(
        [df_solved[df_solved['planner_name'] == p]['wall_clock_time'].values for p in planners],
        positions=range(len(planners)),
        showmeans=True,
        showmedians=True
    )

    for pc in parts['bodies']:
        pc.set_facecolor('#2E86AB')
        pc.set_alpha(0.7)

    ax.set_xticks(range(len(planners)))
    ax.set_xticklabels(planners, rotation=45, ha='right')
    ax.set_ylabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.set_title("Time Distribution (Violin Plot)",
                 fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
    logger.info(f"✓ {output_path}")
    plt.close()


# ============================================================================
# PLOT 7: CUMULATIVE DISTRIBUTION
# ============================================================================

def plot_cumulative_distribution(df: 'pd.DataFrame', output_path: str):
    """Cumulative distribution of solve times."""
    if not HAS_MATPLOTLIB or df is None:
        return

    plt.style.use(PLOT_CONFIG["style"])
    fig, ax = plt.subplots(figsize=PLOT_CONFIG["figsize"], dpi=PLOT_CONFIG["dpi"])

    df_solved = df[df['solved']]

    for planner in sorted(df_solved['planner_name'].unique()):
        df_p = df_solved[df_solved['planner_name'] == planner]['wall_clock_time'].sort_values()
        cumulative = np.arange(1, len(df_p) + 1) / len(df_p)
        ax.plot(df_p.values, cumulative, marker='o', label=planner, linewidth=2,
                color=COLORS.get(planner, "#555"))

    ax.set_xlabel("Time (seconds)", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.set_ylabel("Cumulative Fraction Solved", fontsize=PLOT_CONFIG["font_size"] + 1, fontweight='bold')
    ax.legend(fontsize=PLOT_CONFIG["font_size"])
    ax.set_title("Cumulative Distribution of Solve Times",
                 fontsize=PLOT_CONFIG["font_size"] + 2, fontweight='bold')
    ax.grid(True, alpha=0.3)
    ax.set_xscale('log')

    plt.tight_layout()
    plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
    logger.info(f"✓ {output_path}")
    plt.close()


# ============================================================================
# PLOT 8: STATISTICAL SUMMARY
# ============================================================================

def plot_statistical_summary(df: 'pd.DataFrame', output_path: str):
    """Summary statistics comparison."""
    if not HAS_MATPLOTLIB or df is None:
        return

    plt.style.use(PLOT_CONFIG["style"])
    fig = plt.figure(figsize=(16, 10), dpi=PLOT_CONFIG["dpi"])
    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)

    df_solved = df[df['solved']]
    planners = sorted(df_solved['planner_name'].unique())

    # 1. Solve Rate
    ax1 = fig.add_subplot(gs[0, 0])
    solve_rates = [(df[df['planner_name'] == p]['solved'].sum() / len(df[df['planner_name'] == p]) * 100) for p in
                   planners]
    colors = [COLORS.get(p, "#555") for p in planners]
    ax1.bar(range(len(planners)), solve_rates, color=colors, alpha=0.8, edgecolor='black')
    ax1.set_ylabel("Solve Rate (%)", fontsize=10, fontweight='bold')
    ax1.set_title("Solve Rate", fontsize=11, fontweight='bold')
    ax1.set_xticks(range(len(planners)))
    ax1.set_xticklabels(planners, rotation=45, ha='right', fontsize=9)
    ax1.set_ylim([0, 105])
    ax1.grid(axis='y', alpha=0.3)

    # 2. Median Time
    ax2 = fig.add_subplot(gs[0, 1])
    median_times = [df_solved[df_solved['planner_name'] == p]['wall_clock_time'].median() for p in planners]
    ax2.bar(range(len(planners)), median_times, color=colors, alpha=0.8, edgecolor='black')
    ax2.set_ylabel("Median Time (s)", fontsize=10, fontweight='bold')
    ax2.set_title("Median Time (Solved)", fontsize=11, fontweight='bold')
    ax2.set_xticks(range(len(planners)))
    ax2.set_xticklabels(planners, rotation=45, ha='right', fontsize=9)
    ax2.grid(axis='y', alpha=0.3)

    # 3. Mean Expansions
    ax3 = fig.add_subplot(gs[1, 0])
    mean_exps = [df_solved[df_solved['planner_name'] == p]['nodes_expanded'].mean() for p in planners]
    ax3.bar(range(len(planners)), mean_exps, color=colors, alpha=0.8, edgecolor='black')
    ax3.set_ylabel("Mean Expansions", fontsize=10, fontweight='bold')
    ax3.set_title("Mean Nodes Expanded (Solved)", fontsize=11, fontweight='bold')
    ax3.set_xticks(range(len(planners)))
    ax3.set_xticklabels(planners, rotation=45, ha='right', fontsize=9)
    ax3.set_yscale('log')
    ax3.grid(axis='y', alpha=0.3)

    # 4. Mean Plan Cost
    ax4 = fig.add_subplot(gs[1, 1])
    mean_costs = [df_solved[df_solved['planner_name'] == p]['plan_cost'].mean() for p in planners]
    ax4.bar(range(len(planners)), mean_costs, color=colors, alpha=0.8, edgecolor='black')
    ax4.set_ylabel("Mean Plan Cost", fontsize=10, fontweight='bold')
    ax4.set_title("Mean Plan Cost (Solved)", fontsize=11, fontweight='bold')
    ax4.set_xticks(range(len(planners)))
    ax4.set_xticklabels(planners, rotation=45, ha='right', fontsize=9)
    ax4.grid(axis='y', alpha=0.3)

    fig.suptitle("Statistical Summary", fontsize=14, fontweight='bold', y=0.995)
    plt.savefig(output_path, dpi=PLOT_CONFIG["dpi"], bbox_inches='tight')
    logger.info(f"✓ {output_path}")
    plt.close()


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Analysis and visualization")
    parser.add_argument("--results", required=True, help="Path to evaluation_results.csv")
    parser.add_argument("--output", default="plots", help="Output directory")

    args = parser.parse_args()

    # Create output directory
    Path(args.output).mkdir(parents=True, exist_ok=True)

    # Load results
    if not HAS_PANDAS:
        logger.error("pandas required")
        return 1

    df = load_results_csv(args.results)
    if df is None:
        return 1

    logger.info("\nGenerating plots...\n")

    # Generate all plots
    plot_solve_rate_comparison(df, os.path.join(args.output, "01_solve_rate_comparison.png"))
    plot_time_comparison(df, os.path.join(args.output, "02_time_comparison.png"))
    plot_expansions_comparison(df, os.path.join(args.output, "03_expansions_comparison.png"))
    plot_efficiency_frontier(df, os.path.join(args.output, "04_efficiency_frontier.png"))
    plot_per_problem_heatmap(df, os.path.join(args.output, "05_per_problem_heatmap.png"))
    plot_time_violin(df, os.path.join(args.output, "06_time_violin.png"))
    plot_cumulative_distribution(df, os.path.join(args.output, "07_cumulative_distribution.png"))
    plot_statistical_summary(df, os.path.join(args.output, "08_statistical_summary.png"))

    logger.info(f"\n✅ All plots generated in {args.output}/")
    return 0


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file evaluation_comprehensive.py code is this:
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
COMPREHENSIVE EVALUATION FRAMEWORK - ENHANCED
==============================================
Complete rewrite with:
  ✓ Robust baseline runner with all major FD planners
  ✓ GNN policy runner using MergeEnv with real FD
  ✓ Detailed metric extraction (20+ metrics per run)
  ✓ Statistical analysis (mean, median, std, IQR)
  ✓ Multiple output formats (CSV, JSON, TXT)
  ✓ Per-problem and aggregate reporting
  ✓ Error tracking and diagnostics
"""

import sys
import os
import logging
import glob
import json
import subprocess
import time
import re
import csv
import argparse
import numpy as np
import shutil
import traceback
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from datetime import datetime
from dataclasses import dataclass, asdict
from collections import defaultdict

sys.path.insert(0, os.getcwd())

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("evaluation_comprehensive.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


# ============================================================================
# DATA STRUCTURES - COMPREHENSIVE METRICS
# ============================================================================

@dataclass
class DetailedMetrics:
    """Complete set of metrics for a single run."""
    problem_name: str
    planner_name: str

    # Solve status
    solved: bool
    wall_clock_time: float

    # Planning quality
    plan_cost: int = 0
    plan_length: int = 0

    # Search metrics
    nodes_expanded: int = 0
    nodes_generated: int = 0
    search_depth: int = 0
    branching_factor: float = 1.0

    # Memory metrics (if available)
    peak_memory_kb: int = 0

    # Time breakdown
    search_time: float = 0.0
    translate_time: float = 0.0
    preprocess_time: float = 0.0

    # Solution quality
    solution_length: int = 0
    plan_optimality: float = 1.0  # 1.0 if optimal

    # Heuristic quality (if evaluator)
    initial_heuristic: int = 0
    average_heuristic: float = 0.0

    # Error info
    error_type: Optional[str] = None
    error_message: Optional[str] = None

    # GNN-specific
    gnn_decisions: int = 0  # Number of merge decisions made
    merge_episodes: int = 0

    def efficiency_score(self) -> float:
        """Score: lower is better. 0 is best."""
        if not self.solved:
            return float('inf')

        # Normalize by problem size (approximated by plan cost)
        if self.nodes_expanded > 0 and self.plan_cost > 0:
            return (self.nodes_expanded / (self.plan_cost * 100.0)) + (self.wall_clock_time / 10.0)
        return self.wall_clock_time

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for CSV/JSON."""
        d = asdict(self)
        d['efficiency_score'] = self.efficiency_score()
        return d


@dataclass
class AggregateStatistics:
    """Summary statistics across all problems."""
    planner_name: str
    num_problems_total: int
    num_problems_solved: int
    solve_rate_pct: float

    # Time stats (solved only)
    mean_time_sec: float
    median_time_sec: float
    std_time_sec: float
    min_time_sec: float
    max_time_sec: float
    q1_time_sec: float  # 25th percentile
    q3_time_sec: float  # 75th percentile

    # Expansions stats
    mean_expansions: int
    median_expansions: int
    std_expansions: int

    # Plan quality stats
    mean_plan_cost: int
    median_plan_cost: int
    std_plan_cost: int

    # Efficiency
    mean_efficiency_score: float

    # Coverage metrics
    unsolved_count: int
    error_count: int
    timeout_count: int

    # Aggregate times
    total_wall_clock_time_sec: float

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


# ============================================================================
# BASELINE RUNNER - COMPLETE REFACTORING
# ============================================================================

class BaselineRunner:
    """Enhanced baseline runner with comprehensive metrics extraction."""

    # Baseline configurations with ALL important FD variants
    BASELINES = [
        {
            "name": "FD_LM-Cut",
            "search": "astar(lmcut())"
        },
        {
            "name": "FD_Blind",
            "search": "astar(blind())"
        },
        {
            "name": "FD_Add",
            "search": "astar(add())"
        },
        {
            "name": "FD_Max",
            "search": "astar(max())"
        },
        {
            "name": "FD_M&S_DFP",
            "search": (
                "astar(merge_and_shrink("
                "merge_strategy=merge_stateless("
                "merge_selector=score_based_filtering("
                "scoring_functions=[goal_relevance(),dfp(),total_order()])),"
                "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
                "label_reduction=exact(before_shrinking=true,before_merging=false),"
                "max_states=50000,threshold_before_merge=1))"
            )
        },
        {
            "name": "FD_M&S_SCC",
            "search": (
                "astar(merge_and_shrink("
                "merge_strategy=merge_sccs("
                "order_of_sccs=topological,"
                "merge_selector=score_based_filtering("
                "scoring_functions=[goal_relevance(),dfp(),total_order()])),"
                "shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),"
                "label_reduction=exact(before_shrinking=true,before_merging=false),"
                "max_states=50000,threshold_before_merge=1))"
            )
        },
    ]

    def __init__(self, timeout_sec: int = 300):
        self.timeout_sec = timeout_sec
        self.fd_bin = os.path.abspath("downward/builds/release/bin/downward.exe")
        self.fd_translate = os.path.abspath("downward/builds/release/bin/translate/translate.py")

        if not os.path.exists(self.fd_bin) or not os.path.exists(self.fd_translate):
            raise FileNotFoundError("Fast Downward binary not found")

    def run(self, domain_file: str, problem_file: str, search_config: str) -> DetailedMetrics:
        """Run FD with comprehensive metrics extraction."""
        problem_name = os.path.basename(problem_file)

        try:
            # TRANSLATE PHASE
            logger.info(f"    [TRANSLATE] {problem_name}...")

            work_dir = os.path.abspath("evaluation_temp")
            os.makedirs(work_dir, exist_ok=True)
            sas_file = os.path.join(work_dir, "output.sas")

            translate_start = time.time()

            result = subprocess.run(
                f'python "{self.fd_translate}" "{os.path.abspath(domain_file)}" '
                f'"{os.path.abspath(problem_file)}" --sas-file "{sas_file}"',
                shell=True,
                cwd=os.path.abspath("."),
                capture_output=True,
                text=True,
                timeout=self.timeout_sec
            )

            translate_time = time.time() - translate_start

            if result.returncode != 0 or not os.path.exists(sas_file):
                logger.debug(f"    [TRANSLATE] Failed: {result.stderr[:200]}")
                return DetailedMetrics(
                    problem_name=problem_name,
                    planner_name="FD",
                    solved=False,
                    wall_clock_time=translate_time,
                    error_type="translate_error",
                    error_message=result.stderr[:500]
                )

            logger.debug(f"    [TRANSLATE] Success ({os.path.getsize(sas_file)} bytes)")

            # SEARCH PHASE
            logger.debug(f"    [SEARCH] Starting...")

            search_start = time.time()

            result = subprocess.run(
                f'"{self.fd_bin}" --search "{search_config}" < "{sas_file}"',
                shell=True,
                cwd=os.path.dirname(self.fd_bin),
                capture_output=True,
                text=True,
                timeout=self.timeout_sec
            )

            search_time = time.time() - search_start
            total_time = translate_time + search_time

            output_text = result.stdout + result.stderr

            logger.debug(f"    [SEARCH] Completed in {search_time:.2f}s")

            # CHECK FOR SOLUTION
            if "Solution found" not in output_text and "Plan length:" not in output_text:
                logger.debug(f"    [PARSE] No solution found")
                return DetailedMetrics(
                    problem_name=problem_name,
                    planner_name="FD",
                    solved=False,
                    wall_clock_time=total_time,
                    translate_time=translate_time,
                    search_time=search_time,
                    error_type="no_solution"
                )

            # EXTRACT METRICS
            metrics_dict = self._parse_fd_output(output_text)

            if metrics_dict is None:
                logger.debug(f"    [PARSE] Could not extract metrics")
                return DetailedMetrics(
                    problem_name=problem_name,
                    planner_name="FD",
                    solved=True,
                    wall_clock_time=total_time,
                    translate_time=translate_time,
                    search_time=search_time,
                    error_type="parse_error"
                )

            # BUILD RESULT
            result_metrics = DetailedMetrics(
                problem_name=problem_name,
                planner_name="FD",
                solved=True,
                wall_clock_time=total_time,
                translate_time=translate_time,
                search_time=search_time,
                plan_cost=metrics_dict.get('cost', 0),
                plan_length=metrics_dict.get('cost', 0),
                nodes_expanded=metrics_dict.get('expansions', 0),
                search_depth=metrics_dict.get('search_depth', 0),
                branching_factor=metrics_dict.get('branching_factor', 1.0),
                peak_memory_kb=metrics_dict.get('memory', 0),
            )

            logger.debug(f"    [SUCCESS] cost={result_metrics.plan_cost}, "
                         f"exp={result_metrics.nodes_expanded}")

            return result_metrics

        except subprocess.TimeoutExpired:
            logger.debug(f"    [TIMEOUT] Exceeded {self.timeout_sec}s")
            return DetailedMetrics(
                problem_name=problem_name,
                planner_name="FD",
                solved=False,
                wall_clock_time=self.timeout_sec,
                error_type="timeout"
            )

        except Exception as e:
            logger.error(f"    [ERROR] {e}")
            return DetailedMetrics(
                problem_name=problem_name,
                planner_name="FD",
                solved=False,
                wall_clock_time=0,
                error_type="exception",
                error_message=str(e)[:500]
            )

    @staticmethod
    def _parse_fd_output(output_text: str) -> Optional[Dict[str, Any]]:
        """Extract comprehensive metrics from FD output."""
        metrics = {}

        # Plan cost
        match = re.search(r'Plan length:\s*(\d+)', output_text)
        if match:
            metrics['cost'] = int(match.group(1))

        # Expansions (take last)
        matches = list(re.finditer(r'Expanded\s+(\d+)\s+state', output_text))
        if matches:
            metrics['expansions'] = int(matches[-1].group(1))

        # Generated states
        matches = list(re.finditer(r'Generated\s+(\d+)\s+state', output_text))
        if matches:
            metrics['generated'] = int(matches[-1].group(1))

        # Search depth
        match = re.search(r'Search depth:\s*(\d+)', output_text)
        if match:
            metrics['search_depth'] = int(match.group(1))

        # Branching factor
        match = re.search(r'Branching factor:\s*([\d.]+)', output_text)
        if match:
            metrics['branching_factor'] = float(match.group(1))

        # Memory
        match = re.search(r'Peak memory:\s*(\d+)\s*KB', output_text)
        if match:
            metrics['memory'] = int(match.group(1))

        # Require at least cost and expansions
        if 'cost' not in metrics or 'expansions' not in metrics:
            return None

        return metrics


# ============================================================================
# GNN POLICY RUNNER - USING MergeEnv
# ============================================================================

class GNNPolicyRunner:
    """GNN policy runner using real MergeEnv with FD feedback."""

    def __init__(self, model_path: str, timeout_sec: int = 300):
        self.model_path = model_path
        self.timeout_sec = timeout_sec

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")

        from stable_baselines3 import PPO
        from merge_env import MergeEnv
        self.PPO = PPO
        self.MergeEnv = MergeEnv

    def run(self, domain_file: str, problem_file: str) -> DetailedMetrics:
        """Run GNN policy on a problem."""
        problem_name = os.path.basename(problem_file)

        try:
            start_time = time.time()

            logger.debug(f"    [LOAD] Loading model...")
            model = self.PPO.load(self.model_path)
            logger.debug(f"    [LOAD] Model loaded")

            logger.debug(f"    [ENV] Creating environment...")
            env = self.MergeEnv(
                domain_file=os.path.abspath(domain_file),
                problem_file=os.path.abspath(problem_file),
                max_merges=50,
                debug=False,
                reward_variant='astar_search',
                w_search_efficiency=0.30,
                w_solution_quality=0.20,
                w_f_stability=0.35,
                w_state_control=0.15,
            )
            logger.debug(f"    [ENV] Environment created")

            logger.debug(f"    [RESET] Resetting environment...")
            solve_start = time.time()
            obs, info = env.reset()
            logger.debug(f"    [RESET] Environment reset")

            total_reward = 0.0
            steps = 0
            max_steps = 50
            gnn_decisions = 0

            logger.debug(f"    [INFERENCE] Starting...")

            while steps < max_steps:
                try:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, done, truncated, info = env.step(int(action))
                    total_reward += reward
                    steps += 1
                    gnn_decisions += 1

                    if done or truncated:
                        break

                except KeyboardInterrupt:
                    logger.warning("Interrupted by user")
                    break
                except Exception as e:
                    logger.error(f"Step failed: {e}")
                    break

            solve_time = time.time() - solve_start
            total_time = time.time() - start_time

            logger.debug(f"    [INFERENCE] Completed in {steps} steps")

            # Extract metrics from FD output
            plan_cost, expansions, nodes_expanded, search_depth, solution_found = \
                self._extract_fd_metrics()

            logger.debug(f"    [EXTRACT] FD metrics extracted")

            try:
                env.close()
            except:
                pass

            result = DetailedMetrics(
                problem_name=problem_name,
                planner_name="GNN",
                solved=solution_found,
                wall_clock_time=total_time,
                plan_cost=plan_cost,
                nodes_expanded=nodes_expanded,
                search_depth=search_depth,
                gnn_decisions=gnn_decisions,
                merge_episodes=steps,
            )

            return result

        except Exception as e:
            logger.error(f"GNN run failed: {e}")
            logger.error(traceback.format_exc())

            try:
                env.close()
            except:
                pass

            return DetailedMetrics(
                problem_name=problem_name,
                planner_name="GNN",
                solved=False,
                wall_clock_time=0,
                error_type="exception",
                error_message=str(e)[:500]
            )

    @staticmethod
    def _extract_fd_metrics() -> Tuple[int, int, int, int, bool]:
        """Extract metrics from FD output files."""
        plan_cost = 0
        expansions = 0
        nodes_expanded = 0
        search_depth = 0
        solution_found = False

        log_file = os.path.join("downward", "fd_output", "log.txt")

        if not os.path.exists(log_file):
            return plan_cost, expansions, nodes_expanded, search_depth, solution_found

        try:
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()

            # Extract plan cost
            match = re.search(r'Plan length:\s*(\d+)', content)
            if match:
                plan_cost = int(match.group(1))
                solution_found = True

            # Extract expansions
            matches = list(re.finditer(r'Expanded\s+(\d+)\s+state', content))
            if matches:
                expansions = int(matches[-1].group(1))

            # Extract search depth
            match = re.search(r'Search depth:\s*(\d+)', content)
            if match:
                search_depth = int(match.group(1))

            nodes_expanded = expansions

        except Exception as e:
            logger.warning(f"Error extracting FD metrics: {e}")

        return plan_cost, expansions, nodes_expanded, search_depth, solution_found


# ============================================================================
# EVALUATION ORCHESTRATOR
# ============================================================================

class EvaluationFramework:
    """Main evaluation orchestrator."""

    def __init__(self, output_dir: str = "evaluation_results"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.results: List[DetailedMetrics] = []

    def run_comprehensive_evaluation(
            self,
            domain_file: str,
            problem_pattern: str,
            model_path: str,
            timeout_sec: int = 300,
            include_baselines: bool = True
    ) -> Dict[str, Any]:
        """Run complete evaluation."""

        print_section("COMPREHENSIVE EVALUATION FRAMEWORK")

        # Load problems
        logger.info(f"\nLoading problems matching: {problem_pattern}")
        problems = sorted(glob.glob(problem_pattern))

        if not problems:
            logger.error("No problems found!")
            return {}

        logger.info(f"Found {len(problems)} problem(s)\n")

        # Run baselines
        if include_baselines:
            self._run_all_baselines(domain_file, problems, timeout_sec)

        # Run GNN
        self._run_gnn(domain_file, problems, model_path, timeout_sec)

        # Generate reports
        return self._generate_comprehensive_report()

    def _run_all_baselines(self, domain_file: str, problems: List[str], timeout_sec: int):
        """Run all baseline configurations."""
        print_subsection("RUNNING BASELINE PLANNERS")

        baseline_runner = BaselineRunner(timeout_sec)

        for baseline_config in baseline_runner.BASELINES:
            logger.info(f"\n{baseline_config['name']}")
            logger.info("-" * 60)

            for i, problem in enumerate(problems, 1):
                logger.info(f"  [{i}/{len(problems)}] {os.path.basename(problem)}")

                result = baseline_runner.run(
                    domain_file,
                    problem,
                    baseline_config['search']
                )
                result.planner_name = baseline_config['name']
                self.results.append(result)

    def _run_gnn(self, domain_file: str, problems: List[str], model_path: str, timeout_sec: int):
        """Run GNN policy."""
        print_subsection("RUNNING GNN POLICY")

        gnn_runner = GNNPolicyRunner(model_path, timeout_sec)

        for i, problem in enumerate(problems, 1):
            logger.info(f"  [{i}/{len(problems)}] {os.path.basename(problem)}")

            result = gnn_runner.run(domain_file, problem)
            self.results.append(result)

    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate reports and statistics."""
        print_section("GENERATING COMPREHENSIVE REPORT")

        # Group by planner
        by_planner = defaultdict(list)
        for result in self.results:
            by_planner[result.planner_name].append(result)

        # Compute statistics
        summaries = {}
        for planner_name, results_list in by_planner.items():
            summary = self._compute_statistics(planner_name, results_list)
            summaries[planner_name] = summary

            logger.info(f"\n{planner_name}:")
            logger.info(f"  Solve rate: {summary.solve_rate_pct:.1f}%")
            logger.info(f"  Avg time: {summary.mean_time_sec:.2f}s")
            logger.info(f"  Avg expansions: {summary.mean_expansions}")

        # Export results
        self._export_all_results(summaries)

        return {
            "summaries": {name: summary.to_dict() for name, summary in summaries.items()},
            "timestamp": datetime.now().isoformat()
        }

    def _compute_statistics(self, planner_name: str, results_list: List[DetailedMetrics]) -> AggregateStatistics:
        """Compute comprehensive statistics."""

        solved = [r for r in results_list if r.solved]
        num_solved = len(solved)
        num_total = len(results_list)

        times = [r.wall_clock_time for r in solved]
        expansions = [r.nodes_expanded for r in solved]
        costs = [r.plan_cost for r in solved]
        efficiency_scores = [r.efficiency_score() for r in results_list if r.solved]

        unsolved = [r for r in results_list if not r.solved]
        errors = [r for r in unsolved if r.error_type]
        timeouts = [r for r in unsolved if r.error_type == "timeout"]

        return AggregateStatistics(
            planner_name=planner_name,
            num_problems_total=num_total,
            num_problems_solved=num_solved,
            solve_rate_pct=(num_solved / max(num_total, 1)) * 100,
            mean_time_sec=np.mean(times) if times else 0,
            median_time_sec=np.median(times) if times else 0,
            std_time_sec=np.std(times) if times else 0,
            min_time_sec=np.min(times) if times else 0,
            max_time_sec=np.max(times) if times else 0,
            q1_time_sec=np.percentile(times, 25) if times else 0,
            q3_time_sec=np.percentile(times, 75) if times else 0,
            mean_expansions=int(np.mean(expansions)) if expansions else 0,
            median_expansions=int(np.median(expansions)) if expansions else 0,
            std_expansions=int(np.std(expansions)) if expansions else 0,
            mean_plan_cost=int(np.mean(costs)) if costs else 0,
            median_plan_cost=int(np.median(costs)) if costs else 0,
            std_plan_cost=int(np.std(costs)) if costs else 0,
            mean_efficiency_score=np.mean(efficiency_scores) if efficiency_scores else float('inf'),
            unsolved_count=len(unsolved),
            error_count=len(errors),
            timeout_count=len(timeouts),
            total_wall_clock_time_sec=sum(times) if times else 0,
        )

    def _export_all_results(self, summaries: Dict[str, AggregateStatistics]):
        """Export results in all formats."""

        # CSV: detailed
        csv_path = self.output_dir / "evaluation_results.csv"
        fieldnames = list(self.results[0].to_dict().keys()) if self.results else []

        with open(csv_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            for result in self.results:
                writer.writerow(result.to_dict())

        logger.info(f"✓ CSV: {csv_path}")

        # JSON: summary
        json_path = self.output_dir / "evaluation_summary.json"
        with open(json_path, 'w') as f:
            json.dump(
                {name: summary.to_dict() for name, summary in summaries.items()},
                f,
                indent=2
            )

        logger.info(f"✓ JSON: {json_path}")

        # TXT: comparison report
        self._write_text_report(summaries)

    def _write_text_report(self, summaries: Dict[str, AggregateStatistics]):
        """Write formatted text comparison report."""

        report_path = self.output_dir / "comparison_report.txt"

        try:
            with open(report_path, 'w') as f:
                f.write("=" * 90 + "\n")
                f.write("COMPREHENSIVE EVALUATION - COMPARISON REPORT\n")
                f.write("=" * 90 + "\n\n")

                f.write(f"Timestamp: {datetime.now().isoformat()}\n")
                num_problems = summaries[list(summaries.keys())[0]].num_problems_total if summaries else 0
                f.write(f"Total problems evaluated: {num_problems}\n\n")

                # Summary table
                f.write("SUMMARY TABLE\n")
                f.write("-" * 90 + "\n")
                header = (
                    f"{'Planner':<25} {'Solved':<18} {'Avg Time (s)':<15} "
                    f"{'Med Time (s)':<15} {'Avg Exp.':<15}"
                )
                f.write(header + "\n")
                f.write("-" * 90 + "\n")

                # Sort planners, maybe put 'GNN' first if present
                planner_names = sorted(summaries.keys())
                if "GNN" in planner_names:
                    planner_names.insert(0, planner_names.pop(planner_names.index("GNN")))

                for planner_name in planner_names:
                    if planner_name not in summaries: continue
                    summary = summaries[planner_name]
                    solved_str = (
                        f"{summary.num_problems_solved}/{summary.num_problems_total} "
                        f"({summary.solve_rate_pct:.1f}%)"
                    )
                    avg_exp_str = f"{summary.mean_expans:,}" if summary.num_problems_solved > 0 else "N/A" # Added comma formatting

                    line = (
                        f"{planner_name:<25} {solved_str:<18} {summary.mean_time_sec:<15.2f} "
                        f"{summary.median_time_sec:<15.2f} {avg_exp_str:<15}"
                    )
                    f.write(line + "\n")

                f.write("-" * 90 + "\n\n")

                # Detailed stats per planner (optional, could make report long)
                # You can add more details here if needed, similar to the JSON summary

            logger.info(f"✓ Text report: {report_path}")

        except Exception as e:
            logger.error(f"Failed to write text report: {e}")
            logger.error(traceback.format_exc())

def print_section(title: str, width: int = 90):
    logger.info("\n" + "=" * width)
    logger.info(f"// {title.upper()}")
    logger.info("=" * width + "\n")


def print_subsection(title: str):
    logger.info("\n" + "-" * 80)
    logger.info(f">>> {title}")
    logger.info("-" * 80 + "\n")


def main():
    parser = argparse.ArgumentParser(description="Comprehensive evaluation framework")
    parser.add_argument("--model", required=True, help="Path to trained model")
    parser.add_argument("--domain", required=True, help="Path to domain PDDL")
    parser.add_argument("--problems", required=True, help="Glob pattern for problems")
    parser.add_argument("--timeout", type=int, default=300, help="Timeout per problem")
    parser.add_argument("--output", default="evaluation_results", help="Output directory")
    parser.add_argument("--skip-baselines", action="store_true", help="Skip baselines")

    args = parser.parse_args()

    framework = EvaluationFramework(args.output)
    results = framework.run_comprehensive_evaluation(
        domain_file=args.domain,
        problem_pattern=args.problems,
        model_path=args.model,
        timeout_sec=args.timeout,
        include_baselines=not args.skip_baselines
    )

    print_section("EVALUATION COMPLETE")
    logger.info("✅ All evaluations complete!")
    logger.info(f"Results saved to: {os.path.abspath(args.output)}")

    return 0


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file run_full_evaluation.py code is this:
# !/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
END-TO-END EVALUATION ORCHESTRATOR
==================================
Master script that runs comprehensive evaluation: baselines + GNN + analysis.

This is the primary interface for evaluating your GNN model against baselines.

Usage:
    python run_all_evaluation.py \
        --model mvp_output/gnn_model.zip \
        --domain domain.pddl \
        --problems "problem_small_*.pddl" \
        --output evaluation_results/ \
        --timeout 300

Features:
  ✓ Single entry point for all evaluation
  ✓ Automatic input validation
  ✓ Progressive execution with checkpointing
  ✓ Comprehensive error handling
  ✓ Final report generation
  ✓ Result visualization

Output Structure:
    evaluation_results/
    ├── evaluation_results.csv        # Detailed per-problem results
    ├── evaluation_summary.json        # Aggregate statistics
    ├── comparison_report.txt          # Human-readable comparison
    ├── evaluation.log                 # Full debug log
    ├── plots/
    │   ├── solve_rate_comparison.png
    │   ├── time_comparison.png
    │   ├── expansions_comparison.png
    │   ├── efficiency_frontier.png
    │   ├── per_problem_comparison.png
    │   ├── statistical_analysis.png
    │   └── summary_dashboard.html
    └── checkpoints/
        └── evaluation_checkpoint.json # Resume point
"""

import sys
import os
import logging
import argparse
import subprocess
import json
from pathlib import Path
from datetime import datetime
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("evaluation_orchestrator.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


# ============================================================================
# STAGE 1: INPUT VALIDATION
# ============================================================================

class EvaluationValidator:
    """Validates all inputs before starting evaluation."""

    @staticmethod
    def validate_model(model_path: str) -> bool:
        """Check that model file exists and is readable."""
        if not model_path:
            logger.error("Model path not provided")
            return False

        if not os.path.exists(model_path):
            logger.error(f"Model file not found: {model_path}")
            return False

        if not model_path.endswith('.zip'):
            logger.warning(f"Model file does not end with .zip: {model_path}")

        try:
            # Try to read as ZIP to verify it's a valid model file
            import zipfile
            with zipfile.ZipFile(model_path, 'r') as z:
                if 'data' not in z.namelist() and 'policy.optimizer_states' not in z.namelist():
                    logger.warning(f"Model file may not be a valid PPO model: {model_path}")
        except Exception as e:
            logger.warning(f"Could not validate model ZIP: {e}")

        logger.info(f"✓ Model validated: {model_path}")
        return True

    @staticmethod
    def validate_domain(domain_path: str) -> bool:
        """Check that domain PDDL file exists."""
        if not domain_path:
            logger.error("Domain path not provided")
            return False

        if not os.path.exists(domain_path):
            logger.error(f"Domain file not found: {domain_path}")
            return False

        if not domain_path.endswith('.pddl'):
            logger.warning(f"Domain file does not end with .pddl: {domain_path}")

        # Minimal syntax check
        try:
            with open(domain_path, 'r') as f:
                content = f.read()
                if '(define (domain' not in content:
                    logger.warning(f"Domain file may not be valid PDDL: {domain_path}")
        except Exception as e:
            logger.error(f"Could not read domain file: {e}")
            return False

        logger.info(f"✓ Domain validated: {domain_path}")
        return True

    @staticmethod
    def validate_problems(problem_pattern: str) -> bool:
        """Check that problem files exist matching the pattern."""
        if not problem_pattern:
            logger.error("Problem pattern not provided")
            return False

        import glob
        problems = sorted(glob.glob(problem_pattern))

        if not problems:
            logger.error(f"No problems found matching pattern: {problem_pattern}")
            return False

        logger.info(f"✓ Found {len(problems)} problem(s) matching pattern")

        # Validate first few
        for prob in problems[:min(3, len(problems))]:
            if not prob.endswith('.pddl'):
                logger.warning(f"Problem file does not end with .pddl: {prob}")
            try:
                with open(prob, 'r') as f:
                    content = f.read()
                    if '(define (problem' not in content:
                        logger.warning(f"File may not be valid PDDL: {prob}")
            except Exception as e:
                logger.error(f"Could not read problem file {prob}: {e}")
                return False

        return True

    @staticmethod
    def validate_output_dir(output_dir: str) -> bool:
        """Check that output directory is writable."""
        try:
            os.makedirs(output_dir, exist_ok=True)

            # Try to write a test file
            test_file = os.path.join(output_dir, ".write_test")
            with open(test_file, 'w') as f:
                f.write("test")
            os.remove(test_file)

            logger.info(f"✓ Output directory ready: {output_dir}")
            return True

        except Exception as e:
            logger.error(f"Cannot write to output directory: {e}")
            return False

    @staticmethod
    def validate_all(model_path: str, domain_path: str, problem_pattern: str, output_dir: str) -> bool:
        """Run all validations."""
        logger.info("\n" + "=" * 80)
        logger.info("STAGE 0: INPUT VALIDATION")
        logger.info("=" * 80 + "\n")

        checks = [
            ("Model file", lambda: EvaluationValidator.validate_model(model_path)),
            ("Domain PDDL", lambda: EvaluationValidator.validate_domain(domain_path)),
            ("Problem files", lambda: EvaluationValidator.validate_problems(problem_pattern)),
            ("Output directory", lambda: EvaluationValidator.validate_output_dir(output_dir)),
        ]

        results = []
        for name, check in checks:
            try:
                result = check()
                results.append((name, result))
            except Exception as e:
                logger.error(f"✗ {name} validation failed: {e}")
                results.append((name, False))

        # Summary
        passed = sum(1 for _, r in results if r)
        total = len(results)

        logger.info(f"\nValidation Summary: {passed}/{total} passed")

        return all(r for _, r in results)


# ============================================================================
# STAGE 2: RUN COMPREHENSIVE EVALUATION
# ============================================================================

def run_comprehensive_evaluation(
        model_path: str,
        domain_path: str,
        problem_pattern: str,
        output_dir: str,
        timeout: int,
        skip_baselines: bool
) -> bool:
    """Execute comprehensive evaluation framework."""

    logger.info("\n" + "=" * 80)
    logger.info("STAGE 1: COMPREHENSIVE EVALUATION")
    logger.info("=" * 80 + "\n")

    try:
        cmd = [
            "python", "evaluation_comprehensive.py",
            "--model", model_path,
            "--domain", domain_path,
            "--problems", problem_pattern,
            "--output", output_dir,
            "--timeout", str(timeout)
        ]

        if skip_baselines:
            cmd.append("--skip-baselines")

        logger.info(f"Running command: {' '.join(cmd)}\n")

        result = subprocess.run(
            cmd,
            capture_output=False,
            text=True
        )

        if result.returncode != 0:
            logger.error(f"Comprehensive evaluation failed with code {result.returncode}")
            return False

        logger.info("\n✅ Comprehensive evaluation completed")
        return True

    except Exception as e:
        logger.error(f"Failed to run comprehensive evaluation: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False


# ============================================================================
# STAGE 3: RUN ANALYSIS AND VISUALIZATION
# ============================================================================

def run_analysis_and_visualization(
        results_csv: str,
        output_dir: str
) -> bool:
    """Execute analysis and visualization framework."""

    logger.info("\n" + "=" * 80)
    logger.info("STAGE 2: ANALYSIS AND VISUALIZATION")
    logger.info("=" * 80 + "\n")

    try:
        # Check if CSV exists
        if not os.path.exists(results_csv):
            logger.warning(f"Results CSV not found: {results_csv}")
            logger.warning("Skipping analysis and visualization")
            return False

        plots_dir = os.path.join(output_dir, "plots")

        cmd = [
            "python", "analysis_and_visualization.py",
            "--results", results_csv,
            "--output", plots_dir
        ]

        logger.info(f"Running command: {' '.join(cmd)}\n")

        result = subprocess.run(
            cmd,
            capture_output=False,
            text=True
        )

        if result.returncode != 0:
            logger.warning(f"Analysis and visualization exited with code {result.returncode}")
            logger.warning("Continuing with evaluation results anyway")
            return False

        logger.info("\n✅ Analysis and visualization completed")
        return True

    except Exception as e:
        logger.error(f"Failed to run analysis and visualization: {e}")
        logger.error("Continuing with evaluation results anyway")
        return False


# ============================================================================
# STAGE 4: GENERATE FINAL REPORT
# ============================================================================

def generate_final_report(output_dir: str) -> bool:
    """Create comprehensive final report."""

    logger.info("\n" + "=" * 80)
    logger.info("STAGE 3: FINAL REPORT GENERATION")
    logger.info("=" * 80 + "\n")

    try:
        report_path = os.path.join(output_dir, "EVALUATION_REPORT.txt")

        # CORRECTED LINE
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("=" * 90 + "\n")
            f.write("GNN MERGE STRATEGY - COMPREHENSIVE EVALUATION REPORT\n")
            f.write("=" * 90 + "\n\n")

            f.write(f"Timestamp: {datetime.now().isoformat()}\n\n")

            # Overview
            f.write("EVALUATION OVERVIEW\n")
            f.write("-" * 90 + "\n")
            f.write("This report contains the complete evaluation of the trained GNN policy\n")
            f.write("against baseline Fast Downward planners on a test set of problems.\n\n")

            # Key Files
            f.write("KEY FILES\n")
            f.write("-" * 90 + "\n")
            f.write(f"  Detailed results:     evaluation_results.csv\n")
            f.write(f"  Summary statistics:   evaluation_summary.json\n")
            f.write(f"  Comparison report:    comparison_report.txt\n")
            f.write(f"  Plots directory:      plots/\n\n")

            # How to View Results
            f.write("HOW TO INTERPRET RESULTS\n")
            f.write("-" * 90 + "\n")
            f.write("1. SOLVE RATE: Percentage of problems solved by each planner\n")
            f.write("   - Higher is better. Target: > 80% for GNN on test set.\n\n")

            f.write("2. AVERAGE TIME: Mean solution time for solved problems\n")
            f.write("   - Lower is better. Measures computational efficiency.\n\n")

            f.write("3. EXPANSIONS: Mean number of nodes expanded during search\n")
            f.write("   - Lower is better. Indicates heuristic quality.\n\n")

            f.write("4. PLAN COST: Quality of solutions found\n")
            f.write("   - Lower is better. Indicates solution optimality.\n\n")

            f.write("5. EFFICIENCY FRONTIER: Trade-off between time and expansions\n")
            f.write("   - Curves closer to origin are better.\n\n")

            # Next Steps
            f.write("NEXT STEPS\n")
            f.write("-" * 90 + "\n")
            f.write("1. Review the comparison_report.txt for baseline performance\n")
            f.write("2. Check plots/solve_rate_comparison.png for GNN vs baselines\n")
            f.write("3. Analyze per_problem_comparison.png for problem-specific insights\n")
            f.write("4. Review evaluation_summary.json for detailed statistics\n\n")

            # Recommendations
            f.write("INTERPRETATION GUIDELINES\n")
            f.write("-" * 90 + "\n")
            f.write("✅ EXCELLENT: GNN solves >= 90% of problems with time comparable to LM-Cut\n")
            f.write("✓  GOOD:      GNN solves >= 80% of problems with reasonable time\n")
            f.write("⚠️  FAIR:      GNN solves >= 60% of problems, needs optimization\n")
            f.write("❌ POOR:      GNN solves < 60% of problems, requires retraining\n\n")

            f.write("=" * 90 + "\n")

        logger.info(f"✅ Final report written: {report_path}")

        # Print summary
        logger.info("\n" + "=" * 90)
        logger.info("EVALUATION COMPLETE")
        logger.info("=" * 90 + "\n")

        logger.info("Results saved to:")
        logger.info(f"  {os.path.abspath(output_dir)}\n")

        logger.info("Key files:")
        logger.info(f"  - {os.path.join(output_dir, 'evaluation_results.csv')}")
        logger.info(f"  - {os.path.join(output_dir, 'comparison_report.txt')}")
        logger.info(f"  - {os.path.join(output_dir, 'plots/')}\n")

        logger.info("To view results:")
        logger.info(f"  cat {os.path.join(output_dir, 'EVALUATION_REPORT.txt')}\n")

        return True

    except Exception as e:
        logger.error(f"Failed to generate final report: {e}")
        return False


# ============================================================================
# MAIN ORCHESTRATOR
# ============================================================================

def main():
    """Main execution orchestrator."""
    parser = argparse.ArgumentParser(
        description="End-to-end evaluation orchestrator for GNN merge strategy",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:

  # Basic evaluation
  python run_all_evaluation.py \\
      --model mvp_output/gnn_model.zip \\
      --domain domain.pddl \\
      --problems "problem_small_*.pddl" \\
      --output evaluation_results/


  # Skip baselines (faster)
  python run_all_evaluation.py \\
      --model mvp_output/gnn_model.zip \\
      --domain domain.pddl \\
      --problems "problem_small_*.pddl" \\
      --output evaluation_results/ \\
      --skip-baselines

  # Longer timeout
  python run_all_evaluation.py \\
      --model mvp_output/gnn_model.zip \\
      --domain domain.pddl \\
      --problems "problem_*.pddl" \\
      --output evaluation_results/ \\
      --timeout 600
        """
    )

    parser.add_argument(
        "--model", required=True,
        help="Path to trained GNN model (ZIP file)"
    )
    parser.add_argument(
        "--domain", required=True,
        help="Path to domain PDDL file"
    )
    parser.add_argument(
        "--problems", required=True,
        help="Glob pattern for problem PDDL files"
    )
    parser.add_argument(
        "--output", default="evaluation_results",
        help="Output directory for results (default: evaluation_results)"
    )
    parser.add_argument(
        "--timeout", type=int, default=300,
        help="Timeout per problem in seconds (default: 300)"
    )
    parser.add_argument(
        "--skip-baselines", action="store_true",
        help="Skip baseline evaluation (faster, GNN only)"
    )

    args = parser.parse_args()

    # ====================================================================
    # STAGE 0: VALIDATION
    # ====================================================================

    if not EvaluationValidator.validate_all(
            args.model,
            args.domain,
            args.problems,
            args.output
    ):
        logger.error("\n❌ INPUT VALIDATION FAILED")
        logger.error("Please check your inputs and try again")
        return 1

    # ====================================================================
    # STAGE 1: COMPREHENSIVE EVALUATION
    # ====================================================================

    if not run_comprehensive_evaluation(
            args.model,
            args.domain,
            args.problems,
            args.output,
            args.timeout,
            args.skip_baselines
    ):
        logger.error("\n❌ COMPREHENSIVE EVALUATION FAILED")
        return 1

    # ====================================================================
    # STAGE 2: ANALYSIS AND VISUALIZATION
    # ====================================================================

    results_csv = os.path.join(args.output, "evaluation_results.csv")

    if not run_analysis_and_visualization(results_csv, args.output):
        logger.warning("\n⚠️ ANALYSIS AND VISUALIZATION FAILED (continuing anyway)")

    # ====================================================================
    # STAGE 3: FINAL REPORT
    # ====================================================================

    if not generate_final_report(args.output):
        logger.error("\n❌ FINAL REPORT GENERATION FAILED")
        return 1

    logger.info("✅ EVALUATION PIPELINE COMPLETE\n")
    return 0


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file overfit_experiment.py code is this:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OVERFIT EXPERIMENT INFRASTRUCTURE
==================================
Trains GNN policy on a small set of problems and tests overfitting.

Features:
  ✓ Train on K problems from a domain
  ✓ Test on the same K problems to measure overfitting
  ✓ Track learning curves (reward, plan cost, expansions)
  ✓ Per-problem learning curves
  ✓ Comparison: initial vs final performance
  ✓ Statistical analysis of overfitting

Usage:
    python overfit_experiment.py \
        --domain domain.pddl \
        --problems "problem_small_*.pddl" \
        --num-problems 5 \
        --num-train-episodes 100 \
        --output overfit_results/

Output:
    overfit_results/
    ├── training_log.jsonl          # Per-episode metrics
    ├── overfit_summary.json         # Final statistics
    ├── per_problem_learning.json    # Per-problem curves
    └── plots/
        ├── learning_curve.png       # Overall reward curve
        ├── per_problem_curves.png   # Individual problem curves
        ├── convergence_analysis.png # Convergence rate
        └── overfitting_metrics.png  # Specialization analysis
"""

import sys
import os
import json
import glob
import logging
import argparse
import random
import traceback
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, asdict, field
from collections import defaultdict
import numpy as np
from datetime import datetime

sys.path.insert(0, os.getcwd())

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("overfit_experiment.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class EpisodeMetrics:
    """Metrics for a single training episode."""
    episode: int
    problem_idx: int
    problem_name: str
    reward: float
    plan_cost: int = 0
    num_expansions: int = 0
    solved: bool = False
    total_reward: float = 0.0  # Cumulative reward
    timestamp: float = field(default_factory=lambda: datetime.now().timestamp())

    def to_dict(self) -> Dict:
        return asdict(self)


@dataclass
class ProblemStats:
    """Statistics for a single problem across training."""
    problem_idx: int
    problem_name: str
    num_episodes: int
    avg_reward: float
    best_reward: float
    worst_reward: float
    final_reward: float
    improvement_ratio: float  # (final - worst) / (best - worst)
    avg_plan_cost: float
    solve_rate: float
    episodes_to_convergence: Optional[int] = None

    def to_dict(self) -> Dict:
        return asdict(self)


# FILE: overfit_experiment.py
# REPLACE THIS DATACLASS DEFINITION

@dataclass
class OverfitExperimentSummary:
    """Overall statistics for the overfit experiment."""
    # --- Fields WITHOUT defaults ---
    num_problems: int
    num_train_episodes: int
    total_timesteps: int
    start_time: str
    end_time: str
    duration_seconds: float
    avg_reward_over_all: float
    best_reward_over_all: float
    worst_reward_over_all: float
    per_problem_stats: List[Dict]
    reward_variance: float
    plan_cost_improvement_ratio: float
    solve_rate_improvement: float
    early_convergence_episodes: int
    learning_rate_estimate: float  # Moved up

    # --- Fields WITH defaults (must come AFTER non-defaults) ---
    convergence_threshold: float = 0.05
    plateau_episode: Optional[int] = None

    def to_dict(self) -> Dict:
        return asdict(self)


# ============================================================================
# PROBLEM SELECTION
# ============================================================================

def select_training_problems(
        domain_file: str,
        problem_pattern: str,
        num_problems: int,
        seed: int = 42
) -> Tuple[str, List[Tuple[str, str]]]:
    """
    Select K problems for overfitting experiment.

    Returns:
        (domain_path, [(domain, problem), ...])
    """
    logger.info("\n" + "=" * 80)
    logger.info("STEP 1: SELECT TRAINING PROBLEMS")
    logger.info("=" * 80 + "\n")

    domain_path = os.path.abspath(domain_file)

    if not os.path.exists(domain_path):
        raise FileNotFoundError(f"Domain not found: {domain_path}")

    logger.info(f"Domain: {domain_path}\n")

    # Load all problems matching pattern
    all_problems = sorted(glob.glob(problem_pattern))

    if not all_problems:
        raise ValueError(f"No problems found matching: {problem_pattern}")

    logger.info(f"Found {len(all_problems)} total problems matching pattern")
    logger.info(f"Selecting {num_problems} for overfitting experiment\n")

    # Random selection
    random.seed(seed)
    selected = random.sample(all_problems, min(num_problems, len(all_problems)))
    selected = sorted(selected)

    # Convert to absolute paths
    benchmarks = [(domain_path, os.path.abspath(p)) for p in selected]

    logger.info("Selected problems:")
    for i, (domain, problem) in enumerate(benchmarks, 1):
        logger.info(f"  [{i}] {os.path.basename(problem)}")

    return domain_path, benchmarks


# ============================================================================
# TRAINING PHASE
# ============================================================================

class OverfitTrainer:
    """Trains GNN on a fixed set of problems."""

    def __init__(self, benchmarks: List[Tuple[str, str]], output_dir: str):
        self.benchmarks = benchmarks
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.episode_log: List[EpisodeMetrics] = []
        self.start_time = datetime.now()

        # Import here to avoid issues
        from stable_baselines3 import PPO
        from gnn_policy import GNNPolicy
        from merge_env import MergeEnv
        from stable_baselines3.common.monitor import Monitor

        self.PPO = PPO
        self.GNNPolicy = GNNPolicy
        self.MergeEnv = MergeEnv
        self.Monitor = Monitor

    def run_training(
            self,
            num_episodes: int,
            timesteps_per_episode: int = 50,
            save_interval: int = 10
    ) -> str:
        """
        Train GNN on problems with overfitting measurement.

        Returns:
            Path to trained model
        """
        logger.info("\n" + "=" * 80)
        logger.info("STEP 2: TRAINING PHASE")
        logger.info("=" * 80 + "\n")

        model = None
        episode = 0

        try:
            # Create problem iterator (cycle through problems for multiple episodes)
            problem_cycle = [self.benchmarks[i % len(self.benchmarks)]
                             for i in range(num_episodes)]

            for episode in range(num_episodes):
                domain_file, problem_file = problem_cycle[episode]
                problem_idx = episode % len(self.benchmarks)
                problem_name = os.path.basename(problem_file)

                logger.info(f"\nEpisode {episode + 1}/{num_episodes}: {problem_name}")

                try:
                    # Create environment
                    env = self.MergeEnv(
                        domain_file=domain_file,
                        problem_file=problem_file,
                        max_merges=50,
                        debug=False,
                        reward_variant='astar_search',
                        w_search_efficiency=0.30,
                        w_solution_quality=0.20,
                        w_f_stability=0.35,
                        w_state_control=0.15,
                    )

                    env = self.Monitor(env)

                    # Create or continue model
                    if model is None:
                        logger.info("  Creating new PPO model...")
                        model = self.PPO(
                            policy=self.GNNPolicy,
                            env=env,
                            learning_rate=0.0003,
                            n_steps=64,
                            batch_size=32,
                            ent_coef=0.01,
                            verbose=0,
                            tensorboard_log=str(self.output_dir / "tb_logs"),
                            policy_kwargs={"hidden_dim": 64},
                        )
                    else:
                        model.set_env(env)

                    # Train for this episode
                    logger.info(f"  Training for {timesteps_per_episode} timesteps...")
                    model.learn(
                        total_timesteps=timesteps_per_episode,
                        tb_log_name=f"overfit_episode_{episode}",
                        reset_num_timesteps=False,
                    )

                    # Extract metrics
                    obs, _ = env.reset()
                    episode_reward = 0.0
                    steps = 0

                    for step in range(20):  # Quick test run
                        action, _ = model.predict(obs, deterministic=True)
                        obs, reward, done, truncated, info = env.step(int(action))
                        episode_reward += reward
                        steps += 1

                        if done or truncated:
                            break

                    # Log metrics
                    metrics = EpisodeMetrics(
                        episode=episode,
                        problem_idx=problem_idx,
                        problem_name=problem_name,
                        reward=episode_reward,
                        plan_cost=info.get('plan_cost', 0),
                        num_expansions=info.get('num_expansions', 0),
                        solved=info.get('plan_cost', 0) > 0,
                        total_reward=sum(m.reward for m in self.episode_log) + episode_reward
                    )

                    self.episode_log.append(metrics)

                    logger.info(f"  Reward: {episode_reward:.4f}")
                    logger.info(f"  Cumulative: {metrics.total_reward:.4f}")

                    env.close()

                    # Save checkpoint
                    if (episode + 1) % save_interval == 0:
                        model_path = self.output_dir / f"model_ep{episode+1}.zip"
                        model.save(model_path)
                        logger.info(f"  ✓ Saved checkpoint: {model_path}")

                except Exception as e:
                    logger.error(f"  ✗ Episode {episode} failed: {e}")
                    logger.error(traceback.format_exc())
                    continue

            # Save final model
            if model is not None:
                final_model_path = self.output_dir / "model_final.zip"
                model.save(final_model_path)
                logger.info(f"\n✅ Training complete! Final model: {final_model_path}")
                return str(final_model_path)

        except Exception as e:
            logger.error(f"Training failed: {e}")
            logger.error(traceback.format_exc())
            return None

    def save_training_log(self):
        """Save episode metrics to JSONL file."""
        log_path = self.output_dir / "training_log.jsonl"

        with open(log_path, 'w') as f:
            for metrics in self.episode_log:
                f.write(json.dumps(metrics.to_dict()) + '\n')

        logger.info(f"Saved training log: {log_path}")
        return log_path


# ============================================================================
# EVALUATION PHASE
# ============================================================================

class OverfitEvaluator:
    """Tests overfitting on the training problems."""

    def __init__(self, model_path: str, benchmarks: List[Tuple[str, str]], output_dir: str):
        self.model_path = model_path
        self.benchmarks = benchmarks
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        from stable_baselines3 import PPO
        from merge_env import MergeEnv

        self.PPO = PPO
        self.MergeEnv = MergeEnv

    def evaluate(self, num_runs_per_problem: int = 5) -> Dict:
        """
        Test on training problems.

        Returns:
            Per-problem statistics
        """
        logger.info("\n" + "=" * 80)
        logger.info("STEP 3: EVALUATION PHASE")
        logger.info("=" * 80 + "\n")

        # Load model
        logger.info(f"Loading model: {self.model_path}")
        model = self.PPO.load(self.model_path)
        logger.info("✓ Model loaded\n")

        results = {}

        for prob_idx, (domain_file, problem_file) in enumerate(self.benchmarks):
            problem_name = os.path.basename(problem_file)
            logger.info(f"Problem {prob_idx + 1}/{len(self.benchmarks)}: {problem_name}")

            rewards = []
            plan_costs = []
            expansions = []

            for run in range(num_runs_per_problem):
                try:
                    env = self.MergeEnv(
                        domain_file=domain_file,
                        problem_file=problem_file,
                        max_merges=50,
                        debug=False,
                        reward_variant='astar_search',
                    )

                    obs, _ = env.reset()
                    total_reward = 0.0

                    for step in range(20):
                        action, _ = model.predict(obs, deterministic=True)
                        obs, reward, done, truncated, info = env.step(int(action))
                        total_reward += reward

                        if done or truncated:
                            break

                    rewards.append(total_reward)
                    plan_costs.append(info.get('plan_cost', 0))
                    expansions.append(info.get('num_expansions', 0))

                    logger.info(f"  Run {run + 1}: reward={total_reward:.4f}")
                    env.close()

                except Exception as e:
                    logger.error(f"  Run {run + 1} failed: {e}")
                    continue

            # Aggregate stats
            if rewards:
                results[problem_name] = {
                    'problem_idx': prob_idx,
                    'avg_reward': np.mean(rewards),
                    'std_reward': np.std(rewards),
                    'max_reward': np.max(rewards),
                    'min_reward': np.min(rewards),
                    'avg_plan_cost': np.mean(plan_costs) if plan_costs else 0,
                    'avg_expansions': int(np.mean(expansions)) if expansions else 0,
                }

                logger.info(f"  Average reward: {results[problem_name]['avg_reward']:.4f} "
                            f"(±{results[problem_name]['std_reward']:.4f})")

        return results


# ============================================================================
# ANALYSIS
# ============================================================================

def analyze_overfitting(
        training_log: List[EpisodeMetrics],
        eval_results: Dict,
        benchmarks: List[Tuple[str, str]]
) -> OverfitExperimentSummary:
    """
    Analyze overfitting indicators.

    Returns:
        Comprehensive summary with overfitting metrics
    """
    logger.info("\n" + "=" * 80)
    logger.info("STEP 4: ANALYSIS")
    logger.info("=" * 80 + "\n")

    # Group by problem
    by_problem = defaultdict(list)
    for metrics in training_log:
        by_problem[metrics.problem_name].append(metrics)

    # Per-problem statistics
    per_problem_stats = []

    for problem_idx, (domain, problem_file) in enumerate(benchmarks):
        problem_name = os.path.basename(problem_file)

        if problem_name in by_problem:
            episodes = by_problem[problem_name]
            rewards = [e.reward for e in episodes]

            # Improvement analysis
            initial_reward = rewards[0] if rewards else 0
            final_reward = rewards[-1] if rewards else 0
            best_reward = max(rewards) if rewards else 0
            worst_reward = min(rewards) if rewards else 0

            if best_reward != worst_reward:
                improvement_ratio = (final_reward - worst_reward) / (best_reward - worst_reward)
            else:
                improvement_ratio = 0.0

            # Convergence detection
            episodes_to_convergence = None
            if len(rewards) > 10:
                for i in range(10, len(rewards)):
                    recent_avg = np.mean(rewards[i-10:i])
                    older_avg = np.mean(rewards[max(0, i-20):i-10])
                    if older_avg > 0 and abs(recent_avg - older_avg) / abs(older_avg) < 0.05:
                        episodes_to_convergence = i
                        break

            stats = ProblemStats(
                problem_idx=problem_idx,
                problem_name=problem_name,
                num_episodes=len(episodes),
                avg_reward=np.mean(rewards),
                best_reward=best_reward,
                worst_reward=worst_reward,
                final_reward=final_reward,
                improvement_ratio=improvement_ratio,
                avg_plan_cost=np.mean([e.plan_cost for e in episodes]) if episodes else 0,
                solve_rate=sum(1 for e in episodes if e.solved) / len(episodes) if episodes else 0,
                episodes_to_convergence=episodes_to_convergence
            )

            per_problem_stats.append(stats)

            logger.info(f"\n{problem_name}:")
            logger.info(f"  Episodes: {len(episodes)}")
            logger.info(f"  Reward: {stats.avg_reward:.4f} (best: {best_reward:.4f})")
            logger.info(f"  Improvement: {improvement_ratio*100:.1f}%")
            logger.info(f"  Convergence: {episodes_to_convergence} episodes")

    # Overall statistics
    all_rewards = [m.reward for m in training_log]

    summary = OverfitExperimentSummary(
        num_problems=len(benchmarks),
        num_train_episodes=len(training_log),
        total_timesteps=len(training_log) * 50,  # Assuming 50 timesteps per episode
        start_time=training_log[0].timestamp if training_log else datetime.now().isoformat(),
        end_time=training_log[-1].timestamp if training_log else datetime.now().isoformat(),
        duration_seconds=training_log[-1].timestamp - training_log[0].timestamp if len(training_log) > 1 else 0,
        avg_reward_over_all=np.mean(all_rewards) if all_rewards else 0,
        best_reward_over_all=np.max(all_rewards) if all_rewards else 0,
        worst_reward_over_all=np.min(all_rewards) if all_rewards else 0,
        per_problem_stats=[s.to_dict() for s in per_problem_stats],
        reward_variance=np.var(all_rewards) if all_rewards else 0,
        plan_cost_improvement_ratio=0.0,  # Can be computed from eval_results
        solve_rate_improvement=0.0,
        early_convergence_episodes=min([s.episodes_to_convergence for s in per_problem_stats
                                        if s.episodes_to_convergence], default=None) or 0,
        learning_rate_estimate=0.0,
    )

    return summary


# ============================================================================
# VISUALIZATION
# ============================================================================

def generate_plots(
        training_log: List[EpisodeMetrics],
        eval_results: Dict,
        output_dir: Path
):
    """Generate learning curve plots."""

    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
    except ImportError:
        logger.warning("matplotlib not available - skipping plots")
        return

    logger.info("\nGenerating plots...")

    plots_dir = output_dir / "plots"
    plots_dir.mkdir(exist_ok=True)

    # Plot 1: Overall learning curve
    fig, ax = plt.subplots(figsize=(12, 6))

    episodes = [m.episode for m in training_log]
    rewards = [m.reward for m in training_log]

    ax.plot(episodes, rewards, alpha=0.3, label='Per-episode')

    # Rolling average
    window = min(10, len(rewards) // 4)
    if window > 1:
        rolling_avg = np.convolve(rewards, np.ones(window)/window, mode='valid')
        ax.plot(range(window-1, len(rewards)), rolling_avg, linewidth=2, label=f'Rolling avg (window={window})')

    ax.set_xlabel('Episode')
    ax.set_ylabel('Reward')
    ax.set_title('Learning Curve - Overall')
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(plots_dir / "learning_curve.png", dpi=150)
    logger.info(f"  ✓ {plots_dir / 'learning_curve.png'}")
    plt.close()

    # Plot 2: Per-problem curves
    by_problem = defaultdict(list)
    for m in training_log:
        by_problem[m.problem_name].append(m)

    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()

    for idx, (problem_name, metrics) in enumerate(sorted(by_problem.items())[:4]):
        ax = axes[idx]

        episodes = [m.episode for m in metrics]
        rewards = [m.reward for m in metrics]

        ax.plot(episodes, rewards, marker='o', label=problem_name)
        ax.set_xlabel('Episode')
        ax.set_ylabel('Reward')
        ax.set_title(problem_name)
        ax.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(plots_dir / "per_problem_curves.png", dpi=150)
    logger.info(f"  ✓ {plots_dir / 'per_problem_curves.png'}")
    plt.close()

    logger.info("✅ Plots generated")


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="Overfit experiment infrastructure")
    parser.add_argument("--domain", required=True, help="Domain PDDL file")
    parser.add_argument("--problems", required=True, help="Problem glob pattern")
    parser.add_argument("--num-problems", type=int, default=5, help="Number of problems to train on")
    parser.add_argument("--num-train-episodes", type=int, default=100, help="Training episodes")
    parser.add_argument("--output", default="overfit_results", help="Output directory")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")

    args = parser.parse_args()

    try:
        # Step 1: Select problems
        domain_file, benchmarks = select_training_problems(
            args.domain,
            args.problems,
            args.num_problems,
            args.seed
        )

        # Step 2: Train
        trainer = OverfitTrainer(benchmarks, args.output)
        model_path = trainer.run_training(args.num_train_episodes)
        trainer.save_training_log()

        if not model_path:
            logger.error("Training failed")
            return 1

        # Step 3: Evaluate
        evaluator = OverfitEvaluator(model_path, benchmarks, args.output)
        eval_results = evaluator.evaluate()

        # Step 4: Analyze
        summary = analyze_overfitting(trainer.episode_log, eval_results, benchmarks)

        # Save summary
        summary_path = Path(args.output) / "overfit_summary.json"
        with open(summary_path, 'w') as f:
            json.dump(summary.to_dict(), f, indent=2, default=str)
        logger.info(f"Saved summary: {summary_path}")

        # Generate plots
        generate_plots(trainer.episode_log, eval_results, Path(args.output))

        logger.info("\n" + "=" * 80)
        logger.info("✅ OVERFIT EXPERIMENT COMPLETE")
        logger.info("=" * 80)
        logger.info(f"\nResults saved to: {os.path.abspath(args.output)}")
        logger.info(f"  - Training log: training_log.jsonl")
        logger.info(f"  - Summary: overfit_summary.json")
        logger.info(f"  - Plots: plots/")

        return 0

    except Exception as e:
        logger.error(f"❌ Experiment failed: {e}")
        logger.error(traceback.format_exc())
        return 1


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file problem_generalization_experiment.py code is this:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SINGLE-DOMAIN TRAIN/TEST SPLIT EXPERIMENT
==========================================
Trains a GNN policy on a fixed subset of problems from ONE domain
and evaluates performance on a separate test set.

Features:
  ✓ Single domain focus (e.g., blocks_world, gripper, logistics)
  ✓ Automatic train/test splitting (configurable ratio)
  ✓ Cross-validation support (multiple splits)
  ✓ Baseline comparison (optional)
  ✓ Detailed metrics and visualizations
  ✓ Reproducible experiments (seed-based)

Usage:
    python train_test_split_experiment.py \
        --domain domain.pddl \
        --problems "problem_small_*.pddl" \
        --domain-name blocks_world \
        --train-ratio 0.8 \
        --output experiment_results/

    # For cross-validation (multiple splits):
    python train_test_split_experiment.py \
        --domain domain.pddl \
        --problems "problem_small_*.pddl" \
        --domain-name blocks_world \
        --cross-validation 5 \
        --output experiment_results_cv/

Environment Variables:
    REWARD_VARIANT: Reward function to use (default: astar_search)
    TRAIN_RATIO: Train/test split ratio (default: 0.8)
    RANDOM_SEED: Seed for reproducibility (default: 42)
    NUM_TRAIN_EPOCHS: Training epochs per problem (default: 1)
"""

import sys
import os
import logging
import glob
import json
import random
import traceback
import argparse
import subprocess
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from datetime import datetime
import numpy as np
from dataclasses import dataclass, asdict

# Setup paths
sys.path.insert(0, os.getcwd())
os.makedirs("downward/gnn_output", exist_ok=True)
os.makedirs("downward/fd_output", exist_ok=True)
os.makedirs("logs", exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("train_test_split_experiment.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class ExperimentConfig:
    """Configuration for a single experiment."""
    domain_file: str
    domain_name: str
    train_problems: List[str]
    test_problems: List[str]
    train_ratio: float
    random_seed: int
    output_dir: str
    reward_variant: str = 'astar_search'
    total_timesteps: int = 5000
    timesteps_per_problem: int = 500
    max_merges: int = 50
    timeout_per_eval: int = 300
    include_baselines: bool = True
    cv_fold: Optional[int] = None
    cv_total: Optional[int] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'domain_name': self.domain_name,
            'train_problems_count': len(self.train_problems),
            'test_problems_count': len(self.test_problems),
            'train_ratio': self.train_ratio,
            'random_seed': self.random_seed,
            'reward_variant': self.reward_variant,
            'total_timesteps': self.total_timesteps,
            'timeout_per_eval': self.timeout_per_eval,
            'include_baselines': self.include_baselines,
            'cv_fold': self.cv_fold,
            'cv_total': self.cv_total,
            'timestamp': datetime.now().isoformat(),
        }


@dataclass
class ExperimentResults:
    """Results from a single experiment."""
    config: ExperimentConfig
    training_time_sec: float
    training_timesteps: int
    training_problems_used: int

    # Test set metrics (GNN only initially)
    gnn_solve_rate: float
    gnn_avg_time: float
    gnn_avg_expansions: int

    # Baseline metrics (if enabled)
    baseline_solve_rates: Dict[str, float] = None  # planner_name -> rate
    baseline_avg_times: Dict[str, float] = None

    # Derived metrics
    improvement_over_baseline: Optional[float] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        d = {
            'config': self.config.to_dict(),
            'training_time_sec': self.training_time_sec,
            'training_timesteps': self.training_timesteps,
            'training_problems_used': self.training_problems_used,
            'gnn_solve_rate': self.gnn_solve_rate,
            'gnn_avg_time': self.gnn_avg_time,
            'gnn_avg_expansions': self.gnn_avg_expansions,
        }

        if self.baseline_solve_rates:
            d['baseline_solve_rates'] = self.baseline_solve_rates
            d['baseline_avg_times'] = self.baseline_avg_times
            d['improvement_over_baseline'] = self.improvement_over_baseline

        return d


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def print_section(title: str, width: int = 90):
    """Print formatted section header."""
    logger.info("\n" + "=" * width)
    logger.info(f"// {title.upper()}")
    logger.info("=" * width + "\n")


def print_subsection(title: str):
    """Print formatted subsection header."""
    logger.info("\n" + "-" * 80)
    logger.info(f">>> {title}")
    logger.info("-" * 80 + "\n")


# ============================================================================
# PHASE 0: DATASET PREPARATION
# ============================================================================

def load_and_split_problems(
        domain_file: str,
        problem_pattern: str,
        train_ratio: float = 0.8,
        random_seed: int = 42
) -> Tuple[List[str], List[str]]:
    """
    Load all problems matching pattern and split into train/test.

    Args:
        domain_file: Path to domain PDDL
        problem_pattern: Glob pattern for problems
        train_ratio: Fraction for training
        random_seed: Seed for reproducibility

    Returns:
        (train_problems, test_problems)
    """
    print_subsection("Loading and Splitting Problems")

    # Verify domain exists
    if not os.path.exists(domain_file):
        raise FileNotFoundError(f"Domain file not found: {domain_file}")

    logger.info(f"Domain: {domain_file}")

    # Load problems
    all_problems = sorted(glob.glob(problem_pattern))

    if not all_problems:
        raise ValueError(f"No problems found matching: {problem_pattern}")

    logger.info(f"Found {len(all_problems)} total problem(s)")

    if len(all_problems) < 2:
        raise ValueError("Need at least 2 problems for train/test split")

    # Set seed for reproducibility
    random.seed(random_seed)
    problems_shuffled = all_problems.copy()
    random.shuffle(problems_shuffled)

    # Split
    split_idx = int(len(problems_shuffled) * train_ratio)
    train_problems = sorted(problems_shuffled[:split_idx])
    test_problems = sorted(problems_shuffled[split_idx:])

    logger.info(f"\nTrain/test split (ratio={train_ratio:.1%}, seed={random_seed}):")
    logger.info(f"  Train: {len(train_problems)} problems")
    for i, p in enumerate(train_problems[:3], 1):
        logger.info(f"    {i}. {os.path.basename(p)}")
    if len(train_problems) > 3:
        logger.info(f"    ... and {len(train_problems) - 3} more")

    logger.info(f"  Test:  {len(test_problems)} problems")
    for i, p in enumerate(test_problems[:3], 1):
        logger.info(f"    {i}. {os.path.basename(p)}")
    if len(test_problems) > 3:
        logger.info(f"    ... and {len(test_problems) - 3} more")

    return train_problems, test_problems


# ============================================================================
# PHASE 1: TRAINING
# ============================================================================

def train_gnn_on_split(
        config: ExperimentConfig
) -> Tuple[str, float]:
    """
    Train GNN on the training set.

    Returns:
        (model_path, training_time_sec)
    """
    print_section("PHASE 1: TRAINING GNN ON TRAINING SET")

    try:
        from common_utils import train_model

        logger.info(f"Configuration:")
        logger.info(f"  Domain:              {config.domain_name}")
        logger.info(f"  Training problems:   {len(config.train_problems)}")
        logger.info(f"  Test problems:       {len(config.test_problems)}")
        logger.info(f"  Total timesteps:     {config.total_timesteps}")
        logger.info(f"  Timesteps/problem:   {config.timesteps_per_problem}")
        logger.info(f"  Reward variant:      {config.reward_variant}")
        logger.info(f"  Random seed:         {config.random_seed}")

        if config.cv_fold is not None:
            logger.info(f"  CV fold:             {config.cv_fold}/{config.cv_total}")

        # Create benchmark list (train set)
        benchmarks = [(config.domain_file, p) for p in config.train_problems]

        # Prepare model path
        os_makedirs(config.output_dir, exist_ok=True)

        if config.cv_fold is not None:
            model_name = f"gnn_model_fold_{config.cv_fold}.zip"
        else:
            model_name = "gnn_model_trained.zip"

        model_path = os.path.join(config.output_dir, model_name)

        # Define hyperparameters
        hyperparams = {
            'learning_rate': 0.0003,
            'n_steps': 64,
            'batch_size': 32,
            'ent_coef': 0.01,
            'reward_variant': config.reward_variant,
            'w_search_efficiency': 0.30,
            'w_solution_quality': 0.20,
            'w_f_stability': 0.35,
            'w_state_control': 0.15,
        }

        logger.info(f"\nHyperparameters:")
        for k, v in hyperparams.items():
            logger.info(f"  {k:<30} {v}")

        print_subsection("Starting Training")
        logger.info(f"Training on {len(benchmarks)} problem(s)...\n")

        import time
        start_time = time.time()

        model = train_model(
            model_save_path=model_path,
            benchmarks=benchmarks,
            hyperparams=hyperparams,
            total_timesteps=config.total_timesteps,
            tb_log_dir="tb_logs/",
            tb_log_name=f"train_test_{config.domain_name}" + (f"_fold{config.cv_fold}" if config.cv_fold else ""),
            debug_mode=False,  # REAL FD
        )

        elapsed = time.time() - start_time

        if model is None:
            logger.error("Training failed - model is None")
            return None, 0.0

        if not os.path.exists(model_path):
            logger.error(f"Model not saved: {model_path}")
            return None, 0.0

        logger.info(f"\n✅ Training complete!")
        logger.info(f"  Time: {elapsed:.1f}s")
        logger.info(f"  Model: {model_path}")

        return model_path, elapsed

    except Exception as e:
        logger.error(f"\n❌ Training FAILED: {e}")
        logger.error(traceback.format_exc())
        return None, 0.0


# ============================================================================
# PHASE 2: EVALUATION
# ============================================================================

def evaluate_on_test_set(
        config: ExperimentConfig,
        model_path: str
) -> Dict[str, Any]:
    """
    Evaluate trained model on test set.
    Uses evaluation_comprehensive framework.
    """
    print_section("PHASE 2: EVALUATION ON TEST SET")

    try:
        from evaluation_comprehensive import (
            EvaluationFramework,
            GNNPolicyRunner,
            BaselineRunner,
            BenchmarkConfig
        )

        eval_output_dir = os.path.join(config.output_dir, "evaluation")
        os.makedirs(eval_output_dir, exist_ok=True)

        framework = EvaluationFramework(eval_output_dir)

        # Prepare test problem pattern
        test_problems_str = "|".join(config.test_problems)

        logger.info(f"Evaluation configuration:")
        logger.info(f"  Model:              {model_path}")
        logger.info(f"  Test problems:      {len(config.test_problems)}")
        logger.info(f"  Include baselines:  {config.include_baselines}")
        logger.info(f"  Timeout per eval:   {config.timeout_per_eval}s")

        # Run evaluation
        results = framework.run_comprehensive_evaluation(
            domain_file=config.domain_file,
            problem_pattern="|".join(config.test_problems),  # Use alternation for glob
            model_path=model_path,
            timeout_sec=config.timeout_per_eval,
            include_baselines=config.include_baselines
        )

        # Extract and summarize results
        summaries = results.get("summaries", {})

        gnn_results = summaries.get("GNN", {})
        baseline_results = {name: summary for name, summary in summaries.items()
                            if name != "GNN"}

        logger.info(f"\n✅ Evaluation complete!")
        logger.info(f"\nGNN Results:")
        logger.info(f"  Solve rate:     {gnn_results.get('solve_rate_pct', 0):.1f}%")
        logger.info(f"  Avg time:       {gnn_results.get('avg_time_sec', 0):.2f}s")
        logger.info(f"  Avg expansions: {gnn_results.get('avg_expansions', 0)}")

        if baseline_results:
            logger.info(f"\nBaseline Results:")
            for name, summary in baseline_results.items():
                logger.info(f"  {name}:")
                logger.info(f"    Solve rate: {summary.get('solve_rate_pct', 0):.1f}%")
                logger.info(f"    Avg time:   {summary.get('avg_time_sec', 0):.2f}s")

        return {
            'gnn': gnn_results,
            'baselines': baseline_results,
            'all_summaries': summaries,
        }

    except Exception as e:
        logger.error(f"\n❌ Evaluation FAILED: {e}")
        logger.error(traceback.format_exc())
        return {}


# ============================================================================
# PHASE 3: ANALYSIS AND REPORTING
# ============================================================================

def generate_experiment_report(
        config: ExperimentConfig,
        training_time: float,
        eval_results: Dict[str, Any]
) -> ExperimentResults:
    """
    Generate comprehensive report and create result object.
    """
    print_section("PHASE 3: ANALYSIS AND REPORTING")

    gnn_results = eval_results.get('gnn', {})
    baseline_results = eval_results.get('baselines', {})

    # Extract GNN metrics
    gnn_solve_rate = gnn_results.get('solve_rate_pct', 0) / 100.0
    gnn_avg_time = gnn_results.get('avg_time_sec', 0)
    gnn_avg_expansions = int(gnn_results.get('avg_expansions', 0))

    # Extract baseline metrics
    baseline_solve_rates = {}
    baseline_avg_times = {}
    best_baseline_rate = 0.0

    for name, summary in baseline_results.items():
        rate = summary.get('solve_rate_pct', 0) / 100.0
        baseline_solve_rates[name] = rate
        baseline_avg_times[name] = summary.get('avg_time_sec', 0)
        best_baseline_rate = max(best_baseline_rate, rate)

    # Compute improvement
    improvement = None
    if best_baseline_rate > 0:
        improvement = (gnn_solve_rate - best_baseline_rate) / best_baseline_rate * 100

    # Create result object
    results = ExperimentResults(
        config=config,
        training_time_sec=training_time,
        training_timesteps=config.total_timesteps,
        training_problems_used=len(config.train_problems),
        gnn_solve_rate=gnn_solve_rate,
        gnn_avg_time=gnn_avg_time,
        gnn_avg_expansions=gnn_avg_expansions,
        baseline_solve_rates=baseline_solve_rates if baseline_solve_rates else None,
        baseline_avg_times=baseline_avg_times if baseline_avg_times else None,
        improvement_over_baseline=improvement,
    )

    # Write report
    report_path = os.path.join(config.output_dir, "experiment_report.txt")

    with open(report_path, 'w') as f:
        f.write("=" * 90 + "\n")
        f.write("TRAIN/TEST SPLIT EXPERIMENT REPORT\n")
        f.write("=" * 90 + "\n\n")

        f.write(f"Timestamp: {datetime.now().isoformat()}\n")
        f.write(f"Domain: {config.domain_name}\n\n")

        f.write("EXPERIMENT CONFIGURATION\n")
        f.write("-" * 90 + "\n")
        f.write(f"Training problems:  {len(config.train_problems)}\n")
        f.write(f"Test problems:      {len(config.test_problems)}\n")
        f.write(f"Train ratio:        {config.train_ratio:.1%}\n")
        f.write(f"Random seed:        {config.random_seed}\n")
        f.write(f"Reward variant:     {config.reward_variant}\n")
        f.write(f"Total timesteps:    {config.total_timesteps}\n\n")

        if config.cv_fold is not None:
            f.write(f"Cross-validation:   Fold {config.cv_fold}/{config.cv_total}\n\n")

        f.write("TRAINING RESULTS\n")
        f.write("-" * 90 + "\n")
        f.write(f"Training time:      {results.training_time_sec:.1f}s\n")
        f.write(f"Problems used:      {results.training_problems_used}\n\n")

        f.write("TEST SET RESULTS\n")
        f.write("-" * 90 + "\n")
        f.write(f"GNN Solve Rate:     {results.gnn_solve_rate * 100:.1f}%\n")
        f.write(f"GNN Avg Time:       {results.gnn_avg_time:.2f}s\n")
        f.write(f"GNN Avg Expansions: {results.gnn_avg_expansions}\n\n")

        if results.baseline_solve_rates:
            f.write("BASELINE COMPARISON\n")
            f.write("-" * 90 + "\n")

            for name in sorted(results.baseline_solve_rates.keys()):
                rate = results.baseline_solve_rates[name]
                time = results.baseline_avg_times[name]
                f.write(f"{name:<30} Solve: {rate * 100:5.1f}%  Time: {time:7.2f}s\n")

            f.write("\n")

            if results.improvement_over_baseline is not None:
                f.write(f"GNN Improvement over best baseline: {results.improvement_over_baseline:+.1f}%\n")

        f.write("\n" + "=" * 90 + "\n")

    logger.info(f"✅ Report written: {report_path}")

    # Save results as JSON
    json_path = os.path.join(config.output_dir, "experiment_results.json")

    with open(json_path, 'w') as f:
        json.dump(results.to_dict(), f, indent=2)

    logger.info(f"✅ Results saved: {json_path}")

    return results


# ============================================================================
# CROSS-VALIDATION SUPPORT
# ============================================================================

def run_cross_validation(
        domain_file: str,
        problem_pattern: str,
        domain_name: str,
        num_folds: int = 5,
        output_dir: str = "experiment_results_cv",
        **kwargs
) -> List[ExperimentResults]:
    """
    Run multiple train/test splits (cross-validation).
    """
    print_section(f"CROSS-VALIDATION: {num_folds}-FOLD")

    all_problems = sorted(glob.glob(problem_pattern))

    if len(all_problems) < num_folds * 2:
        raise ValueError(f"Need at least {num_folds * 2} problems for {num_folds}-fold CV")

    logger.info(f"Total problems: {len(all_problems)}")
    logger.info(f"Number of folds: {num_folds}\n")

    fold_size = len(all_problems) // num_folds
    results_list = []

    for fold_idx in range(num_folds):
        print_section(f"FOLD {fold_idx + 1}/{num_folds}")

        # Create fold-specific splits
        fold_start = fold_idx * fold_size
        fold_end = fold_start + fold_size if fold_idx < num_folds - 1 else len(all_problems)

        test_problems = all_problems[fold_start:fold_end]
        train_problems = all_problems[:fold_start] + all_problems[fold_end:]

        logger.info(f"Fold {fold_idx + 1}:")
        logger.info(f"  Train: problems 0-{fold_start - 1}, {fold_end}-{len(all_problems) - 1} "
                    f"({len(train_problems)} total)")
        logger.info(f"  Test:  problems {fold_start}-{fold_end - 1} ({len(test_problems)} total)\n")

        # Create fold output directory
        fold_output_dir = os.path.join(output_dir, f"fold_{fold_idx + 1}")
        os.makedirs(fold_output_dir, exist_ok=True)

        # Create config
        config = ExperimentConfig(
            domain_file=domain_file,
            domain_name=domain_name,
            train_problems=train_problems,
            test_problems=test_problems,
            output_dir=fold_output_dir,
            cv_fold=fold_idx + 1,
            cv_total=num_folds,
            **kwargs
        )

        # Run single fold
        try:
            # Train
            model_path, training_time = train_gnn_on_split(config)

            if model_path is None:
                logger.error(f"Fold {fold_idx + 1} training failed - skipping")
                continue

            # Evaluate
            eval_results = evaluate_on_test_set(config, model_path)

            # Report
            fold_results = generate_experiment_report(config, training_time, eval_results)
            results_list.append(fold_results)

        except Exception as e:
            logger.error(f"Fold {fold_idx + 1} FAILED: {e}")
            logger.error(traceback.format_exc())
            continue

    # Generate cross-validation summary
    print_section("CROSS-VALIDATION SUMMARY")

    if not results_list:
        logger.error("No folds completed successfully!")
        return []

    logger.info(f"Completed: {len(results_list)}/{num_folds} folds\n")

    # Aggregate metrics
    gnn_solve_rates = [r.gnn_solve_rate for r in results_list]
    gnn_avg_times = [r.gnn_avg_time for r in results_list]
    training_times = [r.training_time_sec for r in results_list]

    logger.info("GNN Performance Across Folds:")
    logger.info(f"  Solve rate:  {np.mean(gnn_solve_rates) * 100:.1f}% ± {np.std(gnn_solve_rates) * 100:.1f}%")
    logger.info(f"  Avg time:    {np.mean(gnn_avg_times):.2f}s ± {np.std(gnn_avg_times):.2f}s")
    logger.info(f"  Training:    {np.mean(training_times):.1f}s ± {np.std(training_times):.1f}s")

    # Write CV summary
    cv_summary_path = os.path.join(output_dir, "cv_summary.json")

    cv_summary = {
        'num_folds': num_folds,
        'completed_folds': len(results_list),
        'domain': domain_name,
        'gnn_solve_rate_mean': float(np.mean(gnn_solve_rates)),
        'gnn_solve_rate_std': float(np.std(gnn_solve_rates)),
        'gnn_avg_time_mean': float(np.mean(gnn_avg_times)),
        'gnn_avg_time_std': float(np.std(gnn_avg_times)),
        'training_time_mean': float(np.mean(training_times)),
        'fold_results': [r.to_dict() for r in results_list],
        'timestamp': datetime.now().isoformat(),
    }

    with open(cv_summary_path, 'w') as f:
        json.dump(cv_summary, f, indent=2)

    logger.info(f"\n✅ CV summary saved: {cv_summary_path}")

    return results_list


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Single-domain train/test split experiment"
    )

    parser.add_argument("--domain", required=True, help="Path to domain PDDL")
    parser.add_argument("--problems", required=True, help="Glob pattern for problems")
    parser.add_argument("--domain-name", required=True, help="Name of domain (blocks_world, etc)")

    parser.add_argument("--train-ratio", type=float, default=0.8,
                        help="Fraction of problems for training")
    parser.add_argument("--random-seed", type=int, default=42,
                        help="Seed for reproducibility")

    parser.add_argument("--cross-validation", type=int, default=0,
                        help="Number of folds for CV (0=no CV)")

    parser.add_argument("--reward-variant", default="astar_search",
                        help="Reward function variant")
    parser.add_argument("--total-timesteps", type=int, default=5000,
                        help="Total training timesteps")
    parser.add_argument("--timeout-eval", type=int, default=300,
                        help="Timeout per evaluation (seconds)")

    parser.add_argument("--skip-baselines", action="store_true",
                        help="Skip baseline evaluation")

    parser.add_argument("--output", default="experiment_results",
                        help="Output directory")

    args = parser.parse_args()

    # Create output directory
    os.makedirs(args.output, exist_ok=True)

    print_section("SINGLE-DOMAIN TRAIN/TEST SPLIT EXPERIMENT")

    try:
        # Load and split problems
        train_problems, test_problems = load_and_split_problems(
            domain_file=args.domain,
            problem_pattern=args.problems,
            train_ratio=args.train_ratio,
            random_seed=args.random_seed
        )

        if args.cross_validation > 0:
            # Run cross-validation
            results = run_cross_validation(
                domain_file=args.domain,
                problem_pattern=args.problems,
                domain_name=args.domain_name,
                num_folds=args.cross_validation,
                output_dir=args.output,
                train_ratio=args.train_ratio,
                random_seed=args.random_seed,
                reward_variant=args.reward_variant,
                total_timesteps=args.total_timesteps,
                timeout_per_eval=args.timeout_eval,
                include_baselines=not args.skip_baselines,
            )
        else:
            # Run single train/test split
            config = ExperimentConfig(
                domain_file=args.domain,
                domain_name=args.domain_name,
                train_problems=train_problems,
                test_problems=test_problems,
                train_ratio=args.train_ratio,
                random_seed=args.random_seed,
                output_dir=args.output,
                reward_variant=args.reward_variant,
                total_timesteps=args.total_timesteps,
                timeout_per_eval=args.timeout_eval,
                include_baselines=not args.skip_baselines,
            )

            # Train
            model_path, training_time = train_gnn_on_split(config)

            if model_path is None:
                logger.error("Training failed!")
                return 1

            # Evaluate
            eval_results = evaluate_on_test_set(config, model_path)

            if not eval_results:
                logger.error("Evaluation failed!")
                return 1

            # Report
            results = generate_experiment_report(config, training_time, eval_results)

        print_section("EXPERIMENT COMPLETE")
        logger.info(f"✅ Results saved to: {os.path.abspath(args.output)}")
        logger.info(f"\nKey files:")
        logger.info(f"  - experiment_report.txt")
        logger.info(f"  - experiment_results.json")
        logger.info(f"  - evaluation/evaluation_results.csv")
        logger.info(f"  - evaluation/comparison_report.txt")

        return 0

    except Exception as e:
        logger.error(f"\n❌ EXPERIMENT FAILED: {e}")
        logger.error(traceback.format_exc())
        return 1


# Helper to avoid conflicts with os.makedirs
def os_makedirs(*args, **kwargs):
    return os.makedirs(*args, **kwargs)


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

--------------------------------------------------------------------------------

The file scale_generalization_experiment.py code is this:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SCALE GENERALIZATION TRAINING & TESTING
========================================
Complete pipeline to train a GNN merge policy on small/medium problems and
evaluate its generalization to larger, harder problems.

Features:
  ✓ Generate synthetic PDDL problems at multiple scales
  ✓ Train GNN policy on small/medium problems
  ✓ Evaluate on medium/large problems (different from training set)
  ✓ Compare against baseline planners
  ✓ Detailed analysis of scale generalization
  ✓ HTML report with visualizations

Run with:
    python train_and_test_scale_generalization.py

Or with options:
    python train_and_test_scale_generalization.py \
        --domain blocks_world \
        --train-sizes small medium \
        --test-sizes large \
        --problems-per-size 10 \
        --training-timesteps 5000 \
        --output results/scale_generalization/
"""

import sys
import os
import logging
import json
import glob
import subprocess
import argparse
import traceback
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import numpy as np

# Setup paths
sys.path.insert(0, os.getcwd())
os.makedirs("benchmarks", exist_ok=True)
os.makedirs("downward/gnn_output", exist_ok=True)
os.makedirs("downward/fd_output", exist_ok=True)
os.makedirs("logs", exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - [%(name)s] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("scale_generalization.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


# ============================================================================
# PHASE 0: PROBLEM GENERATION
# ============================================================================

def generate_benchmark_problems(
    domain: str = "blocks_world",
    sizes: List[str] = None,
    problems_per_size: int = 5
) -> Dict[str, List[Tuple[str, str]]]:
    """
    Generate PDDL problems at multiple scales.

    Args:
        domain: "blocks_world", "logistics", or "gripper"
        sizes: List of sizes ("small", "medium", "large")
        problems_per_size: Number of problems to generate per size

    Returns:
        Dict mapping size → list of (domain_path, problem_path) tuples
    """
    if sizes is None:
        sizes = ["small", "medium", "large"]

    logger.info("\n" + "=" * 80)
    logger.info("PHASE 0: GENERATING BENCHMARK PROBLEMS")
    logger.info("=" * 80 + "\n")

    try:
        from pddl_generator.problem_generator import PDDLProblemGenerator
        from pddl_generator.size_config import SIZE_CONFIGS

        generator = PDDLProblemGenerator(output_dir="benchmarks")

        benchmarks = {}

        for size in sizes:
            logger.info(f"\nGenerating {domain} - {size} problems...")

            domain_path, problem_paths = generator.save_domain_and_problems(
                domain=domain,
                size=size,
                num_problems=problems_per_size
            )

            benchmarks[size] = [(domain_path, p) for p in problem_paths]
            logger.info(f"  ✓ Generated {len(benchmarks[size])} {size} problem(s)")

        return benchmarks

    except ImportError:
        logger.warning("PDDL generator not available - using existing benchmarks")
        return {}
    except Exception as e:
        logger.error(f"Problem generation failed: {e}")
        logger.error(traceback.format_exc())
        return {}


# ============================================================================
# PHASE 1: TRAINING
# ============================================================================

def train_on_problems(
    benchmarks: Dict[str, List[Tuple[str, str]]],
    train_sizes: List[str] = None,
    training_timesteps: int = 5000,
    reward_variant: str = "astar_search"
) -> Optional[str]:
    """
    Train a GNN policy on problems from specified sizes.

    Args:
        benchmarks: Generated problems by size
        train_sizes: Sizes to train on ("small", "medium")
        training_timesteps: Total timesteps for training
        reward_variant: Reward function to use

    Returns:
        Path to trained model, or None if training failed
    """
    if train_sizes is None:
        train_sizes = ["small", "medium"]

    logger.info("\n" + "=" * 80)
    logger.info("PHASE 1: TRAINING ON SMALL/MEDIUM PROBLEMS")
    logger.info("=" * 80 + "\n")

    try:
        from common_utils import train_model

        # Collect all training problems
        train_benchmarks = []
        for size in train_sizes:
            if size in benchmarks:
                train_benchmarks.extend(benchmarks[size])

        if not train_benchmarks:
            logger.error("No training problems available")
            return None

        logger.info(f"Training on {len(train_benchmarks)} problems:")
        for domain, problem in train_benchmarks[:3]:
            logger.info(f"  - {os.path.basename(problem)}")
        if len(train_benchmarks) > 3:
            logger.info(f"  ... and {len(train_benchmarks) - 3} more")

        # Hyperparameters
        hyperparams = {
            'learning_rate': 0.0001,
            'n_steps': 128,
            'batch_size': 32,
            'ent_coef': 0.02,
            'reward_variant': reward_variant,
            'w_search_efficiency': 0.30,
            'w_solution_quality': 0.20,
            'w_f_stability': 0.35,
            'w_state_control': 0.15,
        }

        logger.info(f"\nHyperparameters:")
        for k, v in hyperparams.items():
            logger.info(f"  {k:<30} = {v}")

        # Train
        logger.info(f"\nTraining for {training_timesteps} timesteps...")

        model_path = f"mvp_output/gnn_model_scale_gen_{reward_variant}.zip"
        os.makedirs("mvp_output", exist_ok=True)

        model = train_model(
            model_save_path=model_path,
            benchmarks=train_benchmarks,
            hyperparams=hyperparams,
            total_timesteps=training_timesteps,
            tb_log_dir="tb_logs/",
            tb_log_name="ScaleGeneralization_Training",
            debug_mode=False,  # REAL FD
        )

        if model is None:
            logger.error("Training failed")
            return None

        logger.info(f"\n✅ Training complete!")
        logger.info(f"   Model saved: {model_path}")

        return model_path

    except Exception as e:
        logger.error(f"Training failed: {e}")
        logger.error(traceback.format_exc())
        return None


# ============================================================================
# PHASE 2: EVALUATION
# ============================================================================

def evaluate_scale_generalization(
    model_path: str,
    benchmarks: Dict[str, List[Tuple[str, str]]],
    test_sizes: List[str] = None,
    timeout_sec: int = 300
) -> Dict[str, Dict]:
    """
    Evaluate trained model on larger problems (scale generalization test).

    Args:
        model_path: Path to trained model
        benchmarks: Problems by size
        test_sizes: Sizes to test on ("medium", "large")
        timeout_sec: Timeout per problem

    Returns:
        Dict with evaluation results
    """
    if test_sizes is None:
        test_sizes = ["medium", "large"]

    logger.info("\n" + "=" * 80)
    logger.info("PHASE 2: SCALE GENERALIZATION EVALUATION")
    logger.info("=" * 80 + "\n")

    try:
        from evaluation_comprehensive import (
            EvaluationFramework,
            GNNPolicyRunner,
            BenchmarkConfig
        )

        framework = EvaluationFramework(output_dir="scale_gen_results")

        # Run GNN evaluation on test sizes
        results_by_size = {}

        for size in test_sizes:
            if size not in benchmarks or not benchmarks[size]:
                logger.warning(f"No problems available for size: {size}")
                continue

            logger.info(f"\n{'='*60}")
            logger.info(f"Evaluating on {size.upper()} problems")
            logger.info(f"{'='*60}\n")

            test_benchmarks = benchmarks[size]
            logger.info(f"Testing on {len(test_benchmarks)} {size} problem(s):")
            for domain, problem in test_benchmarks[:3]:
                logger.info(f"  - {os.path.basename(problem)}")
            if len(test_benchmarks) > 3:
                logger.info(f"  ... and {len(test_benchmarks) - 3} more")

            # Run evaluation
            gnn_runner = GNNPolicyRunner(model_path, timeout_sec)

            size_results = {
                'problems_tested': 0,
                'problems_solved': 0,
                'avg_time': 0.0,
                'avg_expansions': 0,
                'avg_plan_cost': 0,
                'solve_rate': 0.0,
                'details': []
            }

            times = []
            expansions = []
            costs = []
            solved_count = 0

            for domain_path, problem_path in test_benchmarks:
                logger.info(f"\n  Testing: {os.path.basename(problem_path)}")

                try:
                    result = gnn_runner.run(domain_path, problem_path)

                    size_results['problems_tested'] += 1

                    detail = {
                        'problem': os.path.basename(problem_path),
                        'solved': result.solved,
                        'time': result.time_sec,
                        'expansions': result.expansions,
                        'plan_cost': result.plan_cost,
                    }

                    if result.solved:
                        solved_count += 1
                        times.append(result.time_sec)
                        expansions.append(result.expansions)
                        costs.append(result.plan_cost)
                        detail['status'] = "✅ SOLVED"
                    else:
                        detail['status'] = "❌ UNSOLVED"

                    size_results['details'].append(detail)
                    logger.info(f"    {detail['status']}")

                except Exception as e:
                    logger.error(f"    Evaluation failed: {e}")
                    size_results['details'].append({
                        'problem': os.path.basename(problem_path),
                        'status': f"❌ ERROR: {str(e)[:50]}"
                    })

            # Compute statistics
            size_results['problems_solved'] = solved_count
            size_results['solve_rate'] = (solved_count / max(size_results['problems_tested'], 1)) * 100

            if times:
                size_results['avg_time'] = np.mean(times)
                size_results['median_time'] = np.median(times)
            if expansions:
                size_results['avg_expansions'] = int(np.mean(expansions))
                size_results['median_expansions'] = int(np.median(expansions))
            if costs:
                size_results['avg_plan_cost'] = int(np.mean(costs))
                size_results['median_plan_cost'] = int(np.median(costs))

            results_by_size[size] = size_results

            # Log summary
            logger.info(f"\n{'-'*60}")
            logger.info(f"SUMMARY FOR {size.upper()} PROBLEMS:")
            logger.info(f"  Solve Rate:      {size_results['solve_rate']:.1f}% ({solved_count}/{size_results['problems_tested']})")
            if times:
                logger.info(f"  Avg Time:        {size_results['avg_time']:.2f}s")
                logger.info(f"  Avg Expansions:  {size_results['avg_expansions']}")

        return results_by_size

    except Exception as e:
        logger.error(f"Evaluation failed: {e}")
        logger.error(traceback.format_exc())
        return {}


# ============================================================================
# PHASE 3: ANALYSIS & REPORTING
# ============================================================================

def generate_scale_generalization_report(
    results_by_size: Dict[str, Dict],
    output_dir: str = "scale_gen_results"
) -> bool:
    """
    Generate detailed analysis of scale generalization results.

    Args:
        results_by_size: Evaluation results by problem size
        output_dir: Where to save the report

    Returns:
        True if successful
    """
    logger.info("\n" + "=" * 80)
    logger.info("PHASE 3: SCALE GENERALIZATION ANALYSIS")
    logger.info("=" * 80 + "\n")

    try:
        os.makedirs(output_dir, exist_ok=True)

        # Generate text report
        report_path = Path(output_dir) / "scale_generalization_report.txt"

        with open(report_path, 'w') as f:
            f.write("=" * 90 + "\n")
            f.write("SCALE GENERALIZATION ANALYSIS REPORT\n")
            f.write("=" * 90 + "\n\n")

            f.write(f"Timestamp: {datetime.now().isoformat()}\n\n")

            # Summary table
            f.write("SOLVE RATE BY PROBLEM SIZE\n")
            f.write("-" * 50 + "\n")
            f.write(f"{'Size':<15} {'Solved':<20} {'Solve Rate':<15}\n")
            f.write("-" * 50 + "\n")

            for size, results in results_by_size.items():
                solved = results['problems_solved']
                total = results['problems_tested']
                rate = results['solve_rate']
                f.write(f"{size:<15} {solved}/{total:<18} {rate:>6.1f}%\n")

            f.write("-" * 50 + "\n\n")

            # Performance metrics
            f.write("PERFORMANCE METRICS BY SIZE\n")
            f.write("-" * 50 + "\n")

            for size, results in results_by_size.items():
                f.write(f"\n{size.upper()} PROBLEMS:\n")
                f.write(f"  Avg Time:           {results.get('avg_time', 'N/A')}\n")
                f.write(f"  Avg Expansions:     {results.get('avg_expansions', 'N/A')}\n")
                f.write(f"  Avg Plan Cost:      {results.get('avg_plan_cost', 'N/A')}\n")

            # Detailed results
            f.write("\n" + "=" * 90 + "\n")
            f.write("DETAILED RESULTS BY PROBLEM\n")
            f.write("=" * 90 + "\n\n")

            for size, results in results_by_size.items():
                f.write(f"\n{size.upper()} PROBLEMS:\n")
                f.write("-" * 50 + "\n")

                for detail in results.get('details', []):
                    f.write(f"  Problem: {detail['problem']}\n")
                    f.write(f"    Status: {detail['status']}\n")

                    if 'time' in detail:
                        f.write(f"    Time: {detail['time']:.2f}s\n")
                    if 'expansions' in detail:
                        f.write(f"    Expansions: {detail['expansions']}\n")

            # Analysis & conclusions
            f.write("\n" + "=" * 90 + "\n")
            f.write("SCALE GENERALIZATION ANALYSIS\n")
            f.write("=" * 90 + "\n\n")

            sizes_sorted = sorted(results_by_size.keys(),
                                 key=lambda x: ['small', 'medium', 'large'].index(x) if x in ['small', 'medium', 'large'] else 999)

            solve_rates = [results_by_size[s]['solve_rate'] for s in sizes_sorted if s in results_by_size]

            if len(solve_rates) >= 2:
                degradation = solve_rates[0] - solve_rates[-1]
                f.write(f"Generalization Degradation:\n")
                f.write(f"  From {sizes_sorted[0]} ({solve_rates[0]:.1f}%) → {sizes_sorted[-1]} ({solve_rates[-1]:.1f}%)\n")
                f.write(f"  Absolute Drop: {degradation:.1f} percentage points\n")

                if degradation < 10:
                    f.write(f"  ✅ EXCELLENT: Less than 10% degradation\n")
                elif degradation < 25:
                    f.write(f"  ✓ GOOD: Less than 25% degradation\n")
                elif degradation < 50:
                    f.write(f"  ⚠️ MODERATE: 25-50% degradation\n")
                else:
                    f.write(f"  ❌ POOR: Greater than 50% degradation\n")

            f.write("\n" + "=" * 90 + "\n")

        logger.info(f"✓ Report saved: {report_path}")

        # Save JSON results
        json_path = Path(output_dir) / "scale_generalization_results.json"
        with open(json_path, 'w') as f:
            json.dump(results_by_size, f, indent=2, default=str)

        logger.info(f"✓ JSON results saved: {json_path}")

        # Print summary to console
        logger.info("\n" + "=" * 80)
        logger.info("SCALE GENERALIZATION SUMMARY")
        logger.info("=" * 80 + "\n")

        for size, results in sorted(results_by_size.items()):
            logger.info(f"{size.upper()}:")
            logger.info(f"  Solve Rate: {results['solve_rate']:.1f}% ({results['problems_solved']}/{results['problems_tested']})")
            if results.get('avg_time'):
                logger.info(f"  Avg Time: {results['avg_time']:.2f}s")
            logger.info()

        return True

    except Exception as e:
        logger.error(f"Report generation failed: {e}")
        logger.error(traceback.format_exc())
        return False


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Scale Generalization Training & Testing Framework"
    )

    parser.add_argument(
        "--domain",
        default="blocks_world",
        choices=["blocks_world", "logistics", "gripper"],
        help="Domain to generate problems for"
    )

    parser.add_argument(
        "--train-sizes",
        nargs="+",
        default=["small", "medium"],
        help="Sizes to train on"
    )

    parser.add_argument(
        "--test-sizes",
        nargs="+",
        default=["medium", "large"],
        help="Sizes to test on"
    )

    parser.add_argument(
        "--problems-per-size",
        type=int,
        default=5,
        help="Number of problems to generate per size"
    )

    parser.add_argument(
        "--training-timesteps",
        type=int,
        default=5000,
        help="Total training timesteps"
    )

    parser.add_argument(
        "--reward-variant",
        default="astar_search",
        help="Reward function variant"
    )

    parser.add_argument(
        "--output",
        default="scale_gen_results",
        help="Output directory"
    )

    parser.add_argument(
        "--skip-generation",
        action="store_true",
        help="Skip problem generation (use existing)"
    )

    parser.add_argument(
        "--skip-training",
        action="store_true",
        help="Skip training (use existing model)"
    )

    args = parser.parse_args()

    logger.info("\n" + "=" * 90)
    logger.info("SCALE GENERALIZATION: TRAIN SMALL/MEDIUM → TEST LARGE")
    logger.info("=" * 90)

    logger.info(f"\nConfiguration:")
    logger.info(f"  Domain:                 {args.domain}")
    logger.info(f"  Train sizes:            {', '.join(args.train_sizes)}")
    logger.info(f"  Test sizes:             {', '.join(args.test_sizes)}")
    logger.info(f"  Problems per size:      {args.problems_per_size}")
    logger.info(f"  Training timesteps:     {args.training_timesteps}")
    logger.info(f"  Reward variant:         {args.reward_variant}")
    logger.info(f"  Output directory:       {args.output}\n")

    # Phase 0: Generate problems
    if not args.skip_generation:
        all_sizes = list(set(args.train_sizes + args.test_sizes))
        benchmarks = generate_benchmark_problems(
            domain=args.domain,
            sizes=all_sizes,
            problems_per_size=args.problems_per_size
        )

        if not benchmarks:
            logger.error("Problem generation failed")
            return 1
    else:
        logger.info("Skipping problem generation")
        benchmarks = {}

    # Phase 1: Train
    if not args.skip_training:
        model_path = train_on_problems(
            benchmarks=benchmarks,
            train_sizes=args.train_sizes,
            training_timesteps=args.training_timesteps,
            reward_variant=args.reward_variant
        )

        if model_path is None:
            logger.error("Training failed")
            return 1
    else:
        logger.info("Skipping training")
        model_path = f"mvp_output/gnn_model_scale_gen_{args.reward_variant}.zip"

    # Phase 2: Evaluate
    results_by_size = evaluate_scale_generalization(
        model_path=model_path,
        benchmarks=benchmarks,
        test_sizes=args.test_sizes,
        timeout_sec=300
    )

    if not results_by_size:
        logger.error("Evaluation failed")
        return 1

    # Phase 3: Report
    if not generate_scale_generalization_report(results_by_size, args.output):
        logger.error("Report generation failed")
        return 1

    # Final summary
    logger.info("\n" + "=" * 90)
    logger.info("SCALE GENERALIZATION FRAMEWORK COMPLETE")
    logger.info("=" * 90)
    logger.info(f"\nResults saved to: {os.path.abspath(args.output)}")
    logger.info(f"Report: {os.path.join(args.output, 'scale_generalization_report.txt')}")

    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

--------------------------------------------------------------------------------

The file curriculum_experiment.py code is this:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CURRICULUM LEARNING TRAINING - Progressive Difficulty
=======================================================
Trains a GNN policy using curriculum learning: small → medium → large problems.

Features:
  ✓ Progressive difficulty training (curriculum learning)
  ✓ Continuous monitoring and detailed logging
  ✓ Save checkpoints after each difficulty level
  ✓ Validation on large problems after training
  ✓ Performance tracking across difficulties
  ✓ TensorBoard visualization

Run with:
    python train_curriculum.py

Environment Variables:
    REWARD_VARIANT: Which reward function to use (default: astar_search)
    TOTAL_TIMESTEPS: Total training timesteps (default: 50000)
    SMALL_TIMESTEPS: Timesteps per small problem (default: 500)
    MEDIUM_TIMESTEPS: Timesteps per medium problem (default: 750)
    LARGE_TIMESTEPS: Timesteps per large problem (default: 1000)
    NUM_PROBLEMS_PER_DIFFICULTY: Problems per difficulty (default: 10)
"""

import sys
import os
import logging
import glob
import json
import traceback
import random
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime

# Setup paths
sys.path.insert(0, os.getcwd())
os.makedirs("downward/gnn_output", exist_ok=True)
os.makedirs("downward/fd_output", exist_ok=True)
os.makedirs("logs", exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)-8s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("training_curriculum.log", encoding='utf-8'),
    ],
    force=True
)
logger = logging.getLogger(__name__)


# ============================================================================
# CONFIGURATION
# ============================================================================

class CurriculumConfig:
    """Configuration for curriculum learning."""

    REWARD_VARIANT = os.environ.get('REWARD_VARIANT', 'astar_search')

    SMALL_TIMESTEPS = int(os.environ.get('SMALL_TIMESTEPS', '500'))
    MEDIUM_TIMESTEPS = int(os.environ.get('MEDIUM_TIMESTEPS', '750'))
    LARGE_TIMESTEPS = int(os.environ.get('LARGE_TIMESTEPS', '1000'))
    TOTAL_TIMESTEPS = int(os.environ.get('TOTAL_TIMESTEPS', '50000'))

    NUM_PROBLEMS_PER_DIFFICULTY = int(os.environ.get('NUM_PROBLEMS_PER_DIFFICULTY', '10'))

    REWARD_KWARGS = {
        'w_search_efficiency': 0.30,
        'w_solution_quality': 0.20,
        'w_f_stability': 0.35,
        'w_state_control': 0.15,
    }


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def print_section(title: str, width: int = 90):
    """Print formatted section header."""
    logger.info("\n" + "=" * width)
    logger.info(f"// {title.upper()}")
    logger.info("=" * width + "\n")


def print_subsection(title: str):
    """Print formatted subsection header."""
    logger.info("\n" + "-" * 80)
    logger.info(f">>> {title}")
    logger.info("-" * 80 + "\n")


# ============================================================================
# PHASE 0: BENCHMARK LOADING
# ============================================================================

def load_benchmarks_by_difficulty() -> Dict[str, List[Tuple[str, str]]]:
    """Load benchmarks organized by difficulty (small/medium/large)."""
    print_section("PHASE 0: LOAD BENCHMARKS BY DIFFICULTY")

    benchmarks_dir = os.path.abspath("benchmarks")

    if not os.path.isdir(benchmarks_dir):
        logger.error(f"Benchmarks directory not found: {benchmarks_dir}")
        logger.error("Expected structure:")
        logger.error("  benchmarks/small/   (with domain.pddl and problem_*.pddl)")
        logger.error("  benchmarks/medium/  (with domain.pddl and problem_*.pddl)")
        logger.error("  benchmarks/large/   (with domain.pddl and problem_*.pddl)")
        return {}

    logger.info(f"Benchmarks directory: {benchmarks_dir}\n")

    difficulties = ["small", "medium", "large"]
    all_benchmarks = {}

    for difficulty in difficulties:
        difficulty_dir = os.path.join(benchmarks_dir, difficulty)
        logger.info(f"Loading {difficulty.upper()} difficulty problems...")

        if not os.path.isdir(difficulty_dir):
            logger.warning(f"  ⚠️ Directory not found: {difficulty_dir}")
            all_benchmarks[difficulty] = []
            continue

        # Find domain
        domain_file = os.path.join(difficulty_dir, "domain.pddl")
        if not os.path.exists(domain_file):
            logger.warning(f"  ⚠️ Domain file not found: {domain_file}")
            all_benchmarks[difficulty] = []
            continue

        logger.info(f"  ✓ Domain: {domain_file}")

        # Find problems
        problems = sorted(glob.glob(os.path.join(difficulty_dir, "problem_*.pddl")))

        if not problems:
            logger.warning(f"  ⚠️ No problem files found")
            all_benchmarks[difficulty] = []
            continue

        logger.info(f"  ✓ Found {len(problems)} problem(s)")
        for i, prob in enumerate(problems[:3], 1):
            logger.info(f"    {i}. {os.path.basename(prob)}")
        if len(problems) > 3:
            logger.info(f"    ... and {len(problems) - 3} more")

        # Create benchmark list
        benchmarks_list = [
            (os.path.abspath(domain_file), os.path.abspath(prob))
            for prob in problems
        ]

        all_benchmarks[difficulty] = benchmarks_list
        logger.info(f"  ✓ Loaded {len(benchmarks_list)} benchmark(s)\n")

    # Summary
    total = sum(len(b) for b in all_benchmarks.values())
    if total == 0:
        logger.error("❌ No benchmarks loaded!")
        return {}

    logger.info(f"✅ Loaded {total} total benchmarks:")
    for difficulty in difficulties:
        count = len(all_benchmarks[difficulty])
        logger.info(f"   {difficulty:<10} {count:>3} problems")

    return all_benchmarks


def create_curriculum_sequence(
        all_benchmarks: Dict[str, List[Tuple[str, str]]],
        num_per_difficulty: int = 10
) -> List[Tuple[str, str, str]]:
    """Create curriculum sequence: small → medium → large."""
    print_subsection("Create Curriculum Sequence")

    sequence = []
    difficulties = ["small", "medium", "large"]

    for difficulty in difficulties:
        if difficulty not in all_benchmarks or not all_benchmarks[difficulty]:
            logger.warning(f"No problems for difficulty: {difficulty}")
            continue

        problems_this_difficulty = all_benchmarks[difficulty]
        sampled = random.sample(
            problems_this_difficulty,
            min(num_per_difficulty, len(problems_this_difficulty))
        )

        for domain_file, problem_file in sampled:
            sequence.append((domain_file, problem_file, difficulty))

        logger.info(f"✓ Added {len(sampled)} {difficulty} problems (total: {len(sequence)})")

    logger.info(f"\n✅ Curriculum sequence ready:")
    logger.info(f"   Small:  {len([s for s in sequence if s[2] == 'small'])} problems")
    logger.info(f"   Medium: {len([s for s in sequence if s[2] == 'medium'])} problems")
    logger.info(f"   Large:  {len([s for s in sequence if s[2] == 'large'])} problems")

    return sequence


# ============================================================================
# PHASE 1: CURRICULUM TRAINING
# ============================================================================

def run_curriculum_training(
        curriculum_sequence: List[Tuple[str, str, str]],
        reward_variant: str = 'astar_search',
        **reward_kwargs
) -> Optional[str]:
    """
    Run curriculum learning training.

    Returns:
        Path to final model, or None if training failed
    """
    print_section("PHASE 1: CURRICULUM LEARNING TRAINING")

    try:
        from stable_baselines3 import PPO
        from merge_env import MergeEnv
        from gnn_policy import GNNPolicy
        from stable_baselines3.common.monitor import Monitor

        timesteps_config = {
            'small': CurriculumConfig.SMALL_TIMESTEPS,
            'medium': CurriculumConfig.MEDIUM_TIMESTEPS,
            'large': CurriculumConfig.LARGE_TIMESTEPS,
        }

        # Try to load existing model
        model_path = "mvp_output/gnn_model_curriculum_start.zip"
        model = None

        if os.path.exists(model_path):
            logger.info(f"Loading existing model: {model_path}")
            try:
                model = PPO.load(model_path)
                logger.info("✓ Model loaded, continuing training\n")
            except Exception as e:
                logger.warning(f"Could not load: {e}")

        total_steps = 0
        problems_trained = 0
        difficulty_counts = {'small': 0, 'medium': 0, 'large': 0}

        # Iterate through curriculum
        for step, (domain_file, problem_file, difficulty) in enumerate(curriculum_sequence, 1):
            problem_name = os.path.basename(problem_file)
            timesteps_this_problem = timesteps_config.get(difficulty, 500)

            print_subsection(f"Problem {step}/{len(curriculum_sequence)}: [{difficulty.upper()}] {problem_name}")

            logger.info(f"Difficulty: {difficulty}")
            logger.info(f"Timesteps: {timesteps_this_problem}\n")

            # Create environment
            try:
                env = MergeEnv(
                    domain_file=os.path.abspath(domain_file),
                    problem_file=os.path.abspath(problem_file),
                    max_merges=50,
                    debug=False,
                    reward_variant=reward_variant,
                    **reward_kwargs
                )
                env = Monitor(env)
                logger.info("✓ Environment created\n")
            except Exception as e:
                logger.error(f"Failed to create environment: {e}")
                logger.error("Skipping this problem\n")
                continue

            # Create model if needed
            if model is None:
                logger.info("Creating new PPO model with GNN policy...")
                model = PPO(
                    policy=GNNPolicy,
                    env=env,
                    learning_rate=0.0003,
                    n_steps=64,
                    batch_size=32,
                    ent_coef=0.01,
                    verbose=0,
                    tensorboard_log="tb_logs/",
                    policy_kwargs={"hidden_dim": 64},
                )
                logger.info("✓ New model created\n")
            else:
                model.set_env(env)

            # Train
            logger.info(f"Training for {timesteps_this_problem} timesteps...")
            try:
                model.learn(
                    total_timesteps=timesteps_this_problem,
                    tb_log_name=f"curriculum_{difficulty}_{step}",
                    reset_num_timesteps=False,
                )
                logger.info("✓ Training completed\n")

                total_steps += timesteps_this_problem
                problems_trained += 1
                difficulty_counts[difficulty] += 1

            except KeyboardInterrupt:
                logger.warning("⚠️ Training interrupted by user")
                break
            except Exception as e:
                logger.error(f"Training failed: {e}")
                logger.error(traceback.format_exc())
                continue
            finally:
                try:
                    env.close()
                except:
                    pass

            # Save checkpoint
            checkpoint_path = f"mvp_output/gnn_model_curriculum_{difficulty}_{step}.zip"
            model.save(checkpoint_path)
            logger.info(f"✓ Checkpoint saved: {checkpoint_path}\n")

            # Check total timesteps limit
            if total_steps >= CurriculumConfig.TOTAL_TIMESTEPS:
                logger.info(f"✓ Reached total timesteps: {total_steps}/{CurriculumConfig.TOTAL_TIMESTEPS}")
                break

        # Save final model
        if model is not None:
            final_model_path = "mvp_output/gnn_model_curriculum_final.zip"
            model.save(final_model_path)
            logger.info(f"\n✅ Final model saved: {final_model_path}")
            logger.info(f"  Total problems trained: {problems_trained}")
            logger.info(f"  Total timesteps: {total_steps}")
            logger.info(
                f"  Small: {difficulty_counts['small']}, Medium: {difficulty_counts['medium']}, Large: {difficulty_counts['large']}")
            return final_model_path

        return None

    except Exception as e:
        logger.error(f"❌ Curriculum training failed: {e}")
        logger.error(traceback.format_exc())
        return None


# ============================================================================
# PHASE 2: VALIDATION ON LARGE PROBLEMS
# ============================================================================

def validate_on_large_problems(
        model,
        all_benchmarks: Dict[str, List[Tuple[str, str]]],
        num_problems: int = 5
) -> Dict:
    """Validate trained model on large problems."""
    print_section("PHASE 2: VALIDATION ON LARGE PROBLEMS")

    if model is None:
        logger.warning("No model to validate")
        return {}

    if 'large' not in all_benchmarks or not all_benchmarks['large']:
        logger.warning("No large problems available")
        return {}

    try:
        from merge_env import MergeEnv

        large_problems = all_benchmarks['large']
        test_problems = random.sample(
            large_problems,
            min(num_problems, len(large_problems))
        )

        logger.info(f"Testing on {len(test_problems)} large problems:\n")

        total_reward = 0.0
        episodes_completed = 0
        episode_rewards = []

        for i, (domain_file, problem_file) in enumerate(test_problems, 1):
            problem_name = os.path.basename(problem_file)
            logger.info(f"  [{i}] {problem_name}")

            try:
                env = MergeEnv(
                    domain_file=os.path.abspath(domain_file),
                    problem_file=os.path.abspath(problem_file),
                    max_merges=50,
                    debug=False,
                    reward_variant=CurriculumConfig.REWARD_VARIANT,
                    **CurriculumConfig.REWARD_KWARGS
                )

                obs, _ = env.reset()
                episode_reward = 0.0
                steps = 0

                while steps < 20:
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, done, truncated, info = env.step(int(action))
                    episode_reward += reward
                    steps += 1
                    if done or truncated:
                        break

                total_reward += episode_reward
                episodes_completed += 1
                episode_rewards.append(episode_reward)

                logger.info(f"      ✓ Reward: {episode_reward:+.4f} ({steps} steps)")
                env.close()

            except Exception as e:
                logger.warning(f"      ⚠️ Failed: {e}")

        if episodes_completed > 0:
            avg_reward = total_reward / episodes_completed
            logger.info(f"\n✅ Validation Results:")
            logger.info(f"  Episodes: {episodes_completed}/{len(test_problems)}")
            logger.info(f"  Avg reward: {avg_reward:+.4f}")
            logger.info(f"  Max reward: {max(episode_rewards):+.4f}")
            logger.info(f"  Min reward: {min(episode_rewards):+.4f}")

            return {
                'avg_reward': avg_reward,
                'max_reward': max(episode_rewards),
                'min_reward': min(episode_rewards),
                'episodes': episodes_completed,
            }

        return {}

    except Exception as e:
        logger.error(f"❌ Validation failed: {e}")
        logger.error(traceback.format_exc())
        return {}


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Main execution."""
    print_section("CURRICULUM LEARNING TRAINING", "=", 95)

    logger.info(f"Configuration:")
    logger.info(f"  Reward variant: {CurriculumConfig.REWARD_VARIANT}")
    logger.info(f"  Total timesteps: {CurriculumConfig.TOTAL_TIMESTEPS}")
    logger.info(f"  Small: {CurriculumConfig.SMALL_TIMESTEPS} ts/problem")
    logger.info(f"  Medium: {CurriculumConfig.MEDIUM_TIMESTEPS} ts/problem")
    logger.info(f"  Large: {CurriculumConfig.LARGE_TIMESTEPS} ts/problem")
    logger.info(f"  Problems per difficulty: {CurriculumConfig.NUM_PROBLEMS_PER_DIFFICULTY}\n")

    # Phase 0: Load
    all_benchmarks = load_benchmarks_by_difficulty()
    if not all_benchmarks or sum(len(b) for b in all_benchmarks.values()) == 0:
        logger.error("No benchmarks loaded")
        return 1

    # Create curriculum
    curriculum_sequence = create_curriculum_sequence(
        all_benchmarks,
        num_per_difficulty=CurriculumConfig.NUM_PROBLEMS_PER_DIFFICULTY
    )

    if not curriculum_sequence:
        logger.error("No curriculum sequence created")
        return 1

    # Phase 1: Train
    model_path = run_curriculum_training(
        curriculum_sequence,
        reward_variant=CurriculumConfig.REWARD_VARIANT,
        **CurriculumConfig.REWARD_KWARGS
    )

    if not model_path:
        logger.error("Training failed")
        return 1

    # Phase 2: Validate
    try:
        from stable_baselines3 import PPO
        model = PPO.load(model_path)
        validate_on_large_problems(model, all_benchmarks, num_problems=5)
    except Exception as e:
        logger.warning(f"Validation skipped: {e}")

    # Summary
    print_section("TRAINING COMPLETE", "=", 95)
    logger.info("✅ Curriculum learning pipeline completed!\n")
    logger.info("Next steps:")
    logger.info(f"  1. TensorBoard: tensorboard --logdir={os.path.abspath('tb_logs/')}")
    logger.info(f"  2. Model: {model_path}")
    logger.info(f"  3. Log: {os.path.abspath('training_curriculum.log')}")

    return 0


if __name__ == "__main__":
    sys.exit(main())

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.cc code is this:
#include "merge_and_shrink_algorithm.h"

#include "distances.h"
#include "factored_transition_system.h"
#include "fts_factory.h"
#include "label_reduction.h"
#include "labels.h"
#include "merge_and_shrink_representation.h"
#include "merge_strategy.h"
#include "merge_strategy_factory.h"
#include "shrink_strategy.h"
#include "transition_system.h"
#include "types.h"
#include "utils.h"

#include "../plugins/plugin.h"
#include "../task_utils/task_properties.h"
#include "../utils/component_errors.h"
#include "../utils/countdown_timer.h"
#include "../utils/markup.h"
#include "../utils/math.h"
#include "../utils/system.h"
#include "../utils/timer.h"

#include <cassert>
#include <iostream>
#include <limits>
#include <string>
#include <utility>
#include <vector>

#include <filesystem>

#include <nlohmann/json.hpp>
#include <fstream>

#include <cmath>     // for NaN/Inf handling if needed
#include <limits>    // for numeric_limits

using json = nlohmann::json;

using namespace std;
using plugins::Bounds;
using utils::ExitCode;

namespace merge_and_shrink {
static void log_progress(const utils::Timer &timer, const string &msg, utils::LogProxy &log) {
    log << "M&S algorithm timer: " << timer << " (" << msg << ")" << endl;
}
MergeAndShrinkAlgorithm::MergeAndShrinkAlgorithm(
    const shared_ptr<MergeStrategyFactory> &merge_strategy,
    const shared_ptr<ShrinkStrategy> &shrink_strategy,
    const shared_ptr<LabelReduction> &label_reduction,
    bool prune_unreachable_states, bool prune_irrelevant_states,
    int max_states, int max_states_before_merge,
    int threshold_before_merge, double main_loop_max_time,
    utils::Verbosity verbosity)
    : merge_strategy_factory(merge_strategy),
      shrink_strategy(shrink_strategy),
      label_reduction(label_reduction),
      max_states(max_states),
      max_states_before_merge(max_states_before_merge),
      shrink_threshold_before_merge(threshold_before_merge),
      prune_unreachable_states(prune_unreachable_states),
      prune_irrelevant_states(prune_irrelevant_states),
      log(utils::get_log_for_verbosity(verbosity)),
      main_loop_max_time(main_loop_max_time),
      starting_peak_memory(0) {
    handle_shrink_limit_defaults();
    // Asserting fields (not parameters).
    assert(this->max_states_before_merge >= 1);
    assert(this->max_states >= this->max_states_before_merge);
}

void MergeAndShrinkAlgorithm::handle_shrink_limit_defaults() {
    // If none of the two state limits has been set: set default limit.
    if (max_states == -1 && max_states_before_merge == -1) {
        max_states = 50000;
    }

    // If one of the max_states options has not been set, set the other
    // so that it imposes no further limits.
    if (max_states_before_merge == -1) {
        max_states_before_merge = max_states;
    } else if (max_states == -1) {
        if (utils::is_product_within_limit(
                max_states_before_merge, max_states_before_merge, INF)) {
            max_states = max_states_before_merge * max_states_before_merge;
        } else {
            max_states = INF;
        }
    }

    if (max_states_before_merge > max_states) {
        max_states_before_merge = max_states;
        if (log.is_warning()) {
            log << "WARNING: "
                << "max_states_before_merge exceeds max_states, "
                << "correcting max_states_before_merge." << endl;
        }
    }

    utils::verify_argument(max_states >= 1,
                           "Transition system size must be at least 1.");

    utils::verify_argument(max_states_before_merge >= 1,
                           "Transition system size before merge must be at least 1.");

    if (shrink_threshold_before_merge == -1) {
        shrink_threshold_before_merge = max_states;
    }

    utils::verify_argument(shrink_threshold_before_merge >= 1,
                           "Threshold must be at least 1.");

    if (shrink_threshold_before_merge > max_states) {
        shrink_threshold_before_merge = max_states;
        if (log.is_warning()) {
            log << "WARNING: "
                << "threshold exceeds max_states, "
                << "correcting threshold." << endl;
        }
    }
}

void MergeAndShrinkAlgorithm::report_peak_memory_delta(bool final) const {
    if (final)
        log << "Final";
    else
        log << "Current";
    log << " peak memory increase of merge-and-shrink algorithm: "
        << utils::get_peak_memory_in_kb() - starting_peak_memory << " KB"
        << endl;
}

void MergeAndShrinkAlgorithm::dump_options() const {
    if (log.is_at_least_normal()) {
        if (merge_strategy_factory) { // deleted after merge strategy extraction
            merge_strategy_factory->dump_options();
            log << endl;
        }

        log << "Options related to size limits and shrinking: " << endl;
        log << "Transition system size limit: " << max_states << endl
            << "Transition system size limit right before merge: "
            << max_states_before_merge << endl;
        log << "Threshold to trigger shrinking right before merge: "
            << shrink_threshold_before_merge << endl;
        log << endl;

        shrink_strategy->dump_options(log);
        log << endl;

        log << "Pruning unreachable states: "
            << (prune_unreachable_states ? "yes" : "no") << endl;
        log << "Pruning irrelevant states: "
            << (prune_irrelevant_states ? "yes" : "no") << endl;
        log << endl;

        if (label_reduction) {
            label_reduction->dump_options(log);
        } else {
            log << "Label reduction disabled" << endl;
        }
        log << endl;

        log << "Main loop max time in seconds: " << main_loop_max_time << endl;
        log << endl;
    }
}

void MergeAndShrinkAlgorithm::warn_on_unusual_options() const {
    string dashes(79, '=');
    if (!label_reduction) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! You did not enable label reduction. " << endl
                << "This may drastically reduce the performance of merge-and-shrink!"
                << endl << dashes << endl;
        }
    } else if (label_reduction->reduce_before_merging() && label_reduction->reduce_before_shrinking()) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! You set label reduction to be applied twice in each merge-and-shrink" << endl
                << "iteration, both before shrinking and merging. This double computation effort" << endl
                << "does not pay off for most configurations!"
                << endl << dashes << endl;
        }
    } else {
        if (label_reduction->reduce_before_shrinking() &&
            (shrink_strategy->get_name() == "f-preserving"
             || shrink_strategy->get_name() == "random")) {
            if (log.is_warning()) {
                log << dashes << endl
                    << "WARNING! Bucket-based shrink strategies such as f-preserving random perform" << endl
                    << "best if used with label reduction before merging, not before shrinking!"
                    << endl << dashes << endl;
            }
        }
        if (label_reduction->reduce_before_merging() &&
            shrink_strategy->get_name() == "bisimulation") {
            if (log.is_warning()) {
                log << dashes << endl
                    << "WARNING! Shrinking based on bisimulation performs best if used with label" << endl
                    << "reduction before shrinking, not before merging!"
                    << endl << dashes << endl;
            }
        }
    }

    if (!prune_unreachable_states || !prune_irrelevant_states) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! Pruning is (partially) turned off!" << endl
                << "This may drastically reduce the performance of merge-and-shrink!"
                << endl << dashes << endl;
        }
    }
}

bool MergeAndShrinkAlgorithm::ran_out_of_time(
    const utils::CountdownTimer &timer) const {
    if (timer.is_expired()) {
        if (log.is_at_least_normal()) {
            log << "Ran out of time, stopping computation." << endl;
            log << endl;
        }
        return true;
    }
    return false;
}

// ============================================================================
// END: CORRECTED HELPER FUNCTION
// ============================================================================

void MergeAndShrinkAlgorithm::main_loop(
    FactoredTransitionSystem &fts,
    const TaskProxy &task_proxy) {

    utils::CountdownTimer timer(main_loop_max_time);
    if (log.is_at_least_normal()) {
        log << "Starting main loop ";
        if (main_loop_max_time == numeric_limits<double>::infinity()) {
            log << "without a time limit." << endl;
        } else {
            log << "with a time limit of " << main_loop_max_time << "s." << endl;
        }
    }

    int maximum_intermediate_size = 0;
    for (int i = 0; i < fts.get_size(); ++i) {
        int size = fts.get_transition_system(i).get_size();
        if (size > maximum_intermediate_size) {
            maximum_intermediate_size = size;
        }
    }

    if (label_reduction) {
        label_reduction->initialize(task_proxy);
    }

    unique_ptr<MergeStrategy> merge_strategy =
        merge_strategy_factory->compute_merge_strategy(task_proxy, fts);
    merge_strategy_factory = nullptr;

    auto log_main_loop_progress = [&timer, this](const string &msg) {
        log << "M&S algorithm main loop timer: "
            << timer.get_elapsed_time()
            << " (" << msg << ")" << endl;
    };

    // ✅ SETUP: Get output directories ONCE at the start
    std::string fd_output_dir = std::filesystem::absolute("fd_output").string();
    std::filesystem::create_directories(fd_output_dir);
    std::cout << "[M&S] Using fd_output: " << fd_output_dir << std::endl;

    int iteration = 0;

    while (fts.get_num_active_entries() > 1) {
        // ====================================================================
        // PHASE 1: GET NEXT MERGE PAIR FROM STRATEGY
        // ====================================================================

        pair<int, int> merge_indices = merge_strategy->get_next();
        if (ran_out_of_time(timer)) break;

        int merge_index1 = merge_indices.first;
        int merge_index2 = merge_indices.second;

        assert(merge_index1 != merge_index2);
        if (log.is_at_least_normal()) {
            log << "Next pair of indices: ("
                << merge_index1 << ", " << merge_index2 << ")" << endl;
            if (log.is_at_least_verbose()) {
                fts.statistics(merge_index1, log);
                fts.statistics(merge_index2, log);
            }
            log_main_loop_progress("after computation of next merge");
        }

        // ====================================================================
        // PHASE 2: LABEL REDUCTION (BEFORE SHRINKING)
        // ====================================================================

        bool reduced = false;
        if (label_reduction && label_reduction->reduce_before_shrinking()) {
            reduced = label_reduction->reduce(merge_indices, fts, log);
            if (log.is_at_least_normal() && reduced) {
                log_main_loop_progress("after label reduction");
            }
        }
        if (ran_out_of_time(timer)) break;

        // ====================================================================
        // PHASE 3: SHRINKING
        // ====================================================================

        bool shrunk = shrink_before_merge_step(
            fts,
            merge_index1,
            merge_index2,
            max_states,
            max_states_before_merge,
            shrink_threshold_before_merge,
            *shrink_strategy,
            log);
        if (log.is_at_least_normal() && shrunk) {
            log_main_loop_progress("after shrinking");
        }
        if (ran_out_of_time(timer)) break;

        // ====================================================================
        // PHASE 4: LABEL REDUCTION (BEFORE MERGING)
        // ====================================================================

        if (label_reduction && label_reduction->reduce_before_merging()) {
            reduced = label_reduction->reduce(merge_indices, fts, log);
            if (log.is_at_least_normal() && reduced) {
                log_main_loop_progress("after label reduction");
            }
        }
        if (ran_out_of_time(timer)) break;

        // ====================================================================
        // PHASE 5: EXPORT MERGE SIGNALS (BEFORE DATA) - SIMPLIFIED INLINE
        // ====================================================================

        {
            const auto& init1 = fts.get_distances(merge_index1).get_init_distances();
            const auto& goal1 = fts.get_distances(merge_index1).get_goal_distances();
            const auto& init2 = fts.get_distances(merge_index2).get_init_distances();
            const auto& goal2 = fts.get_distances(merge_index2).get_goal_distances();

            std::vector<int> f1(init1.size()), f2(init2.size());
            for (size_t i = 0; i < f1.size(); ++i) {
                f1[i] = (init1[i] == INF || goal1[i] == INF) ? INF : init1[i] + goal1[i];
            }
            for (size_t j = 0; j < f2.size(); ++j) {
                f2[j] = (init2[j] == INF || goal2[j] == INF) ? INF : init2[j] + goal2[j];
            }

            int ts1_transitions = 0;
            for (auto it = fts.get_transition_system(merge_index1).begin();
                 it != fts.get_transition_system(merge_index1).end(); ++it) {
                ts1_transitions += (*it).get_transitions().size();
            }

            int ts2_transitions = 0;
            for (auto it = fts.get_transition_system(merge_index2).begin();
                 it != fts.get_transition_system(merge_index2).end(); ++it) {
                ts2_transitions += (*it).get_transitions().size();
            }

            int ts1_size = (int)f1.size();
            int ts2_size = (int)f2.size();

            json product_mapping;
            for (int s = 0; s < ts1_size * ts2_size; ++s) {
                int s1 = s / ts2_size;
                int s2 = s % ts2_size;
                product_mapping[std::to_string(s)] = {{"s1", s1}, {"s2", s2}};
            }

            json before_data;
            before_data["ts1_id"] = merge_index1;
            before_data["ts2_id"] = merge_index2;
            before_data["iteration"] = iteration;
            before_data["ts1_f_values"] = f1;
            before_data["ts2_f_values"] = f2;
            before_data["ts1_size"] = ts1_size;
            before_data["ts2_size"] = ts2_size;
            before_data["ts1_num_transitions"] = ts1_transitions;
            before_data["ts2_num_transitions"] = ts2_transitions;
            before_data["product_mapping"] = product_mapping;

            // ✅ DIRECT WRITE: No helper function, just write it
            std::string before_path = fd_output_dir + "/merge_before_" + std::to_string(iteration) + ".json";
            std::ofstream before_file(before_path, std::ios::out | std::ios::trunc);
            if (!before_file.is_open()) {
                std::cerr << "[M&S] ERROR: Cannot create merge_before file: " << before_path << std::endl;
                throw std::runtime_error("Cannot create merge_before file");
            }
            before_file << before_data.dump(2);
            before_file.close();
            std::cout << "[M&S] ✅ Wrote merge_before_" << iteration << ".json" << std::endl;
        }

        // ====================================================================
        // PHASE 6: PERFORM ACTUAL MERGE
        // ====================================================================

        int merged_index = fts.merge(merge_index1, merge_index2, log);

        // ====================================================================
        // PHASE 7: EXPORT MERGE SIGNALS (AFTER DATA) - SIMPLIFIED INLINE
        // ====================================================================

        {
            const auto& init_dist = fts.get_distances(merged_index).get_init_distances();
            const auto& goal_dist = fts.get_distances(merged_index).get_goal_distances();

            std::vector<int> f_after(init_dist.size());
            for (size_t s = 0; s < f_after.size(); ++s) {
                f_after[s] = (init_dist[s] == INF || goal_dist[s] == INF) ? INF : init_dist[s] + goal_dist[s];
            }

            const TransitionSystem& merged_ts = fts.get_transition_system(merged_index);
            int num_goals = 0;
            for (size_t i = 0; i < f_after.size(); ++i) {
                if (merged_ts.is_goal_state(i)) num_goals++;
            }

            int merged_transitions = 0;
            for (auto it = merged_ts.begin(); it != merged_ts.end(); ++it) {
                merged_transitions += (*it).get_transitions().size();
            }

            // ✅ COMPUTE A* METRICS (inline, simple version)
            int nodes_expanded = 0;
            for (int i = 0; i < merged_ts.get_size(); ++i) {
                if (init_dist[i] != INF && goal_dist[i] != INF) {
                    nodes_expanded++;
                }
            }

            int reachable_states = nodes_expanded;
            double branching_factor = 1.0;
            if (reachable_states > 0 && merged_transitions > 0) {
                branching_factor = (double)merged_transitions / (double)reachable_states;
                if (std::isnan(branching_factor) || std::isinf(branching_factor) || branching_factor < 1.0) {
                    branching_factor = 1.0;
                }
            }

            int search_depth = 0;
            long long sum_goal_dist = 0;
            int reachable_goal_count = 0;
            for (int i = 0; i < merged_ts.get_size(); ++i) {
                if (init_dist[i] != INF && goal_dist[i] != INF) {
                    sum_goal_dist += goal_dist[i];
                    reachable_goal_count++;
                }
            }
            if (reachable_goal_count > 0) {
                search_depth = (int)std::round((double)sum_goal_dist / reachable_goal_count);
            }

            int best_goal_f = INF;
            for (int i = 0; i < merged_ts.get_size(); ++i) {
                if (merged_ts.is_goal_state(i) && init_dist[i] != INF && goal_dist[i] != INF) {
                    int f = init_dist[i] + goal_dist[i];
                    if (f < best_goal_f) {
                        best_goal_f = f;
                    }
                }
            }
            bool solution_found = (best_goal_f != INF);
            int solution_cost = solution_found ? best_goal_f : 0;

            json after_data;
            after_data["ts1_id"] = merge_index1;
            after_data["ts2_id"] = merge_index2;
            after_data["merged_id"] = merged_index;
            after_data["iteration"] = iteration;
            after_data["f_values"] = f_after;
            after_data["num_states"] = (int)f_after.size();
            after_data["num_goal_states"] = num_goals;
            after_data["num_transitions"] = merged_transitions;

            json search_signals;
            search_signals["nodes_expanded"] = nodes_expanded;
            search_signals["search_depth"] = search_depth;
            search_signals["solution_cost"] = solution_cost;
            search_signals["branching_factor"] = branching_factor;
            search_signals["solution_found"] = solution_found;
            after_data["search_signals"] = search_signals;

            // ✅ DIRECT WRITE: No helper function
            std::string after_path = fd_output_dir + "/merge_after_" + std::to_string(iteration) + ".json";
            std::ofstream after_file(after_path, std::ios::out | std::ios::trunc);
            if (!after_file.is_open()) {
                std::cerr << "[M&S] ERROR: Cannot create merge_after file: " << after_path << std::endl;
                throw std::runtime_error("Cannot create merge_after file");
            }
            after_file << after_data.dump(2);
            after_file.close();
            std::cout << "[M&S] ✅ Wrote merge_after_" << iteration << ".json" << std::endl;
        }

        // ====================================================================
        // PHASE 8: EXPORT MERGED TS JSON - SIMPLIFIED INLINE
        // ====================================================================

        {
            const TransitionSystem& ts = fts.get_transition_system(merged_index);

            json ts_json;
            ts_json["iteration"] = iteration;
            ts_json["num_states"] = ts.get_size();
            ts_json["init_state"] = ts.get_init_state();
            ts_json["transformed"] = (shrunk || reduced);

            std::vector<int> goal_states;
            for (int i = 0; i < ts.get_size(); ++i) {
                if (ts.is_goal_state(i)) {
                    goal_states.push_back(i);
                }
            }
            ts_json["goal_states"] = goal_states;
            ts_json["incorporated_variables"] = ts.get_incorporated_variables();

            std::vector<json> transitions;
            for (auto it = ts.begin(); it != ts.end(); ++it) {
                const auto& info = *it;
                const auto& label_group = info.get_label_group();
                const auto& trans_vec = info.get_transitions();

                for (int label : label_group) {
                    for (const auto& trans : trans_vec) {
                        transitions.push_back({
                            {"src", trans.src},
                            {"target", trans.target},
                            {"label", label}
                        });
                    }
                }
            }
            ts_json["transitions"] = transitions;

            std::string ts_path = fd_output_dir + "/ts_" + std::to_string(iteration) + ".json";

            // ✅ DIRECT WRITE: Simple, no helper function
            std::ofstream ts_file(ts_path, std::ios::out | std::ios::trunc);
            if (!ts_file.is_open()) {
                std::cerr << "[M&S] ERROR: Cannot create ts file: " << ts_path << std::endl;
                throw std::runtime_error("Cannot create ts file");
            }
            ts_file << ts_json.dump(2);
            ts_file.close();
            std::cout << "[M&S] ✅ Wrote ts_" << iteration << ".json with " << ts.get_size() << " states" << std::endl;
        }

        // ====================================================================
        // PHASE 9: INCREMENT ITERATION COUNTER
        // ====================================================================

        iteration++;  // ✅ MOVED HERE for clarity

        int abs_size = fts.get_transition_system(merged_index).get_size();
        if (abs_size > maximum_intermediate_size) {
            maximum_intermediate_size = abs_size;
        }

        if (log.is_at_least_normal()) {
            if (log.is_at_least_verbose()) {
                fts.statistics(merged_index, log);
            }
            log_main_loop_progress("after merging");
        }

        if (ran_out_of_time(timer)) {
            break;
        }

        // Pruning
        if (prune_unreachable_states || prune_irrelevant_states) {
            bool pruned = prune_step(
                fts,
                merged_index,
                prune_unreachable_states,
                prune_irrelevant_states,
                log);
            if (log.is_at_least_normal() && pruned) {
                if (log.is_at_least_verbose()) {
                    fts.statistics(merged_index, log);
                }
                log_main_loop_progress("after pruning");
            }
        }

        if (!fts.is_factor_solvable(merged_index)) {
            if (log.is_at_least_normal()) {
                log << "Abstract problem is unsolvable, stopping computation." << endl << endl;
            }
            break;
        }

        if (ran_out_of_time(timer)) {
            break;
        }

        if (log.is_at_least_verbose()) {
            report_peak_memory_delta();
        }
        if (log.is_at_least_normal()) {
            log << endl;
        }
    }

    log << "End of merge-and-shrink algorithm, statistics:" << endl;
    log << "Main loop runtime: " << timer.get_elapsed_time() << endl;
    log << "Maximum intermediate abstraction size: "
        << maximum_intermediate_size << endl;
    shrink_strategy = nullptr;
    label_reduction = nullptr;
}


FactoredTransitionSystem MergeAndShrinkAlgorithm::build_factored_transition_system(
    const TaskProxy &task_proxy) {
    if (starting_peak_memory) {
        cerr << "Calling build_factored_transition_system twice is not "
             << "supported!" << endl;
        utils::exit_with(utils::ExitCode::SEARCH_CRITICAL_ERROR);
    }
    starting_peak_memory = utils::get_peak_memory_in_kb();

    utils::Timer timer;
    log << "Running merge-and-shrink algorithm..." << endl;
    task_properties::verify_no_axioms(task_proxy);
    dump_options();
    warn_on_unusual_options();
    log << endl;

    const bool compute_init_distances =
        shrink_strategy->requires_init_distances() ||
        merge_strategy_factory->requires_init_distances() ||
        prune_unreachable_states;
    const bool compute_goal_distances =
        shrink_strategy->requires_goal_distances() ||
        merge_strategy_factory->requires_goal_distances() ||
        prune_irrelevant_states;
    FactoredTransitionSystem fts =
        create_factored_transition_system(
            task_proxy,
            compute_init_distances,
            compute_goal_distances,
            log);
    if (log.is_at_least_normal()) {
        log_progress(timer, "after computation of atomic factors", log);
    }

    /*
      Prune all atomic factors according to the chosen options. Stop early if
      one factor is unsolvable.

      TODO: think about if we can prune already while creating the atomic FTS.
    */
    bool pruned = false;
    bool unsolvable = false;
    for (int index = 0; index < fts.get_size(); ++index) {
        assert(fts.is_active(index));
        if (prune_unreachable_states || prune_irrelevant_states) {
            bool pruned_factor = prune_step(
                fts,
                index,
                prune_unreachable_states,
                prune_irrelevant_states,
                log);
            pruned = pruned || pruned_factor;
        }
        if (!fts.is_factor_solvable(index)) {
            log << "Atomic FTS is unsolvable, stopping computation." << endl;
            unsolvable = true;
            break;
        }
    }
    if (log.is_at_least_normal()) {
        if (pruned) {
            log_progress(timer, "after pruning atomic factors", log);
        }
        log << endl;
    }

    // ####################################################################################################
    // === Export Atomic Transition Systems ===
    {
        std::string filename = "merged_transition_systems.json";
        json all_ts;

        // Try to load existing merged systems if any
        std::ifstream infile(filename);
        if (infile) {
            infile >> all_ts;
            infile.close();
        }

        for (int i = 0; i < fts.get_size(); ++i) {
            if (!fts.is_active(i)) continue;

            const TransitionSystem& ts = fts.get_transition_system(i);

            json ts_json;
            ts_json["iteration"] = -1;  // Use -1 to mark atomic TS
            ts_json["num_states"] = ts.get_size();
            ts_json["init_state"] = ts.get_init_state();

            std::vector<int> goal_states;
            for (int j = 0; j < ts.get_size(); ++j)
                if (ts.is_goal_state(j))
                    goal_states.push_back(j);
            ts_json["goal_states"] = goal_states;

            ts_json["incorporated_variables"] = ts.get_incorporated_variables();

            std::vector<json> transitions;
            for (auto it = ts.begin(); it != ts.end(); ++it) {
                const auto& info = *it;
                const auto& label_group = info.get_label_group();
                const auto& trans_vec = info.get_transitions();

                for (int label : label_group) {
                    for (const auto& trans : trans_vec) {
                        transitions.push_back({
                            {"src", trans.src},
                            {"target", trans.target},
                            {"label", label}
                            });
                    }
                }
            }
            ts_json["transitions"] = transitions;

            all_ts.push_back(ts_json);
        }

        std::ofstream outfile(filename);
        outfile << all_ts.dump(4);  // Pretty print
        outfile.close();
    }
    // ####################################################################################################

    if (!unsolvable && main_loop_max_time > 0) {
        main_loop(fts, task_proxy);
    }
    const bool final = true;
    report_peak_memory_delta(final);
    log << "Merge-and-shrink algorithm runtime: " << timer << endl;
    log << endl;
    return fts;
}

void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature) {
    // Merge strategy option.
    feature.add_option<shared_ptr<MergeStrategyFactory>>(
        "merge_strategy",
        "See detailed documentation for merge strategies. "
        "We currently recommend SCC-DFP, which can be achieved using "
        "{{{merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector="
        "score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order"
        "]))}}}");

    // Shrink strategy option.
    feature.add_option<shared_ptr<ShrinkStrategy>>(
        "shrink_strategy",
        "See detailed documentation for shrink strategies. "
        "We currently recommend non-greedy shrink_bisimulation, which can be "
        "achieved using {{{shrink_strategy=shrink_bisimulation(greedy=false)}}}");

    // Label reduction option.
    feature.add_option<shared_ptr<LabelReduction>>(
        "label_reduction",
        "See detailed documentation for labels. There is currently only "
        "one 'option' to use label_reduction, which is {{{label_reduction=exact}}} "
        "Also note the interaction with shrink strategies.",
        plugins::ArgumentInfo::NO_DEFAULT);

    // Pruning options.
    feature.add_option<bool>(
        "prune_unreachable_states",
        "If true, prune abstract states unreachable from the initial state.",
        "true");
    feature.add_option<bool>(
        "prune_irrelevant_states",
        "If true, prune abstract states from which no goal state can be "
        "reached.",
        "true");

    add_transition_system_size_limit_options_to_feature(feature);

    feature.add_option<double>(
        "main_loop_max_time",
        "A limit in seconds on the runtime of the main loop of the algorithm. "
        "If the limit is exceeded, the algorithm terminates, potentially "
        "returning a factored transition system with several factors. Also "
        "note that the time limit is only checked between transformations "
        "of the main loop, but not during, so it can be exceeded if a "
        "transformation is runtime-intense.",
        "infinity",
        Bounds("0.0", "infinity"));
}

tuple<shared_ptr<MergeStrategyFactory>, shared_ptr<ShrinkStrategy>,
      shared_ptr<LabelReduction>, bool, bool, int, int, int, double>
get_merge_and_shrink_algorithm_arguments_from_options(
    const plugins::Options &opts) {
    return tuple_cat(
        make_tuple(
            opts.get<shared_ptr<MergeStrategyFactory>>("merge_strategy"),
            opts.get<shared_ptr<ShrinkStrategy>>("shrink_strategy"),
            opts.get<shared_ptr<LabelReduction>>(
                "label_reduction", nullptr),
            opts.get<bool>("prune_unreachable_states"),
            opts.get<bool>("prune_irrelevant_states")),
        get_transition_system_size_limit_arguments_from_options(opts),
        make_tuple(opts.get<double>("main_loop_max_time"))
        );
}

void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature) {
    feature.add_option<int>(
        "max_states",
        "maximum transition system size allowed at any time point.",
        "-1",
        Bounds("-1", "infinity"));
    feature.add_option<int>(
        "max_states_before_merge",
        "maximum transition system size allowed for two transition systems "
        "before being merged to form the synchronized product.",
        "-1",
        Bounds("-1", "infinity"));
    feature.add_option<int>(
        "threshold_before_merge",
        "If a transition system, before being merged, surpasses this soft "
        "transition system size limit, the shrink strategy is called to "
        "possibly shrink the transition system.",
        "-1",
        Bounds("-1", "infinity"));
}

tuple<int, int, int>
get_transition_system_size_limit_arguments_from_options(
    const plugins::Options &opts) {
    return make_tuple(
        opts.get<int>("max_states"),
        opts.get<int>("max_states_before_merge"),
        opts.get<int>("threshold_before_merge")
        );
}
}

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.h code is this:
//#ifndef MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H
//#define MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H
//
//#include <memory>
//#include <vector>
//
//// Include these BEFORE opening namespace to avoid pollution
//#include "types.h"           // Provides INF, PRUNED_STATE
//#include "../utils/logging.h"
//
//// Forward declarations
//class TaskProxy;
//
//namespace merge_and_shrink {
//// Now types.h content is available and INF is visible
//
//namespace merge_and_shrink {
//
//// Forward declarations for external types
//namespace utils {
//class LogProxy;
//class CountdownTimer;
//enum class Verbosity;
//}  // namespace utils
//
//
//namespace plugins {
//class ConstructContext;
//class Feature;
//class Options;
//}
//
//namespace utils {
//class CountdownTimer;
//}
//
//namespace merge_and_shrink {
//class FactoredTransitionSystem;
//class LabelReduction;
//class MergeStrategyFactory;
//class ShrinkStrategy;
//
//class MergeAndShrinkAlgorithm {
//
//    // TODO: when the option parser supports it, the following should become
//    // unique pointers.
//    std::shared_ptr<MergeStrategyFactory> merge_strategy_factory;
//    std::shared_ptr<ShrinkStrategy> shrink_strategy;
//    std::shared_ptr<LabelReduction> label_reduction;
//
//    // Options for shrinking
//    // Hard limit: the maximum size of a transition system at any point.
//    int max_states;
//    // Hard limit: the maximum size of a transition system before being merged.
//    int max_states_before_merge;
//    /* A soft limit for triggering shrinking even if the hard limits
//       max_states and max_states_before_merge are not violated. */
//    int shrink_threshold_before_merge;
//
//    // Options for pruning
//    const bool prune_unreachable_states;
//    const bool prune_irrelevant_states;
//
//    mutable utils::LogProxy log;
//    const double main_loop_max_time;
//
//    long starting_peak_memory;
//
//    void report_peak_memory_delta(bool final = false) const;
//    void dump_options() const;
//    void warn_on_unusual_options() const;
//    bool ran_out_of_time(const utils::CountdownTimer &timer) const;
//    void statistics(int maximum_intermediate_size) const;
//    void main_loop(
//        FactoredTransitionSystem &fts,
//        const TaskProxy &task_proxy);
//    void handle_shrink_limit_defaults();
//public:
//    MergeAndShrinkAlgorithm(
//        const std::shared_ptr<MergeStrategyFactory> &merge_strategy,
//        const std::shared_ptr<ShrinkStrategy> &shrink_strategy,
//        const std::shared_ptr<LabelReduction> &label_reduction,
//        bool prune_unreachable_states, bool prune_irrelevant_states,
//        int max_states, int max_states_before_merge,
//        int threshold_before_merge, double main_loop_max_time,
//        utils::Verbosity verbosity);
//    FactoredTransitionSystem build_factored_transition_system(const TaskProxy &task_proxy);
//};
//
//extern void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature);
//std::tuple<std::shared_ptr<MergeStrategyFactory>,
//           std::shared_ptr<ShrinkStrategy>,
//           std::shared_ptr<LabelReduction>, bool, bool, int, int, int,
//           double>
//get_merge_and_shrink_algorithm_arguments_from_options(
//    const plugins::Options &opts);
//extern void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature);
//std::tuple<int, int, int>
//get_transition_system_size_limit_arguments_from_options(
//    const plugins::Options &opts);
//}
//
//#endif

#ifndef MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H
#define MERGE_AND_SHRINK_MERGE_AND_SHRINK_ALGORITHM_H

#include "../utils/logging.h"

#include <memory>

class TaskProxy;

namespace plugins {
class ConstructContext;
class Feature;
class Options;
}

namespace utils {
class CountdownTimer;
}

namespace merge_and_shrink {
class FactoredTransitionSystem;
class LabelReduction;
class MergeStrategyFactory;
class ShrinkStrategy;

class MergeAndShrinkAlgorithm {
    // TODO: when the option parser supports it, the following should become
    // unique pointers.
    std::shared_ptr<MergeStrategyFactory> merge_strategy_factory;
    std::shared_ptr<ShrinkStrategy> shrink_strategy;
    std::shared_ptr<LabelReduction> label_reduction;

    // Options for shrinking
    // Hard limit: the maximum size of a transition system at any point.
    int max_states;
    // Hard limit: the maximum size of a transition system before being merged.
    int max_states_before_merge;
    /* A soft limit for triggering shrinking even if the hard limits
       max_states and max_states_before_merge are not violated. */
    int shrink_threshold_before_merge;

    // Options for pruning
    const bool prune_unreachable_states;
    const bool prune_irrelevant_states;

    mutable utils::LogProxy log;
    const double main_loop_max_time;

    long starting_peak_memory;

    void report_peak_memory_delta(bool final = false) const;
    void dump_options() const;
    void warn_on_unusual_options() const;
    bool ran_out_of_time(const utils::CountdownTimer &timer) const;
    void statistics(int maximum_intermediate_size) const;
    void main_loop(
        FactoredTransitionSystem &fts,
        const TaskProxy &task_proxy);
    void handle_shrink_limit_defaults();
public:
    MergeAndShrinkAlgorithm(
        const std::shared_ptr<MergeStrategyFactory> &merge_strategy,
        const std::shared_ptr<ShrinkStrategy> &shrink_strategy,
        const std::shared_ptr<LabelReduction> &label_reduction,
        bool prune_unreachable_states, bool prune_irrelevant_states,
        int max_states, int max_states_before_merge,
        int threshold_before_merge, double main_loop_max_time,
        utils::Verbosity verbosity);
    FactoredTransitionSystem build_factored_transition_system(const TaskProxy &task_proxy);
};

extern void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature);
std::tuple<std::shared_ptr<MergeStrategyFactory>,
           std::shared_ptr<ShrinkStrategy>,
           std::shared_ptr<LabelReduction>, bool, bool, int, int, int,
           double>
get_merge_and_shrink_algorithm_arguments_from_options(
    const plugins::Options &opts);
extern void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature);
std::tuple<int, int, int>
get_transition_system_size_limit_arguments_from_options(
    const plugins::Options &opts);
}

#endif


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_heuristic.cc code is this:
#include "merge_and_shrink_heuristic.h"

#include "distances.h"
#include "factored_transition_system.h"
#include "merge_and_shrink_algorithm.h"
#include "merge_and_shrink_representation.h"
#include "transition_system.h"
#include "types.h"

#include "../plugins/plugin.h"
#include "../task_utils/task_properties.h"
#include "../utils/markup.h"
#include "../utils/system.h"

#include <cassert>
#include <iostream>
#include <utility>

using namespace std;
using utils::ExitCode;

namespace merge_and_shrink {
MergeAndShrinkHeuristic::MergeAndShrinkHeuristic(
    const shared_ptr<MergeStrategyFactory> &merge_strategy,
    const shared_ptr<ShrinkStrategy> &shrink_strategy,
    const shared_ptr<LabelReduction> &label_reduction,
    bool prune_unreachable_states, bool prune_irrelevant_states,
    int max_states, int max_states_before_merge,
    int threshold_before_merge, double main_loop_max_time,
    const shared_ptr<AbstractTask> &transform, bool cache_estimates,
    const string &description, utils::Verbosity verbosity)
    : Heuristic(transform, cache_estimates, description, verbosity) {
    log << "Initializing merge-and-shrink heuristic..." << endl;
    MergeAndShrinkAlgorithm algorithm(
        merge_strategy, shrink_strategy, label_reduction,
        prune_unreachable_states, prune_irrelevant_states,
        max_states, max_states_before_merge, threshold_before_merge,
        main_loop_max_time, verbosity);
    FactoredTransitionSystem fts = algorithm.build_factored_transition_system(task_proxy);
    extract_factors(fts);
    log << "Done initializing merge-and-shrink heuristic." << endl << endl;
}

void MergeAndShrinkHeuristic::extract_factor(
    FactoredTransitionSystem &fts, int index) {
    /*
      Extract the factor at the given index from the given factored transition
      system, compute goal distances if necessary and store the M&S
      representation, which serves as the heuristic.
    */
    auto final_entry = fts.extract_factor(index);
    unique_ptr<MergeAndShrinkRepresentation> mas_representation = move(final_entry.first);
    unique_ptr<Distances> distances = move(final_entry.second);
    if (!distances->are_goal_distances_computed()) {
        const bool compute_init = false;
        const bool compute_goal = true;
        distances->compute_distances(compute_init, compute_goal, log);
    }
    assert(distances->are_goal_distances_computed());
    mas_representation->set_distances(*distances);
    mas_representations.push_back(move(mas_representation));
}

bool MergeAndShrinkHeuristic::extract_unsolvable_factor(FactoredTransitionSystem &fts) {
    /* Check if there is an unsolvable factor. If so, extract and store it and
       return true. Otherwise, return false. */
    for (int index : fts) {
        if (!fts.is_factor_solvable(index)) {
            mas_representations.reserve(1);
            extract_factor(fts, index);
            if (log.is_at_least_normal()) {
                log << fts.get_transition_system(index).tag()
                    << "use this unsolvable factor as heuristic."
                    << endl;
            }
            return true;
        }
    }
    return false;
}

void MergeAndShrinkHeuristic::extract_nontrivial_factors(FactoredTransitionSystem &fts) {
    // Iterate over remaining factors and extract and store the nontrivial ones.
    for (int index : fts) {
        if (fts.is_factor_trivial(index)) {
            if (log.is_at_least_verbose()) {
                log << fts.get_transition_system(index).tag()
                    << "is trivial." << endl;
            }
        } else {
            extract_factor(fts, index);
        }
    }
}

void MergeAndShrinkHeuristic::extract_factors(FactoredTransitionSystem &fts) {
    /*
      TODO: This method has quite a bit of fiddling with aspects of
      transition systems and the merge-and-shrink representation (checking
      whether distances have been computed; computing them) that we would
      like to have at a lower level. See also the TODO in
      factored_transition_system.h on improving the interface of that class
      (and also related classes like TransitionSystem etc).
    */
    assert(mas_representations.empty());

    int num_active_factors = fts.get_num_active_entries();
    if (log.is_at_least_normal()) {
        log << "Number of remaining factors: " << num_active_factors << endl;
    }

    bool unsolvalbe = extract_unsolvable_factor(fts);
    if (!unsolvalbe) {
        extract_nontrivial_factors(fts);
    }

    int num_factors_kept = mas_representations.size();
    if (log.is_at_least_normal()) {
        log << "Number of factors kept: " << num_factors_kept << endl;
    }
}

int MergeAndShrinkHeuristic::compute_heuristic(const State &ancestor_state) {
    State state = convert_ancestor_state(ancestor_state);
    int heuristic = 0;
    for (const unique_ptr<MergeAndShrinkRepresentation> &mas_representation : mas_representations) {
        int cost = mas_representation->get_value(state);
        if (cost == PRUNED_STATE || cost == INF) {
            // If state is unreachable or irrelevant, we encountered a dead end.
            return DEAD_END;
        }
        heuristic = max(heuristic, cost);
    }
    return heuristic;
}

class MergeAndShrinkHeuristicFeature
    : public plugins::TypedFeature<Evaluator, MergeAndShrinkHeuristic> {
public:
    MergeAndShrinkHeuristicFeature() : TypedFeature("merge_and_shrink") {
        document_title("Merge-and-shrink heuristic");
        document_synopsis(
            "This heuristic implements the algorithm described in the following "
            "paper:" + utils::format_conference_reference(
                {"Silvan Sievers", "Martin Wehrle", "Malte Helmert"},
                "Generalized Label Reduction for Merge-and-Shrink Heuristics",
                "https://ai.dmi.unibas.ch/papers/sievers-et-al-aaai2014.pdf",
                "Proceedings of the 28th AAAI Conference on Artificial"
                " Intelligence (AAAI 2014)",
                "2358-2366",
                "AAAI Press",
                "2014") + "\n" +
            "For a more exhaustive description of merge-and-shrink, see the journal "
            "paper" + utils::format_journal_reference(
                {"Silvan Sievers", "Malte Helmert"},
                "Merge-and-Shrink: A Compositional Theory of Transformations "
                "of Factored Transition Systems",
                "https://ai.dmi.unibas.ch/papers/sievers-helmert-jair2021.pdf",
                "Journal of Artificial Intelligence Research",
                "71",
                "781-883",
                "2021") + "\n" +
            "The following paper describes how to improve the DFP merge strategy "
            "with tie-breaking, and presents two new merge strategies (dyn-MIASM "
            "and SCC-DFP):" + utils::format_conference_reference(
                {"Silvan Sievers", "Martin Wehrle", "Malte Helmert"},
                "An Analysis of Merge Strategies for Merge-and-Shrink Heuristics",
                "https://ai.dmi.unibas.ch/papers/sievers-et-al-icaps2016.pdf",
                "Proceedings of the 26th International Conference on Automated "
                "Planning and Scheduling (ICAPS 2016)",
                "294-298",
                "AAAI Press",
                "2016") + "\n" +
            "Details of the algorithms and the implementation are described in the "
            "paper" + utils::format_conference_reference(
                {"Silvan Sievers"},
                "Merge-and-Shrink Heuristics for Classical Planning: Efficient "
                "Implementation and Partial Abstractions",
                "https://ai.dmi.unibas.ch/papers/sievers-socs2018.pdf",
                "Proceedings of the 11th Annual Symposium on Combinatorial Search "
                "(SoCS 2018)",
                "90-98",
                "AAAI Press",
                "2018")
            );

        add_merge_and_shrink_algorithm_options_to_feature(*this);
        add_heuristic_options_to_feature(*this, "merge_and_shrink");

        document_note(
            "Note",
            "Conditional effects are supported directly. Note, however, that "
            "for tasks that are not factored (in the sense of the JACM 2014 "
            "merge-and-shrink paper), the atomic transition systems on which "
            "merge-and-shrink heuristics are based are nondeterministic, "
            "which can lead to poor heuristics even when only perfect shrinking "
            "is performed.");
        document_note(
            "Note",
            "When pruning unreachable states, admissibility and consistency is "
            "only guaranteed for reachable states and transitions between "
            "reachable states. While this does not impact regular A* search which "
            "will never encounter any unreachable state, it impacts techniques "
            "like symmetry-based pruning: a reachable state which is mapped to an "
            "unreachable symmetric state (which hence is pruned) would falsely be "
            "considered a dead-end and also be pruned, thus violating optimality "
            "of the search.");
        document_note(
            "Note",
            "When using a time limit on the main loop of the merge-and-shrink "
            "algorithm, the heuristic will compute the maximum over all heuristics "
            "induced by the remaining factors if terminating the merge-and-shrink "
            "algorithm early. Exception: if there is an unsolvable factor, it will "
            "be used as the exclusive heuristic since the problem is unsolvable.");
        document_note(
            "Note",
            "A currently recommended good configuration uses bisimulation "
            "based shrinking, the merge strategy SCC-DFP, and the appropriate "
            "label reduction setting (max_states has been altered to be between "
            "10k and 200k in the literature). As merge-and-shrink heuristics "
            "can be expensive to compute, we also recommend limiting time by "
            "setting {{{main_loop_max_time}}} to a finite value. A sensible "
            "value would be half of the time allocated for the planner.\n"
            "{{{\nmerge_and_shrink(shrink_strategy=shrink_bisimulation(greedy=false),"
            "merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector="
            "score_based_filtering(scoring_functions=[goal_relevance(),dfp(),"
            "total_order()])),label_reduction=exact(before_shrinking=true,"
            "before_merging=false),max_states=50k,threshold_before_merge=1)\n}}}\n");

        document_language_support("action costs", "supported");
        document_language_support("conditional effects", "supported (but see note)");
        document_language_support("axioms", "not supported");

        document_property("admissible", "yes (but see note)");
        document_property("consistent", "yes (but see note)");
        document_property("safe", "yes");
        document_property("preferred operators", "no");
    }

    virtual shared_ptr<MergeAndShrinkHeuristic>
    create_component(const plugins::Options &opts) const override {
        return plugins::make_shared_from_arg_tuples<MergeAndShrinkHeuristic>(
            get_merge_and_shrink_algorithm_arguments_from_options(opts),
            get_heuristic_arguments_from_options(opts)
            );
    }
};

static plugins::FeaturePlugin<MergeAndShrinkHeuristicFeature> _plugin;
}


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_heuristic.h code is this:
#ifndef MERGE_AND_SHRINK_MERGE_AND_SHRINK_HEURISTIC_H
#define MERGE_AND_SHRINK_MERGE_AND_SHRINK_HEURISTIC_H

#include "../heuristic.h"

#include <memory>

namespace merge_and_shrink {
class FactoredTransitionSystem;
class MergeAndShrinkRepresentation;

class MergeStrategyFactory;
class ShrinkStrategy;
class LabelReduction;

class MergeAndShrinkHeuristic : public Heuristic {
    // The final merge-and-shrink representations, storing goal distances.
    std::vector<std::unique_ptr<MergeAndShrinkRepresentation>> mas_representations;

    void extract_factor(FactoredTransitionSystem &fts, int index);
    bool extract_unsolvable_factor(FactoredTransitionSystem &fts);
    void extract_nontrivial_factors(FactoredTransitionSystem &fts);
    void extract_factors(FactoredTransitionSystem &fts);
protected:
    virtual int compute_heuristic(const State &ancestor_state) override;
public:
    MergeAndShrinkHeuristic(
        const std::shared_ptr<MergeStrategyFactory> &merge_strategy,
        const std::shared_ptr<ShrinkStrategy> &shrink_strategy,
        const std::shared_ptr<LabelReduction> &label_reduction,
        bool prune_unreachable_states, bool prune_irrelevant_states,
        int max_states, int max_states_before_merge,
        int threshold_before_merge, double main_loop_max_time,
        const std::shared_ptr<AbstractTask> &transform,
        bool cache_estimates, const std::string &description,
        utils::Verbosity verbosity);
};
}

#endif


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_representation.cc code is this:
#include "merge_and_shrink_representation.h"

#include "distances.h"
#include "types.h"

#include "../task_proxy.h"

#include "../utils/logging.h"

#include <algorithm>
#include <cassert>
#include <iostream>
#include <numeric>

using namespace std;

namespace merge_and_shrink {
MergeAndShrinkRepresentation::MergeAndShrinkRepresentation(int domain_size)
    : domain_size(domain_size) {
}

MergeAndShrinkRepresentation::~MergeAndShrinkRepresentation() {
}

int MergeAndShrinkRepresentation::get_domain_size() const {
    return domain_size;
}


MergeAndShrinkRepresentationLeaf::MergeAndShrinkRepresentationLeaf(
    int var_id, int domain_size)
    : MergeAndShrinkRepresentation(domain_size),
      var_id(var_id),
      lookup_table(domain_size) {
    iota(lookup_table.begin(), lookup_table.end(), 0);
}

void MergeAndShrinkRepresentationLeaf::set_distances(
    const Distances &distances) {
    assert(distances.are_goal_distances_computed());
    for (int &entry : lookup_table) {
        if (entry != PRUNED_STATE) {
            entry = distances.get_goal_distance(entry);
        }
    }
}

void MergeAndShrinkRepresentationLeaf::apply_abstraction_to_lookup_table(
    const vector<int> &abstraction_mapping) {
    int new_domain_size = 0;
    for (int &entry : lookup_table) {
        if (entry != PRUNED_STATE) {
            entry = abstraction_mapping[entry];
            new_domain_size = max(new_domain_size, entry + 1);
        }
    }
    domain_size = new_domain_size;
}

int MergeAndShrinkRepresentationLeaf::get_value(const State &state) const {
    int value = state[var_id].get_value();
    return lookup_table[value];
}

bool MergeAndShrinkRepresentationLeaf::is_total() const {
    for (int entry : lookup_table) {
        if (entry == PRUNED_STATE) {
            return false;
        }
    }
    return true;
}

void MergeAndShrinkRepresentationLeaf::dump(utils::LogProxy &log) const {
    if (log.is_at_least_debug()) {
        log << "lookup table (leaf): ";
        for (const auto &value : lookup_table) {
            log << value << ", ";
        }
        log << endl;
    }
}


MergeAndShrinkRepresentationMerge::MergeAndShrinkRepresentationMerge(
    unique_ptr<MergeAndShrinkRepresentation> left_child_,
    unique_ptr<MergeAndShrinkRepresentation> right_child_)
    : MergeAndShrinkRepresentation(left_child_->get_domain_size() *
                                   right_child_->get_domain_size()),
      left_child(move(left_child_)),
      right_child(move(right_child_)),
      lookup_table(left_child->get_domain_size(),
                   vector<int>(right_child->get_domain_size())) {
    int counter = 0;
    for (vector<int> &row : lookup_table) {
        for (int &entry : row) {
            entry = counter;
            ++counter;
        }
    }
}

void MergeAndShrinkRepresentationMerge::set_distances(
    const Distances &distances) {
    assert(distances.are_goal_distances_computed());
    for (vector<int> &row : lookup_table) {
        for (int &entry : row) {
            if (entry != PRUNED_STATE) {
                entry = distances.get_goal_distance(entry);
            }
        }
    }
}

void MergeAndShrinkRepresentationMerge::apply_abstraction_to_lookup_table(
    const vector<int> &abstraction_mapping) {
    int new_domain_size = 0;
    for (vector<int> &row : lookup_table) {
        for (int &entry : row) {
            if (entry != PRUNED_STATE) {
                entry = abstraction_mapping[entry];
                new_domain_size = max(new_domain_size, entry + 1);
            }
        }
    }
    domain_size = new_domain_size;
}

int MergeAndShrinkRepresentationMerge::get_value(
    const State &state) const {
    int state1 = left_child->get_value(state);
    int state2 = right_child->get_value(state);
    if (state1 == PRUNED_STATE || state2 == PRUNED_STATE)
        return PRUNED_STATE;
    return lookup_table[state1][state2];
}

bool MergeAndShrinkRepresentationMerge::is_total() const {
    for (const vector<int> &row : lookup_table) {
        for (int entry : row) {
            if (entry == PRUNED_STATE) {
                return false;
            }
        }
    }
    return left_child->is_total() && right_child->is_total();
}

void MergeAndShrinkRepresentationMerge::dump(utils::LogProxy &log) const {
    if (log.is_at_least_debug()) {
        log << "lookup table (merge): " << endl;
        for (const auto &row : lookup_table) {
            for (const auto &value : row) {
                log << value << ", ";
            }
            log << endl;
        }
        log << "left child:" << endl;
        left_child->dump(log);
        log << "right child:" << endl;
        right_child->dump(log);
    }
}
}


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_representation.h code is this:
#ifndef MERGE_AND_SHRINK_MERGE_AND_SHRINK_REPRESENTATION_H
#define MERGE_AND_SHRINK_MERGE_AND_SHRINK_REPRESENTATION_H

#include <memory>
#include <vector>

class State;

namespace utils {
class LogProxy;
}

namespace merge_and_shrink {
class Distances;
class MergeAndShrinkRepresentation {
protected:
    int domain_size;

public:
    explicit MergeAndShrinkRepresentation(int domain_size);
    virtual ~MergeAndShrinkRepresentation() = 0;

    int get_domain_size() const;

    // Store distances instead of abstract state numbers.
    virtual void set_distances(const Distances &) = 0;
    virtual void apply_abstraction_to_lookup_table(
        const std::vector<int> &abstraction_mapping) = 0;
    /*
      Return the value that state is mapped to. This is either an abstract
      state (if set_distances has not been called) or a distance (if it has).
      If the represented function is not total, the returned value is DEAD_END
      if the abstract state is PRUNED_STATE or if the (distance) value is INF.
    */
    virtual int get_value(const State &state) const = 0;
    /* Return true iff the represented function is total, i.e., does not map
       to PRUNED_STATE. */
    virtual bool is_total() const = 0;
    virtual void dump(utils::LogProxy &log) const = 0;
};


class MergeAndShrinkRepresentationLeaf : public MergeAndShrinkRepresentation {
    const int var_id;

    std::vector<int> lookup_table;
public:
    MergeAndShrinkRepresentationLeaf(int var_id, int domain_size);
    virtual ~MergeAndShrinkRepresentationLeaf() = default;

    virtual void set_distances(const Distances &) override;
    virtual void apply_abstraction_to_lookup_table(
        const std::vector<int> &abstraction_mapping) override;
    virtual int get_value(const State &state) const override;
    virtual bool is_total() const override;
    virtual void dump(utils::LogProxy &log) const override;
};


class MergeAndShrinkRepresentationMerge : public MergeAndShrinkRepresentation {
    std::unique_ptr<MergeAndShrinkRepresentation> left_child;
    std::unique_ptr<MergeAndShrinkRepresentation> right_child;
    std::vector<std::vector<int>> lookup_table;
public:
    MergeAndShrinkRepresentationMerge(
        std::unique_ptr<MergeAndShrinkRepresentation> left_child,
        std::unique_ptr<MergeAndShrinkRepresentation> right_child);
    virtual ~MergeAndShrinkRepresentationMerge() = default;

    virtual void set_distances(const Distances &distances) override;
    virtual void apply_abstraction_to_lookup_table(
        const std::vector<int> &abstraction_mapping) override;
    virtual int get_value(const State &state) const override;
    virtual bool is_total() const override;
    virtual void dump(utils::LogProxy &log) const override;
};
}

#endif


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy.cc code is this:
#include "merge_strategy.h"

using namespace std;

namespace merge_and_shrink {
MergeStrategy::MergeStrategy(
    const FactoredTransitionSystem &fts)
    : fts(fts) {
}
}


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy.h code is this:
#ifndef MERGE_AND_SHRINK_MERGE_STRATEGY_H
#define MERGE_AND_SHRINK_MERGE_STRATEGY_H

#include <utility>

namespace merge_and_shrink {
class FactoredTransitionSystem;

/*
  A merge strategy dictates the order in which transition systems of the
  factored transition system maintained by the merge-and-shrink heuristic
  should be merged.

  We distinguish three types of merge strategies: a stateless type, a
  precomputed type, and a third unspecified type, which does not fit
  either category.

  Stateless merge strategies: they do not store any information and determine
  the next merge soley based on the current factored transition system.

  Precomputed merge strategies: they are represented in the form of a merge
  tree (see class MergeTree). They return the next merge based on that
  precomputed tree. This requires that the actual merge performed always
  matches the one dictated by the precomputed strategy, i.e. it is mandatory
  that the merge tree maintained by the precomputed strategy remains
  synchronized with the current factored transition system.

  Special merge strategies: these do not fit either of the other categories
  and are usually a combination of existing stateless and/or precomputed merge
  strategies. For example, the SCCs merge strategy (Sievers et al, ICAPS 2016)
  needs to know which of the SCCs have been merged. While merging all variables
  within an SCC, it makes use of a stateless merge strategy or a merge tree,
  until that SCC has been entirely processed. There is currently no such merge
  strategy in the Fast Downward repository.

  NOTE: While stateless merge strategies have full control over the merge
  order, this is not true for the specific implementation of merge tree,
  because we always perform the next "left-most" merge in the merge tree.
  See also the documentation in merge_tree.h.
*/
class MergeStrategy {
protected:
    const FactoredTransitionSystem &fts;
public:
    explicit MergeStrategy(const FactoredTransitionSystem &fts);
    virtual ~MergeStrategy() = default;
    virtual std::pair<int, int> get_next() = 0;
};
}

#endif


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_factory.cc code is this:
#include "merge_strategy_factory.h"

#include "../plugins/plugin.h"

#include <iostream>

using namespace std;

namespace merge_and_shrink {
MergeStrategyFactory::MergeStrategyFactory(utils::Verbosity verbosity)
    : log(utils::get_log_for_verbosity(verbosity)) {
}

void MergeStrategyFactory::dump_options() const {
    if (log.is_at_least_normal()) {
        log << "Merge strategy options:" << endl;
        log << "Type: " << name() << endl;
        dump_strategy_specific_options();
    }
}

void add_merge_strategy_options_to_feature(plugins::Feature &feature) {
    utils::add_log_options_to_feature(feature);
}

tuple<utils::Verbosity> get_merge_strategy_arguments_from_options(
    const plugins::Options &opts) {
    return utils::get_log_arguments_from_options(opts);
}


static class MergeStrategyFactoryCategoryPlugin : public plugins::TypedCategoryPlugin<MergeStrategyFactory> {
public:
    MergeStrategyFactoryCategoryPlugin() : TypedCategoryPlugin("MergeStrategy") {
        document_synopsis(
            "This page describes the various merge strategies supported "
            "by the planner.");
    }
}
_category_plugin;
}


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_factory.h code is this:
#ifndef MERGE_AND_SHRINK_MERGE_STRATEGY_FACTORY_H
#define MERGE_AND_SHRINK_MERGE_STRATEGY_FACTORY_H

#include "../utils/logging.h"

#include <memory>
#include <string>

class TaskProxy;

namespace plugins {
class Options;
class Feature;
}

namespace merge_and_shrink {
class FactoredTransitionSystem;
class MergeStrategy;

class MergeStrategyFactory {
protected:
    mutable utils::LogProxy log;

    virtual std::string name() const = 0;
    virtual void dump_strategy_specific_options() const = 0;
public:
    MergeStrategyFactory(utils::Verbosity verbosity);
    virtual ~MergeStrategyFactory() = default;
    void dump_options() const;
    virtual std::unique_ptr<MergeStrategy> compute_merge_strategy(
        const TaskProxy &task_proxy,
        const FactoredTransitionSystem &fts) = 0;
    virtual bool requires_init_distances() const = 0;
    virtual bool requires_goal_distances() const = 0;
};

extern void add_merge_strategy_options_to_feature(plugins::Feature &feature);
extern std::tuple<utils::Verbosity>
get_merge_strategy_arguments_from_options(
    const plugins::Options &opts);
}

#endif


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_gnn.cc code is this:
// FILE: downward/src/search/merge_and_shrink/merge_strategy_gnn.cc

#include "merge_strategy_gnn.h"
#include "../utils/system.h"

#include <nlohmann/json.hpp>
#include <fstream>
#include <thread>
#include <chrono>
#include <iostream>
#include <filesystem>
#include <stdexcept>
#include <cstdlib>

using namespace std;
using json = nlohmann::json;
using namespace merge_and_shrink;

MergeStrategyGNN::MergeStrategyGNN(const FactoredTransitionSystem& fts)
    : MergeStrategy(fts),
      iteration(0) {
    cout << "\n[GNN::INIT] MergeStrategyGNN initialized\n" << endl;
}

// ============================================================================
// ✅ COMPLETELY REFACTORED: Robust cross-platform path resolution
// ============================================================================
std::filesystem::path get_project_root() {
    std::filesystem::path cwd = std::filesystem::current_path();
    std::cout << "[GNN::PATH] Current working directory: " << cwd.string() << std::endl;

    // If we're already in downward/, go up one level
    if (cwd.filename() == "downward") {
        cwd = cwd.parent_path();
        std::cout << "[GNN::PATH] Detected cwd is downward/, adjusting to parent" << std::endl;
    }

    // Search upward, starting from cwd
    std::filesystem::path search_path = cwd;

    for (int i = 0; i < 15; ++i) {
        std::filesystem::path downward_subdir = search_path / "downward";

        if (std::filesystem::is_directory(downward_subdir)) {
            std::filesystem::path gnn_output_dir = downward_subdir / "gnn_output";
            std::filesystem::path fd_output_dir = downward_subdir / "fd_output";

            if (std::filesystem::is_directory(gnn_output_dir) &&
                std::filesystem::is_directory(fd_output_dir)) {
                std::cout << "[GNN::PATH] ✅ Found project root: " << search_path.string() << std::endl;
                return search_path.string();
            }
        }

        std::filesystem::path parent = search_path.parent_path();
        if (parent == search_path) break;
        search_path = parent;
    }

    std::cout << "[GNN::PATH] ⚠️ Could not find project root, defaulting to cwd" << std::endl;
    return cwd.string();
}

std::pair<int, int> MergeStrategyGNN::get_next() {
    int idx = iteration;

    // ✅ FIX: Use std::filesystem for ALL path operations (cross-platform!)
    std::filesystem::path project_root_str = get_project_root();
    std::filesystem::path project_root = std::filesystem::absolute(project_root_str);

    std::filesystem::path gnn_input_dir = project_root / "downward" / "gnn_output";
    std::filesystem::path fd_output_dir = project_root / "downward" / "fd_output";

    std::filesystem::path in_path = gnn_input_dir / ("merge_" + std::to_string(idx) + ".json");
    std::filesystem::path ack_path = fd_output_dir / ("gnn_ack_" + std::to_string(idx) + ".json");

    std::cout << "\n[GNN::ITERATION " << idx << "] ========================================" << std::endl;
    std::cout << "[GNN::ITERATION " << idx << "] Waiting for GNN merge decision..." << std::endl;
    std::cout << "[GNN::ITERATION " << idx << "] Input file:  " << in_path.string() << std::endl;
    std::cout << "[GNN::ITERATION " << idx << "] ACK file:    " << ack_path.string() << std::endl;

    // ✅ WAIT FOR INPUT FILE with timeout
    const int sleep_ms = 50;  // 50ms polling (fast feedback)
    const int max_wait_ms = 600000;  // 10 minutes timeout
    int elapsed_ms = 0;

    while (!std::filesystem::exists(in_path)) {
        // Log progress every 10 seconds
        if (elapsed_ms > 0 && elapsed_ms % 10000 == 0) {
            std::cout << "[GNN::ITERATION " << idx << "] ... waiting (" << elapsed_ms / 1000 << "s) ..."
                      << std::endl;
        }

        if (elapsed_ms > max_wait_ms) {
            std::cerr << "\n[GNN::ERROR] ❌ TIMEOUT: No merge decision received!" << std::endl;
            std::cerr << "[GNN::ERROR] Expected file: " << in_path.string() << std::endl;
            std::cerr << "[GNN::ERROR] Timeout: " << max_wait_ms / 1000 << " seconds" << std::endl;

            // Diagnostic logging
            std::cout << "[GNN::DIAG] Files in " << gnn_input_dir.string() << ":" << std::endl;
            try {
                if (std::filesystem::exists(gnn_input_dir)) {
                    for (const auto& entry : std::filesystem::directory_iterator(gnn_input_dir)) {
                        std::cout << "[GNN::DIAG]   - " << entry.path().filename().string() << std::endl;
                    }
                } else {
                    std::cout << "[GNN::DIAG]   (directory doesn't exist)" << std::endl;
                }
            } catch (const std::exception& e) {
                std::cout << "[GNN::DIAG] Error listing directory: " << e.what() << std::endl;
            }

            throw std::runtime_error("GNN merge decision timeout: " + in_path.string());
        }

        std::this_thread::sleep_for(std::chrono::milliseconds(sleep_ms));
        elapsed_ms += sleep_ms;
    }

    std::cout << "[GNN::ITERATION " << idx << "] ✅ Merge decision file found!" << std::endl;

    // ✅ READ MERGE DECISION WITH VALIDATION
    json merge_decision;
    try {
        std::ifstream fin(in_path);
        if (!fin.is_open()) {
            throw std::runtime_error("Cannot open merge decision file: " + in_path.string());
        }

        fin >> merge_decision;
        fin.close();

        // Validate JSON structure
        if (!merge_decision.contains("merge_pair")) {
            throw std::runtime_error("Missing 'merge_pair' key in merge decision");
        }
        if (!merge_decision["merge_pair"].is_array() ||
            merge_decision["merge_pair"].size() != 2) {
            throw std::runtime_error("'merge_pair' must be an array of exactly 2 integers");
        }

        std::cout << "[GNN::ITERATION " << idx << "] ✅ Successfully parsed merge decision JSON"
                  << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[GNN::ERROR] Failed to read/parse merge decision: " << e.what() << std::endl;
        throw;
    }

    int u = merge_decision["merge_pair"][0].get<int>();
    int v = merge_decision["merge_pair"][1].get<int>();

    std::cout << "[GNN::ITERATION " << idx << "] Merge decision: (" << u << ", " << v << ")"
              << std::endl;

    // ✅ WRITE ACKNOWLEDGMENT ATOMICALLY
    try {
        // Ensure output directory exists
        std::filesystem::create_directories(fd_output_dir);

        json ack_data;
        ack_data["iteration"] = idx;
        ack_data["merge_pair"] = {u, v};
        ack_data["received"] = true;
        ack_data["timestamp"] = std::time(nullptr);

        // Write to temporary file first
        std::filesystem::path temp_path = ack_path.string() + ".tmp";
        {
            std::ofstream fout(temp_path);
            if (!fout.is_open()) {
                throw std::runtime_error("Cannot open temp ACK file: " + temp_path.string());
            }

            fout << ack_data.dump(2);  // Pretty print
            fout.flush();

            // Explicit close to ensure all data is written
            fout.close();
            if (fout.fail()) {
                throw std::runtime_error("Failed to write or close temp ACK file");
            }
        }

        // Atomic rename: temp → final
        try {
            std::filesystem::rename(temp_path, ack_path);
        } catch (const std::filesystem::filesystem_error& e) {
            std::cerr << "[GNN::WARNING] Failed to rename ACK file: " << e.what() << std::endl;
            // Try direct write as fallback
            std::ofstream fout(ack_path);
            if (fout.is_open()) {
                fout << ack_data.dump(2);
                fout.close();
            }
        }

        std::cout << "[GNN::ITERATION " << idx << "] ✅ Wrote ACK to: " << ack_path.string()
                  << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "[GNN::WARNING] Could not write ACK file: " << e.what() << std::endl;
        // Don't throw—we still got the merge decision
    }

    iteration++;

    std::cout << "[GNN::ITERATION " << idx << "] ✅ Returning merge pair (" << u << ", " << v
              << ")" << std::endl;
    std::cout << "[GNN::ITERATION " << idx << "] ========================================\n"
              << std::endl;

    return {u, v};
}

void MergeStrategyGNN::set_next_merge_pair(
    const FactoredTransitionSystem&,
    const std::vector<std::shared_ptr<ShrinkStrategy>>&) {
    // Not used
}

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_gnn.h code is this:
#pragma once

#include "merge_strategy.h"
#include "factored_transition_system.h"
#include "shrink_strategy.h"

#include <vector>
#include <memory>
#include <string>

namespace merge_and_shrink {

    class MergeStrategyGNN : public MergeStrategy {
    private:
        int iteration;
        std::string gnn_output_dir;
//        std::string fd_output_dir;

    public:
        explicit MergeStrategyGNN(const FactoredTransitionSystem& fts);

        std::pair<int, int> get_next() override;

        void set_next_merge_pair(
            const FactoredTransitionSystem&,
            const std::vector<std::shared_ptr<ShrinkStrategy>>&);
    };

}


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_factory_gnn.cc code is this:
﻿//#include "merge_strategy_factory_gnn.h"
//#include "merge_strategy_gnn.h"
//
//#include "../plugins/plugin.h"            // for TypedFeature
//#include "../utils/logging.h"             // brings in utils::Verbosity
//
//using namespace std;

#include "merge_strategy_factory_gnn.h"
#include "merge_strategy_gnn.h"

#include "../plugins/plugin.h"
#include "../utils/logging.h"

using std::string;
using std::unique_ptr;
using std::make_unique;
using std::shared_ptr;

namespace merge_and_shrink {

    // 1) Constructor: forward verbosity to base
    MergeStrategyFactoryGNN::MergeStrategyFactoryGNN(utils::Verbosity verbosity)
        : MergeStrategyFactory(verbosity) {
    }

    // 2) Instantiation
    unique_ptr<MergeStrategy> MergeStrategyFactoryGNN::compute_merge_strategy(
        const TaskProxy& /*task_proxy*/,
        const FactoredTransitionSystem& fts) {
        return make_unique<MergeStrategyGNN>(fts);
    }

    string MergeStrategyFactoryGNN::name() const {
        return "merge_gnn";
    }

    void MergeStrategyFactoryGNN::dump_strategy_specific_options() const {
        // No extra options here
    }

    // 3) Plugin registration, exactly like the others
    class MergeStrategyFactoryGNNFeature
        : public plugins::TypedFeature<MergeStrategyFactory, MergeStrategyFactoryGNN> {
    public:
        MergeStrategyFactoryGNNFeature()
            : TypedFeature("merge_gnn") {
            document_title("GNN-based merge strategy");
            document_synopsis(
                "Reads the next merge pair from an external JSON file "
                "produced by a GNN.");
            add_merge_strategy_options_to_feature(*this);
        }

        // This unpacks (verbosity) from opts into the ctor
        shared_ptr<MergeStrategyFactoryGNN> create_component(
            const plugins::Options& opts) const override {
            return plugins::make_shared_from_arg_tuples<MergeStrategyFactoryGNN>(
                get_merge_strategy_arguments_from_options(opts)
            );
        }
    };

    // static registration
    static plugins::FeaturePlugin<MergeStrategyFactoryGNNFeature> _plugin;

}  // namespace merge_and_shrink


--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_strategy_factory_gnn.h code is this:
#ifndef MERGE_AND_SHRINK_MERGE_STRATEGY_FACTORY_GNN_H
#define MERGE_AND_SHRINK_MERGE_STRATEGY_FACTORY_GNN_H

#include "merge_strategy_factory.h"

namespace merge_and_shrink {

    class MergeStrategyFactoryGNN : public MergeStrategyFactory {
    public:
        // Match the other factories: take verbosity
        explicit MergeStrategyFactoryGNN(utils::Verbosity verbosity);

        // Create the strategy object
        std::unique_ptr<MergeStrategy> compute_merge_strategy(
            const TaskProxy& task_proxy,
            const FactoredTransitionSystem& fts) override;

        std::string name() const override;
        void dump_strategy_specific_options() const override;
        bool requires_init_distances()  const override { return false; }
        bool requires_goal_distances()  const override { return false; }
    };

}  // namespace merge_and_shrink

#endif


--------------------------------------------------------------------------------

