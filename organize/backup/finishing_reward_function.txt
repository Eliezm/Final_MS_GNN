The file gnn_model.py code code is this:
# FILE: gnn_model.py (COMPLETE REWRITE WITH ATTENTION & EDGE FEATURES)
import torch
from torch import Tensor, nn
from torch_geometric.nn import GCNConv, GATConv
import torch.nn as nn
from typing import Tuple, Optional
import numpy as np

import logging
logger = logging.getLogger(__name__)


# ============================================================================
# ATTENTION BACKBONE: Improved GCN with Multi-Head Attention
# ============================================================================

class GCNWithAttention(nn.Module):
    """✅ NEW: GCN backbone with multi-head attention for focusing on key relationships."""

    def __init__(self, input_dim: int, hidden_dim: int, n_layers: int = 3, n_heads: int = 4):
        super().__init__()

        # GCN layers
        layers = []
        dims = [input_dim] + [hidden_dim] * (n_layers - 1) + [hidden_dim]
        for i in range(n_layers):
            layers.append(GCNConv(dims[i], dims[i + 1]))
        self.convs = nn.ModuleList(layers)


        # ✅ NEW: Graph Attention Network layer for learning which relationships matter
        self.attention = GATConv(
            in_channels=hidden_dim,
            out_channels=hidden_dim,
            heads=n_heads,
            concat=True,
            dropout=0.1,
            add_self_loops=False
        )

        # Post-attention projection (if concat=True, attention outputs n_heads*out_channels)
        self.attention_proj = nn.Linear(hidden_dim * n_heads, hidden_dim)

        self.activation = nn.ReLU()
        self.dropout = nn.Dropout(0.1)

    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:
        """Forward pass with attention mechanism."""
        device = x.device
        dtype = x.dtype
        edge_index = edge_index.to(device, dtype=torch.long)

        # Standard GCN pass
        for conv in self.convs:
            x = self.activation(conv(x, edge_index))
            x = self.dropout(x)

        # ✅ NEW: Apply attention on top of GCN embeddings
        # This learns which node pairs are important for merge decisions
        if edge_index.numel() > 0:
            try:
                attn_out = self.attention(x, edge_index)
                # Project back to hidden_dim
                attn_out = self.activation(self.attention_proj(attn_out))
                # Residual connection: blend GCN output with attention output
                x = x + attn_out * 0.3  # Small weight to preserve GCN learning
            except Exception as e:
                # Fallback if attention fails
                logger.warning(f"Attention layer failed: {e}, skipping")

        return x


# ============================================================================
# EDGE FEATURE ENCODER: Encode merge candidate properties
# ============================================================================

class EdgeFeatureEncoder(nn.Module):
    """✅ NEW: Encodes rich features about merge candidates."""

    def __init__(self, num_edge_features: int = 8, output_dim: int = 16):
        super().__init__()

        # Map edge features through neural net
        self.encoder = nn.Sequential(
            nn.Linear(num_edge_features, 32),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(32, output_dim),
            nn.ReLU()
        )

        self.output_dim = output_dim

    def forward(self, edge_features: Tensor) -> Tensor:
        """
        Encode edge features into embeddings.

        Args:
            edge_features: [E, num_edge_features] raw edge features

        Returns:
            [E, output_dim] encoded edge features
        """
        if edge_features.numel() == 0:
            return torch.zeros(0, self.output_dim, device=edge_features.device)

        return self.encoder(edge_features)


# ============================================================================
# ATTENTION-WEIGHTED EDGE SCORER
# ============================================================================

class AttentionWeightedEdgeScorer(nn.Module):
    """✅ NEW: Score edges using attention + edge features + node embeddings."""

    def __init__(self, hidden_dim: int, edge_feature_dim: int = 16):
        super().__init__()

        # Combine node embeddings with edge features
        # Input: [src_embedding, tgt_embedding, edge_features]
        total_dim = 2 * hidden_dim + edge_feature_dim

        self.mlp = nn.Sequential(
            nn.Linear(total_dim, 2 * total_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(2 * total_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 1),
        )

        # ✅ NEW: Attention weights for edge features
        # Learn which edge features are most important
        self.edge_attention = nn.Sequential(
            nn.Linear(edge_feature_dim, 16),
            nn.Tanh(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        )

    def forward(
            self,
            node_embs: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tensor:
        """
        Score edges using attention-weighted combination of features.

        Args:
            node_embs: [N, H] node embeddings
            edge_index: [2, E] edge list in COO format
            edge_features: [E, D] edge features (optional)

        Returns:
            [E] edge scores
        """
        # ✅ SAFETY: Handle empty edge lists
        if edge_index.numel() == 0 or edge_index.shape[1] == 0:
            return torch.zeros(0, device=node_embs.device, dtype=torch.float32)

        src_idx, tgt_idx = edge_index
        num_nodes = node_embs.shape[0]

        # ✅ SAFETY: Validate indices
        max_idx = max(src_idx.max().item(), tgt_idx.max().item()) if len(src_idx) > 0 else -1
        if max_idx >= num_nodes:
            print(f"WARNING: Edge index contains invalid node ID {max_idx} >= {num_nodes}")
            return torch.zeros(len(src_idx), device=node_embs.device, dtype=torch.float32)

        src_emb = node_embs[src_idx]  # [E, H]
        tgt_emb = node_embs[tgt_idx]  # [E, H]

        # ✅ NEW: Handle edge features with attention
        if edge_features is not None and edge_features.numel() > 0:
            # Attention weights for edge features
            edge_attn_weights = self.edge_attention(edge_features)  # [E, 1]

            # Scale edge features by attention weights
            edge_feats_weighted = edge_features * edge_attn_weights  # [E, D]

            # Concatenate node embeddings with weighted edge features
            edge_feat = torch.cat([src_emb, tgt_emb, edge_feats_weighted], dim=1)  # [E, 2H+D]
        else:
            # Fallback: just use node embeddings
            edge_feat = torch.cat([src_emb, tgt_emb], dim=1)  # [E, 2H]

        score = self.mlp(edge_feat).squeeze(-1)  # [E]

        # ✅ SAFETY: Clamp to avoid explosion
        score = torch.clamp(score, min=-1e6, max=1e6)

        # ✅ SAFETY: Replace NaN/Inf with safe defaults
        score = torch.nan_to_num(score, nan=0.0, posinf=1e6, neginf=-1e6)

        return score


# ============================================================================
# UNIFIED GNN MODEL WITH ATTENTION & EDGE FEATURES
# ============================================================================

class GNNModel(nn.Module):
    """✅ COMPLETE: Full GNN with attention, edge features, and robust validation."""

    def __init__(
            self,
            input_dim: int,
            hidden_dim: int,
            n_layers: int = 3,
            n_heads: int = 4,
            edge_feature_dim: int = 8
    ):
        super().__init__()

        # ✅ NEW: GCN with attention backbone
        self.backbone = GCNWithAttention(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            n_layers=n_layers,
            n_heads=n_heads
        )

        # ✅ NEW: Edge feature encoder
        self.edge_encoder = EdgeFeatureEncoder(
            num_edge_features=edge_feature_dim,
            output_dim=16
        )

        # ✅ NEW: Attention-weighted edge scorer
        self.scorer = AttentionWeightedEdgeScorer(
            hidden_dim=hidden_dim,
            edge_feature_dim=16
        )

    def forward(
            self,
            x: Tensor,
            edge_index: Tensor,
            edge_features: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        """Forward pass with validation."""
        # ✅ INPUT VALIDATION
        if x.dim() != 2:
            raise ValueError(f"Node features must be 2D, got {x.dim()}D")
        if edge_index.dim() != 2 or edge_index.shape[0] != 2:
            raise ValueError(f"Edge index must be [2, E], got {edge_index.shape}")

        device = x.device
        edge_index = edge_index.to(device, dtype=torch.long)

        # ✅ EDGE INDEX VALIDATION - CRITICAL!
        num_nodes = x.shape[0]
        if edge_index.numel() > 0:
            max_idx = edge_index.max().item()
            if max_idx >= num_nodes:
                logger.warning(f"Edge index {max_idx} >= num_nodes {num_nodes}. Clamping...")
                edge_index = torch.clamp(edge_index, 0, num_nodes - 1)

            # ✅ NEW: Check for negative indices
            min_idx = edge_index.min().item()
            if min_idx < 0:
                logger.warning(f"Negative edge index {min_idx} found. Clamping...")
                edge_index = torch.clamp(edge_index, 0, num_nodes - 1)

        # Rest of forward pass...
        node_embs = self.backbone(x, edge_index)

        if torch.isnan(node_embs).any():
            logger.warning("NaN in node embeddings, replacing with 0")
            node_embs = torch.nan_to_num(node_embs, nan=0.0)

        encoded_edge_features = None
        if edge_features is not None and edge_features.numel() > 0:
            try:
                encoded_edge_features = self.edge_encoder(edge_features.float())
            except Exception as e:
                logger.warning(f"Could not encode edge features: {e}")

        edge_logits = self.scorer(node_embs, edge_index, encoded_edge_features)

        return edge_logits, node_embs

--------------------------------------------------------------------------------

The file gnn_policy.py code code is this:
# FILE: gnn_policy.py (CRITICAL FIXES)
import traceback

import numpy as np
import torch
from torch import nn, Tensor
from torch.distributions import Categorical
from stable_baselines3.common.policies import ActorCriticPolicy
from typing import Tuple, Dict, Any, Optional
from gnn_model import GNNModel

import logging
logger = logging.getLogger(__name__)


class GNNExtractor(nn.Module):
    """✅ UPDATED: Wraps GNNModel with edge feature support."""

    def __init__(self, input_dim: int, hidden_dim: int, edge_feature_dim: int = 8):
        super().__init__()
        self.gnn = GNNModel(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            n_layers=3,
            n_heads=4,  # ✅ NEW: Attention heads
            edge_feature_dim=edge_feature_dim  # ✅ NEW: Edge feature support
        )
        self.edge_feature_dim = edge_feature_dim

    def forward(
        self,
        x: Tensor,
        edge_index: Tensor,
        edge_features: Optional[Tensor] = None
    ) -> Tuple[Tensor, Tensor]:
        """✅ UPDATED: Pass edge features to GNN."""
        edge_logits, node_embs = self.gnn(x, edge_index, edge_features)
        return edge_logits, node_embs


class GNNPolicy(ActorCriticPolicy):
    """✅ FIXED: Robust policy with action validation."""

    def __init__(
            self,
            observation_space,
            action_space,
            lr_schedule,
            net_arch=None,
            activation_fn=nn.ReLU,
            hidden_dim: int = 128,
            **kwargs
    ):
        super().__init__(
            observation_space,
            action_space,
            lr_schedule,
            net_arch=[],
            activation_fn=activation_fn,
            **kwargs
        )

        self.node_feat_dim = observation_space["x"].shape[-1]
        self.hidden_dim = hidden_dim

        self.extractor = GNNExtractor(input_dim=self.node_feat_dim, hidden_dim=self.hidden_dim)
        self.value_net = nn.Linear(self.hidden_dim, 1)
        self.action_net = nn.Identity()

        self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1))

    def _mask_invalid_edges(self, logits: Tensor, num_edges: int) -> Tensor:
        """
        ✅ FIXED: Mask invalid edges without breaking everything.

        GUARANTEE: At least one edge remains unmasked.
        """
        E = logits.size(0)

        # ✅ SAFETY: Clamp num_edges to valid range [1, E]
        num_edges_clamped = max(1, min(int(num_edges), E))

        mask = torch.arange(E, device=logits.device) < num_edges_clamped

        # ✅ GUARANTEE: At least one edge is available
        if not mask.any():
            mask[0] = True

        masked = logits.clone()
        masked[~mask] = -1e9

        return masked

    def _sample_or_argmax(self, logits: Tensor, deterministic: bool) -> Tuple[Tensor, Tensor]:
        """
        ✅ FIXED: Sample action safely with fallback.

        Returns (action, log_prob)
        """
        try:
            logits = torch.clamp(logits, min=-100, max=100)
            probs = torch.softmax(logits, dim=0)

            # ✅ SAFETY: Check if distribution is valid
            if torch.isnan(probs).any() or torch.isinf(probs).any():
                print("WARNING: Invalid probabilities, using uniform")
                probs = torch.ones_like(logits) / len(logits)

            dist = Categorical(probs=probs)

            if deterministic:
                action = probs.argmax(dim=0)
            else:
                action = dist.sample()

            logp = dist.log_prob(action)

            # ✅ SAFETY: Check log_prob
            if torch.isnan(logp) or torch.isinf(logp):
                logp = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)

            return action, logp

        except Exception as e:
            print(f"WARNING: Sampling failed: {e}, using argmax fallback")
            action = logits.argmax(dim=0)
            logp = torch.tensor(0.0, device=logits.device, dtype=logits.dtype)
            return action, logp

    @torch.no_grad()
    def predict(
            self,
            observation: Dict[str, Any],
            state=None,
            mask=None,
            deterministic=False
    ):
        """✅ FIXED: Properly handle observation batching from Stable-Baselines3."""
        self.eval()
        device = self.device

        # ================================================================
        # PHASE 1: EXTRACT OBSERVATIONS WITH PROPER BATCHING DETECTION
        # ================================================================

        x = torch.as_tensor(observation["x"], dtype=torch.float32, device=device)
        ei = torch.as_tensor(observation["edge_index"], dtype=torch.long, device=device)

        # ✅ FIX: Extract edge_features with proper batching handling
        edge_features = None
        if "edge_features" in observation:
            ef_raw = observation["edge_features"]
            edge_features = torch.as_tensor(ef_raw, dtype=torch.float32, device=device)

        # ================================================================
        # PHASE 2: DETECT BATCH DIMENSION
        # ================================================================

        # Determine if observations are already batched
        if x.ndim == 2:
            # Unbatched: [N, feat_dim]
            is_batched = False
            B = 1
            x = x.unsqueeze(0)  # [1, N, feat_dim]
            ei = ei.unsqueeze(0)  # [1, 2, E]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)  # [1, E, 8]

        elif x.ndim == 3:
            # Already batched: [batch, N, feat_dim]
            is_batched = True
            B = x.shape[0]

            # ✅ CRITICAL FIX: Ensure edge_features is also batched
            if edge_features is not None:
                if edge_features.ndim == 2:
                    # edge_features is [E, 8], need to broadcast to [batch, E, 8]
                    edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)
                # else: already [batch, E, 8], keep as is
        else:
            raise ValueError(f"Invalid observation dimension: {x.ndim}")

        # ================================================================
        # PHASE 3: EXTRACT DIMENSIONS
        # ================================================================

        # Handle num_nodes/num_edges which might be scalars or arrays
        ne = observation.get("num_edges", None)
        num_nodes_obs = observation.get("num_nodes", None)

        # Parse num_edges
        if ne is None:
            ne = [ei.shape[-1]] * B
        elif isinstance(ne, np.ndarray):
            if ne.ndim == 0:
                ne = [int(ne)] * B
            else:
                ne = ne.reshape(-1).tolist()
        elif isinstance(ne, (int, float)):
            ne = [int(ne)] * B
        elif isinstance(ne, torch.Tensor):
            ne = ne.cpu().numpy().reshape(-1).tolist()
        else:
            ne = [int(n) for n in ne] if hasattr(ne, '__iter__') else [int(ne)] * B

        # Ensure ne list has correct length
        if len(ne) == 1 and B > 1:
            ne = ne * B
        elif len(ne) != B:
            logger.warning(f"num_edges length {len(ne)} != batch size {B}, padding")
            ne = (ne + [ei.shape[-1]] * B)[:B]

        # ================================================================
        # PHASE 4: INPUT VALIDATION
        # ================================================================

        try:
            if x.dim() < 2:
                raise ValueError(f"x must be at least 2D, got {x.dim()}D: {x.shape}")
            if ei.dim() < 2:
                raise ValueError(f"edge_index must be at least 2D, got {ei.dim()}D: {ei.shape}")

            num_nodes = x.shape[-2]

            if ei.numel() > 0:
                max_idx = ei.max().item()
                if max_idx >= num_nodes:
                    logger.warning(f"Edge index {max_idx} >= num_nodes {num_nodes}. Clamping.")
                    ei = torch.clamp(ei, max=num_nodes - 1)

        except Exception as e:
            logger.error(f"Input validation failed: {e}")
            return np.zeros(B, dtype=int), None

        # ================================================================
        # PHASE 5: PROCESS EACH SAMPLE
        # ================================================================

        actions = []

        for i in range(B):
            try:
                # Extract sample from batch
                x_i = x[i]  # [N, feat_dim]
                ei_i = ei[i]  # [2, E]

                # ✅ FIX: Properly extract edge features for this sample
                if edge_features is not None:
                    edge_feats_i = edge_features[i]  # [E, 8]
                else:
                    edge_feats_i = None

                # ✅ VALIDATE SHAPES before passing to extractor
                if x_i.ndim != 2:
                    logger.error(f"Sample {i}: x_i has wrong shape {x_i.shape}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue
                if ei_i.numel() > 0 and ei_i.ndim != 2:
                    logger.error(f"Sample {i}: ei_i has wrong shape {ei_i.ndim}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue
                if edge_feats_i is not None and edge_feats_i.ndim != 2:
                    logger.error(f"Sample {i}: edge_feats_i has wrong shape {edge_feats_i.shape}")
                    actions.append(torch.tensor(0, dtype=torch.long, device=device))
                    continue

                # Forward through GNN
                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                # Validate logits
                if torch.isnan(logits_i).any() or torch.isinf(logits_i).any():
                    logger.warning(f"Sample {i}: Invalid logits, using uniform")
                    logits_i = torch.ones_like(logits_i)

                num_edges = int(ne[i]) if i < len(ne) else logits_i.shape[0]

                if num_edges <= 0 or logits_i.shape[0] == 0:
                    a_i = torch.tensor(0, dtype=torch.long, device=device)
                else:
                    masked = self._mask_invalid_edges(logits_i, num_edges)
                    a_i, _ = self._sample_or_argmax(masked, deterministic)

                # Validate action
                if a_i < 0 or (logits_i.shape[0] > 0 and a_i >= logits_i.shape[0]):
                    logger.warning(f"Sample {i}: Invalid action {a_i}, clamping to 0")
                    a_i = torch.tensor(0, dtype=torch.long, device=device)

                actions.append(a_i)

            except Exception as e:
                logger.error(f"Sample {i} processing failed: {e}")
                logger.error(traceback.format_exc())
                actions.append(torch.tensor(0, dtype=torch.long, device=device))

        # ================================================================
        # PHASE 6: RETURN RESULT
        # ================================================================

        actions_tensor = torch.stack(actions) if actions else torch.zeros(B, dtype=torch.long, device=device)
        return actions_tensor.cpu().numpy(), None

    def forward(self, obs: Dict[str, Any], deterministic: bool = False
                ) -> Tuple[Tensor, Tensor, Tensor]:
        """✅ FIXED: Properly handle batched observations."""
        device = self.device

        # ================================================================
        # EXTRACT AND VALIDATE OBSERVATIONS
        # ================================================================

        x = torch.as_tensor(obs["x"], dtype=torch.float32, device=device)
        ei = torch.as_tensor(obs["edge_index"], dtype=torch.long, device=device)

        # Extract edge_features with proper batching
        edge_features = None
        if "edge_features" in obs:
            edge_features = torch.as_tensor(obs["edge_features"], dtype=torch.float32, device=device)

        # ================================================================
        # DETECT AND HANDLE BATCHING
        # ================================================================

        if x.ndim == 2:
            # Unbatched input
            x = x.unsqueeze(0)  # [1, N, feat]
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)  # [1, E, feat]
        elif x.ndim == 3:
            # Already batched - ensure edge_features is also batched
            B = x.shape[0]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)

        # ================================================================
        # EXTRACT ACTUAL DIMENSIONS
        # ================================================================

        num_nodes_obs = obs.get("num_nodes", None)
        if num_nodes_obs is not None:
            if isinstance(num_nodes_obs, torch.Tensor):
                actual_num_nodes = int(num_nodes_obs.item()) if num_nodes_obs.dim() == 0 else int(
                    num_nodes_obs[0].item())
            else:
                actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0])
        else:
            actual_num_nodes = x.shape[-2]

        ne_obs = obs.get("num_edges", None)
        if ne_obs is not None:
            if isinstance(ne_obs, torch.Tensor):
                actual_num_edges = int(ne_obs.item()) if ne_obs.dim() == 0 else int(ne_obs[0].item())
            else:
                actual_num_edges = int(np.asarray(ne_obs).flat[0])
        else:
            actual_num_edges = ei.shape[-1]

        # ================================================================
        # TRIM TO ACTUAL SIZES
        # ================================================================

        x_trimmed = x[..., :actual_num_nodes, :]

        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        # Trim edge_features accordingly
        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # ================================================================
        # PROCESS BATCH
        # ================================================================

        B = x_trimmed.shape[0]
        actions, values, logps = [], [], []

        for i in range(B):
            try:
                # Extract sample
                x_i = x_trimmed[i]  # [N, feat]
                ei_i = ei_trimmed[i]  # [2, E]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None  # [E, 8]

                # Forward through GNN
                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                # Compute value
                if node_embs_i.dim() == 1:
                    node_embs_i = node_embs_i.unsqueeze(0)

                if node_embs_i.shape[0] > 0:
                    v_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True)).squeeze(-1)
                else:
                    v_i = torch.zeros((), dtype=torch.float32, device=device)

                # Sample action
                ne_i = actual_num_edges

                if ne_i <= 0 or logits_i.shape[0] == 0:
                    a_i = torch.zeros((), dtype=torch.long, device=device)
                    lp_i = torch.zeros((), dtype=torch.float32, device=device)
                else:
                    masked = self._mask_invalid_edges(logits_i, ne_i)
                    a_i, lp_i = self._sample_or_argmax(masked, deterministic)

                actions.append(a_i)
                values.append(v_i)
                logps.append(lp_i)

            except Exception as e:
                logger.error(f"Batch {i} forward failed: {e}")
                logger.error(traceback.format_exc())
                actions.append(torch.zeros((), dtype=torch.long, device=device))
                values.append(torch.zeros((), device=device))
                logps.append(torch.zeros((), device=device))

        actions = torch.stack(actions)
        values = torch.stack(values).unsqueeze(-1)
        logps = torch.stack(logps)

        return actions, values, logps

    def evaluate_actions(self, obs: Dict[str, Tensor], actions: Tensor
                         ) -> Tuple[Tensor, Tensor, Tensor]:
        """✅ FIXED: Evaluate actions with proper batching."""
        device = self.device

        x = obs["x"].to(device, dtype=torch.float32)
        ei = obs["edge_index"].to(device, dtype=torch.long)

        # Extract edge_features with batching awareness
        edge_features = None
        if "edge_features" in obs:
            edge_features = obs["edge_features"].to(device, dtype=torch.float32)

        # ✅ Handle batching
        if x.ndim == 2:
            x = x.unsqueeze(0)
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)
        elif x.ndim == 3:
            B = x.shape[0]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)

        # Extract dimensions
        num_nodes_obs = obs.get("num_nodes", None)
        actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0]) if num_nodes_obs is not None else x.shape[-2]

        ne = obs.get("num_edges", None)
        actual_num_edges = int(np.asarray(ne).flat[0]) if ne is not None else ei.shape[-1]

        # Trim
        x_trimmed = x[..., :actual_num_nodes, :]
        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # Process batch
        B = x_trimmed.shape[0]
        values, logps, ents = [], [], []

        for i in range(B):
            try:
                x_i = x_trimmed[i]
                ei_i = ei_trimmed[i]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None

                logits_i, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                if node_embs_i.shape[0] > 0:
                    v_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True)).squeeze(-1)
                else:
                    v_i = torch.zeros((), device=device)

                ne_i = actual_num_edges

                if ne_i <= 0 or logits_i.shape[0] == 0:
                    logp_i = torch.zeros((), device=device, requires_grad=True)
                    ent_i = torch.zeros((), device=device, requires_grad=True)
                else:
                    masked = self._mask_invalid_edges(logits_i, ne_i)
                    dist = Categorical(logits=masked)
                    action_clamped = torch.clamp(actions[i], 0, logits_i.shape[0] - 1)
                    logp_i = dist.log_prob(action_clamped)
                    ent_i = dist.entropy()

                values.append(v_i)
                logps.append(logp_i)
                ents.append(ent_i)

            except Exception as e:
                logger.error(f"Evaluate batch {i} failed: {e}")
                values.append(torch.zeros((), device=device, requires_grad=True))
                logps.append(torch.zeros((), device=device, requires_grad=True))
                ents.append(torch.zeros((), device=device, requires_grad=True))

        return torch.stack(values), torch.stack(logps), torch.stack(ents)

    def predict_values(self, obs: Dict[str, Tensor]) -> Tensor:
        """✅ FIXED: Predict values with proper batching."""
        device = self.device

        x = obs["x"].to(device, dtype=torch.float32)
        ei = obs["edge_index"].to(device, dtype=torch.long)

        # Extract edge_features with batching awareness
        edge_features = None
        if "edge_features" in obs:
            edge_features = obs["edge_features"].to(device, dtype=torch.float32)

        # Handle batching
        if x.ndim == 2:
            x = x.unsqueeze(0)
            ei = ei.unsqueeze(0) if ei.ndim == 2 and ei.shape[0] == 2 else ei
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0)
        elif x.ndim == 3:
            B = x.shape[0]
            if edge_features is not None and edge_features.ndim == 2:
                edge_features = edge_features.unsqueeze(0).expand(B, -1, -1)

        # Extract dimensions
        num_nodes_obs = obs.get("num_nodes", None)
        actual_num_nodes = int(np.asarray(num_nodes_obs).flat[0]) if num_nodes_obs is not None else x.shape[-2]

        ne = obs.get("num_edges", None)
        actual_num_edges = int(np.asarray(ne).flat[0]) if ne is not None else ei.shape[-1]

        # Trim
        x_trimmed = x[..., :actual_num_nodes, :]
        if actual_num_edges > 0:
            ei_trimmed = ei[..., :actual_num_edges]
            ei_trimmed = torch.clamp(ei_trimmed, 0, actual_num_nodes - 1)
        else:
            ei_trimmed = torch.zeros((*ei.shape[:-1], 0), dtype=torch.long, device=device)

        if edge_features is not None and actual_num_edges > 0:
            edge_features_trimmed = edge_features[..., :actual_num_edges, :]
        else:
            edge_features_trimmed = None

        # Process batch
        B = x_trimmed.shape[0]
        vals = []

        for i in range(B):
            try:
                x_i = x_trimmed[i]
                ei_i = ei_trimmed[i]
                edge_feats_i = edge_features_trimmed[i] if edge_features_trimmed is not None else None

                _, node_embs_i = self.extractor(x_i, ei_i, edge_feats_i)

                if node_embs_i.shape[0] > 0:
                    val_i = self.value_net(node_embs_i.mean(dim=0, keepdim=True))
                else:
                    val_i = torch.zeros((1, 1), device=device)

                vals.append(val_i)

            except Exception as e:
                logger.error(f"Predict values batch {i} failed: {e}")
                vals.append(torch.zeros((1, 1), device=device))

        return torch.cat(vals, dim=0)

--------------------------------------------------------------------------------

The file graph_tracker.py code code is this:
# -*- coding: utf-8 -*-
"""
This module provides the GraphTracker class, a data structure for managing the
state of the merge-and-shrink heuristic construction process.

It represents the set of transition systems (TS) as nodes in a directed graph,
where edges represent causal dependencies from the Fast Downward planner. The class
is responsible for loading the initial state from planner output, performing
merge operations on nodes, and updating the graph based on new information from
the planner. It serves as the core state management component for the MergeEnv.
"""

# ------------------------------------------------------------------------------
#  Imports
# ------------------------------------------------------------------------------
import json
import logging
import time
from json import JSONDecoder
from typing import List, Union, Dict, Tuple, Any, FrozenSet, Optional

import networkx as nx
import numpy as np

# matplotlib is an optional dependency for visualization
try:
    import matplotlib.pyplot as plt
except ImportError:
    plt = None

logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
#  Configuration and Constants
# ------------------------------------------------------------------------------
# --- Setup basic logging ---
# Consistent logging configuration with merge_env.py
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s'
)

# --- Constants for file I/O ---
FILE_RETRY_COUNT = 60
FILE_RETRY_DELAY_S = 0.2


# ------------------------------------------------------------------------------
#  Helper Functions
# ------------------------------------------------------------------------------

def _load_json_robustly(path: str, retries: int = FILE_RETRY_COUNT, delay: float = FILE_RETRY_DELAY_S) -> Any:
    """
    Parses the first complete JSON object from a file path with retries.

    This function is designed to handle cases where a file might be read while
    another process is writing to it. It ensures the file is not empty and that
    the content appears to be a complete JSON object (ends with '}' or ']')
    before attempting to parse it.

    Method of Action:
    1. Loop for a specified number of `retries`.
    2. Read the file content, ignoring UTF-8 errors.
    3. If the file is empty or doesn't end with a closing brace/bracket,
       it's considered incomplete. Wait and retry.
    4. Use `JSONDecoder.raw_decode` to parse only the *first* valid JSON
       object, which avoids errors from trailing, partially-written data.
    5. If any error occurs (`OSError`, `JSONDecodeError`), wait and retry.
    6. If all retries fail, raise a `RuntimeError` with the last known error.
    """
    decoder = JSONDecoder()
    last_error = None
    for _ in range(retries):
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read().lstrip()

            if not content:
                # File is empty, wait for content to be written.
                raise json.JSONDecodeError("File is empty", content, 0)

            # Heuristic check for completeness to avoid parsing mid-write.
            tail = content.rstrip()
            if not tail or tail[-1] not in ("]", "}"):
                raise json.JSONDecodeError("JSON appears incomplete (no closing bracket/brace)", content, len(content))

            # Decode the first object, ignoring any trailing garbage.
            obj, _ = decoder.raw_decode(content)
            return obj

        except (OSError, json.JSONDecodeError) as e:
            last_error = e
            time.sleep(delay)

    raise RuntimeError(f"Failed to load valid JSON from '{path}' after {retries} retries. Last error: {last_error}")


def product_state_index(s1: int, s2: int, n2: int) -> int:
    """
    Maps a pair of local states to a single index in their Cartesian product.

    This is a standard row-major order mapping. Given two state spaces of sizes
    `n1` and `n2`, a state `s1` from the first space and `s2` from the second
    are mapped to a unique index in the combined space of size `n1 * n2`.

    Args:
        s1 (int): The index of the state in the first transition system.
        s2 (int): The index of the state in the second transition system.
        n2 (int): The total number of states in the second transition system.

    Returns:
        int: The unique index in the product state space.
    """
    return s1 * n2 + s2


# FILE: graph_tracker.py
# REPLACE THIS FUNCTION (around line 143)

def merge_transition_systems(ts1: Dict[str, Any], ts2: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    ✅ OPTIMIZED: Early detection of problematic merges.

    Returns None if merge is obviously bad (saves 90% of computation time).
    """
    n1, n2 = ts1["num_states"], ts2["num_states"]

    # ✅ OPTIMIZATION 1: Reject merges that will exceed reasonable size
    # This prevents attempting trillion-state merges
    SAFE_MERGE_LIMIT = 50_000_000  # 50M states (tunable)

    try:
        # Check if product would overflow
        if n1 > SAFE_MERGE_LIMIT or n2 > SAFE_MERGE_LIMIT:
            product = n1 * n2
        else:
            product = n1 * n2

    except OverflowError:
        logger.error(f"[EARLY REJECT] Merge would overflow: {n1} × {n2}")
        return None

    # Reject if product is unreasonably large
    if product > 10_000_000_000:  # 10 billion
        logger.warning(f"[EARLY REJECT] Merge product too large: {product} (limit: 10B)")
        logger.warning(f"  TS1: {n1} states, TS2: {n2} states")
        return None

    # ✅ OPTIMIZATION 2: Check reachability before creating goal states
    # Only create goal_states if both TS have reachable goals
    reachable_goals_1 = len([f for f in ts1.get("f_before", [])
                             if f != float('inf') and f < 1_000_000_000])
    reachable_goals_2 = len([f for f in ts2.get("f_before", [])
                             if f != float('inf') and f < 1_000_000_000])

    # if reachable_goals_1 == 0 or reachable_goals_2 == 0:
    #     logger.warning(f"[EARLY REJECT] Unreachable goals in merge")
    #     logger.warning(f"  TS1 reachable goals: {reachable_goals_1}, TS2: {reachable_goals_2}")
    #     return None

    # ✅ OPTIMIZATION 3: Lazy goal state computation
    # Don't pre-compute the full list; just store metadata
    goal_states_1 = ts1["goal_states"]
    goal_states_2 = ts2["goal_states"]

    # Only materialize if reasonable size
    num_product_goals = len(goal_states_1) * len(goal_states_2)

    if num_product_goals > 1_000_000:
        # Too many goals; don't enumerate, store as mapping function
        logger.warning(f"[LAZY GOALS] {num_product_goals} product goals; using lazy computation")

        merged_ts = {
            "num_states": n1 * n2,
            "init_state": product_state_index(ts1["init_state"], ts2["init_state"], n2),
            "goal_states": None,  # ✅ Mark as lazy
            "_goal_state_mapping": {
                "ts1_goals": goal_states_1,
                "ts2_goals": goal_states_2,
                "n2": n2,
            },
            "incorporated_variables": ts1["incorporated_variables"] + ts2["incorporated_variables"],
            "iteration": max(ts1.get("iteration", -1), ts2.get("iteration", -1)) + 1,
        }

        logger.info(f"[MERGE] Created lazy goal state mapping (would have {num_product_goals} entries)")
        return merged_ts

    # Otherwise, safe to materialize
    merged_ts = {
        "num_states": n1 * n2,
        "init_state": product_state_index(ts1["init_state"], ts2["init_state"], n2),
        "goal_states": [
            product_state_index(g1, g2, n2)
            for g1 in goal_states_1
            for g2 in goal_states_2
        ],
        "incorporated_variables": ts1["incorporated_variables"] + ts2["incorporated_variables"],
        "iteration": max(ts1.get("iteration", -1), ts2.get("iteration", -1)) + 1,
    }

    return merged_ts


# ✅ ADD THIS NEW HELPER FUNCTION to handle lazy goal states
def is_goal_state_lazy(state_index: int, ts_merged: Dict[str, Any]) -> bool:
    """Check if a state is a goal using lazy mapping (no list materialization)."""
    if ts_merged.get("goal_states") is not None:
        # Already materialized
        return state_index in ts_merged["goal_states"]

    # Use lazy mapping
    mapping = ts_merged.get("_goal_state_mapping")
    if not mapping:
        return False

    ts1_goals = mapping["ts1_goals"]
    ts2_goals = mapping["ts2_goals"]
    n2 = mapping["n2"]

    s1 = state_index // n2
    s2 = state_index % n2

    return s1 in ts1_goals and s2 in ts2_goals

# ------------------------------------------------------------------------------
#  GraphTracker Class
# ------------------------------------------------------------------------------

class GraphTracker:
    """
    Manages the graph of transition systems for the merge-and-shrink process.

    This class holds a `networkx.DiGraph` where each node represents a transition
    system (TS). Initially, nodes correspond to atomic TSs for individual problem
    variables. The class provides methods to merge nodes (creating a new composite
    TS node) and update node properties based on new data from the planner.

    Attributes:
        graph (nx.DiGraph): The graph of transition systems.
        varset_to_node (Dict): A mapping from a frozenset of variable IDs to the
                                corresponding node ID in the graph. This allows for
                                efficient lookups.
        next_node_id (int): A counter for allocating unique IDs to new merged nodes.
    """

    def __init__(self, ts_json_path: str, cg_json_path: str, is_debug: bool = False):
        """Initialize with caching infrastructure."""
        self.graph = nx.DiGraph()
        self.varset_to_node: Dict[FrozenSet, Union[int, str]] = {}
        self.next_node_id: int = 0
        self.is_debug = is_debug
        # ✅ NEW: Persistent caches (survive across observations)
        self._centrality_cache: Optional[Dict] = None
        self._centrality_cache_valid = False
        self._max_vars_cache: Optional[int] = None
        self._max_iter_cache: Optional[int] = None
        self._f_stats_cache: Dict[int, Tuple[float, float, float, float]] = {}  # Cache for f_stats
        self._graph_hash_last = None
        # Note: _edge_features_cache and _node_features_cache were mentioned
        # in the prompt but not used in the provided methods, so omitting them for now.
        # Add them here if needed later:
        # self._edge_features_cache: Optional[np.ndarray] = None
        # self._node_features_cache: Dict[int, np.ndarray] = {}

        logging.info("Initializing GraphTracker...")
        try:
            self._load_atomic_systems(ts_json_path)
            self._load_causal_edges(cg_json_path)
        except Exception as e:
            logging.error(f"Failed during initial graph loading: {e}")
            if not self.is_debug:
                raise
            else:
                logging.warning("Proceeding with an empty graph in debug mode.")

    # --- END OF REPLACEMENT FOR __init__ ---

    # --- ADD THESE NEW METHODS INSIDE THE GraphTracker CLASS ---

    def _get_graph_hash(self) -> str:
        """✅ Quick hash to detect graph changes."""
        # Hash based on edges (nodes are implicit)
        # Using sorted edges ensures hash consistency regardless of internal order
        edges_tuple = tuple(sorted(self.graph.edges()))
        return str(hash(edges_tuple))

    def _invalidate_caches(self):
        """Call ONLY after actual graph modification."""
        logging.debug("Invalidating GraphTracker caches...")  # Added log
        self._centrality_cache_valid = False
        self._f_stats_cache.clear()
        # Invalidate others if added later
        # self._edge_features_cache = None
        # self._node_features_cache.clear()
        self._graph_hash_last = None  # Reset graph hash tracking

    def get_centrality(self, force_recompute: bool = False) -> Dict:
        """✅ CACHED: Return cached centrality or compute once."""
        if force_recompute:
            self._centrality_cache_valid = False

        # Only recompute if cache is marked invalid
        if not self._centrality_cache_valid:
            logging.debug("Computing centrality (cached)...")
            try:
                # Handle potentially empty or disconnected graphs
                if self.graph.number_of_nodes() > 0:
                    self._centrality_cache = nx.closeness_centrality(self.graph)
                else:
                    self._centrality_cache = {}
            except nx.NetworkXError:  # Handles disconnected graph case
                logging.warning(
                    "Graph is disconnected, centrality might be misleading. Computing per component (using closeness_centrality).")
                self._centrality_cache = nx.closeness_centrality(
                    self.graph)  # Or handle components separately if needed
            self._centrality_cache_valid = True  # Mark as valid even if empty/disconnected

        return self._centrality_cache if self._centrality_cache is not None else {}

    def get_max_vars(self) -> int:
        """✅ CACHED: Return max incorporated variables."""
        # Compute only if cache is empty
        if self._max_vars_cache is None:
            logging.debug("Computing max_vars (cached)...")
            self._max_vars_cache = max(
                (len(d.get("incorporated_variables", [])) for _, d in self.graph.nodes(data=True)),
                default=1  # Default if graph is empty
            ) or 1  # Ensure it's at least 1 if max returns 0
        return self._max_vars_cache

    def get_max_iter(self) -> int:
        """✅ CACHED: Return max iteration level."""
        # Compute only if cache is empty
        if self._max_iter_cache is None:
            logging.debug("Computing max_iter (cached)...")
            self._max_iter_cache = max(
                (d.get("iteration", 0) for _, d in self.graph.nodes(data=True)),
                default=0  # Default if graph is empty
            ) or 1  # Ensure it's at least 1 if max returns 0
        return self._max_iter_cache

    def update_graph(self, ts_json_path: str) -> None:
        """
        Updates the graph with new transition system data from a JSON file.

        This method reads a TS list from the given path and updates the properties
        of existing nodes. This is typically used after a merge operation, where
        the planner provides an updated TS file for the newly created node.

        Args:
            ts_json_path (str): The path to the JSON file with TS data.
        """
        logging.info(f"Updating graph from '{ts_json_path}'...")
        try:
            data = _load_json_robustly(ts_json_path)
            ts_list = data if isinstance(data, list) else [data]

            for ts in ts_list:
                if not isinstance(ts, dict):
                    continue
                self._add_or_update_node(ts)

        except Exception as e:
            logging.warning(f"Could not parse or process TS JSON from '{ts_json_path}': {e}")

    # FILE: graph_tracker.py
    # REPLACE THE EXISTING merge_nodes METHOD WITH THIS

    def merge_nodes(self, node_ids: List[Union[int, str]]) -> None:
        """✅ FIXED: Handles rejected merges from merge_transition_systems."""
        if len(node_ids) != 2:
            raise ValueError(f"merge_nodes requires exactly two node IDs, got {len(node_ids)}")

        a, b = node_ids

        # Validation
        if a not in self.graph:
            raise KeyError(f"Node {a} not in graph. Available: {list(self.graph.nodes())}")
        if b not in self.graph:
            raise KeyError(f"Node {b} not in graph. Available: {list(self.graph.nodes())}")
        if a == b:
            raise ValueError(f"Cannot merge node with itself: {a}")
        # Optional: Keep or remove the warning for disconnected nodes
        # if not (self.graph.has_edge(a, b) or self.graph.has_edge(b, a)):
        #     logger.warning(f"Merging potentially disconnected nodes {a}, {b}")

        logging.info(f"Attempting merge of nodes {a} and {b}...")
        ts1 = self.graph.nodes[a]
        ts2 = self.graph.nodes[b]

        # Compute the merged transition system.
        merged_ts = merge_transition_systems(ts1, ts2)

        # ✅ --- FIX: CHECK IF MERGE WAS REJECTED ---
        if merged_ts is None:
            # The merge_transition_systems function decided this merge was bad (e.g., too large).
            # Log it and raise an error to signal failure to the caller (MergeEnv).
            error_msg = f"Merge of nodes {a} and {b} rejected by merge_transition_systems (likely too large or unreachable goals)."
            logger.warning(error_msg)
            raise ValueError(error_msg)  # Raise exception to stop processing this step
        # ✅ --- END FIX ---

        # --- If merge was successful (merged_ts is a dict), proceed ---
        new_id = self.next_node_id
        self.next_node_id += 1

        # Add the new merged node to the graph.
        self.graph.add_node(new_id, **merged_ts)  # Now safe because merged_ts is guaranteed to be a dict
        var_key = frozenset(merged_ts["incorporated_variables"])
        self.varset_to_node[var_key] = new_id

        # Rewire edges from the original nodes to the new node.
        self._rewire_edges(a, new_id)
        self._rewire_edges(b, new_id)

        # Remove the original nodes.
        self.graph.remove_nodes_from([a, b])

        # Invalidate caches AFTER modification
        self._invalidate_caches()

        # Reset max_vars and max_iter caches as they might change
        self._max_vars_cache = None
        self._max_iter_cache = None

        logging.info(f"Successfully merged nodes into new node {new_id} with {merged_ts['num_states']} states.")

    def f_stats(self, node_id: Union[int, str]) -> Tuple[float, float, float, float]:
        """✅ CACHED: Memoized F-statistics computation."""
        # Check cache first
        if node_id in self._f_stats_cache:
            return self._f_stats_cache[node_id]

        # Compute only if not cached
        logging.debug(f"Computing f_stats for node {node_id} (cached)...")  # Added log
        if node_id not in self.graph.nodes:
            result = (0.0, 0.0, 0.0, 0.0)
        else:
            # Filter f_values ONCE
            f_values_raw = self.graph.nodes[node_id].get("f_before", [])
            # Filter out inf and large values more robustly
            f_values = [f for f in f_values_raw if f != float('inf') and f < 1_000_000_000]

            if not f_values:  # Check if list is empty AFTER filtering
                result = (0.0, 0.0, 0.0, 0.0)
            else:
                arr = np.array(f_values, dtype=np.float32)
                # Handle potential empty array after filtering again (safety)
                if arr.size == 0:
                    result = (0.0, 0.0, 0.0, 0.0)
                else:
                    # Use np.nanmin, np.nanmean etc. if NaNs are possible, otherwise regular functions are fine
                    result = (
                        float(np.min(arr)),
                        float(np.mean(arr)),
                        float(np.max(arr)),
                        float(np.std(arr))
                    )

        # Cache for next time
        self._f_stats_cache[node_id] = result
        return result

    # --- END OF REPLACEMENT FOR f_stats ---

    def _load_atomic_systems(self, ts_json_path: str) -> None:
        """
        Loads the initial set of atomic transition systems from a JSON file.

        These form the initial nodes of the graph. Atomic systems are identified
        by having `iteration == -1`.
        """
        logging.info(f"Loading atomic systems from '{ts_json_path}'...")
        data = _load_json_robustly(ts_json_path)
        ts_list = data if isinstance(data, list) else [data]

        num_loaded = 0
        for ts in ts_list:
            if isinstance(ts, dict) and ts.get("iteration", -1) == -1:
                self._add_or_update_node(ts)
                num_loaded += 1

        # Set the next node ID to be higher than any existing integer ID to avoid collisions.
        int_ids = [n for n in self.graph.nodes if isinstance(n, int)]
        self.next_node_id = max(int_ids, default=-1) + 1
        logging.info(f"Loaded {num_loaded} atomic systems. Next node ID set to {self.next_node_id}.")

    # REPLACE THE OLD METHOD WITH THIS NEW ONE
    def _load_causal_edges(self, cg_json_path: str) -> None:
        """Loads the causal graph edges from a JSON file into the graph."""
        logging.info(f"Loading causal edges from '{cg_json_path}'...")
        try:
            with open(cg_json_path, "r") as f:
                data = json.load(f)

            edges = data.get("edges", [])
            if not edges:
                logging.warning("Causal graph file contains no 'edges' key or the list is empty.")
                return

            # Debug: log what nodes exist before trying to add edges
            logging.info(f"Current graph nodes before adding edges: {list(self.graph.nodes())}")

            num_added = 0
            for edge in edges:
                src = edge.get("from")
                tgt = edge.get("to")

                # This check is crucial and now more explicit
                if src is not None and tgt is not None:
                    if self.graph.has_node(src) and self.graph.has_node(tgt):
                        self.graph.add_edge(src, tgt)
                        num_added += 1
                        logging.info(f"Added edge ({src}, {tgt})")
                    else:
                        logging.warning(
                            f"Skipping edge ({src}, {tgt}) because one or both nodes do not exist in the graph. "
                            f"Current nodes: {list(self.graph.nodes())}"
                        )
                else:
                    logging.warning(f"Edge has None values: from={src}, to={tgt}")

            logging.info(f"Loaded {num_added} causal edges. Final edge count: {len(list(self.graph.edges()))}")

        except FileNotFoundError:
            logging.warning(f"Causal graph file '{cg_json_path}' not found. No edges loaded.")
            if not self.is_debug:
                raise
        except Exception as e:
            logging.error(f"An unexpected error occurred while loading causal edges: {e}", exc_info=True)
            if not self.is_debug:
                raise

    def _add_or_update_node(self, ts: Dict[str, Any]) -> None:
        """
        Adds a new node or updates an existing node's data based on a TS dict.

        The identity of a node is determined by its set of "incorporated_variables".
        If a node representing a given set of variables already exists, its
        attributes are updated. Otherwise, a new node is created.

        This method also removes the large 'transitions' list from the node data
        to save memory.

        ✅ NEW: Validates that TS has meaningful data before adding.
        """
        # Validate input
        if not ts or not isinstance(ts, dict):
            logging.warning("Skipping invalid TS: not a dict or empty")
            return

        ts_data = ts.copy()

        # ✅ MEMORY: Don't store full transitions - just count them
        transitions = ts_data.pop("transitions", [])
        ts_data["num_transitions"] = len(transitions)

        ts_data.pop("transitions", None)

        # ✅ NEW: Comprehensive validation
        inc_vars = ts_data.get("incorporated_variables", [])

        if not inc_vars:
            logging.warning("❌ Skipping TS with NO INCORPORATED VARIABLES")
            logging.warning(f"   TS keys: {list(ts.keys())}")
            logging.warning(f"   TS content: {ts}")
            return

        # ✅ NEW: Validate other critical fields
        num_states = ts_data.get("num_states", 0)
        if num_states <= 0:
            logging.warning(f"⚠️  Skipping TS with invalid num_states: {num_states}")
            return

        # ✅ NEW: Check if TS looks complete
        required_fields = ["num_states", "init_state", "goal_states"]
        missing = [k for k in required_fields if k not in ts_data]
        if missing:
            logging.warning(f"⚠️  TS missing fields: {missing}")
            logging.warning(f"   Available fields: {list(ts_data.keys())}")
            # Don't skip - continue with defaults
            for field in missing:
                if field == "init_state":
                    ts_data[field] = 0
                elif field == "goal_states":
                    ts_data[field] = [0]

        # Now proceed with normal logic
        var_key = frozenset(inc_vars)
        existing_node_id = self.varset_to_node.get(var_key)

        if existing_node_id is not None and existing_node_id in self.graph:
            logging.info(f"✓ Updating existing node {existing_node_id} with {num_states} states")
            self.graph.nodes[existing_node_id].update(ts_data)
        else:
            # New node
            is_atomic = ts_data.get("iteration", -1) == -1
            if is_atomic:
                node_id = inc_vars[0]
                logging.info(f"✓ Adding atomic node {node_id} for variable {inc_vars[0]}")
            else:
                node_id = self.next_node_id
                self.next_node_id += 1
                logging.info(f"✓ Adding merged node {node_id} for variables {inc_vars}")

            self.graph.add_node(node_id, **ts_data)
            self.varset_to_node[var_key] = node_id

            # Update counter
            if isinstance(node_id, int):
                self.next_node_id = max(self.next_node_id, node_id + 1)

            logging.info(f"   → Node {node_id} has {num_states} states")


    def _rewire_edges(self, old_id: Union[int, str], new_id: Union[int, str]) -> None:
        """
        Moves all incoming and outgoing edges from an old node to a new node.
        """
        # Rewire incoming edges: for every predecessor `p` of `old_id`, add edge `(p, new_id)`.
        if old_id in self.graph:
            for predecessor in list(self.graph.predecessors(old_id)):
                if predecessor != new_id:  # Avoid self-loops with the other merged node
                    self.graph.add_edge(predecessor, new_id)
            # Rewire outgoing edges: for every successor `s` of `old_id`, add edge `(new_id, s)`.
            for successor in list(self.graph.successors(old_id)):
                if successor != new_id:
                    self.graph.add_edge(new_id, successor)

    def display(self) -> None:
        """
        Renders and displays the current state of the graph using matplotlib.

        Note: This is intended for interactive debugging and requires the
        `matplotlib` library to be installed.
        """
        if plt is None:
            logging.warning("matplotlib is not installed. Cannot display graph.")
            return

        plt.figure(figsize=(12, 9))
        pos = nx.spring_layout(self.graph, seed=42)

        labels = {
            n: f"ID: {n}\n|S|={d.get('num_states', '?')}\nIter: {d.get('iteration', '?')}"
            for n, d in self.graph.nodes(data=True)
        }

        nx.draw_networkx(
            self.graph,
            pos,
            labels=labels,
            node_size=1500,
            node_color="lightblue",
            font_size=8,
            arrows=True,
            arrowstyle="-|>",
            arrowsize=15,
        )
        plt.title("Transition System Causal Graph", fontsize=16)
        plt.axis("off")
        plt.tight_layout()
        plt.show()


if __name__ == "__main__":
    # This block serves as a "smoke test" to verify basic functionality.
    # It requires dummy JSON files to be present in the same directory.
    logging.info("--- Running GraphTracker Smoke Test ---")

    # Create dummy files for testing purposes
    DUMMY_CG_FILE = "causal_graph_test.json"
    DUMMY_TS_FILE = "ts_test.json"

    cg_data = {"edges": [{"from": 0, "to": 1}, {"from": 1, "to": 2}]}
    ts_data = [
        {"num_states": 2, "init_state": 0, "goal_states": [1], "incorporated_variables": [0], "iteration": -1},
        {"num_states": 3, "init_state": 1, "goal_states": [2], "incorporated_variables": [1], "iteration": -1},
        {"num_states": 4, "init_state": 2, "goal_states": [0], "incorporated_variables": [2], "iteration": -1},
    ]

    with open(DUMMY_CG_FILE, "w") as f:
        json.dump(cg_data, f)
    with open(DUMMY_TS_FILE, "w") as f:
        json.dump(ts_data, f)

    try:
        # 1. Test initialization
        tracker = GraphTracker(ts_json_path=DUMMY_TS_FILE, cg_json_path=DUMMY_CG_FILE, is_debug=True)
        print("\nInitial Graph Nodes:", list(tracker.graph.nodes()))
        print("Initial Graph Edges:", list(tracker.graph.edges()))
        # tracker.display() # Uncomment for visual inspection

        # 2. Test merging
        if len(tracker.graph.nodes) >= 2:
            nodes_to_merge = [0, 1]
            tracker.merge_nodes(nodes_to_merge)
            print(f"\nGraph Nodes after merging {nodes_to_merge}:", list(tracker.graph.nodes()))
            print("Graph Edges after merging:", list(tracker.graph.edges()))
            # tracker.display()

            # 3. Test f_stats on the new node
            new_node_id = list(tracker.graph.nodes)[-1]
            # Add some dummy f-values to test f_stats
            tracker.graph.nodes[new_node_id]['f_before'] = [10, 20, 30, 40]
            stats = tracker.f_stats(new_node_id)
            print(
                f"\nF-stats for new node {new_node_id}: min={stats[0]}, mean={stats[1]}, max={stats[2]}, std={stats[3]}")

        else:
            print("\nNot enough nodes to test merge.")

        logging.info("--- Smoke Test Completed Successfully ---")

    except Exception as e:
        logging.error(f"--- Smoke Test FAILED: {e} ---")

    finally:
        # Clean up dummy files
        import os

        if os.path.exists(DUMMY_CG_FILE): os.remove(DUMMY_CG_FILE)
        if os.path.exists(DUMMY_TS_FILE): os.remove(DUMMY_TS_FILE)

--------------------------------------------------------------------------------

The file merge_env.py code code is this:
# FILE: merge_env.py

import re
import tempfile
import glob
import os
import json
import time
import subprocess
import traceback
from json import JSONDecoder
from typing import Tuple, Dict, Optional
import datetime
import gymnasium as gym
import shutil
import numpy as np
from gymnasium import spaces
from torch.utils.tensorboard import SummaryWriter
import networkx as nx

from graph_tracker import GraphTracker
from reward_info_extractor import RewardInfoExtractor, MergeInfo, validate_extracted_info
from reward_function_variants import create_reward_function

import logging

logger = logging.getLogger(__name__)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

TB_LOGDIR = "tb_debug"
writer = SummaryWriter(log_dir=TB_LOGDIR)


def _safe_load_list(path: str, retries: int = 60, delay: float = 0.25) -> list:
    """Robustly load the *first* JSON value from `path` with retries."""
    dec = JSONDecoder()
    last_err = None
    for _ in range(retries):
        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as f:
                s = f.read().lstrip()
            if not s:
                time.sleep(delay)
                continue
            obj, _ = dec.raw_decode(s)
            return obj if isinstance(obj, list) else []
        except (OSError, json.JSONDecodeError) as e:
            last_err = e
            time.sleep(delay)
    return []


def write_json_atomic(obj, final_path: str):
    """Write `obj` to JSON at `final_path` atomically."""
    dir_ = os.path.dirname(final_path) or "."
    fd, tmp_path = tempfile.mkstemp(dir=dir_, suffix=".tmp")
    with os.fdopen(fd, "w") as f:
        json.dump(obj, f)
        f.flush()
        os.fsync(f.fileno())
    os.replace(tmp_path, final_path)


def _wait_json_complete(path: str, timeout: float = 180.0, poll: float = 0.25) -> bool:
    """Wait until `path` exists and looks complete."""
    deadline = time.time() + timeout
    last_size = -1
    while time.time() < deadline:
        try:
            if not os.path.exists(path):
                time.sleep(poll)
                continue
            size = os.path.getsize(path)
            if size <= 0:
                time.sleep(poll)
                continue

            with open(path, "rb") as f:
                if size > 8192:
                    f.seek(-8192, os.SEEK_END)
                tail = f.read()

            tail = tail.rstrip(b"\r\n\t ")
            if not tail:
                time.sleep(poll)
                continue

            last_byte = tail[-1]
            looks_closed = last_byte in (ord("]"), ord("}"))

            if looks_closed and size == last_size:
                return True

            last_size = size
            time.sleep(poll)
        except OSError:
            time.sleep(poll)
    return False


class MergeEnv(gym.Env):
    """
    Gym environment wrapping Fast Downward's merge-and-shrink heuristic.
    ✅ FIXED: Now properly stores domain and problem file paths.
    """

    metadata = {"render.modes": []}

    def __init__(
            self,
            domain_file: str,
            problem_file: str,
            max_merges: int = 20,
            debug: bool = False,
            reward_variant: str = 'rich',
            max_states: int = 4000,
            threshold_before_merge: int = 1,
            **reward_kwargs
    ) -> None:
        """
        Initialize the merge-and-shrink learning environment.

        Args:
            domain_file: Absolute or relative path to domain.pddl
            problem_file: Absolute or relative path to problem.pddl
            max_merges: Maximum number of merges allowed per episode
            debug: If True, use in-memory debug mode (no real FD)
            reward_variant: Which reward function to use ('rich', 'astar_search', etc.)
            max_states: Max abstract states for M&S algorithm
            threshold_before_merge: Threshold for triggering shrinking
            **reward_kwargs: Additional kwargs for reward function (w_f_stability, etc.)
        """
        super().__init__()

        # ✅ CRITICAL: Store file paths as ABSOLUTE paths
        self.domain_file = os.path.abspath(domain_file)
        self.problem_file = os.path.abspath(problem_file)

        # ✅ CRITICAL FIX #1: Initialize fd_base_dir (was missing!)
        self.fd_base_dir = os.path.abspath("downward")

        # Verify files exist (fail early)
        if not os.path.exists(self.domain_file):
            raise FileNotFoundError(f"Domain file not found: {self.domain_file}")
        if not os.path.exists(self.problem_file):
            raise FileNotFoundError(f"Problem file not found: {self.problem_file}")

        logger.info(f"Environment initialized with:")
        logger.info(f"  Domain:  {self.domain_file}")
        logger.info(f"  Problem: {self.problem_file}")
        logger.info(f"  FD base: {self.fd_base_dir}")

        # ✅ Store M&S hyperparameters
        self.max_states = max_states
        self.threshold_before_merge = threshold_before_merge
        logger.info(f"  max_states: {self.max_states}")
        logger.info(f"  threshold_before_merge: {self.threshold_before_merge}")

        # ✅ Setup reward function with ALL parameters
        self.max_merges = max(1, max_merges)
        try:
            self.reward_function = create_reward_function(reward_variant, **reward_kwargs)
            logger.info(f"✓ Initialized reward function: {reward_variant}")
        except Exception as e:
            logger.error(f"Failed to initialize reward function '{reward_variant}': {e}")
            raise

        # ✅ Setup signal extraction
        self.reward_info_extractor = RewardInfoExtractor(fd_output_dir=os.path.join(self.fd_base_dir, "fd_output"))
        logger.info("✓ Initialized reward info extractor")

        # ✅ Initialize state variables
        self.current_merge_step = 0
        self.process = None  # FD subprocess
        self.fd_log_file = None
        self.graph_tracker: GraphTracker = None

        # ✅ Setup logging directory
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        os.makedirs("logs", exist_ok=True)
        self.log_path = f"logs/run_{timestamp}.jsonl"

        # ✅ NEW: GNN METADATA COLLECTION
        self.gnn_metadata_dir = os.path.join("downward", "gnn_metadata")
        os.makedirs(self.gnn_metadata_dir, exist_ok=True)
        self.gnn_decisions_log = []  # Collect all GNN decisions in episode

        # ✅ Initialize observation tracking
        self.prev_total_states = 0
        self.max_vars = 1
        self.max_iter = 1
        self.centrality = {}

        # ✅ Define observation and action spaces (for gym.Env)
        self.feat_dim = 19  # Number of node features per node ✅ NEW: Expanded feature set
        # self.observation_space = spaces.Dict({
        #     "x": spaces.Box(0.0, 1.0, shape=(100, self.feat_dim), dtype=np.float32),
        #     "edge_index": spaces.Box(0, 100, shape=(2, 1000), dtype=np.int64),
        #     "num_nodes": spaces.Box(0, 100, shape=(), dtype=np.int32),
        #     "num_edges": spaces.Box(0, 1000, shape=(), dtype=np.int32),
        # })
        # ✅ UPDATED: Add edge features to observation space
        self.observation_space = spaces.Dict({
            "x": spaces.Box(0.0, 1.0, shape=(100, self.feat_dim), dtype=np.float32),
            "edge_index": spaces.Box(0, 100, shape=(2, 1000), dtype=np.int64),
            "edge_features": spaces.Box(  # ✅ NEW
                -1.0, 1.0,
                shape=(1000, 8),
                dtype=np.float32
            ),
            "num_nodes": spaces.Box(0, 100, shape=(), dtype=np.int32),
            "num_edges": spaces.Box(0, 1000, shape=(), dtype=np.int32),
        })
        self.action_space = spaces.Discrete(1000)  # Max 1000 possible merges

        # ✅ Store debug mode
        self.debug = debug

    def reset(self, *, seed=None, options=None) -> Tuple[Dict, Dict]:
        """Reset the env."""
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                self.process.wait(timeout=5.0)
        except Exception:
            pass
        if self.fd_log_file:
            try:
                self.fd_log_file.close()
            except Exception:
                pass
            self.fd_log_file = None

        super().reset(seed=seed)

        # for folder in ("gnn_output", "fd_output"):
        #     d = os.path.join("downward", folder)
        #     if not os.path.isdir(d):
        #         continue
        #     for fname in os.listdir(d):
        #         if folder == "fd_output" and fname == "log.txt":
        #             continue
        #         try:
        #             os.remove(os.path.join(d, fname))
        #         except Exception:
        #             pass

        # ✅ CRITICAL: Ensure output directories exist BEFORE cleaning
        os.makedirs("downward/gnn_output", exist_ok=True)
        os.makedirs("downward/fd_output", exist_ok=True)

        # ✅ Clean old files
        for folder in ("gnn_output", "fd_output"):
            d = os.path.join("downward", folder)
            if not os.path.isdir(d):
                continue
            for fname in os.listdir(d):
                if folder == "fd_output" and fname == "log.txt":
                    continue
                try:
                    fpath = os.path.join(d, fname)
                    if os.path.isfile(fpath):
                        os.remove(fpath)
                        logger.debug(f"Cleaned: {fpath}")
                except Exception as e:
                    logger.warning(f"Could not clean {fpath}: {e}")

        toy_dir = os.environ.get("TOY_TS", None)
        ts_file, cg_file = "merged_transition_systems.json", "causal_graph.json"
        if toy_dir and self.debug:
            ts_path = os.path.join(toy_dir, ts_file)
            cg_path = os.path.join(toy_dir, cg_file)
        else:
            ts_path = cg_path = None
            for attempt in range(1, 4):
                try:
                    ts_path, cg_path = self._handshake_with_fd()
                    break
                except Exception as e:
                    logger.warning(f"⚠️ Handshake attempt {attempt} failed: {e}")
                    time.sleep(1.0)
            if ts_path is None or cg_path is None:
                if self.debug:
                    logger.warning("⚠️ Handshake failed; falling back to toy data")
                    toy_dir = os.environ.get("TOY_TS", "toy")
                    ts_path = os.path.join(toy_dir, ts_file)
                    cg_path = os.path.join(toy_dir, cg_file)
                else:
                    raise RuntimeError("Handshake with FD failed and debug mode is off")

        # ✅ ADD: Export metadata from previous episode
        if self.gnn_decisions_log:
            self._export_episode_metadata()
            self.gnn_decisions_log = []

        self.current_merge_step = 0
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_path = f"logs/run_{ts}.jsonl"

        self.graph_tracker = GraphTracker(ts_json_path=ts_path, cg_json_path=cg_path, is_debug=self.debug)
        G = self.graph_tracker.graph
        self.max_vars = max(
            (len(d.get("incorporated_variables", [])) for _, d in G.nodes(data=True)),
            default=1
        ) or 1
        self.max_iter = max(
            (d.get("iteration", 0) for _, d in G.nodes(data=True)),
            default=0
        ) or 1
        self.centrality = nx.closeness_centrality(G)

        obs = self._get_observation()
        self.prev_total_states = self._count_total_states()
        self.state = obs
        return obs, {}

    def _handshake_with_fd(self) -> Tuple[str, str]:
        """Launch FD and wait for initial JSONs."""
        dw_dir = os.path.abspath("downward")
        if not os.path.isdir(dw_dir):
            raise RuntimeError(f"Downward folder not found: {dw_dir}")

        def _tail(path: str, n: int = 120) -> str:
            if not os.path.exists(path):
                return "(no log file yet)"
            try:
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    lines = f.readlines()
                return "".join(lines[-n:])
            except Exception as e:
                return f"(failed to read log tail: {e})"

        def _robust_copy(src: str, dst: str, tries: int = 80, delay: float = 0.1):
            for _ in range(tries):
                try:
                    with open(src, "rb") as fin, open(dst, "wb") as fout:
                        fout.write(fin.read())
                    return
                except OSError:
                    time.sleep(delay)
            raise RuntimeError(f"Could not copy {src} -> {dst}")

        def _wait_stable(path: str, grace: float = 0.15, timeout: float = 30.0) -> bool:
            end = time.time() + timeout
            last = -1
            while time.time() < end:
                if os.path.exists(path):
                    try:
                        sz = os.path.getsize(path)
                    except OSError:
                        sz = -1
                    if sz > 0 and sz == last:  # File exists and size hasn't changed
                        return True
                    last = sz
                time.sleep(grace)
            return False

        ## STEP 1: Clean up old files
        logger.info("[Handshake] Cleaning up old files...")
        for fname in ("causal_graph.json", "merged_transition_systems.json", "output.sas"):
            p = os.path.join(dw_dir, fname)
            if os.path.exists(p):
                try:
                    os.remove(p)
                except Exception as e:
                    logger.warning(f"⚠️ Could not delete {p}: {e}")

        for subdir_name in ["gnn_output", "fd_output"]:
            subdir_path = os.path.join(dw_dir, subdir_name)
            os.makedirs(subdir_path, exist_ok=True)
            for f in os.listdir(subdir_path):
                try:
                    os.remove(os.path.join(subdir_path, f))
                except Exception:
                    pass

        ## STEP 2: Safely terminate any previous FD process
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                self.process.wait(timeout=3.0)
        except (subprocess.TimeoutExpired, AttributeError):
            if hasattr(self, 'process') and self.process:
                self.process.kill()
        except Exception:
            pass

        ## STEP 3: Build the FD command with GNN merge strategy
        # ✅ NEW: Build FD command from scratch using stored file paths
        fd_translate_bin = os.path.join(dw_dir, "builds/release/bin/translate/translate.py")
        fd_downward_exe = os.path.join(dw_dir, "builds/release/bin/downward.exe")

        # ✅ Use stored paths (absolute)
        domain_path = self.domain_file
        problem_path = self.problem_file

        logger.info(f"[Handshake] Building FD command:")
        logger.info(f"  Domain:  {domain_path}")
        logger.info(f"  Problem: {problem_path}")

        # ✅ Build complete FD command with merge_gnn strategy
        cmd = (
            f'python "{fd_translate_bin}" "{domain_path}" "{problem_path}" '
            f'--sas-file output.sas && '
            f'"{fd_downward_exe}" '
            r'--search "astar(merge_and_shrink('
            r'merge_strategy=merge_gnn(),'  # ✅ Uses GNN for merge decisions
            r'shrink_strategy=shrink_bisimulation(greedy=false,at_limit=return),'
            r'label_reduction=exact(before_shrinking=true,before_merging=false),'
            f'max_states={self.max_states},'  # ✅ Use stored parameter
            f'threshold_before_merge={self.threshold_before_merge}'  # ✅ Use stored parameter
            r'))" < output.sas'
        )

        logger.info(f"[Handshake] FD Command: {cmd[:150]}...")

        ## STEP 4: Launch FD process
        fd_dir = os.path.join(dw_dir, "fd_output")
        log_path = os.path.join(fd_dir, "log.txt")
        self.fd_log_file = open(log_path, "w", buffering=1, encoding="utf-8")

        self.process = subprocess.Popen(
            cmd,
            shell=True,
            cwd=dw_dir,
            stdout=self.fd_log_file,
            stderr=self.fd_log_file,
        )

        ## STEP 5: Wait for translator
        logger.info("[Handshake] Waiting for translator to produce 'output.sas'...")
        sas_path = os.path.join(dw_dir, "output.sas")
        if not _wait_stable(sas_path, timeout=60.0):
            tail = _tail(log_path)
            raise RuntimeError(f"Translator failed to produce output.sas.\nLog tail:\n{tail}")
        logger.info("[Handshake] 'output.sas' is ready.")

        ## STEP 6: Wait for initial JSONs
        logger.info("[Handshake] Waiting for planner to produce initial JSONs...")
        cg_root = os.path.join(dw_dir, "causal_graph.json")
        ts_root = os.path.join(dw_dir, "merged_transition_systems.json")
        start, timeout_s = time.time(), 240.0

        while not (os.path.exists(cg_root) and os.path.exists(ts_root)):
            if self.process and (self.process.poll() is not None):
                tail = _tail(log_path)
                raise RuntimeError(
                    f"FD exited early (code {self.process.returncode}) while waiting for JSONs.\nLog tail:\n{tail}")
            if time.time() - start > timeout_s:
                tail = _tail(log_path)
                raise RuntimeError(f"Timeout waiting for initial JSONs after {timeout_s:.0f}s.\nLog tail:\n{tail}")
            time.sleep(0.1)

        ## STEP 7: Copy and verify
        logger.info("[Handshake] Initial JSONs found. Verifying and copying...")
        if not (_wait_stable(cg_root) and _wait_stable(ts_root)):
            tail = _tail(log_path)
            raise RuntimeError(f"Root JSONs never stabilized.\nLog tail:\n{tail}")

        fd_cg = os.path.join(fd_dir, "causal_graph.json")
        fd_ts = os.path.join(fd_dir, "merged_transition_systems.json")

        dec = JSONDecoder()
        for _ in range(80):
            _robust_copy(cg_root, fd_cg)
            _robust_copy(ts_root, fd_ts)
            try:
                with open(fd_ts, "r", encoding="utf-8", errors="ignore") as f:
                    s = f.read().lstrip()
                if not s:
                    time.sleep(0.1)
                    continue
                obj, _ = dec.raw_decode(s)
                break
            except json.JSONDecodeError:
                time.sleep(0.1)
        else:
            tail = _tail(log_path)
            raise RuntimeError("Copied TS JSON is not parseable.\n" + tail)

        logger.info("[Handshake] Handshake complete.")
        return fd_ts, fd_cg

    def _log_step(self, src: int, tgt: int, info: dict, reward: float, done: bool):
        """Append merge-step metrics to log file."""
        total_states = self._count_total_states()

        entry = {
            "step": int(self.current_merge_step),
            "merge_choice": [int(src), int(tgt)],
            "plan_cost": int(info.get("plan_cost", 0)),
            "num_expansions": int(info.get("num_expansions", 0)),
            "num_significant_f_changes": int(info.get("num_significant_f_changes", 0)),
            "delta_states": int(info.get("delta_states", 0)),
            "total_states": int(total_states),
            "reward": float(reward),
            "done": bool(done),
            "timestamp": time.time(),
        }

        with open(self.log_path, "a") as f:
            json.dump(entry, f)
            f.write("\n")

    def step(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        """✅ FIXED: Action handling with complete validation."""
        try:
            # Check if FD process crashed
            if self.process is not None:
                ret = self.process.poll()
                if ret is not None and ret < 10:
                    print(f"[ERROR] FD process exited with code {ret}")
                    return self.state, 0.0, True, False, {"error": "process_crashed"}

            # Get current edges
            edges = list(self.graph_tracker.graph.edges)
            if not edges:
                print("[WARNING] No edges available for merging")
                return self.state, 0.0, True, False, {"error": "no_edges"}

            # ✅ FIX: Action validation and clamping
            action = int(action)
            num_valid_edges = len(edges)

            # Clamp to valid range: [0, num_valid_edges-1]
            action_idx = max(0, min(action % max(num_valid_edges, 1), num_valid_edges - 1))

            print(f"[DEBUG] Action {action} → index {action_idx} (edges: {num_valid_edges})")

            src, tgt = edges[action_idx]

            # ✅ VALIDATION: Check nodes exist and are different
            if src == tgt:
                print(f"[ERROR] Self-merge attempted: {src} == {tgt}")
                return self.state, 0.0, True, False, {"error": "self_merge"}

            if src not in self.graph_tracker.graph or tgt not in self.graph_tracker.graph:
                print(f"[ERROR] Merge nodes invalid: ({src}, {tgt})")
                return self.state, 0.0, True, False, {"error": "invalid_nodes"}

            if self.debug:
                # Need to extract (src, tgt) for debug mode
                edges = list(self.graph_tracker.graph.edges())
                action_idx = max(0, min(int(action) % max(len(edges), 1), len(edges) - 1))
                src, tgt = edges[action_idx]
                return self._step_debug(src, tgt)
            else:
                return self._step_real(action)  # ✅ Pass action, not (src, tgt)

        except Exception as e:
            print(f"[ERROR] Exception in step(): {e}")
            import traceback
            traceback.print_exc()
            return self.state, 0.0, True, False, {"error": str(e)}

    def _step_debug(self, src: int, tgt: int):
        """
        ✅ ENHANCED: In-memory merge (debug mode) with detailed logging.

        Shows every step of the merge process with:
        - State before and after
        - Computation details
        - Graph metrics
        - Reward calculation
        """
        logger.info("\n" + "=" * 90)
        logger.info(f"STEP {self.current_merge_step}: DEBUG MODE MERGE")
        logger.info("=" * 90)

        try:
            # ====================================================================
            # PHASE 1: ENTRY LOGGING & VALIDATION
            # ====================================================================

            logger.info("\n[PHASE 1] ENTRY VALIDATION")
            logger.info(f"  Merge indices: ({src}, {tgt})")
            logger.info(f"  Current merge step: {self.current_merge_step}")
            logger.info(f"  Max merges allowed: {self.max_merges}")

            # ====================================================================
            # PHASE 2: PRE-MERGE STATE SNAPSHOT
            # ====================================================================

            logger.info("\n[PHASE 2] PRE-MERGE STATE SNAPSHOT")

            # Count total states before merge
            old_total = self._count_total_states()
            logger.info(f"  Total states before merge: {old_total}")

            # Log node details
            G = self.graph_tracker.graph
            logger.info(f"  Num nodes before: {len(G.nodes)}")
            logger.info(f"  Num edges before: {len(G.edges)}")

            # Log specific nodes being merged
            if src in G.nodes and tgt in G.nodes:
                src_data = G.nodes[src]
                tgt_data = G.nodes[tgt]
                logger.info(f"\n  Node {src} (source):")
                logger.info(f"    - num_states: {src_data.get('num_states', 'N/A')}")
                logger.info(f"    - iteration: {src_data.get('iteration', 'N/A')}")
                logger.info(f"    - incorporated_variables: {src_data.get('incorporated_variables', 'N/A')}")

                logger.info(f"\n  Node {tgt} (target):")
                logger.info(f"    - num_states: {tgt_data.get('num_states', 'N/A')}")
                logger.info(f"    - iteration: {tgt_data.get('iteration', 'N/A')}")
                logger.info(f"    - incorporated_variables: {tgt_data.get('incorporated_variables', 'N/A')}")

            # ====================================================================
            # PHASE 3: PERFORM MERGE
            # ====================================================================

            logger.info("\n[PHASE 3] PERFORMING MERGE")
            logger.info(f"  Calling graph_tracker.merge_nodes([{src}, {tgt}])...")

            self.graph_tracker.merge_nodes([src, tgt])
            self.current_merge_step += 1

            logger.info(f"  ✓ Merge completed, current_merge_step = {self.current_merge_step}")

            # ====================================================================
            # PHASE 4: POST-MERGE STATE SNAPSHOT
            # ====================================================================

            logger.info("\n[PHASE 4] POST-MERGE STATE SNAPSHOT")

            new_total = self._count_total_states()
            delta_states = new_total - old_total

            logger.info(f"  Total states after merge: {new_total}")
            logger.info(f"  Delta states (new - old): {delta_states}")
            logger.info(f"  State explosion ratio: {delta_states / max(old_total, 1):.4f}")

            # Log updated graph
            G = self.graph_tracker.graph
            logger.info(f"  Num nodes after: {len(G.nodes)}")
            logger.info(f"  Num edges after: {len(G.edges)}")

            # ====================================================================
            # PHASE 5: UPDATE GRAPH METRICS
            # ====================================================================

            logger.info("\n[PHASE 5] UPDATE GRAPH METRICS")

            self.max_vars = max(
                (len(d.get("incorporated_variables", [])) for _, d in G.nodes(data=True)),
                default=1
            ) or 1
            logger.info(f"  max_vars: {self.max_vars}")

            self.max_iter = max(
                (d.get("iteration", 0) for _, d in G.nodes(data=True)),
                default=0
            ) or 1
            logger.info(f"  max_iter: {self.max_iter}")

            self.centrality = nx.closeness_centrality(G)
            logger.info(f"  Centrality computed for {len(self.centrality)} nodes")

            # ====================================================================
            # PHASE 6: COMPUTE REWARD
            # ====================================================================

            logger.info("\n[PHASE 6] REWARD COMPUTATION")

            reward = -max(abs(delta_states), 0.1)
            logger.info(f"  Reward formula: -max(|delta_states|, 0.1)")
            logger.info(f"  Computed reward: {reward:.4f}")

            # ====================================================================
            # PHASE 7: TENSORBOARD LOGGING
            # ====================================================================

            logger.info("\n[PHASE 7] TENSORBOARD LOGGING")

            try:
                writer.add_scalar("env/num_nodes", len(self.graph_tracker.graph.nodes),
                                  self.current_merge_step)
                logger.info(f"  ✓ Logged num_nodes: {len(self.graph_tracker.graph.nodes)}")

                writer.add_scalar("env/num_edges", len(self.graph_tracker.graph.edges),
                                  self.current_merge_step)
                logger.info(f"  ✓ Logged num_edges: {len(self.graph_tracker.graph.edges)}")

                writer.add_scalar("env/total_states", new_total, self.current_merge_step)
                logger.info(f"  ✓ Logged total_states: {new_total}")

                writer.add_scalar("env/delta_states", delta_states, self.current_merge_step)
                logger.info(f"  ✓ Logged delta_states: {delta_states}")

                writer.add_scalar("env/reward", reward, self.current_merge_step)
                logger.info(f"  ✓ Logged reward: {reward:.4f}")
            except Exception as e:
                logger.warning(f"  ⚠️ TensorBoard logging error: {e}")

            # ====================================================================
            # PHASE 8: BUILD OBSERVATION
            # ====================================================================

            logger.info("\n[PHASE 8] BUILD OBSERVATION")

            obs = self._get_observation()
            logger.info(f"  Observation built successfully")
            logger.info(f"  - x shape: {obs['x'].shape}")
            logger.info(f"  - edge_index shape: {obs['edge_index'].shape}")
            logger.info(f"  - num_nodes: {obs['num_nodes']}")
            logger.info(f"  - num_edges: {obs['num_edges']}")

            # ====================================================================
            # PHASE 9: TERMINATION CHECK
            # ====================================================================

            logger.info("\n[PHASE 9] TERMINATION CHECK")

            num_remaining = len(G.nodes)
            done = (num_remaining <= 1) or (self.current_merge_step >= self.max_merges)

            logger.info(f"  Num remaining nodes: {num_remaining}")
            logger.info(f"  Current step: {self.current_merge_step} / {self.max_merges}")
            logger.info(f"  Done: {done}")

            if done:
                if num_remaining <= 1:
                    logger.info(f"  Reason: Only {num_remaining} node(s) left")
                else:
                    logger.info(f"  Reason: Max merges reached")

            # ====================================================================
            # PHASE 10: BUILD INFO DICT
            # ====================================================================

            logger.info("\n[PHASE 10] BUILD INFO DICT")

            info = {
                "num_nodes": len(self.graph_tracker.graph.nodes),
                "num_edges": len(self.graph_tracker.graph.edges),
                "total_states": new_total,
                "delta_states": delta_states,
                "plan_cost": 0,
                "num_expansions": 0,
                "error": None,
            }
            logger.info(f"  Info dict built with {len(info)} keys")

            # ====================================================================
            # PHASE 11: LOG STEP & UPDATE STATE
            # ====================================================================

            logger.info("\n[PHASE 11] LOGGING & STATE UPDATE")

            self._log_step(src, tgt, info, reward, done)
            logger.info(f"  ✓ Step logged to file")

            self.state = obs
            logger.info(f"  ✓ State updated")

            # ====================================================================
            # PHASE 12: RETURN
            # ====================================================================

            logger.info("\n[PHASE 12] RETURN")
            logger.info(f"  Returning: obs, reward={reward:.4f}, done={done}, info")
            logger.info("=" * 90 + "\n")



            return obs, reward, done, False, info

        except Exception as e:
            logger.error(f"\n❌ ERROR in _step_debug: {e}")
            logger.error(f"Traceback:\n{traceback.format_exc()}")
            logger.info("=" * 90 + "\n")
            return self.state, 0.0, True, False, {"error": str(e)}

    def _step_real(self, action: int) -> Tuple[Dict, float, bool, bool, Dict]:
        """
        ✅ ENHANCED: Execute one merge step in REAL mode with BAD MERGE DETECTION.

        This function now:
        1. Validates action and extracts merge pair
        2. Writes merge decision to FD
        3. Waits for FD acknowledgment
        4. Waits for updated TS files
        5. Updates graph tracker
        6. ✅ DETECTS BAD MERGE SITUATIONS
        7. Extracts signals and computes reward
        8. Returns result with comprehensive logging
        """

        logger.info("\n" + "=" * 90)
        logger.info(f"STEP {self.current_merge_step}: REAL MODE MERGE WITH BAD MERGE DETECTION")
        logger.info("=" * 90)

        try:
            # ====================================================================
            # PHASE 1: VALIDATE INPUT & EXTRACT MERGE PAIR
            # ====================================================================

            logger.info("\n[PHASE 1] INPUT VALIDATION & ACTION EXTRACTION")
            logger.info(f"  Input action: {action} (type: {type(action).__name__})")

            if not hasattr(self, 'graph_tracker') or self.graph_tracker is None:
                raise RuntimeError("graph_tracker not initialized")

            # Get current edges
            edges = list(self.graph_tracker.graph.edges())
            logger.info(f"  Available edges in graph: {len(edges)}")

            if not edges:
                logger.warning("  ⚠️ No edges available for merging")
                return self.state, 0.0, True, False, {"error": "no_edges"}

            # ✅ FIX: Action validation and extraction
            action = int(action)
            num_valid_edges = len(edges)
            action_idx = max(0, min(action % max(num_valid_edges, 1), num_valid_edges - 1))

            logger.info(f"  Action index calculation:")
            logger.info(
                f"    - action % max(num_valid_edges, 1) = {action} % {max(num_valid_edges, 1)} = {action % max(num_valid_edges, 1)}")
            logger.info(f"    - clamped to [0, {num_valid_edges - 1}]: {action_idx}")

            node_a, node_b = edges[action_idx]
            logger.info(f"  ✓ Extracted merge pair from edge {action_idx}: ({node_a}, {node_b})")

            # ✅ VALIDATION: Check nodes exist and are different
            if node_a == node_b:
                logger.error(f"  ❌ ERROR: Self-merge attempted: {node_a} == {node_b}")
                return self.state, 0.0, True, False, {"error": "self_merge"}

            if node_a not in self.graph_tracker.graph or node_b not in self.graph_tracker.graph:
                logger.error(f"  ❌ ERROR: Merge nodes invalid: ({node_a}, {node_b})")
                return self.state, 0.0, True, False, {"error": "invalid_nodes"}

            src = node_a
            tgt = node_b

            logger.info(f"  ✓ Validation passed")

            # ====================================================================
            # PHASE 2: PRE-MERGE DIAGNOSTICS & DECISION LOGGING
            # ====================================================================

            logger.info(f"\n[PHASE 2] PRE-MERGE STATE ANALYSIS")

            pre_merge_total = sum(d.get('num_states', 0) for _, d in self.graph_tracker.graph.nodes(data=True))
            logger.info(f"    - Total states before merge: {pre_merge_total}")
            logger.info(f"    - Num nodes: {len(self.graph_tracker.graph.nodes)}")
            logger.info(f"    - Num edges: {len(self.graph_tracker.graph.edges)}")

            # Get node-specific data
            src_data = self.graph_tracker.graph.nodes[src]
            tgt_data = self.graph_tracker.graph.nodes[tgt]
            src_size = src_data.get('num_states', 1)
            tgt_size = tgt_data.get('num_states', 1)
            expected_product = src_size * tgt_size

            logger.info(f"\n    Nodes being merged:")
            logger.info(f"      - Node {src}: {src_size} states")
            logger.info(f"      - Node {tgt}: {tgt_size} states")
            logger.info(f"      - Expected product: {expected_product} states")

            # ====================================================================
            # PHASE 3: WRITE MERGE DECISION
            # ====================================================================

            logger.info(f"\n[PHASE 3] WRITE MERGE DECISION")

            merge_decision = {
                "iteration": self.current_merge_step,
                "merge_pair": [int(src), int(tgt)],
                "timestamp": time.time()
            }

            gnn_output_dir = os.path.abspath(os.path.join(self.fd_base_dir, "gnn_output"))
            os.makedirs(gnn_output_dir, exist_ok=True)

            merge_decision_path = os.path.join(gnn_output_dir, f"merge_{self.current_merge_step}.json")

            # ✅ Write atomically
            import tempfile
            try:
                fd, temp_path = tempfile.mkstemp(
                    dir=gnn_output_dir,
                    suffix=".json",
                    prefix=f"merge_{self.current_merge_step}_"
                )
                with os.fdopen(fd, 'w') as f:
                    json.dump(merge_decision, f, indent=2)
                    f.flush()
                    os.fsync(f.fileno())

                os.replace(temp_path, merge_decision_path)
                logger.info(f"  ✓ Wrote merge decision: {merge_decision_path}")

            except Exception as e:
                logger.error(f"  ❌ Failed to write merge decision: {e}")
                raise

            # ====================================================================
            # PHASE 4: WAIT FOR FD ACKNOWLEDGMENT
            # ====================================================================

            logger.info(f"\n[PHASE 4] WAIT FOR FD ACKNOWLEDGMENT")

            ack_path = os.path.join(
                self.fd_base_dir,
                "fd_output",
                f"gnn_ack_{self.current_merge_step}.json"
            )

            logger.info(f"  Expected ACK file: {ack_path}")

            start_time = time.time()
            ACK_TIMEOUT = 30.0

            while time.time() - start_time < ACK_TIMEOUT:
                elapsed = time.time() - start_time

                if os.path.exists(ack_path):
                    try:
                        with open(ack_path) as f:
                            ack_data = json.load(f)
                        elapsed = time.time() - start_time
                        logger.info(f"  ✓ ACK received after {elapsed:.2f}s")
                        break
                    except json.JSONDecodeError:
                        time.sleep(0.1)
                        continue

                if int(elapsed) > 0 and int(elapsed) % 5 == 0:
                    logger.debug(f"  Still waiting... ({elapsed:.0f}s)")

                time.sleep(0.1)
            else:
                elapsed = time.time() - start_time
                raise TimeoutError(
                    f"FD did not acknowledge merge within {ACK_TIMEOUT}s (waited {elapsed:.1f}s)")

            # ====================================================================
            # PHASE 5: WAIT FOR UPDATED TS FILE
            # ====================================================================

            logger.info(f"\n[PHASE 5] WAIT FOR UPDATED TS FILE")

            ts_path = os.path.join(
                self.fd_base_dir,
                "fd_output",
                f"ts_{self.current_merge_step}.json"
            )

            logger.info(f"  Expected TS file: {ts_path}")

            start_time = time.time()
            TS_TIMEOUT = 60.0

            while time.time() - start_time < TS_TIMEOUT:
                elapsed = time.time() - start_time

                if os.path.exists(ts_path):
                    try:
                        with open(ts_path) as f:
                            ts_data = json.load(f)
                        elapsed = time.time() - start_time
                        logger.info(f"  ✓ TS file received after {elapsed:.2f}s")
                        break
                    except (json.JSONDecodeError, IOError):
                        time.sleep(0.2)
                        continue

                if int(elapsed) > 0 and int(elapsed) % 10 == 0:
                    logger.debug(f"  Still waiting... ({elapsed:.0f}s)")

                time.sleep(0.2)
            else:
                elapsed = time.time() - start_time
                raise TimeoutError(
                    f"TS file not produced within {TS_TIMEOUT}s (waited {elapsed:.1f}s)")

            # ====================================================================
            # PHASE 6: UPDATE GRAPH TRACKER
            # ====================================================================

            logger.info(f"\n[PHASE 6] UPDATE GRAPH TRACKER")
            logger.info(f"  Calling graph_tracker.merge_nodes([{src}, {tgt}])...")

            try:
                # ✅ --- WRAP THIS CALL ---
                self.graph_tracker.merge_nodes([src, tgt])
                logger.info(f"  ✓ Merged in graph tracker")
            except ValueError as e_merge:
                # ✅ --- HANDLE THE REJECTED MERGE ---
                logger.error(f"  ❌ Graph merge failed (rejected by tracker): {e_merge}")
                # Treat this as a failed step - return negative reward and terminate episode
                reward = -2.0  # Assign a strong negative reward for invalid merge attempt
                done = True
                info = {"error": "merge_rejected", "message": str(e_merge)}
                obs = self._get_observation()  # Get current observation
                self._log_step(src, tgt, info, reward, done)  # Log the failure
                return obs, reward, done, False, info  # Return immediately
                # ✅ --- END HANDLING ---
            except Exception as e:  # Catch other potential errors during merge
                logger.error(f"  ❌ Graph merge failed: {e}")
                raise  # Re-raise unexpected errors

            # ====================================================================
            # PHASE 7: EXTRACT SIGNALS & COMPUTE REWARD
            # ====================================================================

            logger.info(f"\n[PHASE 7] EXTRACT SIGNALS & COMPUTE REWARD")
            logger.info(f"  Calling reward_info_extractor.extract_merge_info(iteration={self.current_merge_step})...")

            try:
                merge_info = self.reward_info_extractor.extract_merge_info(
                    iteration=self.current_merge_step,
                    timeout=10.0
                )

                if merge_info is None:
                    logger.warning(f"  ⚠️ Failed to extract merge info, using defaults")
                    reward = 0.0
                else:
                    logger.info(f"  ✓ Merge info extracted successfully")

                    # ✅ ENHANCED: Log extracted signals with bad merge context
                    logger.info(f"\n  Extracted signals:")
                    logger.info(f"    - iteration: {merge_info.iteration}")
                    logger.info(f"    - states_before: {merge_info.states_before}")
                    logger.info(f"    - states_after: {merge_info.states_after}")
                    logger.info(f"    - delta_states: {merge_info.delta_states}")
                    logger.info(f"    - f_value_stability: {merge_info.f_value_stability:.4f}")
                    logger.info(f"    - num_significant_f_changes: {merge_info.num_significant_f_changes}")
                    logger.info(f"    - state_explosion_penalty: {merge_info.state_explosion_penalty:.4f}")
                    logger.info(f"    - nodes_expanded: {merge_info.nodes_expanded}")
                    logger.info(f"    - search_depth: {merge_info.search_depth}")
                    logger.info(f"    - solution_cost: {merge_info.solution_cost}")
                    logger.info(f"    - branching_factor: {merge_info.branching_factor:.4f}")
                    logger.info(f"    - solution_found: {merge_info.solution_found}")

                    # ✅ NEW: Pre-reward bad merge diagnostics
                    logger.info(f"\n  [BAD MERGE DIAGNOSTIC CHECKS]:")

                    # Check 1: State explosion
                    if merge_info.states_after > expected_product * 1.5:
                        logger.warning(
                            f"    ⚠️  State explosion detected: {merge_info.states_after} > {expected_product * 1.5:.0f}")

                    # Check 2: Goal reachability
                    goal_reachable = any(f != float('inf') and f < 1_000_000_000 for f in merge_info.f_after)
                    if not goal_reachable:
                        logger.error(f"    ❌ CRITICAL: Goal unreachable after merge!")

                    # Check 3: F-stability
                    if merge_info.f_value_stability < 0.3:
                        logger.warning(f"    ⚠️  Poor F-stability: {merge_info.f_value_stability:.4f}")

                    # Check 4: Unreachable states
                    unreachable_count = sum(1 for f in merge_info.f_after if f == float('inf') or f >= 1_000_000_000)
                    unreachable_ratio = unreachable_count / max(merge_info.states_after, 1)
                    if unreachable_ratio > 0.7:
                        logger.warning(f"    ⚠️  High unreachability: {unreachable_ratio * 100:.1f}%")

                    # Check 5: Branching factor
                    if merge_info.branching_factor > 8.0:
                        logger.warning(f"    ⚠️  High branching factor: {merge_info.branching_factor:.4f}")

                    logger.info(f"\n  Computing reward:")
                    logger.info(f"    - Reward function: {self.reward_function.name}")
                    logger.info(f"    - Calling compute() with merge_info and signals...")

                    reward = self.reward_function.compute(
                        merge_info=merge_info,
                        search_expansions=merge_info.nodes_expanded,
                        plan_cost=merge_info.solution_cost,
                        is_terminal=False
                    )

                    logger.info(f"    ✓ Reward computed: {reward:.4f}")

                    # Log component breakdown
                    components = self.reward_function.get_components_dict()
                    logger.info(f"\n  Reward component breakdown:")
                    for key, val in components.items():
                        logger.info(f"    - {key:<30} {val:.4f}")

            except Exception as e:
                logger.warning(f"  ⚠️ Reward computation failed: {e}")
                logger.warning(f"  Using reward = 0.0")
                reward = 0.0

            # ====================================================================
            # PHASE 8: CHECK EPISODE TERMINATION
            # ====================================================================

            logger.info(f"\n[PHASE 8] TERMINATION CHECK")

            num_remaining = self.graph_tracker.graph.number_of_nodes()
            logger.info(f"  Num remaining nodes: {num_remaining}")
            logger.info(f"  Current merge step: {self.current_merge_step} / {self.max_merges}")

            done = (num_remaining <= 1) or (self.current_merge_step >= self.max_merges - 1)
            logger.info(f"  Done: {done}")

            if done:
                if num_remaining <= 1:
                    logger.info(f"  Reason: Only {num_remaining} node(s) remaining")
                if self.current_merge_step >= self.max_merges - 1:
                    logger.info(f"  Reason: Max merges reached")

            # ====================================================================
            # PHASE 9: BUILD OBSERVATION
            # ====================================================================

            logger.info(f"\n[PHASE 9] BUILD OBSERVATION")

            try:
                obs = self._get_observation()
                logger.info(f"  ✓ Observation built successfully")
                logger.info(f"    - x shape: {obs['x'].shape}")
                logger.info(f"    - edge_index shape: {obs['edge_index'].shape}")
                logger.info(f"    - num_nodes: {obs['num_nodes']}")
                logger.info(f"    - num_edges: {obs['num_edges']}")
            except Exception as e:
                logger.error(f"  ❌ Failed to build observation: {e}")
                obs = self._get_observation()

            # ====================================================================
            # PHASE 10: INCREMENT STEP COUNTER
            # ====================================================================

            logger.info(f"\n[PHASE 10] STEP COUNTER INCREMENT")

            logger.info(f"  Before: current_merge_step = {self.current_merge_step}")
            self.current_merge_step += 1
            logger.info(f"  After: current_merge_step = {self.current_merge_step}")

            # ====================================================================
            # PHASE 11: BUILD INFO DICT & LOG
            # ====================================================================

            logger.info(f"\n[PHASE 11] BUILD INFO DICT & LOG STEP")

            info = {
                "merge_pair": [int(src), int(tgt)],
                "num_nodes": num_remaining,
                "step": self.current_merge_step - 1,
            }

            if merge_info is not None:
                from validate_merge_signals import validate_merge_signals
                is_valid, issues = validate_merge_signals(merge_info)
                if not is_valid:
                    logger.error(f"[SIGNAL VALIDATION] ✗ Merge signals invalid: {issues}")
                    merge_info = None

                info.update(merge_info.to_dict())
                logger.info(f"  ✓ Added merge_info to info dict")

            self._log_step(src, tgt, info, reward, done)
            logger.info(f"  ✓ Step logged")

            # ====================================================================
            # PHASE 12: RETURN
            # ====================================================================

            # In MergeEnv.step(), after computing reward:

            # ✅ ADD THIS BEFORE RETURNING
            self._save_gnn_decision_metadata(
                merge_step=self.current_merge_step,
                action=action,
                src=src,
                tgt=tgt,
                obs=obs,
                reward=reward,
                info=info
            )

            logger.info(f"\n[PHASE 12] RETURN RESULT")
            logger.info(f"  Returning:")
            logger.info(f"    - obs: shape {obs['x'].shape}")
            logger.info(f"    - reward: {reward:.4f}")
            logger.info(f"    - done: {done}")
            logger.info(f"    - truncated: False")
            logger.info(f"    - info: {len(info)} keys")
            logger.info("=" * 90 + "\n")

            return obs, float(reward), done, False, info

        except Exception as e:
            logger.error(f"\n❌ STEP FAILED: {e}")
            logger.error(f"Traceback:\n{traceback.format_exc()}")
            logger.info("=" * 90 + "\n")
            return self.state, 0.0, True, False, {"error": str(e)}

    def _wait_for_json_file_safe(self, path: str, step: int, timeout_seconds: float = 60.0) -> Optional[Dict]:
        """
        ✅ ROBUST: Wait for JSON file with proper validation and diagnostics.

        NOW WITH PHASE TRACKING to identify exactly where handshake breaks down.

        Returns: Parsed JSON dict, or None if timeout/error occurs
        """
        start_time = time.time()
        last_size = -1
        last_modified = -1
        consecutive_parse_errors = 0
        max_consecutive_errors = 3

        # ✅ Track which phase of handshake we're in
        file_basename = os.path.basename(path)

        if "gnn_ack" in file_basename:
            phase_name = "ACK"
            phase_desc = "FD acknowledging merge decision"
        elif "ts_" in file_basename:
            phase_name = "TS"
            phase_desc = "Updated transition system"
        elif "merge_before" in file_basename:
            phase_name = "BEFORE"
            phase_desc = "Pre-merge metrics"
        elif "merge_after" in file_basename:
            phase_name = "AFTER"
            phase_desc = "Post-merge metrics"
        else:
            phase_name = "DATA"
            phase_desc = "Data file"

        logger.info(f"[Step {step}] [PHASE: {phase_name}] Starting wait...")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Waiting for: {path}")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Description: {phase_desc}")
        logger.info(f"[Step {step}] [PHASE: {phase_name}] Timeout: {timeout_seconds}s")

        while time.time() - start_time < timeout_seconds:
            elapsed = time.time() - start_time

            if not os.path.exists(path):
                if int(elapsed) > 0 and int(elapsed) % 15 == 0:  # Every 15 seconds
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] Still waiting... ({elapsed:.0f}s)")
                time.sleep(0.5)
                continue

            try:
                # ✅ Check file size and modification time
                current_size = os.path.getsize(path)
                current_mtime = os.path.getmtime(path)

                if current_size == 0:
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] File exists but EMPTY (0 bytes)")
                    consecutive_parse_errors += 1
                    if consecutive_parse_errors >= max_consecutive_errors:
                        logger.error(
                            f"[Step {step}] [PHASE: {phase_name}] File empty for {consecutive_parse_errors} checks - giving up")
                        return None
                    time.sleep(1.0)
                    continue

                # ✅ Wait for file to stabilize (size and mtime haven't changed)
                if current_size == last_size and current_mtime == last_modified:
                    logger.debug(f"[Step {step}] [PHASE: {phase_name}] File stable at {current_size} bytes, parsing...")

                    try:
                        with open(path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        if not content.strip():
                            logger.warning(f"[Step {step}] [PHASE: {phase_name}] File content is empty/whitespace")
                            consecutive_parse_errors += 1
                            time.sleep(1.0)
                            continue

                        # ✅ Validate JSON structure
                        content_stripped = content.strip()
                        if not (content_stripped.startswith('{') or content_stripped.startswith('[')):
                            logger.error(
                                f"[Step {step}] [PHASE: {phase_name}] Invalid JSON: doesn't start with {{ or [")
                            logger.error(f"[Step {step}] [PHASE: {phase_name}] First 100 chars: {content[:100]}")
                            consecutive_parse_errors += 1
                            time.sleep(1.0)
                            continue

                        # ✅ Try to parse
                        data = json.loads(content)

                        logger.info(
                            f"[Step {step}] [PHASE: {phase_name}] ✅ Successfully parsed JSON ({current_size} bytes)")
                        logger.info(f"[Step {step}] [PHASE: {phase_name}] Elapsed time: {elapsed:.1f}s")
                        consecutive_parse_errors = 0  # Reset on success
                        return data

                    except json.JSONDecodeError as e:
                        logger.warning(
                            f"[Step {step}] [PHASE: {phase_name}] JSON parse error (line {e.lineno}, col {e.colno}): {e.msg}")
                        logger.debug(f"[Step {step}] [PHASE: {phase_name}] Content preview: {content[:200]}")
                        consecutive_parse_errors += 1

                        if consecutive_parse_errors >= max_consecutive_errors:
                            logger.error(
                                f"[Step {step}] [PHASE: {phase_name}] JSON parsing failed {max_consecutive_errors} times - giving up")
                            return None

                        time.sleep(2.0)
                        continue

                else:
                    # File size or mtime changed - still writing
                    logger.debug(
                        f"[Step {step}] [PHASE: {phase_name}] File size changing: {last_size} → {current_size} bytes (still writing)")
                    last_size = current_size
                    last_modified = current_mtime
                    consecutive_parse_errors = 0
                    time.sleep(0.5)
                    continue

            except (OSError, IOError) as e:
                logger.debug(f"[Step {step}] [PHASE: {phase_name}] File I/O error: {e}")
                consecutive_parse_errors += 1
                time.sleep(1.0)
                continue

            except Exception as e:
                logger.error(f"[Step {step}] [PHASE: {phase_name}] Unexpected error: {e}", exc_info=True)
                return None

            # Check if FD died
            if self.process and self.process.poll() is not None:
                rc = self.process.returncode
                logger.error(f"[Step {step}] [PHASE: {phase_name}] ❌ FD process DIED with code {rc}")
                logger.error(
                    f"[Step {step}] [PHASE: {phase_name}] FD crashed before producing {os.path.basename(path)}")
                return None

        # TIMEOUT
        logger.error(
            f"[Step {step}] [PHASE: {phase_name}] ❌ TIMEOUT after {timeout_seconds}s waiting for {os.path.basename(path)}")
        logger.error(f"[Step {step}] [PHASE: {phase_name}] This indicates:")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   1. FD process crashed or hung")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   2. JSON file not being written by FD")
        logger.error(f"[Step {step}] [PHASE: {phase_name}]   3. File I/O or permission problem")

        # Diagnostic: list what files DO exist
        fd_output_dir = "downward/fd_output"
        if os.path.exists(fd_output_dir):
            logger.error(f"[Step {step}] [PHASE: {phase_name}] Files in fd_output/:")
            try:
                for fname in sorted(os.listdir(fd_output_dir)):
                    fpath = os.path.join(fd_output_dir, fname)
                    if os.path.isfile(fpath):
                        size = os.path.getsize(fpath)
                        logger.error(f"[Step {step}] [PHASE: {phase_name}]   - {fname} ({size} bytes)")
            except:
                pass

        return None

    def _diagnose_fd_output(self, step: int) -> None:
        """✅ NEW: Detailed diagnostics when FD output fails."""
        fd_output_dir = "downward/fd_output"

        logger.info(f"\n[DIAGNOSIS] Checking FD output state for step {step}...")

        # Check directory
        if not os.path.exists(fd_output_dir):
            logger.error(f"[DIAGNOSIS] fd_output directory DOES NOT EXIST")
            return

        logger.info(f"[DIAGNOSIS] Files in {fd_output_dir}:")
        try:
            for fname in sorted(os.listdir(fd_output_dir)):
                fpath = os.path.join(fd_output_dir, fname)
                if os.path.isfile(fpath):
                    size = os.path.getsize(fpath)
                    mtime = os.path.getmtime(fpath)
                    age_sec = time.time() - mtime
                    logger.info(f"  - {fname:<30} {size:>10} bytes (age: {age_sec:.1f}s)")
        except Exception as e:
            logger.error(f"[DIAGNOSIS] Error listing files: {e}")

        # Check for expected files
        expected_ts = os.path.join(fd_output_dir, f"ts_{step}.json")
        expected_before = os.path.join(fd_output_dir, f"merge_before_{step}.json")
        expected_after = os.path.join(fd_output_dir, f"merge_after_{step}.json")

        logger.info(f"[DIAGNOSIS] Expected files for step {step}:")
        for expected_path in [expected_ts, expected_before, expected_after]:
            exists = os.path.exists(expected_path)
            status = "✓ EXISTS" if exists else "✗ MISSING"
            logger.info(f"  {status}: {os.path.basename(expected_path)}")

            if exists:
                try:
                    size = os.path.getsize(expected_path)
                    with open(expected_path, 'r') as f:
                        content = f.read()
                    logger.info(f"    Size: {size} bytes")
                    logger.info(f"    Preview: {content[:100]}...")
                except Exception as e:
                    logger.warning(f"    Error reading: {e}")

        # Check FD process
        if self.process:
            retcode = self.process.poll()
            if retcode is not None:
                logger.error(f"[DIAGNOSIS] FD process has EXITED with code {retcode}")
            else:
                logger.info(f"[DIAGNOSIS] FD process is RUNNING (PID: {self.process.pid})")

    def _diagnose_signals(self, iteration: int):
        """✅ NEW: Verify signal correctness."""
        before_path = os.path.join("downward", "fd_output", f"merge_before_{iteration}.json")
        after_path = os.path.join("downward", "fd_output", f"merge_after_{iteration}.json")

        if not os.path.exists(before_path) or not os.path.exists(after_path):
            logger.error(f"Signal files missing for iteration {iteration}")
            logger.error(f"  Before path: {before_path} (exists: {os.path.exists(before_path)})")
            logger.error(f"  After path: {after_path} (exists: {os.path.exists(after_path)})")
            return False

        try:
            with open(before_path) as f:
                before = json.load(f)
            with open(after_path) as f:
                after = json.load(f)
        except Exception as e:
            logger.error(f"Failed to load signal files: {e}")
            return False

        # ✅ VERIFICATION 1: State count consistency
        ts1_size = before.get("ts1_size", 0)
        ts2_size = before.get("ts2_size", 0)
        expected_merged_size = ts1_size * ts2_size
        actual_merged_size = after.get("num_states", 0)

        logger.info(f"[VERIFY] Iteration {iteration}:")
        logger.info(f"  TS1 size: {ts1_size}, TS2 size: {ts2_size}")
        logger.info(f"  Expected merged size: {expected_merged_size}")
        logger.info(f"  Actual merged size: {actual_merged_size}")

        if actual_merged_size > expected_merged_size * 1.1:
            logger.warning(f"  ⚠️ Merged size larger than expected (not properly shrunk?)")

        # ✅ VERIFICATION 2: F-value list lengths
        f1_len = len(before.get("ts1_f_values", []))
        f2_len = len(before.get("ts2_f_values", []))
        f_after_len = len(after.get("f_values", []))

        logger.info(f"  F-values: |f1|={f1_len}, |f2|={f2_len}, |f_after|={f_after_len}")

        if f1_len != ts1_size or f2_len != ts2_size:
            logger.error(f"  ❌ F-value list size mismatch!")
            return False

        # ✅ VERIFICATION 3: A* signals present and valid
        signals = after.get("search_signals", {})
        logger.info(f"  A* signals: {signals}")

        if not signals:
            logger.warning(f"  ⚠️ No A* signals exported")
        else:
            for key in ["nodes_expanded", "branching_factor", "solution_found"]:
                if key not in signals:
                    logger.error(f"  ❌ Missing signal: {key}")
                    return False

        return True
    def _parse_fd_log(self) -> Tuple[int, int]:
        """Read FD log and extract plan cost and expansions."""
        if self.fd_log_file:
            try:
                self.fd_log_file.flush()
            except Exception:
                pass

        log_path = os.path.join("downward", "fd_output", "log.txt")
        plan_cost, expansions = 0, 0
        if os.path.exists(log_path):
            try:
                text = open(log_path, "r", encoding="utf-8", errors="ignore").read()
            except Exception:
                text = ""
            if text:
                m1 = list(re.finditer(r"Plan length:\s*(\d+)", text))
                if m1:
                    plan_cost = int(m1[-1].group(1))
                m2 = list(re.finditer(r"[Ee]xpanded\s+(\d+)\s+state(?:s)?(?:\(\w*\))?", text))
                if m2:
                    expansions = int(m2[-1].group(1))
        return plan_cost, expansions

    # ============================================================================
    # ✅ NEW: Extract rich edge features for GNN learning
    # ============================================================================

    def _extract_edge_features(self) -> np.ndarray:
        """
        ✅ NEW: Extract rich features about merge candidates.

        For each edge (u, v), compute:
        1. Relative size difference
        2. Expected merge size ratio
        3. Shared variable count
        4. Reachability metrics
        5. Iteration difference
        6. Centrality similarity
        7. F-value consistency
        8. Merge risk indicator

        Returns:
            [E, 8] edge features
        """
        G = self.graph_tracker.graph
        edges = list(G.edges())

        if not edges:
            return np.zeros((0, 8), dtype=np.float32)

        edge_features = []

        for u, v in edges:
            u_data = G.nodes[u]
            v_data = G.nodes[v]

            # Feature 1: Relative size difference (normalized)
            u_size = u_data.get("num_states", 1)
            v_size = v_data.get("num_states", 1)
            max_size_in_graph = max(
                d.get("num_states", 1) for _, d in G.nodes(data=True)
            )
            size_diff = float(abs(u_size - v_size)) / max(max_size_in_graph, 1)

            # Feature 2: Expected merged size ratio (% of reachable states)
            # Heuristic: product of sizes as fraction of typical max
            product_size = (u_size * v_size) / max(max_size_in_graph * max_size_in_graph, 1)
            merge_size_ratio = np.clip(product_size, 0.0, 1.0)

            # Feature 3: Shared variables (normalized)
            u_vars = set(u_data.get("incorporated_variables", []))
            v_vars = set(v_data.get("incorporated_variables", []))
            shared_vars = len(u_vars & v_vars)
            total_vars = len(u_vars | v_vars)
            shared_ratio = shared_vars / max(total_vars, 1)

            # Feature 4: Reachability consistency
            u_f = u_data.get("f_before", [])
            v_f = v_data.get("f_before", [])
            u_reachable = sum(1 for f in u_f if f != float('inf') and f < 1_000_000_000) / max(len(u_f), 1)
            v_reachable = sum(1 for f in v_f if f != float('inf') and f < 1_000_000_000) / max(len(v_f), 1)
            reachability_similarity = 1.0 - abs(u_reachable - v_reachable)

            # Feature 5: Iteration difference (normalized)
            u_iter = u_data.get("iteration", 0)
            v_iter = v_data.get("iteration", 0)
            max_iter = max((d.get("iteration", 0) for _, d in G.nodes(data=True)), default=1)
            iter_diff = float(abs(u_iter - v_iter)) / max(max_iter, 1)

            # Feature 6: Centrality similarity
            u_centrality = self.centrality.get(u, 0.0)
            v_centrality = self.centrality.get(v, 0.0)
            centrality_similarity = 1.0 - abs(u_centrality - v_centrality)

            # Feature 7: F-value consistency (std of combined distributions)
            f_combined = u_f + v_f
            if f_combined:
                # Filter to valid values
                f_valid = [f for f in f_combined if f != float('inf') and f < 1_000_000_000]
                if f_valid:
                    f_std = float(np.std(f_valid)) / (1.0 + float(np.mean(f_valid)))
                    f_consistency = np.clip(1.0 - f_std, 0.0, 1.0)
                else:
                    f_consistency = 0.0
            else:
                f_consistency = 0.5

            # Feature 8: Merge risk indicator
            # Combines: degree, transition density, reachability
            u_degree = G.degree(u) / max(G.number_of_nodes(), 1)
            v_degree = G.degree(v) / max(G.number_of_nodes(), 1)
            u_trans = u_data.get("num_transitions", 0) / max(u_size, 1)
            v_trans = v_data.get("num_transitions", 0) / max(v_size, 1)

            merge_risk = np.clip(
                (u_degree + v_degree) * 0.5 + (u_trans + v_trans) * 0.25,
                0.0, 1.0
            )

            edge_features.append([
                size_diff,
                merge_size_ratio,
                shared_ratio,
                reachability_similarity,
                iter_diff,
                centrality_similarity,
                f_consistency,
                merge_risk
            ])

        return np.array(edge_features, dtype=np.float32)

    def _get_observation(self) -> Dict:
        """✅ ULTRA-OPTIMIZED: Persistent array pre-allocation + vectorization."""
        max_nodes, max_edges = 100, 1000

        G = self.graph_tracker.graph

        # ✅ OPTIMIZATION 1: Pre-allocate observation arrays ONCE (not per call)
        if not hasattr(self, '_obs_cache'):
            self._obs_cache = {
                'x': np.zeros((max_nodes, self.feat_dim), dtype=np.float32),
                'edge_index': np.zeros((2, max_edges), dtype=np.int64),
                'edge_features': np.zeros((max_edges, 8), dtype=np.float32),
            }
            self._last_graph_hash = None

        # ✅ OPTIMIZATION 2: Quick hash check - skip rebuild if graph unchanged
        current_hash = self.graph_tracker._get_graph_hash()
        if current_hash == self._last_graph_hash and self._last_graph_hash is not None:
            # Graph hasn't changed - return cached observation
            return {
                "x": self._obs_cache['x'].copy(),
                "edge_index": self._obs_cache['edge_index'].copy(),
                "edge_features": self._obs_cache['edge_features'].copy(),
                "num_nodes": np.int32(self._last_num_nodes),
                "num_edges": np.int32(self._last_num_edges),
            }
        self._last_graph_hash = current_hash

        # ✅ REUSE arrays (clear instead of reallocate)
        x = self._obs_cache['x']
        x.fill(0)

        ei = self._obs_cache['edge_index']
        ei.fill(0)

        ef = self._obs_cache['edge_features']
        ef.fill(0)

        # ✅ OPTIMIZATION 3: Get cached metrics (don't recompute)
        degs = dict(G.degree()).values()
        max_deg = max(max(degs, default=0), 1)
        max_states_node = max((d.get("num_states", 0) for _, d in G.nodes(data=True)), default=1) or 1

        # ✅ OPTIMIZATION 4: Compute F-scale ONCE (vectorized)
        f_values_all = []
        for _, d in G.nodes(data=True):
            f_vals = d.get("f_before", [])
            if f_vals:
                f_values_all.extend(f_vals)

        f_scale = max(f_values_all) if f_values_all else 1.0
        f_scale = max(f_scale, 1.0)

        # ✅ OPTIMIZATION 5: Use cached centrality (not recomputed every step)
        centrality = self.graph_tracker.get_centrality()
        max_vars = self.graph_tracker.get_max_vars()
        max_iter = self.graph_tracker.get_max_iter()

        # ✅ OPTIMIZATION 6: Vectorized node feature computation
        node_features_list = []
        idx = {}

        for i, (nid, data) in enumerate(G.nodes(data=True)):
            if i >= max_nodes:
                break

            # Batch compute all features for this node at once
            ns_raw = float(data.get("num_states", 0))
            num_states = ns_raw / float(max_states_node)
            is_atomic = 1.0 if data.get("iteration", -1) == -1 else 0.0
            d_norm = G.degree(nid) / max_deg
            od_norm = G.out_degree(nid) / max_deg

            # ✅ CACHED F-stats (memoized in graph_tracker)
            f_min_norm, f_mean_norm, f_max_norm, f_std_norm = self.graph_tracker.f_stats(nid)

            if f_scale > 0:
                f_min_norm = max(0.0, min(f_min_norm / f_scale, 1.0))
                f_mean_norm = max(0.0, min(f_mean_norm / f_scale, 1.0))
                f_max_norm = max(0.0, min(f_max_norm / f_scale, 1.0))
                f_std_norm = max(0.0, min(f_std_norm / f_scale, 1.0))
            else:
                f_min_norm = f_mean_norm = f_max_norm = f_std_norm = 0.0

            # Heuristic quality features
            f_vals = np.array(data.get("f_before", []), dtype=np.float32)
            if len(f_vals) > 0 and f_scale > 0:
                valid_f = f_vals[(f_vals != np.inf) & (f_vals >= 0) & (f_vals < 1e9)]
                if len(valid_f) > 0:
                    avg_f_norm = float(np.mean(valid_f)) / f_scale
                    max_f_norm_heur = float(np.max(valid_f)) / f_scale
                    f_median = np.median(valid_f)
                    heuristic_concentration = float(np.std(valid_f)) / (1.0 + f_median)
                    heuristic_concentration = float(np.clip(heuristic_concentration, 0.0, 1.0))
                else:
                    avg_f_norm = 0.0
                    max_f_norm_heur = 0.0
                    heuristic_concentration = 0.0
            else:
                avg_f_norm = 0.0
                max_f_norm_heur = 0.0
                heuristic_concentration = 0.0

            reachable_ratio = 1.0
            if len(f_vals) > 0:
                unreachable = np.sum((f_vals == np.inf) | (f_vals >= 1e9))
                reachable_ratio = float(1.0 - unreachable / len(f_vals))

            num_vars_norm = len(data.get("incorporated_variables", [])) / float(max_vars)
            iter_idx_norm = data.get("iteration", 0) / float(max_iter)
            centrality_norm = float(centrality.get(nid, 0.0))

            num_neighbors = len(list(G.neighbors(nid)))
            neighbor_risk = float(num_neighbors) / max(len(G.nodes), 1)

            # ✅ SINGLE ASSIGNMENT (faster than individual assignments)
            x[i, :] = [
                num_states, is_atomic, d_norm, od_norm,
                avg_f_norm, max_f_norm_heur, heuristic_concentration, reachable_ratio,
                num_vars_norm, iter_idx_norm, centrality_norm,
                f_min_norm, f_mean_norm, f_max_norm, f_std_norm,
                neighbor_risk, np.clip(ns_raw / 10000.0, 0.0, 1.0),
                0.0, 0.0
            ]

            idx[nid] = i

        num_nodes_feat = len(idx)

        # ✅ OPTIMIZATION 7: Vectorized edge processing
        edges = [
            (idx[u], idx[v])
            for u, v in G.edges()
            if u in idx and v in idx
        ]
        ne = len(edges)

        for j, (u, v) in enumerate(edges[:max_edges]):
            ei[0, j] = u
            ei[1, j] = v

        # ✅ OPTIMIZATION 8: Extract edge features (only if needed)
        if ne > 0:
            edge_features = self._extract_edge_features_cached()
            ef[:ne, :] = edge_features[:ne]

        # Store for next check
        self._last_num_nodes = num_nodes_feat
        self._last_num_edges = ne

        return {
            "x": x,
            "edge_index": ei,
            "edge_features": ef,
            "num_nodes": np.int32(num_nodes_feat),
            "num_edges": np.int32(ne),
        }

    def _extract_edge_features_cached(self) -> np.ndarray:
        """✅ NEW: Cached edge feature extraction."""
        G = self.graph_tracker.graph
        edges = list(G.edges())

        if not edges:
            return np.zeros((0, 8), dtype=np.float32)

        # ✅ Quick check: if graph hasn't changed, return cached
        current_hash = self.graph_tracker._get_graph_hash()
        if (hasattr(self, '_edge_features_cache_hash') and
                self._edge_features_cache_hash == current_hash and
                hasattr(self, '_edge_features_cache')):
            return self._edge_features_cache

        # Compute edge features (this is the expensive part)
        edge_features = []

        # Get graph-level metrics ONCE (not per edge)
        max_size_in_graph = max(
            d.get("num_states", 1) for _, d in G.nodes(data=True)
        ) or 1
        max_iter_global = max((d.get("iteration", 0) for _, d in G.nodes(data=True)), default=1) or 1

        for u, v in edges:
            u_data = G.nodes[u]
            v_data = G.nodes[v]

            u_size = u_data.get("num_states", 1)
            v_size = v_data.get("num_states", 1)
            size_diff = float(abs(u_size - v_size)) / max(max_size_in_graph, 1)

            product_size = (u_size * v_size) / max(max_size_in_graph * max_size_in_graph, 1)
            merge_size_ratio = np.clip(product_size, 0.0, 1.0)

            u_vars = set(u_data.get("incorporated_variables", []))
            v_vars = set(v_data.get("incorporated_variables", []))
            shared_vars = len(u_vars & v_vars)
            total_vars = len(u_vars | v_vars)
            shared_ratio = shared_vars / max(total_vars, 1)

            u_f = u_data.get("f_before", [])
            v_f = v_data.get("f_before", [])
            u_reachable = sum(1 for f in u_f if f != float('inf') and f < 1_000_000_000) / max(len(u_f), 1)
            v_reachable = sum(1 for f in v_f if f != float('inf') and f < 1_000_000_000) / max(len(v_f), 1)
            reachability_similarity = 1.0 - abs(u_reachable - v_reachable)

            u_iter = u_data.get("iteration", 0)
            v_iter = v_data.get("iteration", 0)
            iter_diff = float(abs(u_iter - v_iter)) / max(max_iter_global, 1)

            # Use CACHED centrality
            centrality = self.graph_tracker.get_centrality()
            u_centrality = centrality.get(u, 0.0)
            v_centrality = centrality.get(v, 0.0)
            centrality_similarity = 1.0 - abs(u_centrality - v_centrality)

            f_combined = u_f + v_f
            if f_combined:
                f_valid = [f for f in f_combined if f != float('inf') and f < 1_000_000_000]
                if f_valid:
                    f_std = float(np.std(f_valid)) / (1.0 + float(np.mean(f_valid)))
                    f_consistency = np.clip(1.0 - f_std, 0.0, 1.0)
                else:
                    f_consistency = 0.0
            else:
                f_consistency = 0.5

            u_degree = G.degree(u) / max(G.number_of_nodes(), 1)
            v_degree = G.degree(v) / max(G.number_of_nodes(), 1)
            u_trans = u_data.get("num_transitions", 0) / max(u_size, 1)
            v_trans = v_data.get("num_transitions", 0) / max(v_size, 1)

            merge_risk = np.clip(
                (u_degree + v_degree) * 0.5 + (u_trans + v_trans) * 0.25,
                0.0, 1.0
            )

            edge_features.append([
                size_diff,
                merge_size_ratio,
                shared_ratio,
                reachability_similarity,
                iter_diff,
                centrality_similarity,
                f_consistency,
                merge_risk
            ])

        result = np.array(edge_features, dtype=np.float32)

        # Cache the result
        self._edge_features_cache = result
        self._edge_features_cache_hash = current_hash

        return result

    def _save_gnn_decision_metadata(self, merge_step: int, action: int,
                                    src: int, tgt: int, obs: Dict,
                                    reward: float, info: Dict) -> None:
        """✅ NEW: Save GNN decision metadata for analysis."""

        try:
            import json
            from datetime import datetime

            metadata = {
                'episode_step': self.current_merge_step,
                'merge_step': merge_step,
                'action_index': int(action),
                'chosen_edge': [int(src), int(tgt)],
                'observation_shape': {
                    'num_nodes': int(obs.get('num_nodes', 0)),
                    'num_edges': int(obs.get('num_edges', 0)),
                    'node_features_dim': int(obs['x'].shape[-1]) if obs['x'].ndim > 1 else 0,
                },
                'reward_received': float(reward),
                'merge_info': {
                    'plan_cost': info.get('plan_cost', 0),
                    'num_expansions': info.get('num_expansions', 0),
                    'delta_states': info.get('delta_states', 0),
                },
                'timestamp': datetime.now().isoformat(),
                'problem': os.path.basename(self.problem_file),
            }

            self.gnn_decisions_log.append(metadata)

        except Exception as e:
            logger.debug(f"Could not save GNN metadata: {e}")

    def _export_episode_metadata(self) -> None:
        """✅ NEW: Export all GNN decisions from this episode."""

        if not self.gnn_decisions_log:
            return

        try:
            import json
            from datetime import datetime

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            episode_metadata = {
                'problem': os.path.basename(self.problem_file),
                'num_decisions': len(self.gnn_decisions_log),
                'decisions': self.gnn_decisions_log,
                'export_timestamp': datetime.now().isoformat(),
            }

            metadata_file = os.path.join(
                self.gnn_metadata_dir,
                f"episode_{timestamp}_{len(self.gnn_decisions_log)}_decisions.json"
            )

            with open(metadata_file, 'w') as f:
                json.dump(episode_metadata, f, indent=2, default=str)

            logger.info(f"✓ Exported GNN episode metadata: {metadata_file}")

        except Exception as e:
            logger.warning(f"Failed to export episode metadata: {e}")

    def _count_total_states(self) -> int:
        return sum(d["num_states"] for _, d in self.graph_tracker.graph.nodes(data=True))

    def close(self):
        try:
            if self.process and self.process.poll() is None:
                self.process.terminate()
                try:
                    self.process.wait(timeout=3.0)
                except subprocess.TimeoutExpired:
                    self.process.kill()
        except Exception:
            pass
        finally:
            self.process = None

        try:
            if self.fd_log_file:
                self.fd_log_file.flush()
                self.fd_log_file.close()
        except Exception:
            pass
        finally:
            self.fd_log_file = None

--------------------------------------------------------------------------------

The file downward/src/search/merge_and_shrink/merge_and_shrink_algorithm.cc code code is this:
#include "merge_and_shrink_algorithm.h"

#include "distances.h"
#include "factored_transition_system.h"
#include "fts_factory.h"
#include "label_reduction.h"
#include "labels.h"
#include "merge_and_shrink_representation.h"
#include "merge_strategy.h"
#include "merge_strategy_factory.h"
#include "shrink_strategy.h"
#include "transition_system.h"
#include "types.h"
#include "utils.h"

#include "../plugins/plugin.h"
#include "../task_utils/task_properties.h"
#include "../utils/component_errors.h"
#include "../utils/countdown_timer.h"
#include "../utils/markup.h"
#include "../utils/math.h"
#include "../utils/system.h"
#include "../utils/timer.h"

#include <cassert>
#include <iostream>
#include <limits>
#include <string>
#include <utility>
#include <vector>

#include <filesystem>

#include <nlohmann/json.hpp>
#include <fstream>

#include <cmath>     // for NaN/Inf handling if needed
#include <limits>    // for numeric_limits

using json = nlohmann::json;

using namespace std;
using plugins::Bounds;
using utils::ExitCode;

namespace merge_and_shrink {
static void log_progress(const utils::Timer &timer, const string &msg, utils::LogProxy &log) {
    log << "M&S algorithm timer: " << timer << " (" << msg << ")" << endl;
}
MergeAndShrinkAlgorithm::MergeAndShrinkAlgorithm(
    const shared_ptr<MergeStrategyFactory> &merge_strategy,
    const shared_ptr<ShrinkStrategy> &shrink_strategy,
    const shared_ptr<LabelReduction> &label_reduction,
    bool prune_unreachable_states, bool prune_irrelevant_states,
    int max_states, int max_states_before_merge,
    int threshold_before_merge, double main_loop_max_time,
    utils::Verbosity verbosity)
    : merge_strategy_factory(merge_strategy),
      shrink_strategy(shrink_strategy),
      label_reduction(label_reduction),
      max_states(max_states),
      max_states_before_merge(max_states_before_merge),
      shrink_threshold_before_merge(threshold_before_merge),
      prune_unreachable_states(prune_unreachable_states),
      prune_irrelevant_states(prune_irrelevant_states),
      log(utils::get_log_for_verbosity(verbosity)),
      main_loop_max_time(main_loop_max_time),
      starting_peak_memory(0) {
    handle_shrink_limit_defaults();
    // Asserting fields (not parameters).
    assert(this->max_states_before_merge >= 1);
    assert(this->max_states >= this->max_states_before_merge);
}

void MergeAndShrinkAlgorithm::handle_shrink_limit_defaults() {
    // If none of the two state limits has been set: set default limit.
    if (max_states == -1 && max_states_before_merge == -1) {
        max_states = 50000;
    }

    // If one of the max_states options has not been set, set the other
    // so that it imposes no further limits.
    if (max_states_before_merge == -1) {
        max_states_before_merge = max_states;
    } else if (max_states == -1) {
        if (utils::is_product_within_limit(
                max_states_before_merge, max_states_before_merge, INF)) {
            max_states = max_states_before_merge * max_states_before_merge;
        } else {
            max_states = INF;
        }
    }

    if (max_states_before_merge > max_states) {
        max_states_before_merge = max_states;
        if (log.is_warning()) {
            log << "WARNING: "
                << "max_states_before_merge exceeds max_states, "
                << "correcting max_states_before_merge." << endl;
        }
    }

    utils::verify_argument(max_states >= 1,
                           "Transition system size must be at least 1.");

    utils::verify_argument(max_states_before_merge >= 1,
                           "Transition system size before merge must be at least 1.");

    if (shrink_threshold_before_merge == -1) {
        shrink_threshold_before_merge = max_states;
    }

    utils::verify_argument(shrink_threshold_before_merge >= 1,
                           "Threshold must be at least 1.");

    if (shrink_threshold_before_merge > max_states) {
        shrink_threshold_before_merge = max_states;
        if (log.is_warning()) {
            log << "WARNING: "
                << "threshold exceeds max_states, "
                << "correcting threshold." << endl;
        }
    }
}

void MergeAndShrinkAlgorithm::report_peak_memory_delta(bool final) const {
    if (final)
        log << "Final";
    else
        log << "Current";
    log << " peak memory increase of merge-and-shrink algorithm: "
        << utils::get_peak_memory_in_kb() - starting_peak_memory << " KB"
        << endl;
}

void MergeAndShrinkAlgorithm::dump_options() const {
    if (log.is_at_least_normal()) {
        if (merge_strategy_factory) { // deleted after merge strategy extraction
            merge_strategy_factory->dump_options();
            log << endl;
        }

        log << "Options related to size limits and shrinking: " << endl;
        log << "Transition system size limit: " << max_states << endl
            << "Transition system size limit right before merge: "
            << max_states_before_merge << endl;
        log << "Threshold to trigger shrinking right before merge: "
            << shrink_threshold_before_merge << endl;
        log << endl;

        shrink_strategy->dump_options(log);
        log << endl;

        log << "Pruning unreachable states: "
            << (prune_unreachable_states ? "yes" : "no") << endl;
        log << "Pruning irrelevant states: "
            << (prune_irrelevant_states ? "yes" : "no") << endl;
        log << endl;

        if (label_reduction) {
            label_reduction->dump_options(log);
        } else {
            log << "Label reduction disabled" << endl;
        }
        log << endl;

        log << "Main loop max time in seconds: " << main_loop_max_time << endl;
        log << endl;
    }
}

void MergeAndShrinkAlgorithm::warn_on_unusual_options() const {
    string dashes(79, '=');
    if (!label_reduction) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! You did not enable label reduction. " << endl
                << "This may drastically reduce the performance of merge-and-shrink!"
                << endl << dashes << endl;
        }
    } else if (label_reduction->reduce_before_merging() && label_reduction->reduce_before_shrinking()) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! You set label reduction to be applied twice in each merge-and-shrink" << endl
                << "iteration, both before shrinking and merging. This double computation effort" << endl
                << "does not pay off for most configurations!"
                << endl << dashes << endl;
        }
    } else {
        if (label_reduction->reduce_before_shrinking() &&
            (shrink_strategy->get_name() == "f-preserving"
             || shrink_strategy->get_name() == "random")) {
            if (log.is_warning()) {
                log << dashes << endl
                    << "WARNING! Bucket-based shrink strategies such as f-preserving random perform" << endl
                    << "best if used with label reduction before merging, not before shrinking!"
                    << endl << dashes << endl;
            }
        }
        if (label_reduction->reduce_before_merging() &&
            shrink_strategy->get_name() == "bisimulation") {
            if (log.is_warning()) {
                log << dashes << endl
                    << "WARNING! Shrinking based on bisimulation performs best if used with label" << endl
                    << "reduction before shrinking, not before merging!"
                    << endl << dashes << endl;
            }
        }
    }

    if (!prune_unreachable_states || !prune_irrelevant_states) {
        if (log.is_warning()) {
            log << dashes << endl
                << "WARNING! Pruning is (partially) turned off!" << endl
                << "This may drastically reduce the performance of merge-and-shrink!"
                << endl << dashes << endl;
        }
    }
}

bool MergeAndShrinkAlgorithm::ran_out_of_time(
    const utils::CountdownTimer &timer) const {
    if (timer.is_expired()) {
        if (log.is_at_least_normal()) {
            log << "Ran out of time, stopping computation." << endl;
            log << endl;
        }
        return true;
    }
    return false;
}

// ============================================================================
// END: CORRECTED HELPER FUNCTION
// ============================================================================

void MergeAndShrinkAlgorithm::main_loop(
    FactoredTransitionSystem &fts,
    const TaskProxy &task_proxy) {

    utils::CountdownTimer timer(main_loop_max_time);
    if (log.is_at_least_normal()) {
        log << "Starting main loop ";
        if (main_loop_max_time == numeric_limits<double>::infinity()) {
            log << "without a time limit." << endl;
        } else {
            log << "with a time limit of " << main_loop_max_time << "s." << endl;
        }
    }

    int maximum_intermediate_size = 0;
    for (int i = 0; i < fts.get_size(); ++i) {
        int size = fts.get_transition_system(i).get_size();
        if (size > maximum_intermediate_size) {
            maximum_intermediate_size = size;
        }
    }

    if (label_reduction) {
        label_reduction->initialize(task_proxy);
    }

    unique_ptr<MergeStrategy> merge_strategy =
        merge_strategy_factory->compute_merge_strategy(task_proxy, fts);
    merge_strategy_factory = nullptr;

    auto log_main_loop_progress = [&timer, this](const string &msg) {
        log << "M&S algorithm main loop timer: "
            << timer.get_elapsed_time()
            << " (" << msg << ")" << endl;
    };

    // ✅ SETUP: Get output directories ONCE at the start
    std::string fd_output_dir = std::filesystem::absolute("fd_output").string();
    std::filesystem::create_directories(fd_output_dir);
    std::cout << "[M&S] Using fd_output: " << fd_output_dir << std::endl;

    int iteration = 0;

    while (fts.get_num_active_entries() > 1) {
        // ====================================================================
        // PHASE 1: GET NEXT MERGE PAIR FROM STRATEGY
        // ====================================================================

        pair<int, int> merge_indices = merge_strategy->get_next();
        if (ran_out_of_time(timer)) break;

        int merge_index1 = merge_indices.first;
        int merge_index2 = merge_indices.second;

        assert(merge_index1 != merge_index2);
        if (log.is_at_least_normal()) {
            log << "Next pair of indices: ("
                << merge_index1 << ", " << merge_index2 << ")" << endl;
            if (log.is_at_least_verbose()) {
                fts.statistics(merge_index1, log);
                fts.statistics(merge_index2, log);
            }
            log_main_loop_progress("after computation of next merge");
        }

        // ====================================================================
        // PHASE 2: LABEL REDUCTION (BEFORE SHRINKING)
        // ====================================================================

        bool reduced = false;
        if (label_reduction && label_reduction->reduce_before_shrinking()) {
            reduced = label_reduction->reduce(merge_indices, fts, log);
            if (log.is_at_least_normal() && reduced) {
                log_main_loop_progress("after label reduction");
            }
        }
        if (ran_out_of_time(timer)) break;

        // ====================================================================
        // PHASE 3: SHRINKING
        // ====================================================================

        bool shrunk = shrink_before_merge_step(
            fts,
            merge_index1,
            merge_index2,
            max_states,
            max_states_before_merge,
            shrink_threshold_before_merge,
            *shrink_strategy,
            log);
        if (log.is_at_least_normal() && shrunk) {
            log_main_loop_progress("after shrinking");
        }
        if (ran_out_of_time(timer)) break;

        // ====================================================================
        // PHASE 4: LABEL REDUCTION (BEFORE MERGING)
        // ====================================================================

        if (label_reduction && label_reduction->reduce_before_merging()) {
            reduced = label_reduction->reduce(merge_indices, fts, log);
            if (log.is_at_least_normal() && reduced) {
                log_main_loop_progress("after label reduction");
            }
        }
        if (ran_out_of_time(timer)) break;

//        // ====================================================================
//        // PHASE 5: EXPORT MERGE SIGNALS (BEFORE DATA) - SIMPLIFIED INLINE
//        // ====================================================================
//
//        {
//            const auto& init1 = fts.get_distances(merge_index1).get_init_distances();
//            const auto& goal1 = fts.get_distances(merge_index1).get_goal_distances();
//            const auto& init2 = fts.get_distances(merge_index2).get_init_distances();
//            const auto& goal2 = fts.get_distances(merge_index2).get_goal_distances();
//
//            std::vector<int> f1(init1.size()), f2(init2.size());
//            for (size_t i = 0; i < f1.size(); ++i) {
//                f1[i] = (init1[i] == INF || goal1[i] == INF) ? INF : init1[i] + goal1[i];
//            }
//            for (size_t j = 0; j < f2.size(); ++j) {
//                f2[j] = (init2[j] == INF || goal2[j] == INF) ? INF : init2[j] + goal2[j];
//            }
//
//            int ts1_transitions = 0;
//            for (auto it = fts.get_transition_system(merge_index1).begin();
//                 it != fts.get_transition_system(merge_index1).end(); ++it) {
//                ts1_transitions += (*it).get_transitions().size();
//            }
//
//            int ts2_transitions = 0;
//            for (auto it = fts.get_transition_system(merge_index2).begin();
//                 it != fts.get_transition_system(merge_index2).end(); ++it) {
//                ts2_transitions += (*it).get_transitions().size();
//            }
//
//            int ts1_size = (int)f1.size();
//            int ts2_size = (int)f2.size();
//
//            json product_mapping;
//            for (int s = 0; s < ts1_size * ts2_size; ++s) {
//                int s1 = s / ts2_size;
//                int s2 = s % ts2_size;
//                product_mapping[std::to_string(s)] = {{"s1", s1}, {"s2", s2}};
//            }
//
//            json before_data;
//            before_data["ts1_id"] = merge_index1;
//            before_data["ts2_id"] = merge_index2;
//            before_data["iteration"] = iteration;
//            before_data["ts1_f_values"] = f1;
//            before_data["ts2_f_values"] = f2;
//            before_data["ts1_size"] = ts1_size;
//            before_data["ts2_size"] = ts2_size;
//            before_data["ts1_num_transitions"] = ts1_transitions;
//            before_data["ts2_num_transitions"] = ts2_transitions;
//            before_data["product_mapping"] = product_mapping;
//
//            // ✅ DIRECT WRITE: No helper function, just write it
//            std::string before_path = fd_output_dir + "/merge_before_" + std::to_string(iteration) + ".json";
//            std::ofstream before_file(before_path, std::ios::out | std::ios::trunc);
//            if (!before_file.is_open()) {
//                std::cerr << "[M&S] ERROR: Cannot create merge_before file: " << before_path << std::endl;
//                throw std::runtime_error("Cannot create merge_before file");
//            }
//            before_file << before_data.dump(2);
//            before_file.close();
//            std::cout << "[M&S] ✅ Wrote merge_before_" << iteration << ".json" << std::endl;
//        }
//
//        // ====================================================================
//        // PHASE 6: PERFORM ACTUAL MERGE
//        // ====================================================================
//
//        int merged_index = fts.merge(merge_index1, merge_index2, log);
//
//        // ====================================================================
//        // PHASE 7: EXPORT MERGE SIGNALS (AFTER DATA) - SIMPLIFIED INLINE
//        // ====================================================================
//
//        {
//            const auto& init_dist = fts.get_distances(merged_index).get_init_distances();
//            const auto& goal_dist = fts.get_distances(merged_index).get_goal_distances();
//
//            std::vector<int> f_after(init_dist.size());
//            for (size_t s = 0; s < f_after.size(); ++s) {
//                f_after[s] = (init_dist[s] == INF || goal_dist[s] == INF) ? INF : init_dist[s] + goal_dist[s];
//            }
//
//            const TransitionSystem& merged_ts = fts.get_transition_system(merged_index);
//            int num_goals = 0;
//            for (size_t i = 0; i < f_after.size(); ++i) {
//                if (merged_ts.is_goal_state(i)) num_goals++;
//            }
//
//            int merged_transitions = 0;
//            for (auto it = merged_ts.begin(); it != merged_ts.end(); ++it) {
//                merged_transitions += (*it).get_transitions().size();
//            }
//
//            // ✅ COMPUTE A* METRICS (inline, simple version)
//            int nodes_expanded = 0;
//            for (int i = 0; i < merged_ts.get_size(); ++i) {
//                if (init_dist[i] != INF && goal_dist[i] != INF) {
//                    nodes_expanded++;
//                }
//            }
//
//            int reachable_states = nodes_expanded;
//            double branching_factor = 1.0;
//            if (reachable_states > 0 && merged_transitions > 0) {
//                branching_factor = (double)merged_transitions / (double)reachable_states;
//                if (std::isnan(branching_factor) || std::isinf(branching_factor) || branching_factor < 1.0) {
//                    branching_factor = 1.0;
//                }
//            }
//
//            int search_depth = 0;
//            long long sum_goal_dist = 0;
//            int reachable_goal_count = 0;
//            for (int i = 0; i < merged_ts.get_size(); ++i) {
//                if (init_dist[i] != INF && goal_dist[i] != INF) {
//                    sum_goal_dist += goal_dist[i];
//                    reachable_goal_count++;
//                }
//            }
//            if (reachable_goal_count > 0) {
//                search_depth = (int)std::round((double)sum_goal_dist / reachable_goal_count);
//            }
//
//            int best_goal_f = INF;
//            for (int i = 0; i < merged_ts.get_size(); ++i) {
//                if (merged_ts.is_goal_state(i) && init_dist[i] != INF && goal_dist[i] != INF) {
//                    int f = init_dist[i] + goal_dist[i];
//                    if (f < best_goal_f) {
//                        best_goal_f = f;
//                    }
//                }
//            }
//            bool solution_found = (best_goal_f != INF);
//            int solution_cost = solution_found ? best_goal_f : 0;
//
//            json after_data;
//            after_data["ts1_id"] = merge_index1;
//            after_data["ts2_id"] = merge_index2;
//            after_data["merged_id"] = merged_index;
//            after_data["iteration"] = iteration;
//            after_data["f_values"] = f_after;
//            after_data["num_states"] = (int)f_after.size();
//            after_data["num_goal_states"] = num_goals;
//            after_data["num_transitions"] = merged_transitions;
//
//            json search_signals;
//            search_signals["nodes_expanded"] = nodes_expanded;
//            search_signals["search_depth"] = search_depth;
//            search_signals["solution_cost"] = solution_cost;
//            search_signals["branching_factor"] = branching_factor;
//            search_signals["solution_found"] = solution_found;
//            after_data["search_signals"] = search_signals;
//
//            // ✅ DIRECT WRITE: No helper function
//            std::string after_path = fd_output_dir + "/merge_after_" + std::to_string(iteration) + ".json";
//            std::ofstream after_file(after_path, std::ios::out | std::ios::trunc);
//            if (!after_file.is_open()) {
//                std::cerr << "[M&S] ERROR: Cannot create merge_after file: " << after_path << std::endl;
//                throw std::runtime_error("Cannot create merge_after file");
//            }
//            after_file << after_data.dump(2);
//            after_file.close();
//            std::cout << "[M&S] ✅ Wrote merge_after_" << iteration << ".json" << std::endl;
//        }
//
//        // ====================================================================
//        // PHASE 8: EXPORT MERGED TS JSON - SIMPLIFIED INLINE
//        // ====================================================================
//
//        {
//            const TransitionSystem& ts = fts.get_transition_system(merged_index);
//
//            json ts_json;
//            ts_json["iteration"] = iteration;
//            ts_json["num_states"] = ts.get_size();
//            ts_json["init_state"] = ts.get_init_state();
//            ts_json["transformed"] = (shrunk || reduced);
//
//            std::vector<int> goal_states;
//            for (int i = 0; i < ts.get_size(); ++i) {
//                if (ts.is_goal_state(i)) {
//                    goal_states.push_back(i);
//                }
//            }
//            ts_json["goal_states"] = goal_states;
//            ts_json["incorporated_variables"] = ts.get_incorporated_variables();
//
//            std::vector<json> transitions;
//            for (auto it = ts.begin(); it != ts.end(); ++it) {
//                const auto& info = *it;
//                const auto& label_group = info.get_label_group();
//                const auto& trans_vec = info.get_transitions();
//
//                for (int label : label_group) {
//                    for (const auto& trans : trans_vec) {
//                        transitions.push_back({
//                            {"src", trans.src},
//                            {"target", trans.target},
//                            {"label", label}
//                        });
//                    }
//                }
//            }
//            ts_json["transitions"] = transitions;
//
//            std::string ts_path = fd_output_dir + "/ts_" + std::to_string(iteration) + ".json";
//
//            // ✅ DIRECT WRITE: Simple, no helper function
//            std::ofstream ts_file(ts_path, std::ios::out | std::ios::trunc);
//            if (!ts_file.is_open()) {
//                std::cerr << "[M&S] ERROR: Cannot create ts file: " << ts_path << std::endl;
//                throw std::runtime_error("Cannot create ts file");
//            }
//            ts_file << ts_json.dump(2);
//            ts_file.close();
//            std::cout << "[M&S] ✅ Wrote ts_" << iteration << ".json with " << ts.get_size() << " states" << std::endl;
//        }
//
//        // ====================================================================
//        // PHASE 9: INCREMENT ITERATION COUNTER
//        // ====================================================================
//
//        iteration++;  // ✅ MOVED HERE for clarity

        // ====================================================================
        // PHASE 5: EXPORT MERGE SIGNALS (BEFORE DATA) - ENHANCED
        // ====================================================================

        {
            const auto& init1 = fts.get_distances(merge_index1).get_init_distances();
            const auto& goal1 = fts.get_distances(merge_index1).get_goal_distances();
            const auto& init2 = fts.get_distances(merge_index2).get_init_distances();
            const auto& goal2 = fts.get_distances(merge_index2).get_goal_distances();

            std::vector<int> f1(init1.size()), f2(init2.size());
            for (size_t i = 0; i < f1.size(); ++i) {
                f1[i] = (init1[i] == INF || goal1[i] == INF) ? INF : init1[i] + goal1[i];
            }
            for (size_t j = 0; j < f2.size(); ++j) {
                f2[j] = (init2[j] == INF || goal2[j] == INF) ? INF : init2[j] + goal2[j];
            }

            const TransitionSystem& ts1 = fts.get_transition_system(merge_index1);
            const TransitionSystem& ts2 = fts.get_transition_system(merge_index2);

            int ts1_transitions = 0, ts2_transitions = 0;
            for (auto it = ts1.begin(); it != ts1.end(); ++it) {
                ts1_transitions += (*it).get_transitions().size();
            }
            for (auto it = ts2.begin(); it != ts2.end(); ++it) {
                ts2_transitions += (*it).get_transitions().size();
            }

            int ts1_size = (int)f1.size();
            int ts2_size = (int)f2.size();
            int ts1_goal_states = 0, ts2_goal_states = 0;
            for (int i = 0; i < ts1_size; ++i) {
                if (ts1.is_goal_state(i)) ts1_goal_states++;
            }
            for (int j = 0; j < ts2_size; ++j) {
                if (ts2.is_goal_state(j)) ts2_goal_states++;
            }

            // ✅ ENHANCED: Compute F-value statistics INLINE
            auto compute_f_stats = [](const std::vector<int>& f_vals) -> json {
                std::vector<int> valid_f;
                for (int f : f_vals) {
                    if (f != INF && f >= 0 && f < 1000000000) {
                        valid_f.push_back(f);
                    }
                }

                json stats;
                if (valid_f.empty()) {
                    stats["min"] = INF;
                    stats["max"] = INF;
                    stats["mean"] = 0.0;
                    stats["std"] = 0.0;
                    stats["valid_count"] = 0;
                } else {
                    int min_f = *std::min_element(valid_f.begin(), valid_f.end());
                    int max_f = *std::max_element(valid_f.begin(), valid_f.end());
                    double sum = std::accumulate(valid_f.begin(), valid_f.end(), 0.0);
                    double mean = sum / valid_f.size();
                    double variance = 0.0;
                    for (int v : valid_f) {
                        variance += (v - mean) * (v - mean);
                    }
                    double std_dev = std::sqrt(variance / valid_f.size());

                    stats["min"] = min_f;
                    stats["max"] = max_f;
                    stats["mean"] = mean;
                    stats["std"] = std_dev;
                    stats["valid_count"] = valid_f.size();
                }
                return stats;
            };

            json product_mapping;
            for (int s = 0; s < ts1_size * ts2_size; ++s) {
                int s1 = s / ts2_size;
                int s2 = s % ts2_size;
                product_mapping[std::to_string(s)] = {{"s1", s1}, {"s2", s2}};
            }

            json before_data;
            before_data["iteration"] = iteration;
            before_data["ts1_id"] = merge_index1;
            before_data["ts2_id"] = merge_index2;
            before_data["ts1_size"] = ts1_size;
            before_data["ts2_size"] = ts2_size;
            before_data["expected_product_size"] = ts1_size * ts2_size;
            before_data["ts1_transitions"] = ts1_transitions;
            before_data["ts2_transitions"] = ts2_transitions;
            before_data["ts1_density"] = (double)ts1_transitions / std::max(ts1_size, 1);
            before_data["ts2_density"] = (double)ts2_transitions / std::max(ts2_size, 1);
            before_data["ts1_goal_states"] = ts1_goal_states;
            before_data["ts2_goal_states"] = ts2_goal_states;
            before_data["ts1_f_values"] = f1;
            before_data["ts2_f_values"] = f2;
            before_data["ts1_f_stats"] = compute_f_stats(f1);
            before_data["ts2_f_stats"] = compute_f_stats(f2);
            before_data["ts1_variables"] = ts1.get_incorporated_variables();
            before_data["ts2_variables"] = ts2.get_incorporated_variables();
            before_data["product_mapping"] = product_mapping;
            before_data["shrunk"] = shrunk;
            before_data["reduced"] = reduced;

            std::string before_path = fd_output_dir + "/merge_before_" + std::to_string(iteration) + ".json";
            std::ofstream before_file(before_path, std::ios::out | std::ios::trunc);
            if (!before_file.is_open()) {
                std::cerr << "[M&S] ERROR: Cannot create merge_before file: " << before_path << std::endl;
            } else {
                before_file << before_data.dump(2);
                before_file.close();
                std::cout << "[M&S] ✅ Wrote merge_before_" << iteration << ".json" << std::endl;
            }
        }

        // ====================================================================
        // PHASE 6: PERFORM ACTUAL MERGE
        // ====================================================================

        int merged_index = fts.merge(merge_index1, merge_index2, log);

        // ====================================================================
        // PHASE 7: EXPORT MERGE SIGNALS (AFTER DATA) - ENHANCED
        // ====================================================================

        {
            const auto& init_dist = fts.get_distances(merged_index).get_init_distances();
            const auto& goal_dist = fts.get_distances(merged_index).get_goal_distances();

            std::vector<int> f_after(init_dist.size());
            for (size_t s = 0; s < f_after.size(); ++s) {
                f_after[s] = (init_dist[s] == INF || goal_dist[s] == INF) ? INF : init_dist[s] + goal_dist[s];
            }

            const TransitionSystem& merged_ts = fts.get_transition_system(merged_index);
            int num_goals = 0, reachable_count = 0, unreachable_count = 0;

            for (size_t i = 0; i < f_after.size(); ++i) {
                if (merged_ts.is_goal_state(i)) num_goals++;
                if (init_dist[i] != INF && goal_dist[i] != INF) {
                    reachable_count++;
                } else {
                    unreachable_count++;
                }
            }

            int merged_transitions = 0;
            for (auto it = merged_ts.begin(); it != merged_ts.end(); ++it) {
                merged_transitions += (*it).get_transitions().size();
            }

            // ✅ ENHANCED: Compute A* metrics with validation
            double branching_factor = 1.0;
            if (reachable_count > 0 && merged_transitions > 0) {
                branching_factor = (double)merged_transitions / (double)reachable_count;
                if (std::isnan(branching_factor) || std::isinf(branching_factor) || branching_factor < 1.0) {
                    branching_factor = 1.0;
                }
            }

            int search_depth = 0;
            long long sum_goal_dist = 0;
            int reachable_goal_count = 0;
            for (int i = 0; i < (int)f_after.size(); ++i) {
                if (init_dist[i] != INF && goal_dist[i] != INF) {
                    sum_goal_dist += goal_dist[i];
                    reachable_goal_count++;
                }
            }
            if (reachable_goal_count > 0) {
                search_depth = (int)std::round((double)sum_goal_dist / reachable_goal_count);
            }

            int best_goal_f = INF;
            for (int i = 0; i < (int)f_after.size(); ++i) {
                if (merged_ts.is_goal_state(i) && init_dist[i] != INF && goal_dist[i] != INF) {
                    int f = init_dist[i] + goal_dist[i];
                    if (f < best_goal_f) {
                        best_goal_f = f;
                    }
                }
            }
            bool solution_found = (best_goal_f != INF);
            int solution_cost = solution_found ? best_goal_f : 0;

            // ✅ ENHANCED: F-value statistics
            auto compute_f_stats = [](const std::vector<int>& f_vals) -> json {
                std::vector<int> valid_f;
                for (int f : f_vals) {
                    if (f != INF && f >= 0 && f < 1000000000) {
                        valid_f.push_back(f);
                    }
                }

                json stats;
                if (valid_f.empty()) {
                    stats["min"] = INF;
                    stats["max"] = INF;
                    stats["mean"] = 0.0;
                    stats["std"] = 0.0;
                    stats["valid_count"] = 0;
                } else {
                    int min_f = *std::min_element(valid_f.begin(), valid_f.end());
                    int max_f = *std::max_element(valid_f.begin(), valid_f.end());
                    double sum = std::accumulate(valid_f.begin(), valid_f.end(), 0.0);
                    double mean = sum / valid_f.size();
                    double variance = 0.0;
                    for (int v : valid_f) {
                        variance += (v - mean) * (v - mean);
                    }
                    double std_dev = std::sqrt(variance / valid_f.size());

                    stats["min"] = min_f;
                    stats["max"] = max_f;
                    stats["mean"] = mean;
                    stats["std"] = std_dev;
                    stats["valid_count"] = valid_f.size();
                }
                return stats;
            };

            json after_data;
            after_data["iteration"] = iteration;
            after_data["ts1_id"] = merge_index1;
            after_data["ts2_id"] = merge_index2;
            after_data["merged_id"] = merged_index;
            after_data["merged_size"] = (int)f_after.size();
            after_data["merged_goal_states"] = num_goals;
            after_data["merged_transitions"] = merged_transitions;
            after_data["merged_density"] = (double)merged_transitions / std::max((int)f_after.size(), 1);
            after_data["reachable_states"] = reachable_count;
            after_data["unreachable_states"] = unreachable_count;
            after_data["reachability_ratio"] = (double)reachable_count / std::max((int)f_after.size(), 1);
            after_data["f_values"] = f_after;
            after_data["f_stats"] = compute_f_stats(f_after);
            after_data["shrinking_ratio"] = (double)f_after.size() /
                                             std::max(fts.get_transition_system(merge_index1).get_size() *
                                                     fts.get_transition_system(merge_index2).get_size(), 1);

            json search_signals;
            search_signals["nodes_expanded"] = reachable_count;
            search_signals["search_depth"] = search_depth;
            search_signals["solution_cost"] = solution_cost;
            search_signals["branching_factor"] = branching_factor;
            search_signals["solution_found"] = solution_found;
            after_data["search_signals"] = search_signals;

            std::string after_path = fd_output_dir + "/merge_after_" + std::to_string(iteration) + ".json";
            std::ofstream after_file(after_path, std::ios::out | std::ios::trunc);
            if (!after_file.is_open()) {
                std::cerr << "[M&S] ERROR: Cannot create merge_after file: " << after_path << std::endl;
            } else {
                after_file << after_data.dump(2);
                after_file.close();
                std::cout << "[M&S] ✅ Wrote merge_after_" << iteration << ".json" << std::endl;
            }
        }

        // ====================================================================
        // PHASE 8: EXPORT MERGED TS JSON - ENHANCED
        // ====================================================================

        {
            const TransitionSystem& ts = fts.get_transition_system(merged_index);

            json ts_json;
            ts_json["iteration"] = iteration;
            ts_json["num_states"] = ts.get_size();
            ts_json["init_state"] = ts.get_init_state();
            ts_json["transformed"] = (shrunk || reduced);

            std::vector<int> goal_states;
            for (int i = 0; i < ts.get_size(); ++i) {
                if (ts.is_goal_state(i)) {
                    goal_states.push_back(i);
                }
            }
            ts_json["goal_states"] = goal_states;
            ts_json["incorporated_variables"] = ts.get_incorporated_variables();

            std::vector<json> transitions;
            for (auto it = ts.begin(); it != ts.end(); ++it) {
                const auto& info = *it;
                const auto& label_group = info.get_label_group();
                const auto& trans_vec = info.get_transitions();

                for (int label : label_group) {
                    for (const auto& trans : trans_vec) {
                        transitions.push_back({
                            {"src", trans.src},
                            {"target", trans.target},
                            {"label", label}
                        });
                    }
                }
            }
            ts_json["transitions"] = transitions;

            std::string ts_path = fd_output_dir + "/ts_" + std::to_string(iteration) + ".json";
            std::ofstream ts_file(ts_path, std::ios::out | std::ios::trunc);
            if (!ts_file.is_open()) {
                std::cerr << "[M&S] ERROR: Cannot create ts file: " << ts_path << std::endl;
            } else {
                ts_file << ts_json.dump(2);
                ts_file.close();
                std::cout << "[M&S] ✅ Wrote ts_" << iteration << ".json" << std::endl;
            }
        }

        // ====================================================================
        // PHASE 9: INCREMENT ITERATION COUNTER
        // ====================================================================

        iteration++;

        int abs_size = fts.get_transition_system(merged_index).get_size();
        if (abs_size > maximum_intermediate_size) {
            maximum_intermediate_size = abs_size;
        }

        if (log.is_at_least_normal()) {
            if (log.is_at_least_verbose()) {
                fts.statistics(merged_index, log);
            }
            log_main_loop_progress("after merging");
        }

        if (ran_out_of_time(timer)) {
            break;
        }

        // Pruning
        if (prune_unreachable_states || prune_irrelevant_states) {
            bool pruned = prune_step(
                fts,
                merged_index,
                prune_unreachable_states,
                prune_irrelevant_states,
                log);
            if (log.is_at_least_normal() && pruned) {
                if (log.is_at_least_verbose()) {
                    fts.statistics(merged_index, log);
                }
                log_main_loop_progress("after pruning");
            }
        }

        if (!fts.is_factor_solvable(merged_index)) {
            if (log.is_at_least_normal()) {
                log << "Abstract problem is unsolvable, stopping computation." << endl << endl;
            }
            break;
        }

        if (ran_out_of_time(timer)) {
            break;
        }

        if (log.is_at_least_verbose()) {
            report_peak_memory_delta();
        }
        if (log.is_at_least_normal()) {
            log << endl;
        }
    }

    log << "End of merge-and-shrink algorithm, statistics:" << endl;
    log << "Main loop runtime: " << timer.get_elapsed_time() << endl;
    log << "Maximum intermediate abstraction size: "
        << maximum_intermediate_size << endl;
    shrink_strategy = nullptr;
    label_reduction = nullptr;
}


FactoredTransitionSystem MergeAndShrinkAlgorithm::build_factored_transition_system(
    const TaskProxy &task_proxy) {
    if (starting_peak_memory) {
        cerr << "Calling build_factored_transition_system twice is not "
             << "supported!" << endl;
        utils::exit_with(utils::ExitCode::SEARCH_CRITICAL_ERROR);
    }
    starting_peak_memory = utils::get_peak_memory_in_kb();

    utils::Timer timer;
    log << "Running merge-and-shrink algorithm..." << endl;
    task_properties::verify_no_axioms(task_proxy);
    dump_options();
    warn_on_unusual_options();
    log << endl;

    const bool compute_init_distances =
        shrink_strategy->requires_init_distances() ||
        merge_strategy_factory->requires_init_distances() ||
        prune_unreachable_states;
    const bool compute_goal_distances =
        shrink_strategy->requires_goal_distances() ||
        merge_strategy_factory->requires_goal_distances() ||
        prune_irrelevant_states;
    FactoredTransitionSystem fts =
        create_factored_transition_system(
            task_proxy,
            compute_init_distances,
            compute_goal_distances,
            log);
    if (log.is_at_least_normal()) {
        log_progress(timer, "after computation of atomic factors", log);
    }

    /*
      Prune all atomic factors according to the chosen options. Stop early if
      one factor is unsolvable.

      TODO: think about if we can prune already while creating the atomic FTS.
    */
    bool pruned = false;
    bool unsolvable = false;
    for (int index = 0; index < fts.get_size(); ++index) {
        assert(fts.is_active(index));
        if (prune_unreachable_states || prune_irrelevant_states) {
            bool pruned_factor = prune_step(
                fts,
                index,
                prune_unreachable_states,
                prune_irrelevant_states,
                log);
            pruned = pruned || pruned_factor;
        }
        if (!fts.is_factor_solvable(index)) {
            log << "Atomic FTS is unsolvable, stopping computation." << endl;
            unsolvable = true;
            break;
        }
    }
    if (log.is_at_least_normal()) {
        if (pruned) {
            log_progress(timer, "after pruning atomic factors", log);
        }
        log << endl;
    }

    // ####################################################################################################
    // === Export Atomic Transition Systems ===
    {
        std::string filename = "merged_transition_systems.json";
        json all_ts;

        // Try to load existing merged systems if any
        std::ifstream infile(filename);
        if (infile) {
            infile >> all_ts;
            infile.close();
        }

        for (int i = 0; i < fts.get_size(); ++i) {
            if (!fts.is_active(i)) continue;

            const TransitionSystem& ts = fts.get_transition_system(i);

            json ts_json;
            ts_json["iteration"] = -1;  // Use -1 to mark atomic TS
            ts_json["num_states"] = ts.get_size();
            ts_json["init_state"] = ts.get_init_state();

            std::vector<int> goal_states;
            for (int j = 0; j < ts.get_size(); ++j)
                if (ts.is_goal_state(j))
                    goal_states.push_back(j);
            ts_json["goal_states"] = goal_states;

            ts_json["incorporated_variables"] = ts.get_incorporated_variables();

            std::vector<json> transitions;
            for (auto it = ts.begin(); it != ts.end(); ++it) {
                const auto& info = *it;
                const auto& label_group = info.get_label_group();
                const auto& trans_vec = info.get_transitions();

                for (int label : label_group) {
                    for (const auto& trans : trans_vec) {
                        transitions.push_back({
                            {"src", trans.src},
                            {"target", trans.target},
                            {"label", label}
                            });
                    }
                }
            }
            ts_json["transitions"] = transitions;

            all_ts.push_back(ts_json);
        }

        std::ofstream outfile(filename);
        outfile << all_ts.dump(4);  // Pretty print
        outfile.close();
    }
    // ####################################################################################################

    if (!unsolvable && main_loop_max_time > 0) {
        main_loop(fts, task_proxy);
    }
    const bool final = true;
    report_peak_memory_delta(final);
    log << "Merge-and-shrink algorithm runtime: " << timer << endl;
    log << endl;
    return fts;
}

void add_merge_and_shrink_algorithm_options_to_feature(plugins::Feature &feature) {
    // Merge strategy option.
    feature.add_option<shared_ptr<MergeStrategyFactory>>(
        "merge_strategy",
        "See detailed documentation for merge strategies. "
        "We currently recommend SCC-DFP, which can be achieved using "
        "{{{merge_strategy=merge_sccs(order_of_sccs=topological,merge_selector="
        "score_based_filtering(scoring_functions=[goal_relevance,dfp,total_order"
        "]))}}}");

    // Shrink strategy option.
    feature.add_option<shared_ptr<ShrinkStrategy>>(
        "shrink_strategy",
        "See detailed documentation for shrink strategies. "
        "We currently recommend non-greedy shrink_bisimulation, which can be "
        "achieved using {{{shrink_strategy=shrink_bisimulation(greedy=false)}}}");

    // Label reduction option.
    feature.add_option<shared_ptr<LabelReduction>>(
        "label_reduction",
        "See detailed documentation for labels. There is currently only "
        "one 'option' to use label_reduction, which is {{{label_reduction=exact}}} "
        "Also note the interaction with shrink strategies.",
        plugins::ArgumentInfo::NO_DEFAULT);

    // Pruning options.
    feature.add_option<bool>(
        "prune_unreachable_states",
        "If true, prune abstract states unreachable from the initial state.",
        "true");
    feature.add_option<bool>(
        "prune_irrelevant_states",
        "If true, prune abstract states from which no goal state can be "
        "reached.",
        "true");

    add_transition_system_size_limit_options_to_feature(feature);

    feature.add_option<double>(
        "main_loop_max_time",
        "A limit in seconds on the runtime of the main loop of the algorithm. "
        "If the limit is exceeded, the algorithm terminates, potentially "
        "returning a factored transition system with several factors. Also "
        "note that the time limit is only checked between transformations "
        "of the main loop, but not during, so it can be exceeded if a "
        "transformation is runtime-intense.",
        "infinity",
        Bounds("0.0", "infinity"));
}

tuple<shared_ptr<MergeStrategyFactory>, shared_ptr<ShrinkStrategy>,
      shared_ptr<LabelReduction>, bool, bool, int, int, int, double>
get_merge_and_shrink_algorithm_arguments_from_options(
    const plugins::Options &opts) {
    return tuple_cat(
        make_tuple(
            opts.get<shared_ptr<MergeStrategyFactory>>("merge_strategy"),
            opts.get<shared_ptr<ShrinkStrategy>>("shrink_strategy"),
            opts.get<shared_ptr<LabelReduction>>(
                "label_reduction", nullptr),
            opts.get<bool>("prune_unreachable_states"),
            opts.get<bool>("prune_irrelevant_states")),
        get_transition_system_size_limit_arguments_from_options(opts),
        make_tuple(opts.get<double>("main_loop_max_time"))
        );
}

void add_transition_system_size_limit_options_to_feature(plugins::Feature &feature) {
    feature.add_option<int>(
        "max_states",
        "maximum transition system size allowed at any time point.",
        "-1",
        Bounds("-1", "infinity"));
    feature.add_option<int>(
        "max_states_before_merge",
        "maximum transition system size allowed for two transition systems "
        "before being merged to form the synchronized product.",
        "-1",
        Bounds("-1", "infinity"));
    feature.add_option<int>(
        "threshold_before_merge",
        "If a transition system, before being merged, surpasses this soft "
        "transition system size limit, the shrink strategy is called to "
        "possibly shrink the transition system.",
        "-1",
        Bounds("-1", "infinity"));
}

tuple<int, int, int>
get_transition_system_size_limit_arguments_from_options(
    const plugins::Options &opts) {
    return make_tuple(
        opts.get<int>("max_states"),
        opts.get<int>("max_states_before_merge"),
        opts.get<int>("threshold_before_merge")
        );
}
}

--------------------------------------------------------------------------------

The file backup/downward/src/search/merge_and_shrink/merge_strategy_gnn.h code code is this:
[Error reading backup/downward/src/search/merge_and_shrink/merge_strategy_gnn.h: [Errno 2] No such file or directory: 'backup/downward/src/search/merge_and_shrink/merge_strategy_gnn.h']


--------------------------------------------------------------------------------

